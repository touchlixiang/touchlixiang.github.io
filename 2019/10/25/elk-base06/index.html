<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ELK,">










<meta name="keywords" content="ELK">
<meta property="og:type" content="article">
<meta property="og:title" content="06 ELK+Filebeat+Kafka+ZooKeeper 构建大数据日志分析平台">
<meta property="og:url" content="https://touchlixiang.github.io/2019/10/25/elk-base06/index.html">
<meta property="og:site_name" content="My Notes">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://touchlixiang.github.io/img/mix_3.jpg">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_21.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_22.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_23.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_24.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_25.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_27.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_29.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_28.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_33.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_38.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_31.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_34.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_32.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_35.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_36.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_37.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_38.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_39.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_40.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_41.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_42.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_43.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_44.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_45.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_46.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_47.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_48.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_49.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_50.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_51.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_52.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_53.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_54.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_55.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_56.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_57.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_58.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_59.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_60.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_61.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_62.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_63.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_64.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_65.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_66.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_67.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_68.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_69.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_70.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/elk_71.png">
<meta property="og:updated_time" content="2019-10-27T04:45:15.254Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="06 ELK+Filebeat+Kafka+ZooKeeper 构建大数据日志分析平台">
<meta name="twitter:image" content="https://touchlixiang.github.io/img/mix_3.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://touchlixiang.github.io/2019/10/25/elk-base06/">





  <title>06 ELK+Filebeat+Kafka+ZooKeeper 构建大数据日志分析平台 | My Notes</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">My Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">记不住的一定要写在这里</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://touchlixiang.github.io/2019/10/25/elk-base06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Harris Li">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/Greeen.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">06 ELK+Filebeat+Kafka+ZooKeeper 构建大数据日志分析平台</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-25T07:50:59+08:00">
                2019-10-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ELK/" itemprop="url" rel="index">
                    <span itemprop="name">ELK</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/img/mix_3.jpg" width="50%"><br><a id="more"></a></p>
<h2 id="ELK-应用案例"><a href="#ELK-应用案例" class="headerlink" title="ELK 应用案例"></a>ELK 应用案例</h2><p><img src="/img/elk_21.png"></p>
<h3 id="典型-ELK-应用架构"><a href="#典型-ELK-应用架构" class="headerlink" title="典型 ELK 应用架构"></a>典型 ELK 应用架构</h3><p>此架构稍微有些复杂，因此，这里做一下架构解读。 这个架构图从左到右，总共分为5层，每层实现的功能和含义分别介绍如下：</p>
<ol>
<li><p>第一层. 数据采集层</p>
<ul>
<li><p>数据采集层位于最左边的业务服务器集群上，在每个业务服务器上面安装了filebeat做日志收集，然后把采集到的原始日志发送到Kafka+zookeeper集群上。</p>
</li>
<li><p>filebeat 只能做简单的数据过滤,数据此时还是原始数据。</p>
</li>
</ul>
</li>
<li><p>第二层. 消息队列层</p>
<ul>
<li><p>原始日志发送到Kafka+zookeeper集群上后，会进行集中存储，此时，filbeat是消息的生产者，存储的消息可以随时被消费。</p>
</li>
<li><p>通过zookeeper调度和协调kafka工作，比如主节点挂掉了,选取主节点等</p>
</li>
</ul>
</li>
<li><p>第三层. 数据分析层</p>
<ul>
<li><p>Logstash作为消费者，会去Kafka+zookeeper集群节点实时拉取原始日志，然后将获取到的原始日志根据规则进行分析、清洗、过滤，最后将清洗好的日志转发至Elasticsearch集群。</p>
</li>
<li><p>如数据量过大,或者考虑性能,Logstash可以为多台。</p>
</li>
</ul>
</li>
<li><p>第四层. 数据持久化存储</p>
<ul>
<li>Elasticsearch集群在接收到logstash发送过来的数据后，执行写磁盘，建索引库等操作，最后将结构化的数据存储到Elasticsearch集群上。</li>
</ul>
</li>
</ol>
<ol start="5">
<li><p>第五层. 数据查询、展示层</p>
<ul>
<li>Kibana是一个可视化的数据展示平台，当有数据检索请求时，它从Elasticsearch集群上读取数据，然后进行可视化出图和多维度分析。\</li>
</ul>
</li>
</ol>
<h2 id="环境与角色说明"><a href="#环境与角色说明" class="headerlink" title="环境与角色说明"></a>环境与角色说明</h2><h3 id="服务器环境与角色"><a href="#服务器环境与角色" class="headerlink" title="服务器环境与角色"></a>服务器环境与角色</h3><ol>
<li>操作系统统一采用Centos7.5版本，各个服务器角色如下表所示：(我使用阿里云服务器,操作系统可能会是7.6)</li>
</ol>
<p><img src="/img/elk_22.png"></p>
<h3 id="软件环境与版本"><a href="#软件环境与版本" class="headerlink" title="软件环境与版本"></a>软件环境与版本</h3><ol>
<li>下表详细说明了本节安装软件对应的名称和版本号，其中，ELK三款软件推荐选择一样的版本，这里选择的是6.3.2版本。</li>
</ol>
<p><img src="/img/elk_23.png"></p>
<h3 id="安装JDK以及设置环境变量"><a href="#安装JDK以及设置环境变量" class="headerlink" title="安装JDK以及设置环境变量"></a>安装JDK以及设置环境变量</h3><ol>
<li><p>选择合适版本并下载JDK</p>
<ul>
<li>Zookeeper 、elasticsearch和Logstash都依赖于Java环境，并且elasticsearch和Logstash要求JDK版本至少在JDK1.7或者以上。</li>
</ul>
</li>
</ol>
<ol start="2">
<li>安装过程</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# ls</span><br><span class="line">jdk-8u231-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"># 解压</span><br><span class="line">[root@server2 ~]# tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/local/</span><br><span class="line">[root@server2 ~]# ls -l /usr/local/jdk1.8.0_231/</span><br><span class="line"></span><br><span class="line"># 配置环境变量</span><br><span class="line">[root@server2 local]# vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_231</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH</span><br><span class="line"></span><br><span class="line">[root@server2 local]# source /etc/profile</span><br><span class="line"></span><br><span class="line"># 让设置生效</span><br><span class="line">[root@server2 local]# java -version</span><br><span class="line">java version "1.8.0_231"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_231-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)</span><br><span class="line"></span><br><span class="line"># 所有的ES环境都需要安装</span><br></pre></td></tr></table></figure>
<h2 id="安装并配置-elasticsearch-集群"><a href="#安装并配置-elasticsearch-集群" class="headerlink" title="安装并配置 elasticsearch 集群"></a>安装并配置 elasticsearch 集群</h2><h3 id="elasticsearch-集群的架构与角色"><a href="#elasticsearch-集群的架构与角色" class="headerlink" title="elasticsearch 集群的架构与角色"></a>elasticsearch 集群的架构与角色</h3><ol>
<li><p>在ElasticSearch的架构中，有三类角色，分别是Client Node、Data Node和Master Node，</p>
</li>
<li><p>搜索查询的请求一般是经过Client Node来向 Data Node 获取数据，</p>
</li>
<li><p>而索引查询首先请求 Master Node 节点，然后 Master Node 将请求分配到多个Data Node节点完成一次索引查询。</p>
</li>
</ol>
<p><img src="/img/elk_24.png"></p>
<h3 id="集群中每个角色的含义介绍如下："><a href="#集群中每个角色的含义介绍如下：" class="headerlink" title="集群中每个角色的含义介绍如下："></a>集群中每个角色的含义介绍如下：</h3><ol>
<li>master node：<ul>
<li>可以理解为主节点，主要用于元数据(metadata)的处理，比如索引的新增、删除、分片分配等，以及管理集群各个节点的状态。</li>
<li>elasticsearch集群中可以定义多个主节点,但是，在同一时刻，只有一个主节点起作用，其它定义的主节点，是作为主节点的候选节点存在。</li>
<li>当一个主节点故障后，集群会从候选主节点中选举出新的主节点。</li>
</ul>
</li>
</ol>
<ol start="2">
<li>data node：<ul>
<li>数据节点，这些节点上保存了数据分片。它负责数据相关操作，比如分片的CRUD、搜索和整合等操作。</li>
<li>数据节点上面执行的操作都比较消耗 CPU、内存和I/O资源，因此数据节点服务器要选择较好的硬件配置，才能获取高效的存储和分析性能。</li>
</ul>
</li>
</ol>
<ol start="3">
<li>client node：<ul>
<li>客户端节点，属于可选节点，是作为任务分发用的，它里面也会存元数据，但是它不会对元数据做任何修改。</li>
<li>client node存在的好处是可以分担data node的一部分压力，因为elasticsearch的查询是两层汇聚的结果，</li>
<li>第一层是在data node上做查询结果汇聚，然后把结果发给client node，client node接收到data node发来的结果后再做第二次的汇聚，</li>
<li>然后把最终的查询结果返回给用户。这样，client node就替data node分担了部分压力。</li>
</ul>
</li>
</ol>
<h3 id="安装-elasticsearch集群"><a href="#安装-elasticsearch集群" class="headerlink" title="安装 elasticsearch集群"></a>安装 elasticsearch集群</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 环境介绍</span><br><span class="line">server1 172.17.70.229 ES Master、ES NataNode   </span><br><span class="line">server2 172.17.70.230 ES Master、Kibana</span><br><span class="line">server3 172.17.70.231 ES Master、ES NataNode</span><br></pre></td></tr></table></figure>
<ol>
<li>下载ES</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gz</span><br><span class="line">https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gz</span><br><span class="line">https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-linux-x86_64.tar.gz</span><br><span class="line">https://artifacts.elastic.co/downloads/logstash/logstash-6.3.2.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# mkdir -p /app/elk</span><br><span class="line"></span><br><span class="line">[root@server2 elk]# ls -l</span><br><span class="line">total 632464</span><br><span class="line">-rw-r--r-- 1 root root  91452574 Jul 24  2018 elasticsearch-6.3.2.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root  12483335 Oct 25 11:02 filebeat-6.3.2-linux-x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root 194151339 Oct 25 10:24 jdk-8u231-linux-x64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root 205331616 Oct 25 10:51 kibana-6.3.2-linux-x86_64.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root 144211416 Oct 25 11:02 logstash-6.3.2.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 解压安装es</span><br><span class="line">[root@server2 elk]# tar -zxvf elasticsearch-6.3.2.tar.gz -C /usr/local</span><br><span class="line"># 修改目录名</span><br><span class="line">[root@server2 local]# mv /usr/local/elasticsearch-6.3.2  /usr/local/elasticsearch</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 目录说明</span><br><span class="line">[root@server2 local]# cd elasticsearch/</span><br><span class="line">[root@server2 elasticsearch]# ls -l</span><br><span class="line">total 460</span><br><span class="line">drwxr-xr-x  3 root root   4096 Oct 25 11:03 bin             # 启动服务脚本 和 服务</span><br><span class="line">drwxr-xr-x  2 root root   4096 Jul 20  2018 config          # 配置文件</span><br><span class="line">drwxr-xr-x  2 root root   4096 Jul 20  2018 lib</span><br><span class="line">-rw-r--r--  1 root root  13675 Jul 20  2018 LICENSE.txt</span><br><span class="line">drwxr-xr-x  2 root root   4096 Jul 20  2018 logs</span><br><span class="line">drwxr-xr-x 17 root root   4096 Jul 20  2018 modules</span><br><span class="line">-rw-r--r--  1 root root 416018 Jul 20  2018 NOTICE.txt</span><br><span class="line">drwxr-xr-x  2 root root   4096 Jul 20  2018 plugins          # 插件</span><br><span class="line">-rw-r--r--  1 root root   8511 Jul 20  2018 README.textile</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 elasticsearch]# cd config/ </span><br><span class="line">-rw-rw---- 1 root root 2853 Jul 20  2018 elasticsearch.yml      # 主配置文件</span><br><span class="line">-rw-rw---- 1 root root 2937 Jul 20  2018 jvm.options            # JVM内存配置</span><br><span class="line">-rw-rw---- 1 root root 6380 Jul 20  2018 log4j2.properties</span><br><span class="line">-rw-rw---- 1 root root  473 Jul 20  2018 role_mapping.yml</span><br><span class="line">-rw-rw---- 1 root root  197 Jul 20  2018 roles.yml</span><br><span class="line">-rw-rw---- 1 root root    0 Jul 20  2018 users</span><br><span class="line">-rw-rw---- 1 root root    0 Jul 20  2018 users_roles</span><br></pre></td></tr></table></figure>
<h3 id="增加es用户授权"><a href="#增加es用户授权" class="headerlink" title="增加es用户授权"></a>增加es用户授权</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#增加用户组</span><br><span class="line">groupadd elasticsearch</span><br><span class="line"></span><br><span class="line">#增加用户，并规定所属用户组和密码</span><br><span class="line">useradd elasticsearch -g elasticsearch</span><br><span class="line"></span><br><span class="line"># 递归更改文件的拥有者</span><br><span class="line">chown -R elasticsearch:elasticsearch /usr/local/elasticsearch</span><br></pre></td></tr></table></figure>
<h3 id="操作系统调优"><a href="#操作系统调优" class="headerlink" title="操作系统调优"></a>操作系统调优</h3><ol>
<li><p>操作系统以及JVM调优主要是针对安装elasticsearch的机器。对于操作系统，需要调整几个内核参数，将下面内容添加到/etc/sysctl.conf文件中：</p>
</li>
<li><p>fs.file-max主要是配置系统最大打开文件描述符数，建议修改为655360或者更高，</p>
</li>
<li><p>vm.max_map_count影响Java线程数量，用于限制一个进程可以拥有的VMA(虚拟内存区域)的大小，系统默认是65530，建议修改成262144或者更高。</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 config]# vim /etc/sysctl.conf </span><br><span class="line">fs.file-max=655360</span><br><span class="line">vm.max_map_count = 262144</span><br><span class="line"></span><br><span class="line"># 启用生效</span><br><span class="line">[root@server2 config]# sysctl -p</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>调整进程最大打开文件描述符（nofile）、最大用户进程数（nproc）和最大锁定内存地址空间（memlock），添加如下内容到/etc/security/limits.conf文件中：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 config]# vim /etc/security/limits.conf </span><br><span class="line"></span><br><span class="line">* soft nofile 204800</span><br><span class="line">* hard nofile 204800</span><br><span class="line">* soft nproc 655350</span><br><span class="line">* hard nproc 655350</span><br><span class="line">* soft memlock unlimited</span><br><span class="line">* hard memlock unlimited</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>最后，还需要修改/etc/security/limits.d/20-nproc.conf文件（centos7.x系统）</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 config]# vim /etc/security/limits.d/20-nproc.conf</span><br><span class="line">*          soft    nproc     655350</span><br><span class="line"></span><br><span class="line"># 启用生效</span><br><span class="line">重新打开终端即可</span><br><span class="line"></span><br><span class="line">[root@server2 ~]# ulimit -a</span><br></pre></td></tr></table></figure>
<h3 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h3><ol>
<li>JVM调优主要是针对elasticsearch的JVM内存资源进行优化，elasticsearch的内存资源配置文件为jvm.options，</li>
<li>此文件位于/usr/local/elasticsearch/config目录下，打开此文件,修改如下内容：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 ~]# cd /usr/local/elasticsearch/config/</span><br><span class="line">[root@server2 config]# vim jvm.options </span><br><span class="line">-Xms1g</span><br><span class="line">-Xmx1g</span><br><span class="line"></span><br><span class="line"># 我的阿里云主机是2G内存</span><br><span class="line"># 可根据服务器内存大小，修改为合适的值。一般设置为服务器物理内存的一半最佳。</span><br></pre></td></tr></table></figure>
<h3 id="配置-elasticsearch"><a href="#配置-elasticsearch" class="headerlink" title="配置 elasticsearch"></a>配置 elasticsearch</h3><ol>
<li>elasticsearch的配置文件均在elasticsearch根目录下的config文件夹，这里是/usr/local/elasticsearch/config目录，</li>
<li>主要有jvm.options、elasticsearch.yml和log4j2.properties三个主要配置文件。这里重点介绍elasticsearch.yml一些重要的配置项及其含义。</li>
<li>这里配置的elasticsearch.yml文件内容如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 config]# cd /usr/local/elasticsearch/config</span><br><span class="line">[root@server2 config]# vim elasticsearch.yml </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cluster.name: elkbigdata      # 集群名称</span><br><span class="line">node.name: server1            # 节点名称 配置文件这里都不一致</span><br><span class="line">node.master: true             # 是否有权利成为master 默认是true 默认第一台启动的es就是master</span><br><span class="line">node.data: true               # 是否是一个数据节点</span><br><span class="line">path.data: /data1/elasticsearch,/data2/elasticsearch      # 索引数据的存储路径</span><br><span class="line">path.logs: /usr/local/elasticsearch/logs                  # 日志</span><br><span class="line">bootstrap.memory_lock: true                               # 锁住物理内存 不使用swap</span><br><span class="line">network.host: 0.0.0.0</span><br><span class="line">http.port: 9200               # 对外提供http服务端口 通过该端口查看返回的数据信息</span><br><span class="line">discovery.zen.minimum_master_nodes: 1                     # 最少master节点数 集群中不能低于这个数</span><br><span class="line">discovery.zen.ping.timeout:3s # 默认 新增节点集群会去ping 3秒ping不到就认为故障 网络丢包就加大</span><br><span class="line">discovery.zen.ping.unicast.hosts: ["172.17.70.229:9300", "172.17.70.230:9300", "172.17.70.231:9300"]</span><br><span class="line"># master初始化列表</span><br><span class="line"></span><br><span class="line"># 9300 是es集群互相通信端口</span><br></pre></td></tr></table></figure>
<ol start="0">
<li><p>注意node.name肯定不能相同，其他可以相同</p>
</li>
<li><p>cluster.name: elkbigdata</p>
<ul>
<li>配置elasticsearch集群名称，默认是elasticsearch。这里修改为elkbigdata，elasticsearch会自动发现在同一网段下的集群名为elkbigdata的主机。</li>
</ul>
</li>
<li>node.name: server1<ul>
<li>节点名，任意指定一个即可，这里是server1，我们这个集群环境中有三个节点，分别是server1、server2和server3，记得根据主机的不同，要修改相应的节点名称。</li>
</ul>
</li>
<li>node.master: true<ul>
<li>指定该节点是否有资格被选举成为master，默认是true，elasticsearch集群中默认第一台启动的机器为master角色，如果这台服务器宕机就会重新选举新的master。</li>
</ul>
</li>
<li>node.data: true<ul>
<li>指据，默认为true，表示数据存储节点，如果节点配置node.master:false并且node.data: false，则该节点就是client node。</li>
<li>这个client node类似于一个“路由器”，负责将集群层面的请求转发到主节点，将数据相关的请求转发到数据节点。</li>
</ul>
</li>
<li>path.data:/data1/elasticsearch,/data2/elasticsearch<ul>
<li>设置索引数据的存储路径，默认是elasticsearch根目录下的data文件夹，这里自定义了两个路径，可以设置多个存储路径，用逗号隔开。</li>
</ul>
</li>
<li>path.logs: /usr/local/elasticsearch/logs<ul>
<li>设置日志文件的存储路径，默认是elasticsearch根目录下的logs文件夹</li>
</ul>
</li>
<li>bootstrap.memory_lock: true<ul>
<li>此配置项一般设置为true用来锁住物理内存。linux下可以通过“ulimit -l” 命令查看最大锁定内存地址空间（memlock）是不是unlimited</li>
</ul>
</li>
<li>network.host: 0.0.0.0 <ul>
<li>此配置项用来设置elasticsearch提供服务的IP地址，默认值为0.0.0.0，此参数是在elasticsearch新版本中增加的，此值设置为服务器的内网IP地址即可。</li>
</ul>
</li>
<li>http.port: 9200<ul>
<li>设置elasticsearch对外提供服务的http端口，默认为9200。其实，还有一个端口配置选项transport.tcp.port，此配置项用来设置节点间交互通信的TCP端口，默认是9300。</li>
</ul>
</li>
<li>discovery.zen.minimum_master_nodes: 1<ul>
<li>配置当前集群中最少的master节点数，默认为1，也就是说，elasticsearch集群中master节点数不能低于此值，如果低于此值，elasticsearch集群将停止运行。在三个以上节点的集群环境中，建议配置大一点的值，推荐2至4个为好。   </li>
</ul>
</li>
<li>discovery.zen.ping.unicast.hosts: [“172.17.70.229:9300”, “172.17.70.230:9300”,”172.17.70.231:9300”]<ul>
<li>设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。这里需要注意，master节点初始列表中对应的端口是9300。即为集群交互通信端口。</li>
</ul>
</li>
</ol>
<h3 id="创建data目录"><a href="#创建data目录" class="headerlink" title="创建data目录"></a>创建data目录</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># /data1/elasticsearch,/data2/elasticsearch</span><br><span class="line"></span><br><span class="line">[root@server2 config]# mkdir -p /data1/elasticsearch</span><br><span class="line">[root@server2 config]# mkdir -p /data2/elasticsearch</span><br><span class="line">[root@server2 config]# chown -R elasticsearch:elasticsearch /data1/elasticsearch/</span><br><span class="line">[root@server2 config]# chown -R elasticsearch:elasticsearch /data2/elasticsearch/</span><br></pre></td></tr></table></figure>
<h3 id="使用普通用户启动ES服务"><a href="#使用普通用户启动ES服务" class="headerlink" title="使用普通用户启动ES服务"></a>使用普通用户启动ES服务</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># es5版本后 禁止root用户启动es</span><br><span class="line"># 启动elasticsearch服务需要在一个普通用户下完成，如果通过root用户启动elasticsearch的话，可能会收到如下错误：</span><br><span class="line"># java.lang.RuntimeException: can not run elasticsearch as root</span><br><span class="line"># “-d”参数的意思是将elasticsearch放到后台运行。</span><br><span class="line"></span><br><span class="line">[root@server2 config]# su - elasticsearch</span><br><span class="line">[elasticsearch@server2 elasticsearch]$ cd /usr/local/elasticsearch/</span><br><span class="line">[elasticsearch@server2 elasticsearch]$ bin/elasticsearch -d </span><br><span class="line"></span><br><span class="line">[elasticsearch@server2 logs]$ netstat -ntlp </span><br><span class="line">[elasticsearch@server2 logs]$ ps -aux |grep java</span><br></pre></td></tr></table></figure>
<h3 id="验证elasticsearch集群的正确性"><a href="#验证elasticsearch集群的正确性" class="headerlink" title="验证elasticsearch集群的正确性"></a>验证elasticsearch集群的正确性</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 将所有elasticsearch节点的服务启动后，在任意一个节点执行如下命令：</span><br><span class="line">[elasticsearch@server2 logs]$ curl http://172.17.70.230:9200</span><br><span class="line">&#123;</span><br><span class="line">  "name" : "server2",</span><br><span class="line">  "cluster_name" : "elkbigdata",</span><br><span class="line">  "cluster_uuid" : "Qxj3RgRARVCylLtalVOZFA",</span><br><span class="line">  "version" : &#123;</span><br><span class="line">    "number" : "6.3.2",</span><br><span class="line">    "build_flavor" : "default",</span><br><span class="line">    "build_type" : "tar",</span><br><span class="line">    "build_hash" : "053779d",</span><br><span class="line">    "build_date" : "2018-07-20T05:20:23.451332Z",</span><br><span class="line">    "build_snapshot" : false,</span><br><span class="line">    "lucene_version" : "7.3.1",</span><br><span class="line">    "minimum_wire_compatibility_version" : "5.6.0",</span><br><span class="line">    "minimum_index_compatibility_version" : "5.0.0"</span><br><span class="line">  &#125;,</span><br><span class="line">  "tagline" : "You Know, for Search"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 同时查看另外两台节点</span><br><span class="line">curl http://172.17.70.229:9200</span><br><span class="line">curl http://172.17.70.231:9200</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_25.png"></p>
<h2 id="安装与配置-zookeeper-集群"><a href="#安装与配置-zookeeper-集群" class="headerlink" title="安装与配置 zookeeper 集群"></a>安装与配置 zookeeper 集群</h2><ol>
<li>对于集群模式下的ZooKeeper部署，官方建议至少要三台服务器，关于服务器的数量，推荐是奇数个（3、5、7、9等等），以实现ZooKeeper集群的高可用，这里使用三台服务器进行部署</li>
</ol>
<h3 id="下载与安装zookeeper"><a href="#下载与安装zookeeper" class="headerlink" title="下载与安装zookeeper"></a>下载与安装zookeeper</h3><ol>
<li>ZooKeeper是用Java编写的，需要安装Java运行环境，可以从zookeeper官网<a href="https://zookeeper.apache.org/获取zookeeper安装包，这里安装的版本是zookeeper-3.4.13.tar.gz。" target="_blank" rel="noopener">https://zookeeper.apache.org/获取zookeeper安装包，这里安装的版本是zookeeper-3.4.13.tar.gz。</a></li>
<li>将下载下来的安装包直接解压到一个路径下即可完成zookeeper的安装，</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 elk]# tar -zxvf zookeeper-3.4.13.tar.gz -C /usr/local</span><br><span class="line">[root@server2 elk]# mv /usr/local/zookeeper-3.4.13 /usr/local/zookeeper</span><br><span class="line">[root@server2 elk]# cd /usr/local/zookeeper/</span><br></pre></td></tr></table></figure>
<h3 id="配置zookeeper"><a href="#配置zookeeper" class="headerlink" title="配置zookeeper"></a>配置zookeeper</h3><ol>
<li>zookeeper 安装到了/usr/local目录下，因此，zookeeper的配置模板文件/usr/local/zookeeper/conf/zoo_sample.cfg，</li>
<li>拷贝zoo_sample.cfg并重命名为zoo.cfg，重点配置如下内容：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 conf]# cd /usr/local/zookeeper/conf/</span><br><span class="line">[root@server2 conf]# cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 配置属性参数</span><br><span class="line">tickTime=2000                       # 控制心跳超时 2000毫秒 度量单位</span><br><span class="line">initLimit=10                        # Follower服务器初始化连接到Leader时 最长能忍受多少个心跳 10*2000=20秒</span><br><span class="line">syncLimit=5                         # Leader与Follower之间发送消息，请求和应答时间长度 5*2000</span><br><span class="line">dataDir=/data/zookeeper             # 必须配 存储快照 不配置log 也会放在这里</span><br><span class="line">clientPort=2181                     # 监听端口</span><br><span class="line">server.1=172.16.213.51:2888:3888    # 集群服务器信息 .1代表第几台服务器 </span><br><span class="line">server.2=172.16.213.109:2888:3888   # 2888 是与 Leader 通信的端口</span><br><span class="line">server.3=172.16.213.75:2888:3888    # 3888 选举时服务器之间通信端口</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk1 conf]# grep ^'[a-Z]' zoo.cfg </span><br><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/data/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=172.17.70.232:2888:3888</span><br><span class="line">server.2=172.17.70.233:2888:3888</span><br><span class="line">server.3=172.17.70.234:2888:3888</span><br></pre></td></tr></table></figure>
<h3 id="每个配置项含义如下"><a href="#每个配置项含义如下" class="headerlink" title="每个配置项含义如下"></a>每个配置项含义如下</h3><ol>
<li>tickTime：zookeeper使用的基本时间度量单位，以毫秒为单位，它用来控制心跳和超时。2000表示2 tickTime。更低的tickTime值可以更快地发现超时问题。</li>
<li>initLimit：这个配置项是用来配置Zookeeper集群中Follower服务器初始化连接到Leader时，最长能忍受多少个心跳时间间隔数（也就是tickTime）</li>
<li>syncLimit：这个配置项标识Leader与Follower之间发送消息，请求和应答时间长度最长不能超过多少个tickTime的时间长度</li>
<li>dataDir：必须配置项，用于配置存储快照文件的目录。需要事先创建好这个目录，如果没有配置dataLogDir，那么事务日志也会存储在此目录。</li>
<li>clientPort：zookeeper服务进程监听的TCP端口，默认情况下，服务端会监听2181端口。</li>
<li>server.A=B:C:D：其中A是一个数字，表示这是第几个服务器；B是这个服务器的IP地址；<br>C表示的是这个服务器与集群中的Leader服务器通信的端口；D 表示如果集群中的Leader服务器宕机了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</li>
</ol>
<h3 id="myid-文件"><a href="#myid-文件" class="headerlink" title="myid 文件"></a>myid 文件</h3><ol>
<li>除了修改zoo.cfg配置文件外，集群模式下还要配置一个文件myid，</li>
<li>这个文件需要放在dataDir配置项指定的目录下，这个文件里面只有一个数字，如果要写入1，表示第一个服务器，与zoo.cfg文本中的server.1中的1对应，以此类推，</li>
<li>在集群的第二个服务器zoo.cfg配置文件中dataDir配置项指定的目录下创建myid文件，写入2，这个2与zoo.cfg文本中的server.2中的2对应。</li>
<li>Zookeeper在启动时会读取这个文件，得到里面的数据与zoo.cfg里面的配置信息比较，从而判断每个zookeeper server的对应关系。 </li>
<li>为了保证zookeeper集群配置的规范性，建议将zookeeper集群中每台服务器的安装和配置文件路径都保存一致。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server1 myid 1</span><br><span class="line">server2 myid 2</span><br><span class="line">server3 myid 3</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk1 conf]# mkdir -p /data/zookeeper</span><br><span class="line">[root@kafkazk1 conf]# cd /data/zookeeper</span><br><span class="line">[root@kafkazk1 zookeeper]# vim myid</span><br><span class="line">[root@kafkazk1 zookeeper]# cat /data/zookeeper/myid </span><br><span class="line">1</span><br><span class="line"></span><br><span class="line"># 3台集群同样操作</span><br></pre></td></tr></table></figure>
<h3 id="启动-zookeeper"><a href="#启动-zookeeper" class="headerlink" title="启动 zookeeper"></a>启动 zookeeper</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 三台机器一起启动</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 conf]# cd /usr/local/zookeeper/bin</span><br><span class="line">[root@kafkazk1 bin]# ./zkServer.sh  start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 查看启动是否成功</span><br><span class="line">1. Zookeeper启动后，通过jps命令（jdk内置命令）可以看到有一个QuorumPeerMain标识，</span><br><span class="line">2. 这个就是Zookeeper启动的进程，前面的数字是Zookeeper进程的PID。</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 bin]# jps</span><br><span class="line">1334 QuorumPeerMain</span><br><span class="line">1359 Jps</span><br><span class="line"></span><br><span class="line"># 日志文件 会在启动的路径下</span><br><span class="line">[root@kafkazk3 bin]# tail -200 zookeeper.out</span><br><span class="line"></span><br><span class="line"># 端口</span><br><span class="line">[root@kafkazk3 bin]# netstat -tnlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      1334/java           </span><br><span class="line">tcp        0      0 172.17.70.234:3888      0.0.0.0:*               LISTEN      1334/java</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 有时候为了启动Zookeeper方面，也可以添加zookeeper环境变量到系统的/etc/profile中，</span><br><span class="line"># 这样，在任意路径都可以执行“zkServer.sh  start”命令了，添加环境变量的内容为：</span><br><span class="line">vim /etc/profile</span><br><span class="line">export ZOOKEEPER_HOME=/usr/local/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<h2 id="安装并配置-Kafka-Broker-集群"><a href="#安装并配置-Kafka-Broker-集群" class="headerlink" title="安装并配置 Kafka Broker 集群"></a>安装并配置 Kafka Broker 集群</h2><ol>
<li>这里将kafka和zookeeper部署在一起了。另外，由于是部署集群模式的kafka，因此下面的操作需要在每个集群节点都执行一遍。</li>
</ol>
<h3 id="下载与安装Kafka"><a href="#下载与安装Kafka" class="headerlink" title="下载与安装Kafka"></a>下载与安装Kafka</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 选择kafka版本 需要与 filebeat 需求的对应</span><br><span class="line">https://www.elastic.co/guide/en/beats/filebeat/6.3/kafka-output.html</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_27.png" width="50%"></p>
<ol>
<li><p>可以从kafka官网 <a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">https://kafka.apache.org/downloads</a> 获取kafka安装包，这里推荐的版本是kafka_2.10-0.10.0.1.tgz，</p>
</li>
<li><p>将下载下来的安装包直接解压到一个路径下即可完成kafka的安装，这里统一将kafka安装到/usr/local目录下，基本操作过程如下：</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk1 elk]# tar -zxvf kafka_2.10-0.10.0.1.tgz -C /usr/local</span><br><span class="line">[root@kafkazk1 elk]# mv /usr/local/kafka_2.10-0.10.0.1  /usr/local/kafka</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk1 kafka]# ls -l</span><br><span class="line"></span><br><span class="line">drwxr-xr-x 3 root root  4096 Aug  4  2016 bin</span><br><span class="line">drwxr-xr-x 2 root root  4096 Aug  4  2016 config</span><br><span class="line">drwxr-xr-x 2 root root  4096 Oct 26 11:50 libs</span><br><span class="line">-rw-r--r-- 1 root root 28824 Aug  4  2016 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root   336 Aug  4  2016 NOTICE</span><br><span class="line">drwxr-xr-x 2 root root  4096 Aug  4  2016 site-docs</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 主配置文件</span><br><span class="line">[root@kafkazk1 kafka]# ls -l config/server.properties </span><br><span class="line"></span><br><span class="line"># kafka他也自带了zookeeper,但是我们用的是自己的所有不用关心他的配置文件</span><br></pre></td></tr></table></figure>
<h3 id="配置-kafka集群"><a href="#配置-kafka集群" class="headerlink" title="配置 kafka集群"></a>配置 kafka集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 常用配置</span><br><span class="line"></span><br><span class="line">broker.id=1                                 # 节点在集群中的唯一标识 每个节点需要修改</span><br><span class="line">listeners=PLAINTEXT://172.16.213.51:9092    # kafka监听地址和端口 9092默认 每个节点需要修改</span><br><span class="line">log.dirs=/usr/local/kafka/logs              # 日志文件 关键是数据</span><br><span class="line">num.partitions=6                            # topic有多少个分区 &gt;= 消费者数 保证每个消费者都能得到数据</span><br><span class="line">num.recovery.threads.per.data.dir=1         # 可减少启动时 日志的加载时间</span><br><span class="line">log.retention.hours=60                      # 日志保持时间 60小时                            </span><br><span class="line">log.segment.bytes=1073741824                # partition中每个segment数据文件的大小，默认是1G</span><br><span class="line">zookeeper.connect=172.16.213.51:2181,172.16.213.75:2181,172.16.213.109:2181</span><br><span class="line"># zookeeper 所在的地址 三个zookeeper节点</span><br><span class="line">log.retention.check.interval.ms=300000      # 日志检查时间 </span><br><span class="line">zookeeper.connection.timeout.ms=6000        # zookeeper 连接超时时间</span><br><span class="line"></span><br><span class="line">auto.create.topics.enable=true              # 是否自动创建topic     </span><br><span class="line">delete.topic.enable=true                    # 设置可以物理删除topic</span><br></pre></td></tr></table></figure>
<h3 id="每个配置项含义如下-1"><a href="#每个配置项含义如下-1" class="headerlink" title="每个配置项含义如下"></a>每个配置项含义如下</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">1. broker.id：</span><br><span class="line">    - 每一个broker在集群中的唯一表示，要求是正数。</span><br><span class="line">    - 当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况。</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">2. listeners：</span><br><span class="line">    - 设置kafka的监听地址与端口，可以将监听地址设置为主机名或IP地址，这里将监听地址设置为IP地址。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. log.dirs：</span><br><span class="line">    - 这个参数用于配置kafka保存数据的位置，kafka中所有的消息都会存在这个目录下。</span><br><span class="line">    - 可以通过逗号来指定多个路径， kafka会根据最少被使用的原则选择目录分配新的parition。</span><br><span class="line">    - 需要注意的是，kafka在分配parition的时候选择的规则不是按照磁盘的空间大小来定的，而是根据分配的 parition的个数多小而定。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4. num.partitions：</span><br><span class="line">    - 这个参数用于设置新创建的topic有多少个分区，可以根据消费者实际情况配置，配置过小会影响消费性能。</span><br><span class="line">    - 这里配置6个。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5. log.retention.hours：</span><br><span class="line">    - 这个参数用于配置kafka中消息保存的时间，还支持log.retention.minutes和 log.retention.ms配置项。</span><br><span class="line">    - 这三个参数都会控制删除过期数据的时间，推荐使用log.retention.ms。如果多个同时设置，那么会选择最小的那个。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">6. log.segment.bytes：</span><br><span class="line">    - 配置partition中每个segment数据文件的大小，默认是1GB，超过这个大小会自动创建一个新的segment file。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">7. zookeeper.connect：</span><br><span class="line">    - 这个参数用于指定zookeeper所在的地址，它存储了broker的元信息。 </span><br><span class="line">    - 这个值可以通过逗号设置多个值，每个值的格式均为：hostname:port/path，</span><br><span class="line">    - 每个部分的含义如下：</span><br><span class="line">    - hostname：表示zookeeper服务器的主机名或者IP地址，这里设置为IP地址。	</span><br><span class="line">    - port： 表示是zookeeper服务器监听连接的端口号。	</span><br><span class="line">    - /path：表示kafka在zookeeper上的根目录。如果不设置，会使用根目录。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8. auto.create.topics.enable：</span><br><span class="line">    - 这个参数用于设置是否自动创建topic，如果请求一个topic时发现还没有创建， kafka会在broker上自动创建一个topic，</span><br><span class="line">    - 如果需要严格的控制topic的创建，那么可以设置auto.create.topics.enable为false，禁止自动创建topic。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">9. delete.topic.enable：</span><br><span class="line">    - 在0.8.2版本之后，Kafka提供了删除topic的功能，但是默认并不会直接将topic数据物理删除。</span><br><span class="line">    - 如果要从物理上删除（即删除topic后，数据文件也会一同删除），就需要设置此配置项为true。</span><br></pre></td></tr></table></figure>
<h3 id="本次配置"><a href="#本次配置" class="headerlink" title="本次配置"></a>本次配置</h3><ol>
<li>kafka集群都要修改配置文件</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk1 config]# grep ^'[a-Z]' server.properties </span><br><span class="line"></span><br><span class="line">broker.id=1</span><br><span class="line">listeners=PLAINTEXT://172.17.70.232:9092</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/usr/local/kafka/logs</span><br><span class="line">num.partitions=6</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">log.retention.hours=60</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">auto.create.topics.enable=true</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure>
<h3 id="启动kafka集群"><a href="#启动kafka集群" class="headerlink" title="启动kafka集群"></a>启动kafka集群</h3><ol>
<li>在启动kafka集群前，需要确保ZooKeeper集群已经正常启动。</li>
<li>依次在kafka各个节点上执行如下命令即可</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># nohup 后台启动</span><br><span class="line"># &amp; 输出nohup.out 日志在本地</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 kafka]# cd /usr/local/kafka</span><br><span class="line">[root@kafkazk1 kafka]# nohup bin/kafka-server-start.sh config/server.properties &amp;</span><br><span class="line">[1] 1766</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 kafka]# nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">[root@kafkazk1 kafka]# ls</span><br><span class="line">bin  config  libs  LICENSE  logs  nohup.out  NOTICE  site-docs</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这里将kafka放到后台运行，启动后，会在启动kafka的当前目录下生成一个nohup.out文件，</span><br><span class="line">1. 可通过此文件查看kafka的启动和运行状态。</span><br><span class="line">2. 通过jps指令，可以看到有个Kafka标识，这是kafka进程成功启动的标志。</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 kafka]# jps</span><br><span class="line">2032 Jps</span><br><span class="line">1334 QuorumPeerMain</span><br><span class="line">1766 Kafka</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@kafkazk2 kafka]# tail -200 nohup.out </span><br><span class="line">[root@kafkazk1 kafka]# ps -ef|grep java</span><br><span class="line">[root@kafkazk1 kafka]# netstat -tnlp | grep 9092</span><br></pre></td></tr></table></figure>
<h2 id="kafka-集群基本命令操作"><a href="#kafka-集群基本命令操作" class="headerlink" title="kafka 集群基本命令操作"></a>kafka 集群基本命令操作</h2><ol>
<li><p>kefka提供了多个命令用于查看、创建、修改、删除topic信息，</p>
</li>
<li><p>也可以通过命令测试如何生产消息,消费消息等,这些命令位于kafka安装目录的bin目录下,这里是/usr/local/kafka/bin。</p>
</li>
<li><p>登录任意一台kafka集群节点，切换到此目录下，即可进行命令操作。</p>
</li>
<li><p>下面列举kafka的一些常用命令的使用方法:</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 务必掌握</span><br><span class="line">1. 显示topic列表</span><br><span class="line">2. 创建一个topic，并指定topic属性（副本数、分区数等）</span><br><span class="line">3. 查看某个topic的状态</span><br><span class="line">4. 生产消息</span><br><span class="line">5. 消费消息</span><br><span class="line">6. 删除topic</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 显示当前kafka集群中的topic列表</span><br><span class="line">[root@kafkazk1 kafka]# cd /usr/local/kafka</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个topic</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --create --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --replication-factor 1 --partitions 3 --topic mytopic</span><br><span class="line">Created topic "mytopic".</span><br><span class="line"></span><br><span class="line">--replication-factor 1    # 副本信息 保存1份</span><br><span class="line">--partitiions3            # partitiions数量 和消费者数量有关</span><br><span class="line">--topic mytopic           # topic 名称</span><br><span class="line"></span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list</span><br><span class="line">mytopic</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 查看某个topic的属性信息</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopic</span><br><span class="line">Topic:mytopic	PartitionCount:3	ReplicationFactor:1	Configs:</span><br><span class="line">	Topic: mytopic	Partition: 0	Leader: 1	Replicas: 1	Isr: 1</span><br><span class="line">	Topic: mytopic	Partition: 1	Leader: 2	Replicas: 2	Isr: 2</span><br><span class="line">	Topic: mytopic	Partition: 2	Leader: 3	Replicas: 3	Isr: 3</span><br><span class="line"></span><br><span class="line"># Replicas 对应副本</span><br><span class="line"># Isr: 1   活动状态</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 生产消息</span><br><span class="line"># --broker-list 指定broker信息</span><br><span class="line"># 生产消息指定broker地址+端口 kafka集群的节点信息</span><br><span class="line"># --topic 指定消息生产在哪个topic</span><br><span class="line"># 交互命令 敲完命令 回车 输入消息</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-console-producer.sh --broker-list  172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092 --topic mytopic</span><br><span class="line">test data kafka  </span><br><span class="line">123456</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 消费消息 </span><br><span class="line"># 查看消息 就是消费消息</span><br><span class="line"># 在开个终端查看消息</span><br><span class="line"># 消息是实时消费</span><br><span class="line">[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopic</span><br><span class="line">heelo</span><br><span class="line">abcdefg</span><br><span class="line"></span><br><span class="line"># 通过键盘输入消息内容，消费者马上可以看到</span><br><span class="line"># Crtl + c  退出</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_29.png"></p>
<p><img src="/img/elk_28.png"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 从头开始接收 查看所有消息</span><br><span class="line"># 从头开始接收消息并没有顺序 ，只有实时消费查看 是按照生产的顺序</span><br><span class="line">[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopic --from beginning</span><br><span class="line">heelo</span><br><span class="line">test data kafka</span><br><span class="line">abcdefg</span><br><span class="line">123456</span><br><span class="line">leo</span><br></pre></td></tr></table></figure>
<ol>
<li><p>实际生产机制,并不是键盘生产消息，而是通过第三方软件，比如logstash或者filebeat,向kafka生产数据</p>
</li>
<li><p>消费者 可以使ES 也可以使logstash，最终实现数据传递</p>
</li>
<li><p>kafka就是生产和消费数据的中介,他实现数据的传递,消息队列,持久化缓存数据,作用于消息传输和保存数据</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 删除一个topic</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --delete --topic mytopic</span><br><span class="line">Topic mytopic is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line"># 如果没有设置 delete.topic.enable=true 那么就是标记删除(逻辑删除),设置了就是物理删除</span><br><span class="line"></span><br><span class="line"># 再次查看就没有了</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list</span><br><span class="line"></span><br><span class="line"># 调试消费数据会经常用到,确认数据是否到kafa</span><br></pre></td></tr></table></figure>
<h2 id="安装并配置-Filebeat"><a href="#安装并配置-Filebeat" class="headerlink" title="安装并配置 Filebeat"></a>安装并配置 Filebeat</h2><h3 id="为什么要使用-filebeat"><a href="#为什么要使用-filebeat" class="headerlink" title="为什么要使用 filebeat"></a>为什么要使用 filebeat</h3><ol>
<li>Logstash功能虽然强大，但是它依赖java、在数据量大的时候，Logstash进程会消耗过多的系统资源，这将严重影响业务系统的性能，</li>
<li>而filebeat就是一个完美的替代者，filebeat是Beat成员之一，基于Go语言，没有任何依赖，配置文件简单，格式明了，</li>
<li>同时，filebeat比logstash更加轻量级，所以占用系统资源极少，非常适合安装在生产机器上。</li>
</ol>
<h3 id="下载与安装-filebeat"><a href="#下载与安装-filebeat" class="headerlink" title="下载与安装 filebeat"></a>下载与安装 filebeat</h3><ol>
<li>由于filebeat基于go语言开发,无其他任何依赖,因而安装非常简单,</li>
<li>可以从elastic官网<a href="https://www.elastic.co/downloads/beats/filebeat" target="_blank" rel="noopener">https://www.elastic.co/downloads/beats/filebeat</a> 获取filebeat安装包，</li>
<li>这里下载的版本是filebeat-6.3.2-linux-x86_64.tar.gz。</li>
<li>将下载下来的安装包直接解压到一个路径下即可完成filebeat的安装。</li>
<li>根据前面的规划，将filebeat安装到filebeat server主机上，这里设定将filebeat安装到/usr/local目录下，</li>
<li>基本操作过程如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 ~]# mkdir -p /app/elk</span><br><span class="line">[root@filebeat1 ~]# cd /app/elk/</span><br><span class="line">[root@filebeat1 elk]# ls</span><br><span class="line">filebeat-6.3.2-linux-x86_64.tar.gz</span><br><span class="line"></span><br><span class="line">[root@filebeat1 elk]# tar -zxvf filebeat-6.3.2-linux-x86_64.tar.gz -C /usr/local</span><br><span class="line">[root@filebeat1 elk]# mv /usr/local/filebeat-6.3.2-linux-x86_64  /usr/local/filebeat</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 filebeat]# cd /usr/local/filebeat/</span><br><span class="line">[root@filebeat1 filebeat]# ls -l</span><br><span class="line"></span><br><span class="line">-rw-r--r--  1 root root    55717 Jul 20  2018 fields.yml</span><br><span class="line">-rwxr-xr-x  1 root root 47593843 Jul 20  2018 filebeat                    # 启动文件</span><br><span class="line">-rw-r-----  1 root root    58886 Jul 20  2018 filebeat.reference.yml</span><br><span class="line">-rw-------  1 root root     7230 Jul 20  2018 filebeat.yml                # 配置文件</span><br><span class="line">drwxrwxr-x  4 1000 1000     4096 Jul 20  2018 kibana</span><br><span class="line">-rw-r--r--  1 root root    13675 Jul 20  2018 LICENSE.txt</span><br><span class="line">drwxr-xr-x 16 1000 1000     4096 Jul 20  2018 module</span><br><span class="line">drwxr-xr-x  2 root root     4096 Jul 20  2018 modules.d                   # 模块文件 快速配置filebeat</span><br><span class="line">-rw-r--r--  1 root root   143351 Jul 20  2018 NOTICE.txt</span><br><span class="line">-rw-r--r--  1 root root      802 Jul 20  2018 README.md</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 配置方法</span><br><span class="line">1. 手动配置 filebeat.yml  能更清楚运行机制</span><br><span class="line">2. 模块化配置 modules.d 下的已经写好的配置 , 重命名就可以使用</span><br></pre></td></tr></table></figure>
<h3 id="配置-filebeat"><a href="#配置-filebeat" class="headerlink" title="配置 filebeat"></a>配置 filebeat</h3><ol>
<li>filebeat的配置文件目录为/usr/local/filebeat/filebeat.yml，这里仅列出常用的配置项，内容如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- type: log                                # 输入log日志类型</span><br><span class="line">  enabled: true                            # 默认是走模块方式,手工需要配置为true 模块就不启作用了</span><br><span class="line">  paths:                                   # 指定要收集的日志 可以是完整路径,也可以是模糊匹配</span><br><span class="line">   - /var/log/messages</span><br><span class="line">   - /var/log/secure</span><br><span class="line">  fields:                                  # log_topic: osmessages 就是我们定义的topic 的名称 所以搜集的日志会放入</span><br><span class="line">    log_topic: osmessages</span><br><span class="line">  </span><br><span class="line">  #exclude_files: ['.gz$']                 # 过滤 排除.gz结尾的文件</span><br><span class="line">  </span><br><span class="line">name: "172.16.213.157"                     # 服务器的标识 为空就是主机名</span><br><span class="line">output.kafka:                              # kafka 配置</span><br><span class="line">  enabled: true</span><br><span class="line">  hosts: ["172.16.213.51:9092", "172.16.213.75:9092", "172.16.213.109:9092"]        # kafka集群</span><br><span class="line">  version: "0.10"                                                                   # 版本</span><br><span class="line">  topic: '%&#123;[fields][log_topic]&#125;'                                                   # 指定输出到哪个topic</span><br><span class="line">  partition.round_robin:                                                            # rr轮询</span><br><span class="line">    reachable_only: true</span><br><span class="line">  worker: 2</span><br><span class="line">  required_acks: 1                          # 最大限度保证数据写入 leader确定后再进行发送下一条</span><br><span class="line">  compression: gzip                         # 数据压缩</span><br><span class="line">  max_message_bytes: 10000000               # 单条消息的最大长度 10M</span><br><span class="line">logging.level: debug                        # 消息级别 生产的时候可以改成警告或者info</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 实验版本</span><br><span class="line">filebeat.inputs:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - /var/log/messages</span><br><span class="line">    - /var/log/secure</span><br><span class="line"></span><br><span class="line">output.kafka:</span><br><span class="line">  enabled: true</span><br><span class="line">  hosts: ["172.17.70.232:9092", "172.17.70.232:9092", "172.17.70.232:9092"]</span><br><span class="line">  version: "0.10"</span><br><span class="line">  topic: '%&#123;[fields][log_topic]&#125;'</span><br><span class="line">  partition.round_robin:</span><br><span class="line">    reachable_only: true</span><br><span class="line">  worker: 2</span><br><span class="line">  required_acks: 1</span><br><span class="line">  compression: gzip</span><br><span class="line">  max_message_bytes: 10000000</span><br><span class="line">processors:</span><br><span class="line">- drop_fields:</span><br><span class="line">    fields: ["beat", "input", "source", "offset","prospector","host"]</span><br><span class="line">logging.level: debug</span><br><span class="line">name: "172.17.70.235"</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 官方文档支持</span><br><span class="line">https://www.elastic.co/guide/en/beats/filebeat/6.3/configuration-filebeat-options.html</span><br><span class="line">https://www.elastic.co/guide/en/beats/filebeat/6.3/configuring-output.html</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 所有输出的选项 都需要注释 默认的是es</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_33.png" width="50%"></p>
<p><img src="/img/elk_38.png" width="50%"></p>
<p><img src="/img/elk_31.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">收费插件 Xpack</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_34.png" width="50%"></p>
<h3 id="配置项的含义介绍如下："><a href="#配置项的含义介绍如下：" class="headerlink" title="配置项的含义介绍如下："></a>配置项的含义介绍如下：</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">1. filebeat.inputs：</span><br><span class="line">    - 用于定义数据原型。</span><br><span class="line">    </span><br><span class="line">2. type：</span><br><span class="line">    - 指定数据的输入类型，这里是log，即日志，是默认值，还可以指定为stdin，即标准输入。参考官方文档</span><br><span class="line">    </span><br><span class="line">3. enabled: true：</span><br><span class="line">    - 启用手工配置filebeat，而不是采用模块方式配置filebeat。</span><br><span class="line"></span><br><span class="line">4. paths：</span><br><span class="line">    - 用于指定要监控的日志文件，可以指定一个完整路径的文件，也可以是一个模糊匹配格式，例如：</span><br><span class="line">    - - /data/nginx/logs/nginx_*.log，该配置表示将获取/data/nginx/logs目录下的所有以.log结尾的文件，注意这里有个破折号“-”，</span><br><span class="line">    - 要在paths配置项基础上进行缩进，不然启动filebeat会报错，另外破折号前面不能有tab缩进，建议通过空格方式缩进。</span><br><span class="line">    - - /var/log/*.log，该配置表示将获取/var/log目录的所有子目录中以”.log”结尾的文件，而不会去查找/var/log目录下以”.log”结尾的文件。</span><br><span class="line"></span><br><span class="line">5. name: </span><br><span class="line">    - 设置filebeat收集的日志中对应主机的名字，如果配置为空，则使用该服务器的主机名。这里设置为IP，便于区分多台主机的日志信息。\</span><br><span class="line">    </span><br><span class="line">6. output.kafka：</span><br><span class="line">    - filebeat支持多种输出，支持向kafka，logstash，elasticsearch输出数据，这里的设置是将数据输出到kafka。</span><br><span class="line"></span><br><span class="line">7. enabled：</span><br><span class="line">    - 表明这个模块是启动的。</span><br><span class="line">    </span><br><span class="line">8. host:</span><br><span class="line">    - 指定输出数据到kafka集群上，地址为kafka集群IP加端口号。</span><br><span class="line"></span><br><span class="line">9. topic：</span><br><span class="line">    - 指定要发送数据给kafka集群的哪个topic，若指定的topic不存在，则会自动创建此topic。</span><br><span class="line">    - 注意topic的写法，在filebeat6.x之前版本是通过“%&#123;[type]&#125;”来自动获取document_type配置项的值。</span><br><span class="line">    - 而在filebeat6.x之后版本是通过'%&#123;[fields][log_topic]&#125;'来获取日志分类的。</span><br><span class="line"></span><br><span class="line">10. logging.level：</span><br><span class="line">    - 定义filebeat的日志输出级别，有critical、error、warning、info、debug五种级别可选，在调试的时候可选择debug模式。</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 过滤 清除字段</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_32.png" width="50%"></p>
<h3 id="启动-filebeat-收集日志"><a href="#启动-filebeat-收集日志" class="headerlink" title="启动 filebeat 收集日志"></a>启动 filebeat 收集日志</h3><ol>
<li>所有配置完成之后，就可以启动filebeat，开启收集日志进程了，启动方式如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 filebeat]# cd /usr/local/filebeat/</span><br><span class="line"></span><br><span class="line">[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;</span><br><span class="line">[root@filebeat1 filebeat]# tail -200 nohup.out </span><br><span class="line"></span><br><span class="line"># 开始监控文件</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_35.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 看看要收集的日志 格式</span><br><span class="line"></span><br><span class="line">[root@filebeat1 filebeat]# tail /var/log/messages</span><br><span class="line">Oct 26 17:12:01 filebeat1 rsyslogd: [origin software="rsyslogd" swVersion="8.24.0" x-pid="813" x-info="http://www.rsyslog.com"] rsyslogd was HUPed</span><br><span class="line">Oct 26 17:20:01 filebeat1 systemd: Started Session 11 of user root.</span><br><span class="line">Oct 26 17:20:01 filebeat1 systemd: Starting Session 11 of user root.</span><br><span class="line">Oct 26 17:30:01 filebeat1 systemd: Started Session 12 of user root.</span><br><span class="line">Oct 26 17:30:01 filebeat1 systemd: Starting Session 12 of user root.</span><br><span class="line"></span><br><span class="line">[root@filebeat1 filebeat]# tail /var/log/secure</span><br></pre></td></tr></table></figure>
<h3 id="模拟测试-filebeat-输出信息格式解读"><a href="#模拟测试-filebeat-输出信息格式解读" class="headerlink" title="模拟测试 filebeat 输出信息格式解读"></a>模拟测试 filebeat 输出信息格式解读</h3><ol>
<li>开启filebeat的日志查看记录,有变化就会更新到日志中</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 filebeat]# tailf nohup.out</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>模拟一个失败的登录,日志产生到 /var/log/secure 中</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># 找一台服务器访问filebeat主机,故意失败登录,故意输错密码</span><br><span class="line">[root@server2 elk]# ssh 172.17.70.235</span><br><span class="line">root@172.17.70.235's password: </span><br><span class="line">Permission denied, please try again.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@filebeat1 filebeat]# tailf nohup.out </span><br><span class="line"># json格式的输出</span><br><span class="line">...</span><br><span class="line">"@timestamp": "2019-10-26T09:39:23.831Z",   # 收集到日志的具体时间</span><br><span class="line">"source": "/var/log/secure",                # 收集的哪个日志</span><br><span class="line">"offset": 151,                              # 偏移量,消费信息的时候可以使用</span><br><span class="line"></span><br><span class="line">  "prospector": &#123;                           # 收集类型 和 输入日志类型</span><br><span class="line">    "type": "log"</span><br><span class="line">  &#125;,</span><br><span class="line">  "input": &#123;</span><br><span class="line">    "type": "log"</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  "fields": &#123;                               # 自定义的列</span><br><span class="line">    "log_topic": "osmessages"</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  "beat": &#123;                                 # 收集信息 </span><br><span class="line">    "name": "172.17.70.235",</span><br><span class="line">    "hostname": "filebeat1",</span><br><span class="line">    "version": "6.3.2"</span><br><span class="line">  &#125;,</span><br><span class="line">  </span><br><span class="line">  "host": &#123;                                  # 主机信息</span><br><span class="line">    "name": "172.17.70.235"</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  # 收集的数据信息 </span><br><span class="line">  "message": "Oct 26 17:39:14 filebeat1 sshd[1527]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.17.70.230  user=root",</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_36.png" width="50%"></p>
<p><img src="/img/elk_37.png" width="50%"></p>
<h3 id="filebeat输出信息格式解读"><a href="#filebeat输出信息格式解读" class="headerlink" title="filebeat输出信息格式解读"></a>filebeat输出信息格式解读</h3><ol>
<li>从这个输出可以看到，输出日志被修改成了JSON格式，日志总共分为10个字段，</li>
<li>分别是 “@timestamp”、”@metadata”、”beat”、”host”、”source”、<br>“offset”、”message”、”prospector”、”input”和”fields”字段，</li>
<li>每个字段含义如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. @timestamp：时间字段，表示读取到该行内容的时间。</span><br><span class="line">2. @metadata：元数据字段，此字段只有是跟Logstash进行交互使用。</span><br><span class="line">3. beat：beat属性信息，包含beat所在的主机名、beat版本等信息。</span><br><span class="line">4. host： 主机名字段，输出主机名，如果没主机名，输出主机对应的IP。</span><br><span class="line">5. source： 表示监控的日志文件的全路径。</span><br><span class="line">6. offset： 表示该行日志的偏移量。</span><br><span class="line">7. message： 表示真正的日志内容。</span><br><span class="line">8. prospector：filebeat对应的消息类型。</span><br><span class="line">9. input：日志输入的类型，可以有多种输入类型，例如Log、Stdin、redis、Docker、TCP/UDP等</span><br><span class="line">10.fields：topic对应的消息字段或自定义增加的字段。</span><br></pre></td></tr></table></figure>
<h3 id="过滤字段"><a href="#过滤字段" class="headerlink" title="过滤字段"></a>过滤字段</h3><ol start="0">
<li>日志输出格式介绍和字段删减方法</li>
<li>filebeat收集了这么多字段的数据,所有我们要做一个简单的过滤 再交给后面的程序</li>
<li>通过filebeat接收到的内容，默认增加了不少字段，但是有些字段对数据分析来说没有太大用处，</li>
<li>所以有时候需要删除这些没用的字段，在filebeat配置文件中添加如下配置，即可删除不需要的字段：</li>
<li>这个设置表示删除”beat”、”input”、”source”、”offset” 四个字段，其中， </li>
<li>@timestamp 和@metadata字段是不能删除的，就算加上也过滤不了。</li>
<li>做完这个设置后，再次查看kafka中的输出日志，已经不再输出这四个字段信息了。</li>
<li>后面可以通过logstash进行更好的过滤,不用担心</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">processors:</span><br><span class="line">- drop_fields:</span><br><span class="line">    fields: ["beat", "input", "source", "offset"]</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_38.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 重启filebeat </span><br><span class="line">[root@filebeat1 filebeat]# pgrep -f filebeat</span><br><span class="line">1514</span><br><span class="line">[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat`</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 减少输出后</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_39.png" width="50%"></p>
<h3 id="查看kafka集群有没有收到日志"><a href="#查看kafka集群有没有收到日志" class="headerlink" title="查看kafka集群有没有收到日志"></a>查看kafka集群有没有收到日志</h3><ol>
<li><p>登录到任意的kafka集群节点</p>
</li>
<li><p>消费topic osmessages就行</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 先看看有没有创建 我们定义的topic</span><br><span class="line">[root@kafkazk2 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list</span><br><span class="line">osmessages</span><br><span class="line"></span><br><span class="line"># 开启一个消费</span><br><span class="line">[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic osmessages</span><br><span class="line"></span><br><span class="line"># 再模拟一次数据查看 数据有消费</span><br><span class="line"># filebeat 实时查询并生产数据 可以正常发送给 kafka</span><br><span class="line"># logstash 可以进行更好的过滤将想要的数据 匹配给不同的字段</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_40.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 时间概念:</span><br><span class="line"></span><br><span class="line">"@timestamp":"2019-10-26T10:32:52.317Z"         # 收集日志时间</span><br><span class="line">"message":"Oct 26 18:32:51                      # 输出信息:系统日志打印的真实时间</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_41.png" width="50%"></p>
<h2 id="安装并配置-Logstash-服务"><a href="#安装并配置-Logstash-服务" class="headerlink" title="安装并配置 Logstash 服务"></a>安装并配置 Logstash 服务</h2><h3 id="下载与安装-Logstash"><a href="#下载与安装-Logstash" class="headerlink" title="下载与安装 Logstash"></a>下载与安装 Logstash</h3><ol>
<li>可以从elastic官网 <a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="noopener">https://www.elastic.co/downloads/logstash</a> 获取logstash安装包，这里下载的版本是logstash-6.3.2.tar.gz。</li>
<li>将下载下来的安装包直接解压到一个路径下即可完成logstash的安装。根据前面的规划，</li>
<li>将logstash安装到logstash server主机上，这里统一将logstash安装到/usr/local目录下，基本操作过程如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 ~]# mkdir -p /app/elk</span><br><span class="line">[root@filebeat1 ~]# cd /app/elk</span><br><span class="line"></span><br><span class="line">[root@logstash elk]# tar -zxvf logstash-6.3.2.tar.gz -C /usr/local</span><br><span class="line">[root@logstash elk]# mv /usr/local/logstash-6.3.2 /usr/local/logstash</span><br><span class="line"></span><br><span class="line">[root@logstash elk]# cd /usr/local/logstash/</span><br></pre></td></tr></table></figure>
<h3 id="Logstash-是怎么工作的"><a href="#Logstash-是怎么工作的" class="headerlink" title="Logstash 是怎么工作的"></a>Logstash 是怎么工作的</h3><ol>
<li><p>Logstash是一个开源的、服务端的数据处理pipeline（管道），它可以接收多个源的数据、然后对它们进行转换、最终将它们发送到指定类型的目的地。</p>
</li>
<li><p>Logstash是通过 插件机制 实现各种功能的，可以在<a href="https://github.com/logstash-plugins" target="_blank" rel="noopener">https://github.com/logstash-plugins</a> 下载各种功能的插件，也可以自行编写插件。</p>
</li>
<li><p>Logstash实现的功能主要分为接收数据、解析过滤并转换数据、输出数据三个部分，对应的插件依次是input插件、filter插件、output插件，</p>
</li>
<li><p>其中，filter插件是可选的，其它两个是必须插件。也就是说在一个完整的Logstash配置文件中，必须有input插件和output插件。</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 官方文档学习</span><br><span class="line">https://www.elastic.co/guide/en/logstash/6.3/input-plugins.html</span><br></pre></td></tr></table></figure>
<h3 id="常用的-input-插件"><a href="#常用的-input-插件" class="headerlink" title="常用的 input 插件"></a>常用的 input 插件</h3><ol start="0">
<li>input插件主要用于接收数据，Logstash支持接收多种数据源，常用的有如下几种：</li>
<li>file: <ul>
<li>读取一个文件，这个读取功能有点类似于linux下面的tail命令，一行一行的实时读取。</li>
</ul>
</li>
<li>syslog: <ul>
<li>监听系统514端口的syslog messages，并使用RFC3164格式进行解析。</li>
</ul>
</li>
<li>redis: <ul>
<li>Logstash 可以从redis服务器读取数据，此时redis类似于一个消息缓存组件。</li>
</ul>
</li>
<li>kafka：<ul>
<li>Logstash 也可以从kafka集群中读取数据，kafka加Logstash的架构一般用在数据量较大的业务场景，kafka可用作数据的缓冲和存储。</li>
</ul>
</li>
<li>filebeat：<ul>
<li>filebeat是一个文本日志收集器，性能稳定，并且占用系统资源很少，Logstash可以接收filebeat发送过来的数据。</li>
</ul>
</li>
</ol>
<h3 id="常用的-filter"><a href="#常用的-filter" class="headerlink" title="常用的 filter"></a>常用的 filter</h3><ol start="0">
<li>filter插件主要用于数据的过滤、解析和格式化，也就是将非结构化的数据解析成结构化的、可查询的标准化数据。常见的filter插件有如下几个：</li>
<li>grok: 正则捕获<ul>
<li>grok是Logstash最重要的插件，可解析并结构化任意数据，支持正则表达式，并提供了很多内置的规则和模板可供使用。</li>
<li>使用最多，但也最复杂。</li>
</ul>
</li>
<li>mutate: 数据修改<ul>
<li>提供了丰富的基础类型数据处理能力。包括类型转换，字符串处理和字段处理等。</li>
</ul>
</li>
<li>date：时间处理<ul>
<li>可以用来转换你的日志记录中的时间字符串。</li>
</ul>
</li>
<li>GeoIP：地址查询<ul>
<li>可以根据IP地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。</li>
</ul>
</li>
</ol>
<h3 id="常用的-output"><a href="#常用的-output" class="headerlink" title="常用的 output"></a>常用的 output</h3><ol start="0">
<li>output插件用于数据的输出，一个Logstash事件可以穿过多个output，直到所有的output处理完毕，这个事件才算结束。输出插件常见的有如下几种：</li>
<li>elasticsearch: <ul>
<li>发送数据到elasticsearch。</li>
</ul>
</li>
<li>file：<ul>
<li>发送数据到文件中。</li>
</ul>
</li>
<li>redis：<ul>
<li>发送数据到redis中，从这里可以看出，redis插件既可以用在input插件中，也可以用在output插件中。</li>
</ul>
</li>
<li>kafka：<ul>
<li>发送数据到kafka中，与redis插件类似，此插件也可以用在Logstash的输入和输出插件中。</li>
</ul>
</li>
</ol>
<h3 id="Logstash-配置文件入门"><a href="#Logstash-配置文件入门" class="headerlink" title="Logstash 配置文件入门"></a>Logstash 配置文件入门</h3><ol>
<li>/usr/local/logstash/config/, jvm.options是设置JVM内存资源的配置文件，logstash.yml是logstash全局属性配置文件，</li>
<li>另外还需要自己创建一个logstash事件配置文件，这里介绍下logstash事件配置文件的编写方法和使用方式。</li>
<li>在介绍Logstash配置之前，先来认识一下logstash是如何实现输入和输出的。</li>
<li>Logstash提供了一个shell脚本/usr/local/logstash/bin/logstash,</li>
<li>可以方便快速的启动一个logstash进程，在Linux命令行下，运行如下命令启动Logstash进程：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash elk]# cd /usr/local/logstash/</span><br><span class="line">[root@logstash logstash]# bin/logstash -e 'input&#123;stdin&#123;&#125;&#125; output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;'</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_42.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. -e代表执行的意思。</span><br><span class="line">2. input即输入的意思，input里面即是输入的方式，这里选择了stdin，就是标准输入（从终端输入）。</span><br><span class="line">3. output即输出的意思，output里面是输出的方式，这里选择了stdout，就是标准输出（输出到终端）。</span><br><span class="line">4. 这里的codec是个插件，表明格式。这里放在stdout中，表示输出的格式，</span><br><span class="line">5. rubydebug是专门用来做测试的格式，一般用来在终端输出JSON格式。</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 这就是logstash的输出格式。Logstash在输出内容中会给事件添加一些额外信息。</span><br><span class="line">2. 比如"@version"、"host"、"@timestamp" 都是新增的字段， </span><br><span class="line">3. 而最重要的是@timestamp ，用来标记事件的发生时间。</span><br><span class="line">4. 由于这个字段涉及到Logstash内部流转，如果给一个字符串字段重命名为@timestamp的话，Logstash就会直接报错。</span><br><span class="line">5. 另外，也不能删除这个字段。</span><br></pre></td></tr></table></figure>
<h3 id="编写事件文件"><a href="#编写事件文件" class="headerlink" title="编写事件文件"></a>编写事件文件</h3><ol>
<li>在logstash的输出中，常见的字段还有type，表示事件的唯一类型,</li>
<li>tags，表示事件的某方面属性，我们可以随意给事件添加字段或者从事件里删除字段。</li>
<li>使用-e参数在命令行中指定配置是很常用的方式，但是如果logstash需要配置更多规则的话，就必须把配置固化到文件里，这就是logstash事件配置文件</li>
<li>如果把上面在命令行执行的logstash命令，写到一个配置文件logstash-1.conf中，就变成如下内容：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash logstash]# vim logstash-1.conf </span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">  stdin&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# bin/logstash -f logstash-1.conf</span><br><span class="line"></span><br><span class="line"># 通过这种方式也可以启动logstash进程，不过这种方式启动的进程是在前台运行的，要放到后台运行，</span><br><span class="line"># 可通过nohup命令实现，操作如下：</span><br><span class="line">[root@logstash logstash]# nohup bin/logstash -f logstash-simple.conf &amp;</span><br><span class="line"># 这样，logstash进程就放到了后台运行了，在当前目录会生成一个nohup.out文件，</span><br><span class="line"># 可通过此文件查看 logstash 进程的启动状态。</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_43.png" width="50%"></p>
<h3 id="input-输入插件-file"><a href="#input-输入插件-file" class="headerlink" title="input 输入插件 file"></a>input 输入插件 file</h3><ol>
<li>logstash启动后会去监控messages文件</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash logstash]# vim logstash-1.conf </span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">  file&#123;</span><br><span class="line">    path =&gt; "/var/log/messages"</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# bin/logstash -f logstash-1.conf</span><br><span class="line"></span><br><span class="line"># 我们发现日志产生的时间是世界时间,不是东八区,后续我们会用方法修改</span><br><span class="line"># file用来获取文件懂得输入,多了path文件路径字段,实时监控信息</span><br><span class="line"># 对于output插件，这里仍然采用rubydebug的JSON输出格式，这对于调试logstash输出信息是否正常非常有用。</span><br><span class="line"># 如果需要监控多个文件，可以通过逗号分隔即可，例如：</span><br><span class="line"># path =&gt; ["/var/log/*.log","/var/log/message","/var/log/secure"]</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_44.png" width="50%"></p>
<h3 id="output-输出插件-输出到kafka"><a href="#output-输出插件-输出到kafka" class="headerlink" title="output 输出插件 输出到kafka"></a>output 输出插件 输出到kafka</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash logstash]# vim logstash_in_kafka.conf</span><br><span class="line"></span><br><span class="line">input&#123;</span><br><span class="line">    file &#123;</span><br><span class="line">    path =&gt; ["/var/log/messages","/var/log/secure"]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">    bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092"</span><br><span class="line">    topic_id =&gt; "osmessages"</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 这个配置文件中，输入input仍然是file，重点看输出插件，这里定义了output的输出源为kafka，</span><br><span class="line">2. 通过bootstrap_servers选项指定了kafka集群的IP地址和端口。</span><br><span class="line">3. 特别注意这里IP地址的写法，每个IP地址之间通过逗号分隔。</span><br><span class="line">4. output输出中的topic_id选项，是指定输出到kafka中的哪个topic下，</span><br><span class="line">5. 这里是osmessages，如果无此topic，会自动重建topic。</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># kafka 消费数据</span><br><span class="line">[root@kafkazk1 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic osmessages</span><br><span class="line"></span><br><span class="line"># 放到后台 持续收集</span><br><span class="line">[root@logstash logstash]# nohup bin/logstash -f logstash_in_kafka.conf &amp;</span><br><span class="line">[root@logstash logstash]# ps -ef|grep java</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_45.png" width="50%"></p>
<p><img src="/img/elk_46.png" width="50%"></p>
<p><img src="/img/elk_47.png" width="50%"></p>
<h3 id="收集-filebeat端口发来的数据"><a href="#收集-filebeat端口发来的数据" class="headerlink" title="收集 filebeat端口发来的数据"></a>收集 filebeat端口发来的数据</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 5044是接收端口，filebeat把数据发送到他的5044端口上,logstash收集</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# vim logstash_in_filebeat.conf</span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">        port =&gt; 5044</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    stdout&#123;</span><br><span class="line">        codec =&gt; rubydebug</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Starting server on port: 5044  # 本地会被启动5044端口用来接收数据</span><br><span class="line"># 本次不用后台运行,在输出到终端</span><br><span class="line">[root@logstash logstash]# bin/logstash -f logstash_in_filebeat.conf</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# netstat -tnlp | grep 5044</span><br><span class="line">tcp        0      0 0.0.0.0:5044            0.0.0.0:*               LISTEN      1267/java</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_48.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 配置filebeat</span><br><span class="line"># 配置kafka先注释掉,输出改用 logstash</span><br><span class="line"></span><br><span class="line"># 重启filebeat</span><br><span class="line">[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat`</span><br><span class="line">[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;</span><br><span class="line">[root@filebeat1 filebeat]# tail -200 nohup.out </span><br><span class="line"></span><br><span class="line"># ssh连接故意输错密码，在查看logstash终端收集的信息</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_49.png" width="50%"></p>
<p><img src="/img/elk_50.png" width="50%"></p>
<p><img src="/img/elk_51.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 把输出改成kafka</span><br><span class="line"># 必须指定json格式</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# vim logstash_in_filebeat.conf</span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">        port =&gt; 5044</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092"</span><br><span class="line">        topic_id =&gt; "osmessages"</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# bin/logstash -f logstash_in_filebeat.conf</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_52.png" width="50%"></p>
<p><img src="/img/elk_53.png" width="50%"></p>
<h3 id="配置logstash作为转发节点"><a href="#配置logstash作为转发节点" class="headerlink" title="配置logstash作为转发节点"></a>配置logstash作为转发节点</h3><ol>
<li>上面对logstash的使用做了一个基础的介绍，现在回到本节介绍的这个案例中，在这个部署架构中，</li>
<li>logstash是作为一个二级转发节点使用的，也就是它将kafka作为数据接收源，然后将数据发送到elasticsearch集群中，</li>
<li>按照这个需求，新建logstash事件配置文件 kafka_os_into_es.conf，内容如下：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash logstash]# vim kafka_os_into_es.conf</span><br><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">        kafka &#123;</span><br><span class="line">        bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092"</span><br><span class="line">        topics =&gt; ["osmessages"]</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">        hosts =&gt; ["172.17.70.229:9200","172.17.70.230:9200","172.17.70.231:9200"]</span><br><span class="line">        index =&gt; "osmessageslog-%&#123;+YYYY-MM-dd&#125;"</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@logstash logstash]# nohup bin/logstash -f kafka_os_into_es.conf &amp;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 注意</span><br><span class="line">topics</span><br><span class="line">index 指定存到ES里数据 索引 的名称 必须指定 页面可以通过索引查询</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_54.png" width="50%"></p>
<h2 id="安装并配置Kibana展示日志数据"><a href="#安装并配置Kibana展示日志数据" class="headerlink" title="安装并配置Kibana展示日志数据"></a>安装并配置Kibana展示日志数据</h2><h3 id="下载与安装Kibana"><a href="#下载与安装Kibana" class="headerlink" title="下载与安装Kibana"></a>下载与安装Kibana</h3><ol>
<li><p>kibana使用Node.js(JavaScript)语言编写，安装部署十分简单，即下即用，可以从elastic官网<a href="https://www.elastic.co/cn/downloads/kibana" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/kibana</a> 下载所需的版本，</p>
</li>
<li><p>这里需要注意的是Kibana与Elasticsearch的版本必须一致，另外，在安装Kibana时，要确保Elasticsearch、Logstash和kafka已经安装完毕。</p>
</li>
<li><p>这里安装的版本是kibana-6.3.2-linux-x86_64.tar.gz。将下载下来的安装包直接解压到一个路径下即可完成kibana的安装，根据前面的规划，</p>
</li>
<li><p>将kibana安装到server2主机上，然后统一将kibana安装到/usr/local目录下，基本操作过程如下：</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 elk]# tar -zxvf kibana-6.3.2-linux-x86_64.tar.gz -C /usr/local</span><br><span class="line"></span><br><span class="line">[root@server2 elk]# mv /usr/local/kibana-6.3.2-linux-x86_64  /usr/local/kibana</span><br></pre></td></tr></table></figure>
<h3 id="配置-Kibana"><a href="#配置-Kibana" class="headerlink" title="配置 Kibana"></a>配置 Kibana</h3><ol>
<li><p>由于将Kibana安装到了/usr/local目录下，因此，Kibana的配置文件为/usr/local/kibana/config/kibana.yml,</p>
</li>
<li><p>Kibana 配置非常简单，这里仅列出常用的配置项，内容如下：</p>
</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 kibana]# vim /usr/local/kibana/config/kibana.yml</span><br><span class="line">[root@server2 kibana]# grep ^'[a-Z]' /usr/local/kibana/config/kibana.yml </span><br><span class="line">server.port: 5601</span><br><span class="line">server.host: "172.17.70.230"</span><br><span class="line">elasticsearch.url: "http://172.17.70.231:9200"</span><br><span class="line">kibana.index: ".kibana6"</span><br></pre></td></tr></table></figure>
<ol start="0">
<li>每个配置项的含义介绍如下<ul>
<li>阿里云服务器 用公网地址 开访问端口 5601</li>
</ul>
</li>
<li>server.port：<ul>
<li>kibana绑定的监听端口，默认是5601。</li>
</ul>
</li>
<li>server.host：<ul>
<li>kibana绑定的IP地址，如果内网访问，设置为内网地址即可。</li>
</ul>
</li>
<li>elasticsearch.url：<ul>
<li>kibana访问ElasticSearch的地址，如果是ElasticSearch集群，添加任一集群节点IP即可，</li>
<li>官方推荐是设置为ElasticSearch集群中client node角色的节点IP。</li>
</ul>
</li>
<li>kibana.index：<ul>
<li>用于存储kibana数据信息的索引，这个可以在kibanaweb界面中看到。</li>
</ul>
</li>
</ol>
<p><img src="/img/elk_55.png" width="50%"></p>
<h3 id="启动Kibana服务与web配置"><a href="#启动Kibana服务与web配置" class="headerlink" title="启动Kibana服务与web配置"></a>启动Kibana服务与web配置</h3><ol>
<li>启动kibana服务的命令在/usr/local/kibana/bin目录下，执行如下命令启动kibana服务：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@server2 kibana]# cd /usr/local/kibana/</span><br><span class="line"></span><br><span class="line">[root@server2 kibana]# nohup bin/kibana &amp;</span><br><span class="line"></span><br><span class="line">[root@server2 kibana]#  ps -ef|grep node</span><br><span class="line">root      1625  1237  6 10:44 pts/0    00:00:01 bin/../node/bin/node --no-warnings bin/../src/cli</span><br><span class="line">root      1637  1237  0 10:45 pts/0    00:00:00 grep --color=auto node</span><br><span class="line"></span><br><span class="line">http://60.205.217.112:5601/app/kibana</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_56.png" width="50%"></p>
<h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前我们在 logstash定义了 这个索引 </span><br><span class="line">osmessageslog-*</span><br><span class="line"></span><br><span class="line"># 根据时间排序</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_57.png" width="50%"></p>
<p><img src="/img/elk_58.png" width="50%"></p>
<p><img src="/img/elk_59.png" width="50%"></p>
<p><img src="/img/elk_60.png" width="50%"></p>
<h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以后不用再去每台服务器查看日志</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_61.png" width="50%"></p>
<p><img src="/img/elk_62.png" width="50%"></p>
<p><img src="/img/elk_63.png" width="50%"></p>
<p><img src="/img/elk_64.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">自定义 按照字段搜索</span><br><span class="line">message:Failed</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_65.png" width="50%"></p>
<p><img src="/img/elk_66.png" width="50%"></p>
<p><img src="/img/elk_67.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 收集的字段与我们filebeat 过滤的字段 息息相关 如果加上host 就可以知道来自哪台服务器,添加上就有了</span><br><span class="line"></span><br><span class="line">processors:</span><br><span class="line">- drop_fields:</span><br><span class="line">    fields: ["beat", "input", "source", "offset","prospector","host"]</span><br></pre></td></tr></table></figure>
<h3 id="修改filebeat的过滤"><a href="#修改filebeat的过滤" class="headerlink" title="修改filebeat的过滤"></a>修改filebeat的过滤</h3><p><img src="/img/elk_68.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat`</span><br><span class="line">[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;</span><br><span class="line"># 再去访问 添加上host字段</span><br><span class="line"># 在filebeat配置文件里 配置name = 服务器IP 就可现实IP地址，否则是默认主机名</span><br></pre></td></tr></table></figure>
<p><img src="/img/elk_69.png" width="50%"></p>
<p><img src="/img/elk_70.png" width="50%"></p>
<h2 id="调试并验证日志数据流向"><a href="#调试并验证日志数据流向" class="headerlink" title="调试并验证日志数据流向"></a>调试并验证日志数据流向</h2><ol>
<li>经过上面的配置过程，大数据日志分析平台已经基本构建完成，由于整个配置架构比较复杂，这里来梳理下各个功能模块的数据和业务流向。</li>
</ol>
<p><img src="/img/elk_71.png" width="50%"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ELK/" rel="tag"><i class="fa fa-tag"></i> ELK</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/25/elk-base05/" rel="next" title="05 ELK 常见应用架构">
                <i class="fa fa-chevron-left"></i> 05 ELK 常见应用架构
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/27/elk-base07/" rel="prev" title="07 ELK Logstash 配置语法详解">
                07 ELK Logstash 配置语法详解 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/Greeen.jpg" alt="Harris Li">
            
              <p class="site-author-name" itemprop="name">Harris Li</p>
              <p class="site-description motion-element" itemprop="description">Beijing</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">108</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#ELK-应用案例"><span class="nav-number">1.</span> <span class="nav-text">ELK 应用案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#典型-ELK-应用架构"><span class="nav-number">1.1.</span> <span class="nav-text">典型 ELK 应用架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#环境与角色说明"><span class="nav-number">2.</span> <span class="nav-text">环境与角色说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#服务器环境与角色"><span class="nav-number">2.1.</span> <span class="nav-text">服务器环境与角色</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#软件环境与版本"><span class="nav-number">2.2.</span> <span class="nav-text">软件环境与版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装JDK以及设置环境变量"><span class="nav-number">2.3.</span> <span class="nav-text">安装JDK以及设置环境变量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装并配置-elasticsearch-集群"><span class="nav-number">3.</span> <span class="nav-text">安装并配置 elasticsearch 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#elasticsearch-集群的架构与角色"><span class="nav-number">3.1.</span> <span class="nav-text">elasticsearch 集群的架构与角色</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集群中每个角色的含义介绍如下："><span class="nav-number">3.2.</span> <span class="nav-text">集群中每个角色的含义介绍如下：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-elasticsearch集群"><span class="nav-number">3.3.</span> <span class="nav-text">安装 elasticsearch集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增加es用户授权"><span class="nav-number">3.4.</span> <span class="nav-text">增加es用户授权</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#操作系统调优"><span class="nav-number">3.5.</span> <span class="nav-text">操作系统调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JVM调优"><span class="nav-number">3.6.</span> <span class="nav-text">JVM调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-elasticsearch"><span class="nav-number">3.7.</span> <span class="nav-text">配置 elasticsearch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建data目录"><span class="nav-number">3.8.</span> <span class="nav-text">创建data目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用普通用户启动ES服务"><span class="nav-number">3.9.</span> <span class="nav-text">使用普通用户启动ES服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#验证elasticsearch集群的正确性"><span class="nav-number">3.10.</span> <span class="nav-text">验证elasticsearch集群的正确性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装与配置-zookeeper-集群"><span class="nav-number">4.</span> <span class="nav-text">安装与配置 zookeeper 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载与安装zookeeper"><span class="nav-number">4.1.</span> <span class="nav-text">下载与安装zookeeper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置zookeeper"><span class="nav-number">4.2.</span> <span class="nav-text">配置zookeeper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#每个配置项含义如下"><span class="nav-number">4.3.</span> <span class="nav-text">每个配置项含义如下</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#myid-文件"><span class="nav-number">4.4.</span> <span class="nav-text">myid 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动-zookeeper"><span class="nav-number">4.5.</span> <span class="nav-text">启动 zookeeper</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装并配置-Kafka-Broker-集群"><span class="nav-number">5.</span> <span class="nav-text">安装并配置 Kafka Broker 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载与安装Kafka"><span class="nav-number">5.1.</span> <span class="nav-text">下载与安装Kafka</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-kafka集群"><span class="nav-number">5.2.</span> <span class="nav-text">配置 kafka集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#每个配置项含义如下-1"><span class="nav-number">5.3.</span> <span class="nav-text">每个配置项含义如下</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本次配置"><span class="nav-number">5.4.</span> <span class="nav-text">本次配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动kafka集群"><span class="nav-number">5.5.</span> <span class="nav-text">启动kafka集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-集群基本命令操作"><span class="nav-number">6.</span> <span class="nav-text">kafka 集群基本命令操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装并配置-Filebeat"><span class="nav-number">7.</span> <span class="nav-text">安装并配置 Filebeat</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要使用-filebeat"><span class="nav-number">7.1.</span> <span class="nav-text">为什么要使用 filebeat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#下载与安装-filebeat"><span class="nav-number">7.2.</span> <span class="nav-text">下载与安装 filebeat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-filebeat"><span class="nav-number">7.3.</span> <span class="nav-text">配置 filebeat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置项的含义介绍如下："><span class="nav-number">7.4.</span> <span class="nav-text">配置项的含义介绍如下：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动-filebeat-收集日志"><span class="nav-number">7.5.</span> <span class="nav-text">启动 filebeat 收集日志</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模拟测试-filebeat-输出信息格式解读"><span class="nav-number">7.6.</span> <span class="nav-text">模拟测试 filebeat 输出信息格式解读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#filebeat输出信息格式解读"><span class="nav-number">7.7.</span> <span class="nav-text">filebeat输出信息格式解读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过滤字段"><span class="nav-number">7.8.</span> <span class="nav-text">过滤字段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看kafka集群有没有收到日志"><span class="nav-number">7.9.</span> <span class="nav-text">查看kafka集群有没有收到日志</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装并配置-Logstash-服务"><span class="nav-number">8.</span> <span class="nav-text">安装并配置 Logstash 服务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载与安装-Logstash"><span class="nav-number">8.1.</span> <span class="nav-text">下载与安装 Logstash</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logstash-是怎么工作的"><span class="nav-number">8.2.</span> <span class="nav-text">Logstash 是怎么工作的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的-input-插件"><span class="nav-number">8.3.</span> <span class="nav-text">常用的 input 插件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的-filter"><span class="nav-number">8.4.</span> <span class="nav-text">常用的 filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的-output"><span class="nav-number">8.5.</span> <span class="nav-text">常用的 output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logstash-配置文件入门"><span class="nav-number">8.6.</span> <span class="nav-text">Logstash 配置文件入门</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写事件文件"><span class="nav-number">8.7.</span> <span class="nav-text">编写事件文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#input-输入插件-file"><span class="nav-number">8.8.</span> <span class="nav-text">input 输入插件 file</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#output-输出插件-输出到kafka"><span class="nav-number">8.9.</span> <span class="nav-text">output 输出插件 输出到kafka</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#收集-filebeat端口发来的数据"><span class="nav-number">8.10.</span> <span class="nav-text">收集 filebeat端口发来的数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置logstash作为转发节点"><span class="nav-number">8.11.</span> <span class="nav-text">配置logstash作为转发节点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装并配置Kibana展示日志数据"><span class="nav-number">9.</span> <span class="nav-text">安装并配置Kibana展示日志数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#下载与安装Kibana"><span class="nav-number">9.1.</span> <span class="nav-text">下载与安装Kibana</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-Kibana"><span class="nav-number">9.2.</span> <span class="nav-text">配置 Kibana</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启动Kibana服务与web配置"><span class="nav-number">9.3.</span> <span class="nav-text">启动Kibana服务与web配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建索引"><span class="nav-number">9.4.</span> <span class="nav-text">创建索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看数据"><span class="nav-number">9.5.</span> <span class="nav-text">查看数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#修改filebeat的过滤"><span class="nav-number">9.6.</span> <span class="nav-text">修改filebeat的过滤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#调试并验证日志数据流向"><span class="nav-number">10.</span> <span class="nav-text">调试并验证日志数据流向</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Harris Li</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
