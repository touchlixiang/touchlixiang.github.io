<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="k8s,">










<meta name="keywords" content="k8s">
<meta property="og:type" content="article">
<meta property="og:title" content="10 K8S 持久化存储 Ceph">
<meta property="og:url" content="https://touchlixiang.github.io/2020/01/13/k8s-base12/index.html">
<meta property="og:site_name" content="My Notes">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://touchlixiang.github.io/img/mix_3.jpg">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_1.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_2.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_3.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_4.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_5.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_6.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_7.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_8.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_9.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_10.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_11.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_12.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_13.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_14.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_15.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_16.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_17.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_18.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_19.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_20.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_21.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_22.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_23.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_24.png">
<meta property="og:image" content="https://touchlixiang.github.io/img/ceph/ceph_25.png">
<meta property="og:updated_time" content="2020-01-16T11:24:03.979Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="10 K8S 持久化存储 Ceph">
<meta name="twitter:image" content="https://touchlixiang.github.io/img/mix_3.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://touchlixiang.github.io/2020/01/13/k8s-base12/">





  <title>10 K8S 持久化存储 Ceph | My Notes</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">My Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">记不住的一定要写在这里</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://touchlixiang.github.io/2020/01/13/k8s-base12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Harris Li">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/Greeen.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">10 K8S 持久化存储 Ceph</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-13T18:06:08+08:00">
                2020-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index">
                    <span itemprop="name">k8s</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/img/mix_3.jpg" width="50%"><br><a id="more"></a></p>
<h2 id="Ceph-介绍"><a href="#Ceph-介绍" class="headerlink" title="Ceph 介绍"></a>Ceph 介绍</h2><h3 id="为什么要用-Ceph"><a href="#为什么要用-Ceph" class="headerlink" title="为什么要用 Ceph"></a>为什么要用 Ceph</h3><ol>
<li>Ceph是当前非常流行的<strong>开源分布式存储系统</strong>，具有高扩展性、高性能、高可靠性等优点，同时提供块存储服务(rbd)、对象存储服务(rgw)以及文件系统存储服务(cephfs)。</li>
<li>Ceph在存储的时候充分利用存储节点的计算能力，在存储每一个数据时都会通过计算得出该数据的位置，尽量的分布均衡。</li>
<li>目前也是OpenStack的主流后端存储，随着OpenStack在云计算领域的广泛使用，ceph也变得更加炙手可热。</li>
<li>国内目前使用ceph搭建分布式存储系统较为成功的企业有x-sky,深圳元核云，上海UCloud等三家企业。</li>
<li>Ceph设计思想：集群可靠性、集群可扩展性、数据安全性、接口统一性、充分发挥存储设备自身的计算能力、去除中心化。</li>
</ol>
<p><img src="/img/ceph/ceph_1.png" width="50%"></p>
<h3 id="Ceph-架构介绍"><a href="#Ceph-架构介绍" class="headerlink" title="Ceph 架构介绍"></a>Ceph 架构介绍</h3><p><img src="/img/ceph/ceph_2.png" width="50%"></p>
<p><img src="/img/ceph/ceph_3.png" width="50%"></p>
<ol>
<li>Ceph使用RADOS提供对象存储，通过librados封装库提供多种存储方式的文件和对象转换。</li>
<li>外层通过RGW（Object，有原生的API，而且也兼容Swift和S3的API，适合单客户端使用）、RBD（Block，支持精简配置、快照、克隆，适合多客户端有目录结构）、CephFS（File，Posix接口，支持快照，适合更新变动少的数据，没有目录结构不能直接打开）将数据写入存储。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">- 高性能  </span><br><span class="line">  1. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高  </span><br><span class="line">  2. 考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等  </span><br><span class="line">  3. 能够支持上千个存储节点的规模，支持TB到PB级的数据  </span><br><span class="line">- 高可扩展性  </span><br><span class="line">  1. 去中心化  </span><br><span class="line">  2. 扩展灵活  </span><br><span class="line">  3. 随着节点增加而线性增长  </span><br><span class="line">- 特性丰富  </span><br><span class="line">  1. 支持三种存储接口：块存储、文件存储、对象存储  </span><br><span class="line">  2. 支持自定义接口，支持多种语言驱动</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-核心概念"><a href="#Ceph-核心概念" class="headerlink" title="Ceph 核心概念"></a>Ceph 核心概念</h2><p><strong>RADOS</strong> </p>
<ol>
<li>全称Reliable Autonomic Distributed Object Store，即可靠的、自动化的、分布式对象存储系统。</li>
<li>RADOS是Ceph集群的精华，用户实现数据分配、Failover等集群操作。</li>
</ol>
<p><strong>Librados</strong></p>
<ol>
<li>Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的。</li>
<li>目前提供PHP、Ruby、Java、Python、C和C++支持。</li>
</ol>
<p><strong>Crush</strong></p>
<ol>
<li>Crush算法是Ceph的两大创新之一，通过Crush算法的寻址操作，Ceph得以摒弃了传统的集中式存储元数据寻址方案。</li>
<li>而Crush算法在一致性哈希基础上很好的考虑了容灾域的隔离，使得Ceph能够实现各类负载的副本放置规则，例如跨机房、机架感知等。</li>
<li>同时，Crush算法有相当强大的扩展性，理论上可以支持数千个存储节点，这为Ceph在大规模云环境中的应用提供了先天的便利。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Crush 算法 数据平均分配到各个节点上,访问的时候把数据拼起来获取 </span><br><span class="line">2. Crush 类似索引 有地址记录数据存放在哪里</span><br></pre></td></tr></table></figure>
<p><strong>Pool</strong></p>
<ol>
<li>Pool是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略(默认3副本 )。</li>
<li>支持两种类型：副本（replicated）和 纠删码（ Erasure Code）</li>
</ol>
<p><strong>PG</strong></p>
<ol>
<li>PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略。</li>
<li>简单点说就是相同PG内的对象都会放到相同的硬盘上，PG是ceph的逻辑概念，服务端数据均衡和恢复的最小粒度就是PG。</li>
<li>一个PG包含多个OSD,引入PG这一层其实是为了更好的分配数据和定位数据；</li>
</ol>
<p><strong>Object</strong></p>
<ol>
<li>简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。</li>
<li>能否弄一个读写快，利于共享的出来呢。于是就有了对象存储。最底层的存储单元，包含元数据和原始数据。</li>
</ol>
<p><img src="/img/ceph/ceph_4.png" width="50%"></p>
<h2 id="Ceph-核心组件"><a href="#Ceph-核心组件" class="headerlink" title="Ceph 核心组件"></a>Ceph 核心组件</h2><p><strong>OSD</strong></p>
<ol>
<li>OSD是负责物理存储的进程，一般配置成和磁盘一一对应，一块磁盘启动一个OSD进程。</li>
<li>主要功能是存储数据、复制数据、平衡数据、恢复数据，以及与其它OSD间进行心跳检查，负责响应客户端请求返回具体数据的进程等；  </li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Pool、PG和OSD的关系：</span><br><span class="line">1. 1个Pool里有很多PG；  </span><br><span class="line">2. 1个PG里包含一堆对象，1个对象只能属于一个PG；  </span><br><span class="line">3. PG有主从之分，一个PG分布在不同的OSD上（针对三副本类型）;</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_5.png" width="50%"></p>
<p><strong>Monitor</strong></p>
<ol>
<li>1个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。</li>
<li>负责坚实整个Ceph集群运行的Map视图（如OSD Map、Monitor Map、PG Map和CRUSH Map），维护集群的健康状态，维护展示集群状态的各种图表，管理集群客户端认证与授权；</li>
<li>Monitor生产上至少3个组成高可用。</li>
<li>定期探测组件的健康状态。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. osd想要什么数据 去 Monitor 里的 OSD Map 里面获取</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_6.png" width="50%"></p>
<p><strong>MDS</strong></p>
<ol>
<li>MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。</li>
<li>负责保存文件系统的元数据，管理目录结构。</li>
<li><font color="green">对象存储</font>和<font color="orange">块设备存储</font>不需要元数据服务；</li>
</ol>
<p><img src="/img/ceph/ceph_7.png" width="50%"></p>
<p><strong>Mgr</strong></p>
<ol>
<li>ceph 官方开发了 ceph-mgr，主要目标实现 ceph 集群的管理，为外界提供统一的入口。</li>
<li>例如 cephmetrics、zabbix、calamari、promethus。</li>
<li>Mgr可以作为主从模式,挂了不影响集群使用。</li>
</ol>
<p><strong>RGW</strong></p>
<ol>
<li>RGW 全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。</li>
</ol>
<p><strong>Admin</strong></p>
<ol>
<li>Ceph常用管理接口通常都是命令行工具，如rados、ceph、rbd等命令 。</li>
<li>另外Ceph还有可以有一个专用的管理节点，在此节点上面部署专用的管理工具来实现近乎集群的一些管理工作，如集群部署，集群组件管理等。</li>
</ol>
<h2 id="Ceph-三种存储类型"><a href="#Ceph-三种存储类型" class="headerlink" title="Ceph 三种存储类型"></a>Ceph 三种存储类型</h2><p><strong>块存储（RBD）</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">- 优点：</span><br><span class="line">  * 通过Raid与LVM等手段，对数据提供了保护；</span><br><span class="line">  * 多块廉价的硬盘组合起来，提高容量；</span><br><span class="line">  * 多块磁盘组合出来的逻辑盘，提升读写效率；  </span><br><span class="line"></span><br><span class="line">- 缺点：</span><br><span class="line">  * 采用SAN架构组网时，光纤交换机，造价成本高；</span><br><span class="line">  * 主机之间无法共享数据；</span><br><span class="line"></span><br><span class="line">- 使用场景</span><br><span class="line">  * docker容器、虚拟机磁盘存储分配；</span><br><span class="line">  * 日志存储；</span><br><span class="line">  * 文件存储；</span><br></pre></td></tr></table></figure>
<p><strong>文件存储（CephFS）</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">- 优点：</span><br><span class="line">  * 造价低，随便一台机器就可以了；</span><br><span class="line">  * 方便文件共享；</span><br><span class="line"></span><br><span class="line">- 缺点：</span><br><span class="line">  * 读写速率低；</span><br><span class="line">  * 传输速率慢；</span><br><span class="line"></span><br><span class="line">- 使用场景</span><br><span class="line">  * 日志存储；</span><br><span class="line">  * FTP、NFS；</span><br><span class="line">  * 其它有目录结构的文件存储</span><br></pre></td></tr></table></figure>
<p><strong>对象存储（Object）(适合更新变动较少的数据)</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 优点：</span><br><span class="line">  * 具备块存储的读写高速；</span><br><span class="line">  * 具备文件存储的共享等特性；</span><br><span class="line"></span><br><span class="line">- 使用场景</span><br><span class="line">  * 图片存储；</span><br><span class="line">  * 视频存储；</span><br></pre></td></tr></table></figure>
<p><strong>io ceph 流程</strong></p>
<p><img src="/img/ceph/ceph_8.png" width="50%"></p>
<ol>
<li>client 访问 Monitor map 拿数据 </li>
<li>然后去OSD去找文件,OSD里面有主从概念,主提供服务,副本不做改动,通过盘符标识(SSD/HDD),分配主从节点。</li>
</ol>
<p><img src="/img/ceph/ceph_9.png" width="50%"></p>
<ol>
<li>读数据 流程到返回</li>
<li>写数据 强一致性,主副本先写,写完后同步从副本，三个副本之间通信数据一致后,数据才可以继续读取</li>
</ol>
<h3 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h3><ol>
<li>为什么用ceph： 可扩展,节省成本,支持接口多</li>
<li>架构：分布式架构,多接口,RADOS -&gt; Librados -&gt; RGW,RBD,CephFS ,每个组件分布式,数据也是分布式 </li>
<li>三种存储类型: RBD(块存储),CephFS(文件存储),对象存储(object)</li>
</ol>
<h2 id="Ceph-集群部署"><a href="#Ceph-集群部署" class="headerlink" title="Ceph 集群部署"></a>Ceph 集群部署</h2><h3 id="Ceph-版本选择"><a href="#Ceph-版本选择" class="headerlink" title="Ceph 版本选择"></a>Ceph 版本选择</h3><ol>
<li>官网安装 最新版本</li>
<li>手动安装 内网yum源 安装指定版本 </li>
</ol>
<p><strong>Ceph版本来源介绍</strong></p>
<ol>
<li>Ceph 社区最新版本是 14，而 Ceph 12 是市面用的最广的稳定版本。</li>
<li>第一个 Ceph 版本是 0.1 ，要回溯到 2008 年 1 月。</li>
<li>多年来，版本号方案一直没变，直到 2015 年 4 月 0.94.1 （ Hammer 的第一个修正版）发布后，为了避免 0.99 （以及 0.100 或 1.00 ？），制定了新策略。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x.0.z - 开发版（给早期测试者和勇士们）</span><br><span class="line"></span><br><span class="line">x.1.z - 候选版（用于测试集群、高手们）</span><br><span class="line"></span><br><span class="line">x.2.z - 稳定、修正版（给用户们）</span><br><span class="line"></span><br><span class="line">x 将从 9 算起，它代表 Infernalis （ I 是第九个字母），这样第九个发布周期的第一个开发版就是 9.0.0 ；后续的开发版依次是 9.0.1 、 9.0.2 等等。</span><br><span class="line"></span><br><span class="line">ceph不好招人,对数据存在敬畏之心,小心操作,数据的安全性和一致性太重要了</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>版本名称</th>
<th>版本号</th>
<th>发布时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>Argonaut</td>
<td>0.48版本(LTS)</td>
<td>2012年6月3日</td>
</tr>
<tr>
<td>Bobtail</td>
<td>0.56版本(LTS)</td>
<td>2013年5月7日</td>
</tr>
<tr>
<td>Cuttlefish</td>
<td>0.61版本</td>
<td>2013年1月1日</td>
</tr>
<tr>
<td>Dumpling</td>
<td>0.67版本(LTS)</td>
<td>2013年8月14日</td>
</tr>
<tr>
<td>Emperor</td>
<td>0.72版本</td>
<td>2013年11月9</td>
</tr>
<tr>
<td>Firefly</td>
<td>0.80版本(LTS)</td>
<td>2014年5月</td>
</tr>
<tr>
<td>Giant</td>
<td>Giant</td>
<td>October 2014 - April 2015</td>
</tr>
<tr>
<td>Hammer</td>
<td>Hammer</td>
<td>April 2015 - November 2016</td>
</tr>
<tr>
<td>Infernalis</td>
<td>Infernalis</td>
<td>November 2015 - June 2016</td>
</tr>
<tr>
<td>Jewel</td>
<td>10.2.9</td>
<td>2016年4月</td>
</tr>
<tr>
<td>Kraken</td>
<td>11.2.1</td>
<td>2017年10月</td>
</tr>
<tr>
<td>Luminous</td>
<td>12.2.12</td>
<td>2017年10月</td>
</tr>
<tr>
<td>mimic</td>
<td>13.2.7</td>
<td>2018年5月</td>
</tr>
<tr>
<td>nautilus</td>
<td>14.2.5</td>
<td>2019年2月</td>
</tr>
</tbody>
</table>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 本次实验 nautilus</span><br><span class="line"># 成熟版本 Luminous</span><br></pre></td></tr></table></figure>
<h3 id="Luminous-新版本特性"><a href="#Luminous-新版本特性" class="headerlink" title="Luminous 新版本特性"></a>Luminous 新版本特性</h3><p><strong>1. Bluestore</strong> </p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1. ceph-osd的新后端存储BlueStore已经稳定，是新创建的OSD的默认设置。</span><br><span class="line"># Ceph BlueStore 与 FileStore </span><br><span class="line"># FileStore 先格式化系统盘 把数据写入磁盘形成文件 再写入到 RADOS </span><br><span class="line"># BlueStore 直接转化，无需格式化,直接管理裸盘</span><br><span class="line"></span><br><span class="line"># BlueStore通过直接管理物理HDD或SSD而不使用诸如XFS的中间文件系统，来管理每个OSD存储的数据，这提供了更大的性能和功能。</span><br><span class="line"># BlueStore支持Ceph存储的所有的完整的数据和元数据校验。</span><br><span class="line"># BlueStore内嵌支持使用zlib，snappy或LZ4进行压缩。（Ceph还支持zstd进行RGW压缩，但由于性能原因，不为BlueStore推荐使用zstd）</span><br><span class="line"></span><br><span class="line">2. 集群的总体可扩展性有所提高。已经成功测试了多达10,000个OSD的集群。按实际走100个osd应该稳定。</span><br><span class="line"></span><br><span class="line">3. ceph-mgr</span><br><span class="line"># ceph-mgr是一个新的后台进程，这是任何Ceph部署的必须部分。虽然当ceph-mgr停止时，IO可以继续，但是度量不会刷新，并且某些与度量相关的请求（例如，ceph df）可能会被阻止。</span><br><span class="line"># 我们建议您多部署ceph-mgr的几个实例来实现可靠性。</span><br><span class="line"># ceph-mgr守护进程daemon包括基于REST的API管理。注：API仍然是实验性质的，目前有一些限制，但未来会成为API管理的基础。</span><br><span class="line"># ceph-mgr还包括一个Prometheus插件。</span><br><span class="line"># ceph-mgr现在有一个Zabbix插件。使用zabbix_sender，它可以将集群故障事件发送到Zabbix Server主机。这样可以方便地监视Ceph群集的状态，并在发生故障时发送通知。</span><br></pre></td></tr></table></figure>
<h3 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 我自己重新创建了3台 阿里云主机</span><br><span class="line"># cephnode01 172.31.228.59</span><br><span class="line"># cephnode02 172.31.228.60</span><br><span class="line"># cephnode03 172.31.228.61</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">（1）关闭防火墙：</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">（2）关闭selinux：</span><br><span class="line">sed -i 's/enforcing/disabled/' /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">（3）关闭NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line"></span><br><span class="line">（4）添加主机名与IP对应关系：</span><br><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">172.31.228.59 cephnode01</span><br><span class="line">172.31.228.60 cephnode02 </span><br><span class="line">172.31.228.61 cephnode03</span><br><span class="line"></span><br><span class="line">（5）设置主机名：</span><br><span class="line">[root@ceph ~]# hostnamectl set-hostname cephnode01</span><br><span class="line">[root@ceph ~]# hostnamectl set-hostname cephnode02</span><br><span class="line">[root@ceph ~]# hostnamectl set-hostname cephnode03</span><br><span class="line"></span><br><span class="line">（6）同步网络时间和修改时区</span><br><span class="line">systemctl restart chronyd.service &amp;&amp; systemctl enable chronyd.service</span><br><span class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br><span class="line"></span><br><span class="line">（7）设置文件描述符</span><br><span class="line">echo "ulimit -SHn 102400" &gt;&gt; /etc/rc.local</span><br><span class="line">cat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF</span><br><span class="line">* soft nofile 65535</span><br><span class="line">* hard nofile 65535</span><br><span class="line"></span><br><span class="line">（8）内核参数优化</span><br><span class="line">cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF</span><br><span class="line">kernel.pid_max = 4194303</span><br><span class="line">echo "vm.swappiness = 0" /etc/sysctl.conf </span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br><span class="line"></span><br><span class="line">（9）在cephnode01上配置免密登录到cephnode02、cephnode03</span><br><span class="line">ssh-keygen </span><br><span class="line">ls -l .ssh/</span><br><span class="line">ssh-copy-id root@cephnode02</span><br><span class="line">ssh-copy-id root@cephnode03</span><br><span class="line">ssh root@cephnode03</span><br><span class="line">ssh root@cephnode02</span><br><span class="line"></span><br><span class="line">(10)read_ahead,通过数据预读并且记载到随机访问内存方式提高磁盘读操作</span><br><span class="line">echo "8192" &gt; /sys/block/sda/queue/read_ahead_kb</span><br><span class="line"></span><br><span class="line">(11) I/O Scheduler，SSD要用noop，SATA/SAS使用deadline</span><br><span class="line">echo "deadline" &gt;/sys/block/sd[x]/queue/scheduler</span><br><span class="line">echo "noop" &gt;/sys/block/sd[x]/queue/scheduler</span><br></pre></td></tr></table></figure>
<h3 id="安装内网yum源"><a href="#安装内网yum源" class="headerlink" title="安装内网yum源"></a>安装内网yum源</h3><p><strong>这一步有问题 直接用下面的 阿里云源</strong></p>
<p><strong>1. 安装httpd、createrepo 和 epel源</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install httpd createrepo epel-release -y</span><br></pre></td></tr></table></figure>
<p><strong>2. 编辑yum源文件</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 ~]# vim /etc/yum.repos.d/ceph.repo </span><br><span class="line"></span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br></pre></td></tr></table></figure>
<p><strong>3. 下载Ceph 相关rpm包</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 ~]# yum --downloadonly --downloaddir=/var/www/html/ceph/rpm-nautilus/el7/x86_64/ install ceph ceph-radosgw</span><br></pre></td></tr></table></figure>
<p><strong>4. 下载Ceph依赖文件</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 自作源的时候 要下载这些依赖</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-14.2.4-0.el7.src.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-deploy-2.0.1-0.src.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-cloud-14.2.4-0.el7.noarch.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-local-14.2.4-0.el7.noarch.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-rook-14.2.4-0.el7.noarch.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-ssh-14.2.4-0.el7.noarch.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-release-1-1.el7.src.rpm </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-medic-1.0.4-16.g60cf7e9.el7.src.rpm</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/repomd.xml </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/repomd.xml</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite.bz2</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/183278bb826f5b8853656a306258643384a1547c497dd8b601ed6af73907bb22-other.sqlite.bz2 </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/52bf459e39c76b2ea2cff2c5340ac1d7b5e17a105270f5f01b454d5a058adbd2-filelists.sqlite.bz2</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/4f3141aec1132a9187ff5d1b4a017685e2f83a761880884d451a288fcedb154e-primary.sqlite.bz2</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/0c554884aa5600b1311cd8f616aa40d036c1dfc0922e36bcce7fd84e297c5357-other.sqlite.bz2 </span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/597468b64cddfc386937869f88c2930c8e5fda3dd54977c052bab068d7438fcb-primary.sqlite.bz2</span><br><span class="line"></span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/  http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/9a25e39d0c5038776cd0385513e275c6a4fd22c4a420a2097f03bac8d20fc2ab-primary.xml.gz</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/d65e1c530f969af5458c89e0933adbee9a5192ccab209cbeb8f0887c887aee75-primary.xml.gz</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/b98aa90d4606e473035f3cc0fa28dcb68cac506fea2ac1b62ffe5b4f9dcf6c73-filelists.xml.gz</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/eb9fad1d84b372046f07f437219700b5ae5872e65eae396f88b7eaf05da89646-other.xml.gz</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/9334c7e361a2e2ec768c1df34e386f15223abee6227b0fd4c5ff953bab4c3ba2-filelists.xml.gz</span><br><span class="line">wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/af3ec697842e36c456331b347577225f288124b407d87a173a9e08bf9a482164-other.xml.gz</span><br></pre></td></tr></table></figure>
<p><strong>更新yum源</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">createrepo --update  /var/www/html/ceph/rpm-nautilus</span><br><span class="line">[root@cephnode01 rpm-nautilus]# cp -a repodata el7/x86_64/</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 rpm-nautilus]# systemctl start httpd</span><br><span class="line">[root@cephnode01 rpm-nautilus]# netstat -tnlp|grep 80</span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      11639/httpd</span><br></pre></td></tr></table></figure>
<h3 id="使用阿里云yum源-安装Ceph集群"><a href="#使用阿里云yum源-安装Ceph集群" class="headerlink" title="使用阿里云yum源 安装Ceph集群"></a>使用阿里云yum源 安装Ceph集群</h3><p><strong>1. 编辑yum源</strong></p>
<ol>
<li>将yum源同步到其它节点并提前做好 yum makecache</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 三台都操作</span><br><span class="line">[root@cephnode01 html]# vim /etc/yum.repos.d/ceph.repo </span><br><span class="line"></span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>
<p><strong>2. 安装ceph-deploy(确认ceph-deploy版本是否为2.0.1)</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 rpm-nautilus]# yum list|grep ceph</span><br><span class="line">[root@cephnode01 rpm-nautilus]# yum install ceph-deploy -Y</span><br><span class="line"></span><br><span class="line">[root@cephnode01 rpm-nautilus]# ceph-deploy --version</span><br><span class="line">2.0.1</span><br></pre></td></tr></table></figure>
<p><strong>3. 创建一个my-cluster目录</strong></p>
<ol>
<li>所有命令在此目录下进行（文件位置和名字可以随意）</li>
<li>生产上按照项目启目录名字也可以，这个是测试用的</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /my-cluster</span><br><span class="line">cd /my-cluster</span><br></pre></td></tr></table></figure>
<p><strong>4. 创建一个Ceph集群</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph-deploy new cephnode01 cephnode02 cephnode03 </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor cephnode03 at 172.31.228.61</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor initial members are ['cephnode01', 'cephnode02', 'cephnode03']</span><br><span class="line">[ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.228.59', '172.31.228.60', '172.31.228.61']     # Monitor地址</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating a random mon key...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...</span><br><span class="line">[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...</span><br></pre></td></tr></table></figure>
<p><strong>5. 安装Ceph软件（每个节点执行）</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# yum -y install epel-release</span><br><span class="line">[root@cephnode01 my-cluster]# yum install -y ceph</span><br><span class="line">[root@cephnode01 my-cluster]# ceph -v</span><br><span class="line">ceph version 14.2.6 (f0aa067ac7a02ee46ea48aa26c6e298b5ea272e9) nautilus (stable)</span><br></pre></td></tr></table></figure>
<p><strong>6. 生成monitor检测集群所使用的的秘钥</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p><strong>7. 查看基本配置</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ls -l</span><br><span class="line">total 16</span><br><span class="line">-rw-r--r-- 1 root root  253 Jan 15 09:38 ceph.conf                      # 基础配置</span><br><span class="line">-rw-r--r-- 1 root root 5096 Jan 15 09:38 ceph-deploy-ceph.log           # 记录安装过程</span><br><span class="line">-rw------- 1 root root   73 Jan 15 09:38 ceph.mon.keyring</span><br><span class="line">-rw------- 1 root root   113 Jan 15 10:25 ceph.bootstrap-mds.keyring    # 秘钥 用于交互</span><br><span class="line">-rw------- 1 root root   113 Jan 15 10:25 ceph.bootstrap-mgr.keyring</span><br><span class="line">-rw------- 1 root root   113 Jan 15 10:25 ceph.bootstrap-osd.keyring</span><br><span class="line">-rw------- 1 root root   113 Jan 15 10:25 ceph.bootstrap-rgw.keyring</span><br><span class="line">-rw------- 1 root root   151 Jan 15 10:25 ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# cat ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = 4ed819cf-39be-4a7c-9216-effcae715c58                 # 集群编号 </span><br><span class="line">mon_initial_members = cephnode01, cephnode02, cephnode03</span><br><span class="line">mon_host = 172.31.228.59,172.31.228.60,172.31.228.61        # 集群成员</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure>
<p><strong>8. 安装Ceph CLI，方便执行一些管理命令</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph-deploy admin cephnode01 cephnode02 cephnode03</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 其他节点下会生成</span><br><span class="line">[root@cephnode02 ceph]# cd /etc/ceph/</span><br><span class="line">[root@cephnode02 ceph]# ls -l</span><br><span class="line">total 12</span><br><span class="line">-rw------- 1 root root 151 Jan 15 10:28 ceph.client.admin.keyring  # 管理员的秘钥</span><br><span class="line">-rw-r--r-- 1 root root 253 Jan 15 10:28 ceph.conf</span><br><span class="line">-rw-r--r-- 1 root root  92 Jan  9 03:44 rbdmap</span><br><span class="line">-rw------- 1 root root   0 Jan 15 10:25 tmpk0SexV</span><br></pre></td></tr></table></figure>
<p><strong>9. 配置mgr，用于管理集群</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph-deploy mgr create cephnode01 cephnode02 cephnode03</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl start ceph-mgr@cephnode03   # 启动并设置成开机启动</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl enable ceph.target</span><br></pre></td></tr></table></figure>
<p><strong>10. 部署rgw</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 生产上多机器安装 rgw 对象存储 用nginx做负载均衡代理</span><br><span class="line">[root@cephnode01 my-cluster]# yum install -y ceph-radosgw</span><br><span class="line"># 加到集群里</span><br><span class="line">[root@cephnode01 my-cluster]# ceph-deploy rgw create cephnode01</span><br></pre></td></tr></table></figure>
<p><strong>11. 部署MDS（CephFS）</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph-deploy mds create cephnode01 cephnode02 cephnode03 </span><br><span class="line">...</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl start ceph-mds@cephnode03   # 开机自启动</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl enable ceph.target</span><br></pre></td></tr></table></figure>
<p><strong>12. 阿里云购买云盘</strong></p>
<ol>
<li>为每台实例购买云盘,按照分区购买后,挂载到实例上 </li>
<li>高效云盘,20G,可用区C</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# lsblk </span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  40G  0 disk </span><br><span class="line">└─vda1 253:1    0  40G  0 part /</span><br><span class="line">vdb    253:16   0  20G  0 disk 裸盘</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_10.png" width="50%"></p>
<p><strong>13. 添加osd</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 该命令可以将 裸盘 vdb 自动格式化成ceph BlueStore 认识的格式 </span><br><span class="line">[root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode01</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[cephnode01][WARNIN] --&gt; ceph-volume lvm activate successful for osd ID: 0          # osd编号0,之前的版本都要手动执行</span><br><span class="line">[cephnode01][WARNIN] --&gt; ceph-volume lvm create successful for: /dev/vdb</span><br><span class="line">[cephnode01][INFO  ] checking OSD status...</span><br><span class="line">[cephnode01][DEBUG ] find the location of an executable</span><br><span class="line">[cephnode01][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephnode01 is now ready for osd use.</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF </span><br><span class="line">-1       0.01859 root default                                </span><br><span class="line">-3       0.01859     host cephnode01                         </span><br><span class="line"> 0   hdd 0.01859         osd.0           up  1.00000 1.00000      # 加入,标识hdd sata盘</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 查看集群状态</span><br><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">    health: HEALTH_WARN     # 当前是 WARN状态 pg太少了</span><br><span class="line">            Reduced data availability: 8 pgs inactive</span><br><span class="line">            Degraded data redundancy: 8 pgs undersized</span><br><span class="line">            OSD count 1 <span class="tag">&lt; <span class="attr">osd_pool_default_size</span> <span class="attr">3</span></span></span><br><span class="line">            too few PGs per OSD (8 &lt; min 30)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 34m)</span><br><span class="line">    mgr: cephnode01(active, since 29m), standbys: cephnode03, cephnode02</span><br><span class="line">    osd: 1 osds: 1 up (since 2m), 1 in (since 2m)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 8 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   1.0 GiB used, 18 GiB / 19 GiB avail</span><br><span class="line">    pgs:     100.000% pgs not active</span><br><span class="line">             8 undersized+peered</span><br></pre></td></tr></table></figure>
<p><strong>把其他实例上的盘也加入到集群里</strong></p>
<p><img src="/img/ceph/ceph_11.png" width="50%"></p>
<p><img src="/img/ceph/ceph_12.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 别忘记购买啊...</span><br><span class="line">[root@cephnode02 ceph]# lsblk </span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  40G  0 disk </span><br><span class="line">└─vda1 253:1    0  40G  0 part /</span><br><span class="line">vdb    253:16   0  20G  0 disk </span><br><span class="line"></span><br><span class="line">[root@cephnode03 ~]# lsblk </span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  40G  0 disk </span><br><span class="line">└─vda1 253:1    0  40G  0 part /</span><br><span class="line">vdb    253:16   0  20G  0 disk </span><br><span class="line"></span><br><span class="line"># 添加osd 加入集群</span><br><span class="line">[root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode02</span><br><span class="line">[root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode03</span><br><span class="line"></span><br><span class="line">[cephnode02][WARNIN] --&gt; ceph-volume lvm activate successful for osd ID: 1</span><br><span class="line">[cephnode02][WARNIN] --&gt; ceph-volume lvm create successful for: /dev/vdb</span><br><span class="line">[cephnode02][INFO  ] checking OSD status...</span><br><span class="line">[cephnode02][DEBUG ] find the location of an executable</span><br><span class="line">[cephnode02][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host cephnode02 is now ready for osd use.</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 三块盘 每台实例加入一块</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF </span><br><span class="line">-1       0.05576 root default                                </span><br><span class="line">-3       0.01859     host cephnode01                         </span><br><span class="line"> 0   hdd 0.01859         osd.0           up  1.00000 1.00000 </span><br><span class="line">-5       0.01859     host cephnode02                         </span><br><span class="line"> 1   hdd 0.01859         osd.1           up  1.00000 1.00000 </span><br><span class="line">-7       0.01859     host cephnode03                         </span><br><span class="line"> 2   hdd 0.01859         osd.2           up  1.00000 1.00000</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 在磁盘不平衡或者新加盘的时候 会出现 WARN状态</span><br><span class="line"># 等数据填满平衡时 会变成OK </span><br><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 42m)</span><br><span class="line">    mgr: cephnode01(active, since 37m), standbys: cephnode03, cephnode02</span><br><span class="line">    osd: 3 osds: 3 up (since 33s), 3 in (since 33s)</span><br><span class="line">    rgw: 1 daemon active (cephnode01)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1.2 KiB</span><br><span class="line">    usage:   3.0 GiB used, 54 GiB / 57 GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br><span class="line"> </span><br><span class="line">  io:</span><br><span class="line">    recovery: 67 B/s, 4 objects/s</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 有报错的话 会看到详细报错</span><br><span class="line">[root@cephnode01 my-cluster]# ceph health detail</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br><span class="line"># 查看实时日志</span><br><span class="line">[root@cephnode01 my-cluster]# ceph -w</span><br><span class="line"></span><br><span class="line"># 查看每块盘使用的空间</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd df</span><br><span class="line">ID CLASS WEIGHT  REWEIGHT SIZE   RAW USE DATA    OMAP META  AVAIL  %USE VAR  PGS STATUS </span><br><span class="line"> 0   hdd 0.01859  1.00000 19 GiB 1.0 GiB 3.6 MiB  0 B 1 GiB 18 GiB 5.28 1.00  32     up </span><br><span class="line"> 1   hdd 0.01859  1.00000 19 GiB 1.0 GiB 3.6 MiB  0 B 1 GiB 18 GiB 5.28 1.00  32     up </span><br><span class="line"> 2   hdd 0.01859  1.00000 19 GiB 1.0 GiB 3.6 MiB  0 B 1 GiB 18 GiB 5.28 1.00  32     up </span><br><span class="line">                    TOTAL 57 GiB 3.0 GiB  11 MiB  0 B 3 GiB 54 GiB 5.28            </span><br><span class="line"></span><br><span class="line"># AVAIL 可用 里面有元数据信息</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 查看pool</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd lspools</span><br><span class="line">1 .rgw.root</span><br><span class="line">2 default.rgw.control</span><br><span class="line">3 default.rgw.meta</span><br><span class="line">4 default.rgw.log</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 查看pg,pg是逻辑概念,磁盘规置</span><br><span class="line">[root@cephnode01 my-cluster]# ceph pg dump</span><br></pre></td></tr></table></figure>
<p><strong>添加硬盘业务无感知</strong></p>
<ol>
<li>如果新加盘需要清空再键入</li>
<li>最好直接加入裸盘</li>
</ol>
<h3 id="ceph-conf"><a href="#ceph-conf" class="headerlink" title="ceph.conf"></a>ceph.conf</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 原始配置文件 </span><br><span class="line"># 修改配置也在这个文件中</span><br><span class="line"># 完成修改后 通过 ceph-deploy 推送</span><br><span class="line">[root@cephnode01 my-cluster]# cat /my-cluster/ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = 4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">mon_initial_members = cephnode01, cephnode02, cephnode03</span><br><span class="line">mon_host = 172.31.228.59,172.31.228.60,172.31.228.61</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure>
<p><strong>书写格式</strong></p>
<ol>
<li>该配置文件采用init文件语法，#和;为注释，ceph集群在启动的时候会按照顺序加载所有的conf配置文件。 配置文件分为以下几大块配置。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">global： 全局配置。</span><br><span class="line">osd：    osd专用配置，可以使用osd.N，来表示某一个OSD专用配置，N为osd的编号，如0、2、1等。</span><br><span class="line">mon：    mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、ceph-monitor-1等。使用命令 ceph mon dump 可以获取节点的名称。</span><br><span class="line">client： 客户端专用配置。</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mon dump</span><br><span class="line">dumped monmap epoch 1</span><br><span class="line">epoch 1</span><br><span class="line">fsid 4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">last_changed 2020-01-15 10:25:08.611321</span><br><span class="line">created 2020-01-15 10:25:08.611321</span><br><span class="line">min_mon_release 14 (nautilus)</span><br><span class="line">0: [v2:172.31.228.59:3300/0,v1:172.31.228.59:6789/0] mon.cephnode01</span><br><span class="line">1: [v2:172.31.228.60:3300/0,v1:172.31.228.60:6789/0] mon.cephnode02</span><br><span class="line">2: [v2:172.31.228.61:3300/0,v1:172.31.228.61:6789/0] mon.cephnode03</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mgr dump</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd dump</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>配置文件可以从多个地方进行顺序加载，如果冲突将使用最新加载的配置，其加载顺序为。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 不需要改动</span><br><span class="line">$CEPH_CONF环境变量</span><br><span class="line">-c 指定的位置</span><br><span class="line">/etc/ceph/ceph.conf</span><br><span class="line">~/.ceph/ceph.conf</span><br><span class="line">./ceph.conf</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>配置文件还可以使用一些元变量应用到配置文件，如</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$cluster：   当前集群名。</span><br><span class="line">$type：      当前服务类型。</span><br><span class="line">$id：        进程的标识符。</span><br><span class="line">$host：      守护进程所在的主机名。</span><br><span class="line">$name：      值为$type.$id。</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>ceph.conf 详细参数</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"># 全局设置 默认接口 不用调整太多</span><br><span class="line">[global]</span><br><span class="line">fsid = xxxxxxxxxxxxxxx                           # 集群标识ID </span><br><span class="line">mon host = 10.0.1.1,10.0.1.2,10.0.1.3            # monitor IP 地址</span><br><span class="line">auth cluster required = cephx                    # 集群认证</span><br><span class="line">auth service required = cephx                    # 服务认证</span><br><span class="line">auth client required = cephx                     # 客户端认证</span><br><span class="line">osd pool default size = 3                        # 最小副本数 默认是3</span><br><span class="line">osd pool default min size = 1                    # 当副本只剩下1个的时候,数据就无法使用 PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数</span><br><span class="line">public network = 10.0.1.0/24                     # 公共网络(monitorIP段) 用户使用的网络 万M网络 </span><br><span class="line">cluster network = 10.0.2.0/24                    # 集群网络 集群数据同步走的网络 万M网络 两个网段隔离用户网络,走的更快</span><br><span class="line">max open files = 131072                          # 默认0#如果设置了该选项，Ceph会设置系统的max open fds</span><br><span class="line">mon initial members = node1, node2, node3        # 初始monitor (由创建monitor命令而定)</span><br><span class="line"></span><br><span class="line">##############################################################</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon data = /var/lib/ceph/mon/ceph-$id</span><br><span class="line">mon clock drift allowed = 1                      # 默认值0.05 #monitor间的clock drift</span><br><span class="line">mon osd min down reporters = 13                  # 默认值1 向monitor报告down的最小OSD数 超过该数 数据无法使用</span><br><span class="line">mon osd down out interval = 600                  # 默认值300 标记一个OSD状态为down和out之前ceph等待的秒数  自动踢出集群</span><br><span class="line">##############################################################</span><br><span class="line"></span><br><span class="line">[osd]</span><br><span class="line">osd data = /var/lib/ceph/osd/ceph-$id</span><br><span class="line">osd mkfs type = xfs                                 # 格式化系统类型 默认即可</span><br><span class="line">osd max write size = 512                            # 默认值90 #OSD一次可写入的最大值(MB)</span><br><span class="line">osd client message size cap = 2147483648            # 默认值100 #客户端允许在内存中的最大数据(bytes)</span><br><span class="line">osd deep scrub stride = 131072                      # 默认值524288 健康检查 扫描数据一致 在Deep Scrub时候允许读取的字节数(bytes)</span><br><span class="line">osd op threads = 16                                 # 默认值2 并发文件系统操作数</span><br><span class="line">osd disk threads = 4                                # 默认值1 #OSD密集型操作例如恢复和Scrubbing时的线程</span><br><span class="line">osd map cache size = 1024                           # 默认值500 # 保留OSD Map的缓存(MB)</span><br><span class="line">osd map cache bl size = 128                         # 默认值50 # OSD进程在内存中的OSD Map缓存(MB)</span><br><span class="line">osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier"   # 默认值rw,noatime,inode64  #Ceph OSD xfs Mount选项</span><br><span class="line">osd recovery op priority = 2                        # 默认值10 恢复操作优先级，取值1-63，值越高占用资源越高</span><br><span class="line">osd recovery max active = 10                        # 默认值15 同一时间内活跃的恢复请求数 </span><br><span class="line">osd max backfills = 4                               # 默认值10 一个OSD允许的最大backfills数</span><br><span class="line">osd min pg log entries = 30000                      # 默认值3000   修建PGLog是保留的最小PGLog数</span><br><span class="line">osd max pg log entries = 100000                     # 默认值10000  修建PGLog是保留的最大PGLog数</span><br><span class="line">osd mon heartbeat interval = 40                     # 默认值30 OSD ping一个monitor的时间间隔（默认30s）</span><br><span class="line">ms dispatch throttle bytes = 1048576000             # 默认值 104857600 #等待派遣的最大消息数</span><br><span class="line">objecter inflight ops = 819200                      # 默认值1024 客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限</span><br><span class="line">osd op log threshold = 50                           # 默认值5 一次显示多少操作的log</span><br><span class="line">osd crush chooseleaf type = 0                       # 默认值为1 CRUSH规则用到chooseleaf时的bucket的类型</span><br><span class="line">##############################################################</span><br><span class="line">[client]</span><br><span class="line">rbd cache = true                            # 默认值 true RBD缓存</span><br><span class="line">rbd cache size = 335544320                  # 默认值33554432 RBD缓存大小(bytes)</span><br><span class="line">rbd cache max dirty = 134217728             # 默认值25165824 缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</span><br><span class="line">rbd cache max dirty age = 30                # 默认值1 在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</span><br><span class="line">rbd cache writethrough until flush = false  # 默认值true 该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写</span><br><span class="line">                                            # 设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。</span><br><span class="line">rbd cache max dirty object = 2              # 默认值0 最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分</span><br><span class="line">                                            # 每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能</span><br><span class="line">rbd cache target dirty = 235544320          # 默认值16777216 开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-RBD"><a href="#Ceph-RBD" class="headerlink" title="Ceph RBD"></a>Ceph RBD</h2><h3 id="RBD-介绍"><a href="#RBD-介绍" class="headerlink" title="RBD 介绍"></a>RBD 介绍</h3><ol>
<li>RBD即RADOS Block Device的简称，RBD块存储是最稳定且最常用的存储类型。</li>
<li>RBD块设备类似磁盘可以被挂载。 </li>
<li>RBD块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。</li>
<li>如下是对Ceph RBD的理解。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. RBD:                 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；</span><br><span class="line">2. resizable:           这个块可大可小；</span><br><span class="line">3. data striped:        这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；</span><br><span class="line">4. thin-provisioned:    精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。</span><br><span class="line">举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间  ；</span><br><span class="line"></span><br><span class="line"># 总结</span><br><span class="line">1. 快设备就好似一块盘,拥有快照备份的功能。</span><br><span class="line">2. 这个块可以调整</span><br><span class="line">3. 创建出来的块是不占用空间的，用多少使用占用多少</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. ceph可以通过内核模块和librbd库提供块设备支持。</span><br><span class="line">2. 客户端可以通过内核模块挂在rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；</span><br><span class="line">3. 客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>使用场景：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 云平台（OpenStack做为云的存储后端提供镜像存储）</span><br><span class="line">2. K8s容器</span><br><span class="line">3. map成块设备直接使用</span><br><span class="line">4. ISCIS，安装Ceph客户</span><br></pre></td></tr></table></figure>
<h3 id="RBD-常用命令"><a href="#RBD-常用命令" class="headerlink" title="RBD 常用命令"></a>RBD 常用命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>rbd create</td>
<td>创建块设备映像</td>
</tr>
<tr>
<td>rbd ls</td>
<td>列出 rbd 存储池中的块设备</td>
</tr>
<tr>
<td>rbd info</td>
<td>查看块设备信息</td>
</tr>
<tr>
<td>rbd diff</td>
<td>可以统计 rbd 使用量</td>
</tr>
<tr>
<td>rbd map</td>
<td>映射块设备</td>
</tr>
<tr>
<td>rbd showmapped</td>
<td>查看已映射块设备</td>
</tr>
<tr>
<td>rbd remove</td>
<td>删除块设备</td>
</tr>
<tr>
<td>rbd resize</td>
<td>更改块设备的大小</td>
</tr>
</tbody>
</table>
<h3 id="RBD-配置操作"><a href="#RBD-配置操作" class="headerlink" title="RBD 配置操作"></a>RBD 配置操作</h3><h4 id="RBD-挂载到操作系统"><a href="#RBD-挂载到操作系统" class="headerlink" title="RBD 挂载到操作系统"></a>RBD 挂载到操作系统</h4><p><strong>创建rbd使用的pool</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 32 pg_num 32 pgp_num 随着容量在增加扩容 生产要做规划</span><br><span class="line"># pgp_num 描述pg位置</span><br><span class="line"># osd 与 pg 算法 ? </span><br><span class="line">ceph osd pool create rbd  32 32</span><br><span class="line"># 创建标记 rbd</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create rbd  32 32</span><br><span class="line">pool 'rbd' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool ls detail</span><br><span class="line">...</span><br><span class="line">pool 5 'rbd' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 29 flags hashpspool stripe_width 0</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool application enable rbd rbd </span><br><span class="line">enabled application 'rbd' on pool 'rbd'</span><br></pre></td></tr></table></figure>
<p><strong>创建一个块设备</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd create --size 10240 image01</span><br></pre></td></tr></table></figure>
<p><strong>查看快设备</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd ls</span><br><span class="line">image01</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd info image01</span><br><span class="line">rbd image 'image01':</span><br><span class="line">	size 10 GiB in 2560 objects              # 大小 放多少个对象</span><br><span class="line">	order 22 (4 MiB objects)</span><br><span class="line">	snapshot_count: 0</span><br><span class="line">	id: 11797bd74618</span><br><span class="line">	block_name_prefix: rbd_data.11797bd74618</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	op_features: </span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line">	access_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line">	modify_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rados -p rbd ls --all</span><br><span class="line">	rbd_id.image01</span><br><span class="line">	rbd_directory</span><br><span class="line">	rbd_info</span><br><span class="line">	rbd_object_map.11797bd74618</span><br><span class="line">	rbd_header.11797bd74618</span><br></pre></td></tr></table></figure>
<p><strong>禁用当前系统内核不支持的feature</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd feature disable image01  exclusive-lock, object-map, fast-diff, deep-flatten</span><br></pre></td></tr></table></figure>
<p><strong>将块设备映射到系统内核</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd map image01 </span><br><span class="line">/dev/rbd0</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ls -l /dev/rbd0 </span><br><span class="line">brw-rw---- 1 root disk 251, 0 Jan 15 15:26 /dev/rbd0</span><br></pre></td></tr></table></figure>
<p><strong>格式化块设备镜像</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# mkfs.xfs /dev/rbd0</span><br></pre></td></tr></table></figure>
<p><strong>mount到本地</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# mount /dev/rbd0 /mnt</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# umount /mnt</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G  2.3G   36G   6% /</span><br><span class="line">/dev/rbd0        10G   33M   10G   1% /mnt</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 在/mnt下写数据相当于存放到 这个rbd里了</span><br><span class="line">[root@cephnode01 my-cluster]# cd /mnt/</span><br><span class="line">[root@cephnode01 mnt]# touch a</span><br><span class="line">[root@cephnode01 mnt]# ls -l</span><br><span class="line">-rw-r--r-- 1 root root 0 Jan 15 15:30 a</span><br></pre></td></tr></table></figure>
<p><strong>取消挂载</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 mnt]# rbd showmapped</span><br><span class="line">id pool namespace image   snap device    </span><br><span class="line">0  rbd            image01 -    /dev/rbd0 </span><br><span class="line"></span><br><span class="line">[root@cephnode01 /]# umount /mnt</span><br><span class="line"></span><br><span class="line"># 取消块设备和内核映射</span><br><span class="line">[root@cephnode01 /]# rbd unmap image01 </span><br><span class="line">[root@cephnode01 /]# rbd showmapped</span><br><span class="line">[root@cephnode01 /]# rbd ls</span><br><span class="line">image01</span><br><span class="line"></span><br><span class="line">[root@cephnode01 /]# rbd info image01</span><br><span class="line">rbd image 'image01':</span><br><span class="line">	size 10 GiB in 2560 objects</span><br><span class="line">	order 22 (4 MiB objects)</span><br><span class="line">	snapshot_count: 0</span><br><span class="line">	id: 11797bd74618</span><br><span class="line">	block_name_prefix: rbd_data.11797bd74618</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering</span><br><span class="line">	op_features: </span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line">	access_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line">	modify_timestamp: Wed Jan 15 15:20:46 2020</span><br><span class="line"></span><br><span class="line"># 虽然取消了挂载,但是里面的数据还在</span><br></pre></td></tr></table></figure>
<p><strong>删除RBD块设备</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 /]# rbd rm image01</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line">[root@cephnode01 /]# rbd ls</span><br></pre></td></tr></table></figure>
<p><strong>其他机器需要创建</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph common k8s会讲到</span><br><span class="line">创建快需要调用rbd命令 ， 需要再机器上安装ceph common 才可以创建块</span><br></pre></td></tr></table></figure>
<h3 id="RBD-快照配置"><a href="#RBD-快照配置" class="headerlink" title="RBD 快照配置"></a>RBD 快照配置</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd create --size 10240 image02</span><br><span class="line">[root@cephnode01 my-cluster]# rbd ls</span><br><span class="line">image02</span><br></pre></td></tr></table></figure>
<p><strong>创建快照</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd snap create image02@image02_snap01</span><br></pre></td></tr></table></figure>
<p><strong>列出创建的快照</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd snap list image02</span><br><span class="line">SNAPID NAME           SIZE   PROTECTED TIMESTAMP                </span><br><span class="line">     4 image02_snap01 10 GiB           Wed Jan 15 15:58:41 2020 </span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd ls -l</span><br><span class="line">NAME                   SIZE   PARENT FMT PROT LOCK </span><br><span class="line">image02                10 GiB          2           </span><br><span class="line">image02@image02_snap01 10 GiB          2           # 快照 用于很重要的数据 可以用于回滚</span><br></pre></td></tr></table></figure>
<p><strong>查看快照详细信息</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd info image02@image02_snap01</span><br><span class="line">rbd image 'image02':</span><br><span class="line">	size 10 GiB in 2560 objects</span><br><span class="line">	order 22 (4 MiB objects)</span><br><span class="line">	snapshot_count: 1</span><br><span class="line">	id: 11b2d62f36b7</span><br><span class="line">	block_name_prefix: rbd_data.11b2d62f36b7</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	op_features: </span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Wed Jan 15 15:57:55 2020</span><br><span class="line">	access_timestamp: Wed Jan 15 15:57:55 2020</span><br><span class="line">	modify_timestamp: Wed Jan 15 15:57:55 2020</span><br><span class="line">	protected: False</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd info image02</span><br><span class="line">rbd image 'image02':</span><br><span class="line">	size 10 GiB in 2560 objects</span><br><span class="line">	order 22 (4 MiB objects)</span><br><span class="line">	snapshot_count: 1</span><br><span class="line">	id: 11b2d62f36b7</span><br><span class="line">	block_name_prefix: rbd_data.11b2d62f36b7</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	op_features: </span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Wed Jan 15 15:57:55 2020</span><br><span class="line">	access_timestamp: Wed Jan 15 15:57:55 2020</span><br><span class="line">	modify_timestamp: Wed Jan 15 15:57:55 2020</span><br></pre></td></tr></table></figure>
<p><strong>克隆快照（快照必须处于被保护状态才能被克隆）</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 保护状态就无法操作了</span><br><span class="line">[root@cephnode01 my-cluster]# rbd snap protect image02@image02_snap01</span><br><span class="line"></span><br><span class="line"># 创建一个新pool kube</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create kube 16 16</span><br><span class="line">pool 'kube' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd clone rbd/image02@image02_snap01 kube/image02_clone01</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd ls -p kube</span><br><span class="line">image02_clone01</span><br></pre></td></tr></table></figure>
<p><strong>查看快照的children</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 查看快照的子快照</span><br><span class="line">[root@cephnode01 my-cluster]# rbd children image02</span><br><span class="line">kube/image02_clone01</span><br></pre></td></tr></table></figure>
<p><strong>去掉快照的parent</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 取消克隆关系</span><br><span class="line">[root@cephnode01 my-cluster]# rbd flatten kube/image02_clone01</span><br><span class="line">Image flatten: 100% complete...done.</span><br><span class="line">[root@cephnode01 my-cluster]# rbd children image02</span><br><span class="line"># 这就成为了 独立的快照</span><br></pre></td></tr></table></figure>
<p><strong>恢复快照</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 回滚 一开始有5个文件 一直写 想回滚</span><br><span class="line">[root@cephnode01 my-cluster]# rbd snap rollback image02@image02_snap01</span><br><span class="line">Rolling back to snapshot: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p><strong>删除快照</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 取消保护</span><br><span class="line">[root@cephnode01 my-cluster]# rbd snap unprotect image02@image02_snap01</span><br><span class="line">[root@cephnode01 my-cluster]# rbd snap remove image02@image02_snap01</span><br><span class="line">Removing snap: 100% complete...done.</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# rbd snap list image02</span><br></pre></td></tr></table></figure>
<h3 id="RBD-镜像导出导入"><a href="#RBD-镜像导出导入" class="headerlink" title="RBD 镜像导出导入"></a>RBD 镜像导出导入</h3><p><strong>导出RBD镜像</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# rbd export image02 /tmp/image02</span><br><span class="line">Exporting image: 100% complete...done.</span><br><span class="line"></span><br><span class="line">[root@cephnode01 tmp]# ls -lh /tmp/image02 </span><br><span class="line">-rw-r--r-- 1 root root 10G Jan 15 16:11 /tmp/image02</span><br><span class="line"></span><br><span class="line">[root@cephnode01 tmp]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G  2.3G   36G   6% /</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><strong>导入RBD镜像</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 tmp]# rbd ls</span><br><span class="line">image02</span><br><span class="line">[root@cephnode01 tmp]# rbd remove image02</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line">[root@cephnode01 tmp]# rbd ls</span><br><span class="line"></span><br><span class="line">[root@cephnode01 tmp]# rbd import /tmp/image02 rbd/image02 --image-format 2 </span><br><span class="line">Importing image: 100% complete...done.</span><br><span class="line">[root@cephnode01 tmp]# rbd ls</span><br><span class="line">image02</span><br></pre></td></tr></table></figure>
<h3 id="RBD-扩容"><a href="#RBD-扩容" class="headerlink" title="RBD 扩容"></a>RBD 扩容</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 tmp]# rbd info image02|grep size</span><br><span class="line">	size 10 GiB in 2560 objects</span><br><span class="line"></span><br><span class="line">[root@cephnode01 tmp]# rbd --image image02 resize --size 15240</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line">[root@cephnode01 tmp]# rbd info image02|grep size</span><br><span class="line">	size 15 GiB in 3810 objects</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 把rbd映射到文件系统挂载</span><br><span class="line">2. 再通过nfs /etc/exports 暴露,映射共享到其他机器上</span><br><span class="line">3. 快照 备份回滚，快照占用空间,可以定期拍摄快照，对重要的块进行快照,如果太大了 就导出备份</span><br><span class="line">4. 克隆 到另外的集群 </span><br><span class="line">5. 数据分成3份 分到3个pg上 再分到3个osd上</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-文件系统-CephFS"><a href="#Ceph-文件系统-CephFS" class="headerlink" title="Ceph 文件系统 CephFS"></a>Ceph 文件系统 CephFS</h2><ol>
<li>Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. </li>
<li>Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本.</li>
<li>CephFS 需要至少一个元数据服务器 (Metadata Server - MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。  </li>
<li>对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。</li>
</ol>
<h3 id="CephFS-架构"><a href="#CephFS-架构" class="headerlink" title="CephFS 架构"></a>CephFS 架构</h3><ol>
<li>底层是核心集群所依赖的, 包括:</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. OSDs (ceph-osd):     CephFS 的数据和元数据就存储在 OSDs 上</span><br><span class="line">2. MDS (ceph-mds):      Metadata Servers, 管理着 CephFS 的元数据</span><br><span class="line">3. Mons (ceph-mon):     Monitors 管理着集群 Map 的主副本</span><br><span class="line">Ceph 存储集群的协议层是 Ceph 原生的 librados 库, 与核心集群交互.</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_13.png" width="50%"></p>
<ol start="2">
<li>数据访问流程</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. client 访问MDS 获取数据的元数据(文件名,大小等信息) </span><br><span class="line">2. client 去 RADOS(OSD) 获取数据</span><br><span class="line">3. RADOS(OSD) 和 MDS 通过 Journal Metadata 记录文件写入的操作，数据最终都存入RADOS中。</span><br></pre></td></tr></table></figure>
<h3 id="配置-CephFS-MDS"><a href="#配置-CephFS-MDS" class="headerlink" title="配置 CephFS MDS"></a>配置 CephFS MDS</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 当前集群状态</span><br><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 15m)</span><br><span class="line">    mgr: cephnode02(active, since 15m), standbys: cephnode03, cephnode01</span><br><span class="line">    osd: 3 osds: 3 up (since 15m), 3 in (since 21h)</span><br><span class="line">    rgw: 1 daemon active (cephnode01)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   6 pools, 80 pgs</span><br><span class="line">    objects: 196 objects, 1.9 KiB</span><br><span class="line">    usage:   3.0 GiB used, 54 GiB / 57 GiB avail</span><br><span class="line">    pgs:     80 active+clean</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1. 要使用 CephFS， 至少就需要一个 metadata server 进程。</span><br><span class="line">2. 可以手动创建一个 MDS， 也可以使用 ceph-deploy 或者 ceph-ansible 来部署 MDS。 </span><br><span class="line">3. 登录到ceph-deploy工作目录执行</span><br><span class="line">[root@cephnode01 my-cluster]# cd /my-cluster/</span><br><span class="line">[root@cephnode01 my-cluster]# ceph-deploy mds create cephnode01 cephnode02 cephnode03</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl enable ceph-mds@cephnode03</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl start ceph-mds@cephnode03</span><br><span class="line">[cephnode03][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ps -ef|grep mds</span><br><span class="line">ceph         957       1  0 08:34 ?        00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id cephnode01 --setuser ceph --setgroup ceph</span><br><span class="line">root        2640    2102  0 08:56 pts/0    00:00:00 grep --color=auto mds</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# tail -200 /var/log/ceph/ceph-mds.cephnode01.log</span><br></pre></td></tr></table></figure>
<h3 id="部署-Ceph-文件系统"><a href="#部署-Ceph-文件系统" class="headerlink" title="部署 Ceph 文件系统"></a>部署 Ceph 文件系统</h3><ol>
<li>部署一个 CephFS, 步骤如下:</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 在一个 Mon 节点上创建 Ceph 文件系统.</span><br><span class="line">2. 若使用 CephX 认证,需要创建一个访问 CephFS 的客户端</span><br><span class="line">3. 挂载 CephFS 到一个专用的节点.</span><br><span class="line">   - 以 kernel client 形式挂载 CephFS</span><br><span class="line">   - 以 FUSE client 形式挂载 CephFS</span><br></pre></td></tr></table></figure>
<h3 id="创建一个-Ceph-文件系统"><a href="#创建一个-Ceph-文件系统" class="headerlink" title="创建一个 Ceph 文件系统"></a>创建一个 Ceph 文件系统</h3><p><strong>1. CephFS 需要两个 Pools</strong> </p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cephfs-data 和 cephfs-metadata, 分别存储文件数据和文件元数据 </span><br><span class="line"># ceph osd pool create cephfs-data 256 256</span><br><span class="line"># ceph osd pool create cephfs-metadata 64 64</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create cephfs-data 16 16</span><br><span class="line">pool 'cephfs-data' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create cephfs-metadata 16 16 </span><br><span class="line">pool 'cephfs-metadata' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd lspools</span><br><span class="line">1 .rgw.root</span><br><span class="line">2 default.rgw.control</span><br><span class="line">3 default.rgw.meta</span><br><span class="line">4 default.rgw.log</span><br><span class="line">5 rbd</span><br><span class="line">6 kube</span><br><span class="line">7 cephfs-data</span><br><span class="line">8 cephfs-metadata</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 注意</span><br><span class="line">1. 一般 metadata pool 可以从相对较少的 PGs 启动, </span><br><span class="line">2. 之后可以根据需要增加 PGs. 因为 metadata pool 存储着 CephFS 文件的元数据, 为了保证安全, 最好有较多的副本数. 为了能有较低的延迟, 可以考虑将 metadata 存储在 SSDs 上.</span><br></pre></td></tr></table></figure>
<p><strong>2. 创建一个 CephFS, 名字为 cephfs</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph fs new cephfs cephfs-metadata cephfs-data</span><br><span class="line">new fs with metadata pool 8 and data pool 7</span><br></pre></td></tr></table></figure>
<p><strong>3. 验证至少有一个 MDS 已经进入 Active 状态</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph fs status cephfs</span><br><span class="line"></span><br><span class="line"># 现在源数据的进程跑在 cephnode03上</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_14.png" width="50%"></p>
<p><strong>4. 在 Monitor 上, 创建一个用户，用于访问CephFs</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs-data, allow rw pool=cephfs-metadata'</span><br><span class="line">[client.cephfs]</span><br><span class="line">	key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br></pre></td></tr></table></figure>
<p><strong>5. 验证key是否生效</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph auth get client.cephfs</span><br><span class="line">exported keyring for client.cephfs</span><br><span class="line">[client.cephfs]</span><br><span class="line">	key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br><span class="line">	caps mds = "allow rw"          # 读写</span><br><span class="line">	caps mon = "allow r"           # 读</span><br><span class="line">	caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"  # 读写</span><br></pre></td></tr></table></figure>
<p><strong>6. 检查CephFs和mds状态</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph -s</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mds stat</span><br><span class="line">cephfs:1 &#123;0=cephnode03=up:active&#125; 2 up:standby</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs ls</span><br><span class="line">name: cephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]</span><br><span class="line"># 文件系统名称 cephfs </span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs status</span><br><span class="line"># 以上正常说明 cephfs 文件系统创建成功</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_15.png" width="50%"></p>
<h3 id="挂载-CephFS-文件系统"><a href="#挂载-CephFS-文件系统" class="headerlink" title="挂载 CephFS 文件系统"></a>挂载 CephFS 文件系统</h3><h4 id="以-kernel-client-形式挂载-CephFS"><a href="#以-kernel-client-形式挂载-CephFS" class="headerlink" title="以 kernel client 形式挂载 CephFS"></a>以 kernel client 形式挂载 CephFS</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 找一台不在集群中的服务器测试挂载 </span><br><span class="line"># 联系系统内核挂载文件系统，和挂载文件一样</span><br><span class="line"># 缺点: 读写速度有限</span><br><span class="line">1. 创建挂载目录 cephfs</span><br><span class="line">[root@cephnode04 ~]# mkdir /cephfs</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># secret 就是刚生成的秘钥 key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br><span class="line">2. 挂载目录 </span><br><span class="line">mount -t ceph 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789:/ /cephfs/ -o name=cephfs,secret=AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_16.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 开机自动挂载</span><br><span class="line">3. 自动挂载</span><br><span class="line">echo "mon1:6789,mon2:6789,mon3:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,_netdev,noatime 0 0" | sudo tee -a /etc/fstab</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4. 验证是否挂载成功</span><br><span class="line">stat -f /cephfs</span><br></pre></td></tr></table></figure>
<h4 id="以-FUSE-client-形式挂载-CephFS"><a href="#以-FUSE-client-形式挂载-CephFS" class="headerlink" title="以 FUSE client 形式挂载 CephFS"></a>以 FUSE client 形式挂载 CephFS</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1. 安装ceph-common</span><br><span class="line"></span><br><span class="line">[root@cephnode04 cephfs]# vim /etc/yum.repos.d/ceph.repo </span><br><span class="line"></span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[root@cephnode04 cephfs]# yum install -y ceph-common</span><br><span class="line"># 安装完成后 客户端就会有ceph和rbd命令</span><br><span class="line">[root@cephnode04 cephfs]# ceph</span><br><span class="line">[root@cephnode04 cephfs]# rbd</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2. 安装ceph-fuse </span><br><span class="line"># 客户端工具，用ceph的方式挂载</span><br><span class="line">yum install -y ceph-fuse</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3. 将集群的ceph.conf拷贝到客户端</span><br><span class="line">[root@cephnode04 ceph]# scp root@172.31.228.59:/etc/ceph/ceph.conf /etc/ceph/</span><br><span class="line">[root@cephnode04 ceph]# scp root@172.31.228.59:/etc/ceph/ceph.client.admin.keyring /etc/ceph/</span><br><span class="line">[root@cephnode04 ceph]# chmod 644 /etc/ceph/ceph.conf</span><br><span class="line"></span><br><span class="line">[root@cephnode04 ceph]# ls</span><br><span class="line">ceph.conf  rbdmap</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">4. 使用 ceph-fuse 挂载 CephFS </span><br><span class="line"># 挂载之前把刚才的挂载umount </span><br><span class="line">[root@cephnode04 ~]# umount /cephfs/</span><br><span class="line"></span><br><span class="line"># 先去获取一下认证信息</span><br><span class="line">[root@cephnode01 my-cluster]# ceph auth get client.cephfs</span><br><span class="line">exported keyring for client.cephfs</span><br><span class="line">[client.cephfs]</span><br><span class="line">	key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br><span class="line">	caps mds = "allow rw"</span><br><span class="line">	caps mon = "allow r"</span><br><span class="line">	caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"</span><br><span class="line"></span><br><span class="line"># 写入到keyring 文件中 </span><br><span class="line">[root@cephnode04 ~]# vim /etc/ceph/ceph.client.cephfs.keyring</span><br><span class="line"></span><br><span class="line">[client.cephfs]</span><br><span class="line">        key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==</span><br><span class="line">        caps mds = "allow rw"</span><br><span class="line">        caps mon = "allow r"</span><br><span class="line">        caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 挂载</span><br><span class="line"># ceph-fuse 自己的文件系统格式 </span><br><span class="line">[root@cephnode04 ~]# ceph-fuse --keyring  /etc/ceph/ceph.client.cephfs.keyring  --name client.cephfs -m 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789  /cephfs/</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_17.png" width="50%"></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 验证 CephFS 已经成功挂载</span><br><span class="line"></span><br><span class="line">[root@cephnode04 ~]# stat -f /cephfs</span><br><span class="line">  File: "/cephfs"</span><br><span class="line">    ID: 0        Namelen: 255     Type: fuseblk</span><br><span class="line">Block size: 4194304    Fundamental block size: 4194304</span><br><span class="line">Blocks: Total: 4362       Free: 4362       Available: 4362</span><br><span class="line">Inodes: Total: 1          Free: 0</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 自动挂载 </span><br><span class="line">echo "none /cephfs fuse.ceph ceph.id=cephfs[,ceph.conf=/etc/ceph/ceph.conf],_netdev,defaults 0 0"| sudo tee -a /etc/fstab</span><br><span class="line">或</span><br><span class="line">echo "id=cephfs,conf=/etc/ceph/ceph.conf /mnt/ceph2  fuse.ceph _netdev,defaults 0 0"| sudo tee -a /etc/fstab</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 卸载 </span><br><span class="line">fusermount -u /cephfs</span><br></pre></td></tr></table></figure>
<h4 id="存储文件测试并查看"><a href="#存储文件测试并查看" class="headerlink" title="存储文件测试并查看"></a>存储文件测试并查看</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 在目录下写点东西</span><br><span class="line">[root@cephnode04 ~]# touch test</span><br><span class="line"></span><br><span class="line"># 看看存在pool里是什么样子</span><br><span class="line">[root@cephnode01 my-cluster]# rados lspools</span><br><span class="line"></span><br><span class="line"># 创建的是空文件 没有数据</span><br><span class="line">[root@cephnode01 my-cluster]# rados -p cephfs-data ls --all</span><br><span class="line"></span><br><span class="line"># 源文件数据</span><br><span class="line">[root@cephnode01 my-cluster]# rados -p cephfs-metadata ls --all</span><br><span class="line">	601.00000000</span><br><span class="line">	602.00000000</span><br><span class="line">	600.00000000</span><br><span class="line">	603.00000000</span><br><span class="line">	1.00000000.inode</span><br><span class="line">	200.00000000</span><br><span class="line">	200.00000001</span><br><span class="line">	606.00000000</span><br><span class="line">	607.00000000</span><br><span class="line">	mds0_openfiles.0</span><br><span class="line">	608.00000000</span><br><span class="line">	604.00000000</span><br><span class="line">	500.00000000</span><br><span class="line">	mds_snaptable</span><br><span class="line">	605.00000000</span><br><span class="line">	mds0_inotable</span><br><span class="line">	100.00000000</span><br><span class="line">	mds0_sessionmap</span><br><span class="line">	609.00000000</span><br><span class="line">	400.00000000</span><br><span class="line">	100.00000000.inode</span><br><span class="line">	1.00000000</span><br></pre></td></tr></table></figure>
<h3 id="挂载小总结"><a href="#挂载小总结" class="headerlink" title="挂载小总结"></a>挂载小总结</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 推荐ceph-fuse 效率更高</span><br><span class="line"># 实际应用中 其实也没什么区别 只不过ceph-fuse 是源生的可能更好</span><br><span class="line"># 生产上如果追求少安装软件的话 用内核模式也可以</span><br></pre></td></tr></table></figure>
<h3 id="MDS主备与主主切换"><a href="#MDS主备与主主切换" class="headerlink" title="MDS主备与主主切换"></a>MDS主备与主主切换</h3><h4 id="配置主主模式"><a href="#配置主主模式" class="headerlink" title="配置主主模式"></a>配置主主模式</h4><ol>
<li>当cephfs的性能出现问题时，就应该配置多个活动的MDS。</li>
<li>通常是多个客户机应用程序并行的执行大量元数据操作，并且它们分别有自己单独的工作目录。这种情况下很适合使用多主MDS模式。  </li>
<li>配置MDS多主模式  </li>
<li>每个cephfs文件系统都有一个max_mds设置，可以理解为它将控制创建多少个主MDS。注意只有当实际的MDS个数大于或等于max_mds设置的值时，mdx_mds设置才会生效。</li>
<li>例如，如果只有一个MDS守护进程在运行，并且max_mds被设置为两个，则不会创建第二个主MDS。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置完成后 存在两个主</span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs set cephfs  max_mds 2 </span><br><span class="line"></span><br><span class="line"># 还原单主MDS</span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs set cephfs  max_mds 1</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_18.png" width="50%"></p>
<h4 id="配置备用-MDS"><a href="#配置备用-MDS" class="headerlink" title="配置备用 MDS"></a>配置备用 MDS</h4><ol>
<li>即使有多个活动的MDS，如果其中一个MDS出现故障，仍然需要备用守护进程来接管。因此，对于高可用性系统，实际配置max_mds时，最好比系统中MDS的总数少一个。</li>
<li>但如果你确信你的MDS不会出现故障，可以通过以下设置来通知ceph不需要备用MDS，否则会出现insufficient standby daemons available告警信息：</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置不需要备用MDS</span><br><span class="line">ceph fs set <span class="tag">&lt;<span class="name">fsname</span>&gt;</span> standby_count_wanted 0</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-Dashboard"><a href="#Ceph-Dashboard" class="headerlink" title="Ceph Dashboard"></a>Ceph Dashboard</h2><ol>
<li>Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能</li>
<li>通过Dashboard可以获取Ceph集群的各种基本状态信息。</li>
<li>mimic版  (nautilus版)  dashboard 安装。如果是  (nautilus版) 需要安装 ceph-mgr-dashboard </li>
</ol>
<h3 id="配置-Ceph-Dashboard"><a href="#配置-Ceph-Dashboard" class="headerlink" title="配置 Ceph Dashboard"></a>配置 Ceph Dashboard</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. 在每个mgr节点安装</span><br><span class="line"># yum install ceph-mgr-dashboard </span><br><span class="line">2. 开启mgr功能</span><br><span class="line"># ceph mgr module enable dashboard</span><br><span class="line">3. 生成并安装自签名的证书</span><br><span class="line"># ceph dashboard create-self-signed-cert  </span><br><span class="line">4. 创建一个dashboard登录用户名密码</span><br><span class="line"># ceph dashboard ac-user-create guest 1q2w3e4r administrator </span><br><span class="line">5. 查看服务访问方式</span><br><span class="line"># ceph mgr services</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># 出现问题 ImportError: cannot import name UnrewindableBodyError</span><br><span class="line">[root@cephnode01 my-cluster]# tail -200 /var/log/ceph/ceph-mgr.cephnode01.log </span><br><span class="line"></span><br><span class="line"># 需要重新安装 urllib3</span><br><span class="line">pip uninstall urllib3</span><br><span class="line">yum install python-urllib3 -y</span><br><span class="line">yum remove ceph-mgr-dashboard</span><br><span class="line">yum install -y ceph-mgr-dashboard</span><br><span class="line"></span><br><span class="line"># 在所有的mgr上都要装这个包 ,有1个的话只装1个</span><br><span class="line">[root@cephnode01 my-cluster]# ceph mgr module enable dashboard</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mgr module ls </span><br><span class="line">&#123;</span><br><span class="line">    "enabled_modules": [</span><br><span class="line">        "dashboard",</span><br><span class="line">        "iostat",</span><br><span class="line">        "restful"</span><br><span class="line">    ],</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph dashboard create-self-signed-cert  </span><br><span class="line">Self-signed certificate created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph dashboard ac-user-create guest 1q2w3e4r administrator </span><br><span class="line">&#123;"username": "guest", "lastUpdate": 1579144411, "name": null, "roles": ["administrator"], "password": "$2b$12$ZwdxYdVcpKI7FCz2IZyKs.qR.fJrkGnxlNttZN8eoRbNHcqGdRZry", "email": null&#125;</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mgr services</span><br><span class="line">&#123;</span><br><span class="line">    "dashboard": "https://cephnode02:8443/"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># web访问</span><br><span class="line">https://47.240.15.208:8443</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_19.png" width="50%"></p>
<p><img src="/img/ceph/ceph_20.png" width="50%"></p>
<h3 id="修改默认配置命令"><a href="#修改默认配置命令" class="headerlink" title="修改默认配置命令"></a>修改默认配置命令</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">指定集群dashboard的访问端口</span><br><span class="line"># ceph config-key set mgr/dashboard/server_port 7000</span><br><span class="line"></span><br><span class="line">指定集群 dashboard的访问IP</span><br><span class="line"># ceph config-key set mgr/dashboard/server_addr $IP</span><br></pre></td></tr></table></figure>
<h3 id="开启Object-Gateway管理功能"><a href="#开启Object-Gateway管理功能" class="headerlink" title="开启Object Gateway管理功能"></a>开启Object Gateway管理功能</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 这块可能有问题 需要用到对象存储的时候 再网上查查</span><br><span class="line"># 可以参考官方给出的开启方式开启</span><br><span class="line"></span><br><span class="line">1、创建rgw用户</span><br><span class="line"># radosgw-admin user create --uid=user01 --display-name=user01</span><br><span class="line">2、提供Dashboard证书</span><br><span class="line"># ceph dashboard set-rgw-api-access-key $access_key</span><br><span class="line"># ceph dashboard set-rgw-api-secret-key $secret_key</span><br><span class="line">3、配置rgw主机名和端口</span><br><span class="line"># ceph dashboard set-rgw-api-host 10.151.30.125</span><br><span class="line">4、刷新web页面</span><br></pre></td></tr></table></figure>
<h2 id="Promethus-Grafana-监控-Ceph"><a href="#Promethus-Grafana-监控-Ceph" class="headerlink" title="Promethus+Grafana 监控 Ceph"></a>Promethus+Grafana 监控 Ceph</h2><h3 id="安装-grafana"><a href="#安装-grafana" class="headerlink" title="安装 grafana"></a>安装 grafana</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1、配置yum源文件</span><br><span class="line"># vim /etc/yum.repos.d/grafana.repo</span><br><span class="line">[grafana]</span><br><span class="line">name=grafana</span><br><span class="line">baseurl=https://packages.grafana.com/oss/rpm</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://packages.grafana.com/gpg.key</span><br><span class="line">sslverify=1</span><br><span class="line">sslcacert=/etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line"></span><br><span class="line">2.通过yum命令安装grafana</span><br><span class="line"># yum -y install grafana</span><br><span class="line"></span><br><span class="line">3.启动grafana并设为开机自启</span><br><span class="line">[root@cephnode04 ~]# yum list | grep grafana</span><br><span class="line">ceph-grafana-dashboards.noarch           2:14.2.6-0.el7                @ceph-noarch</span><br><span class="line">grafana.x86_64                           6.5.3-1                       @grafana </span><br><span class="line">pcp-webapp-grafana.noarch                4.3.2-3.el7_7                 updates </span><br><span class="line"></span><br><span class="line"># systemctl start grafana-server.service </span><br><span class="line"># systemctl enable grafana-server.service</span><br><span class="line"></span><br><span class="line">[root@cephnode04 ~]# netstat -antlp|grep grafana</span><br><span class="line"></span><br><span class="line"># web访问:</span><br><span class="line">http://47.240.10.142:3000/login  admin/admin</span><br></pre></td></tr></table></figure>
<h3 id="安装-promethus"><a href="#安装-promethus" class="headerlink" title="安装 promethus"></a>安装 promethus</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">1、下载安装包，下载地址</span><br><span class="line">https://prometheus.io/download/</span><br><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v2.14.0/prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">2、解压压缩包</span><br><span class="line"># tar fvxz prometheus-2.14.0.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">3、将解压后的目录改名</span><br><span class="line"># mv prometheus-2.14.0.linux-amd64 /opt/prometheus</span><br><span class="line"></span><br><span class="line">4、查看promethus版本</span><br><span class="line"># ./prometheus --version</span><br><span class="line"></span><br><span class="line">5、配置系统服务启动</span><br><span class="line"># vim /etc/systemd/system/prometheus.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Prometheus Monitoring System</span><br><span class="line">Documentation=Prometheus Monitoring System</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/opt/prometheus/prometheus \</span><br><span class="line">  --config.file /opt/prometheus/prometheus.yml \</span><br><span class="line">  --web.listen-address=:9090</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">6、加载系统服务</span><br><span class="line"># systemctl daemon-reload</span><br><span class="line"></span><br><span class="line">7、启动服务和添加开机自启动</span><br><span class="line"># systemctl start prometheus</span><br><span class="line"># systemctl enable prometheus</span><br><span class="line"></span><br><span class="line"># web访问</span><br><span class="line">http://47.240.10.142:9090/graph</span><br></pre></td></tr></table></figure>
<h3 id="ceph-mgr-prometheus插件配置"><a href="#ceph-mgr-prometheus插件配置" class="headerlink" title="ceph mgr prometheus插件配置"></a>ceph mgr prometheus插件配置</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># ceph mgr module enable prometheus</span><br><span class="line"># netstat -nltp | grep mgr 检查端口</span><br><span class="line"># curl 127.0.0.1:9283/metrics  测试返回值</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph mgr services</span><br><span class="line">&#123;</span><br><span class="line">    "dashboard": "https://cephnode02:8443/",</span><br><span class="line">    "prometheus": "http://cephnode02:9283/"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># web访问 采集数据</span><br><span class="line">http://47.240.15.208:9283/metrics</span><br></pre></td></tr></table></figure>
<h3 id="配置-promethus"><a href="#配置-promethus" class="headerlink" title="配置 promethus"></a>配置 promethus</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode04 prometheus]# vim /opt/prometheus/prometheus.yml </span><br><span class="line">...</span><br><span class="line">scrape_configs:</span><br><span class="line">  # The job name is added as a label `job=<span class="tag">&lt;<span class="name">job_name</span>&gt;</span>` to any timeseries scraped from this config.</span><br><span class="line">  - job_name: 'prometheus'</span><br><span class="line"></span><br><span class="line">    # metrics_path defaults to '/metrics'</span><br><span class="line">    # scheme defaults to 'http'.</span><br><span class="line"></span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: ['localhost:9090']</span><br><span class="line"></span><br><span class="line">  - job_name: 'ceph_cluster'</span><br><span class="line">    honor_labels: true</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: ['172.31.228.60:9283']</span><br><span class="line">        labels:</span><br><span class="line">          instance: ceph</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 重启promethus服务</span><br><span class="line">[root@cephnode04 prometheus]# systemctl restart prometheus</span><br><span class="line">[root@cephnode04 prometheus]# ps -ef|grep prometheus</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 检查prometheus服务器中是否添加成功</span><br><span class="line">http://47.240.10.142:9090/targets</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_21.png" width="50%"></p>
<h3 id="配置-grafana"><a href="#配置-grafana" class="headerlink" title="配置 grafana"></a>配置 grafana</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 浏览器登录 grafana 管理界面  </span><br><span class="line">2. 添加 data sources，点击 configuration --&gt; data sources  </span><br><span class="line">3. 添加 dashboard，点击HOME--》find dashboard on grafana.com  </span><br><span class="line">4. 搜索ceph的dashboard    </span><br><span class="line">5. 点击HOME--&gt;Import dashboard, 选择合适的dashboard,模板编号: 2842</span><br><span class="line">6. avg(ceph_osd_apply_latency_ms) 关注延迟,如果到秒级,集群就变慢了,正常都是毫秒</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_22.png" width="50%"></p>
<p><img src="/img/ceph/ceph_23.png" width="50%"></p>
<h2 id="K8S-接入-Ceph-存储"><a href="#K8S-接入-Ceph-存储" class="headerlink" title="K8S 接入 Ceph 存储"></a>K8S 接入 Ceph 存储</h2><h3 id="PV、PVC-概述"><a href="#PV、PVC-概述" class="headerlink" title="PV、PVC 概述"></a>PV、PVC 概述</h3><ol>
<li>管理存储是管理计算的一个明显问题。PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。</li>
<li>于是引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1. PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 </span><br><span class="line">集群中的资源就像一个节点是一个集群资源。 </span><br><span class="line">PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 </span><br><span class="line">该API对象包含存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 </span><br><span class="line"></span><br><span class="line">2. PersistentVolumeClaim（PVC）是用户存储的请求。 </span><br><span class="line">它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 </span><br><span class="line">pod可以请求特定级别的资源（CPU和内存）。 </span><br><span class="line">权限要求可以请求特定的大小和访问模式。</span><br><span class="line"></span><br><span class="line">3. 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 </span><br><span class="line">管理员需要能够提供多种不同于PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。</span><br><span class="line"></span><br><span class="line">4. StorageClass为集群提供了一种描述他们提供的存储的“类”的方法。 </span><br><span class="line">不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 </span><br><span class="line">Kubernetes本身对于什么类别代表是不言而喻的,这个概念有时在其他存储系统中称为“配置文件”</span><br></pre></td></tr></table></figure>
<h3 id="POD-动态供给"><a href="#POD-动态供给" class="headerlink" title="POD 动态供给"></a>POD 动态供给</h3><ol>
<li>动态供给主要是能够自动帮你创建pv，需要多大的空间就创建多大的pv。</li>
<li>k8s帮助创建pv，创建pvc就直接api调用存储类来寻找pv。</li>
<li>如果是存储静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态。</li>
<li>而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后帮你去连接，再帮你去自动创建pv。</li>
</ol>
<h3 id="小总结-1"><a href="#小总结-1" class="headerlink" title="小总结"></a>小总结</h3><ol>
<li>pv去存储里申请的一块空间,映射到pv上,作为物理卷存活在容器里</li>
<li>POD使用pvc联系pv拿到存储</li>
<li>动态供给,省却手动创建pv,而是调用StorageClass帮我们去创建</li>
</ol>
<h2 id="POD-使用-RBD-做为持久数据卷"><a href="#POD-使用-RBD-做为持久数据卷" class="headerlink" title="POD 使用 RBD 做为持久数据卷"></a>POD 使用 RBD 做为持久数据卷</h2><h3 id="安装与配置"><a href="#安装与配置" class="headerlink" title="安装与配置"></a>安装与配置</h3><ol>
<li>RBD支持ReadWriteOnce，ReadOnlyMany两种模式</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 访问模式包括：</span><br><span class="line">ReadWriteOnce   该volume只能被单个节点以读写的方式映射</span><br><span class="line">ReadOnlyMany    该volume可以被多个节点以只读方式映射</span><br><span class="line">ReadWriteMany   该volume只能被多个节点以读写的方式映射</span><br></pre></td></tr></table></figure>
<h4 id="配置-rbd-provisioner"><a href="#配置-rbd-provisioner" class="headerlink" title="配置 rbd-provisioner"></a>配置 rbd-provisioner</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"># 配置rbd-provisioner </span><br><span class="line">cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["persistentvolumes"]</span><br><span class="line">    verbs: ["get", "list", "watch", "create", "delete"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["persistentvolumeclaims"]</span><br><span class="line">    verbs: ["get", "list", "watch", "update"]</span><br><span class="line">  - apiGroups: ["storage.k8s.io"]</span><br><span class="line">    resources: ["storageclasses"]</span><br><span class="line">    verbs: ["get", "list", "watch"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["events"]</span><br><span class="line">    verbs: ["create", "update", "patch"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["endpoints"]</span><br><span class="line">    verbs: ["get", "list", "watch", "create", "update", "patch"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["services"]</span><br><span class="line">    resourceNames: ["kube-dns"]</span><br><span class="line">    verbs: ["list", "get"]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: rbd-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [""]</span><br><span class="line">  resources: ["secrets"]</span><br><span class="line">  verbs: ["get"]</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: rbd-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: rbd-provisioner</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: rbd-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: rbd-provisioner</span><br><span class="line">        image: "quay.io/external_storage/rbd-provisioner:v2.0.0-k8s1.11"</span><br><span class="line">        env:</span><br><span class="line">        - name: PROVISIONER_NAME</span><br><span class="line">          value: ceph.com/rbd</span><br><span class="line">      serviceAccount: rbd-provisioner</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f external-storage-rbd-provisioner.yaml</span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pod --all-namespaces</span><br><span class="line">kube-system            rbd-provisioner-578dc7dd99-5w7lr             1/1     Running   0          70s</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="配置-storageclass"><a href="#配置-storageclass" class="headerlink" title="配置 storageclass"></a>配置 storageclass</h4><ol>
<li>创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common。</li>
<li>将ceph的ceph.client.admin.keyring和ceph.conf文件拷贝到所有工作节点的/etc/ceph目录下</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# vim /etc/yum.repos.d/ceph.repo </span><br><span class="line"></span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ceph-common</span><br><span class="line">mkdir -p /etc/ceph</span><br><span class="line">cd /etc/ceph</span><br><span class="line">scp root@172.31.228.59:/etc/ceph/ceph.client.admin.keyring .</span><br><span class="line">scp root@172.31.228.59:/etc/ceph/ceph.conf .</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure>
<h4 id="创建-osd-pool"><a href="#创建-osd-pool" class="headerlink" title="创建 osd pool"></a>创建 osd pool</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create kube 16 16</span><br><span class="line">pool 'kube' already exists</span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd lspools</span><br><span class="line">1 .rgw.root</span><br><span class="line">2 default.rgw.control</span><br><span class="line">3 default.rgw.meta</span><br><span class="line">4 default.rgw.log</span><br><span class="line">5 rbd</span><br><span class="line">6 kube</span><br><span class="line">7 cephfs-data</span><br><span class="line">8 cephfs-metadata</span><br></pre></td></tr></table></figure>
<h4 id="创建k8s访问ceph的用户"><a href="#创建k8s访问ceph的用户" class="headerlink" title="创建k8s访问ceph的用户"></a>创建k8s访问ceph的用户</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring</span><br></pre></td></tr></table></figure>
<h4 id="查看-key"><a href="#查看-key" class="headerlink" title="查看 key"></a>查看 key</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph auth get-key client.admin</span><br><span class="line">AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw==</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph auth get-key client.kube</span><br><span class="line">AQCQGyBeHjByOhAAj8oEic0LQOOX3MT6O5FVSg==</span><br></pre></td></tr></table></figure>
<h4 id="创建-admin-secret"><a href="#创建-admin-secret" class="headerlink" title="创建 admin secret"></a>创建 admin secret</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \</span><br><span class="line">--from-literal=key=AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw== \</span><br><span class="line">--namespace=kube-system</span><br></pre></td></tr></table></figure>
<h4 id="在-default-命名空间创建-pvc-用于访问ceph的-secret"><a href="#在-default-命名空间创建-pvc-用于访问ceph的-secret" class="headerlink" title="在 default 命名空间创建 pvc 用于访问ceph的 secret"></a>在 default 命名空间创建 pvc 用于访问ceph的 secret</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret generic ceph-user-secret --type="kubernetes.io/rbd" \</span><br><span class="line">--from-literal=key=AQCQGyBeHjByOhAAj8oEic0LQOOX3MT6O5FVSg== \</span><br><span class="line">--namespace=default</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl get secret --all-namespaces</span><br></pre></td></tr></table></figure>
<h3 id="配置-StorageClass"><a href="#配置-StorageClass" class="headerlink" title="配置 StorageClass"></a>配置 StorageClass</h3><ol>
<li>源码中，monitors需要k8s dns解析，我这里使用外部ceph，肯定没有相关解析。</li>
<li>所以手动添加解析。</li>
<li>而且storageclass配置默认不支持直接修改（只能删除再添加），维护解析比维护storageclass配置要好些。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl create ns ceph</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# vim rbd-monitor-dns.yaml</span><br><span class="line"></span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon-1</span><br><span class="line">  namespace: ceph</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 172.31.228.59.xip.io</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon-2</span><br><span class="line">  namespace: ceph</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 172.31.228.60.xip.io</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-mon-3</span><br><span class="line">  namespace: ceph</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: 172.31.228.61.xip.io</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl create -f rbd-monitor-dns.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get svc -n ceph</span><br><span class="line">NAME         TYPE           CLUSTER-IP   EXTERNAL-IP            PORT(S)   AGE</span><br><span class="line">ceph-mon-1   ExternalName   <span class="tag">&lt;<span class="name">none</span>&gt;</span>       172.31.228.59.xip.io   <span class="tag">&lt;<span class="name">none</span>&gt;</span>    9s</span><br><span class="line">ceph-mon-2   ExternalName   <span class="tag">&lt;<span class="name">none</span>&gt;</span>       172.31.228.60.xip.io   <span class="tag">&lt;<span class="name">none</span>&gt;</span>    9s</span><br><span class="line">ceph-mon-3   ExternalName   <span class="tag">&lt;<span class="name">none</span>&gt;</span>       172.31.228.61.xip.io   <span class="tag">&lt;<span class="name">none</span>&gt;</span>    9s</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# vim storageclass-ceph-rdb.yaml </span><br><span class="line"></span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: dynamic-ceph-rdb</span><br><span class="line">provisioner: ceph.com/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: ceph-mon-1.ceph.svc.cluster.local.:6789,ceph-mon-2.ceph.svc.cluster.local.:6789,ceph-mon-3.ceph.svc.cluster.local.:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-secret</span><br><span class="line">  adminSecretNamespace: kube-system</span><br><span class="line">  pool: kube</span><br><span class="line">  userId: kube</span><br><span class="line">  userSecretName: ceph-user-secret</span><br><span class="line">  fsType: ext4</span><br><span class="line">  imageFormat: "2"</span><br><span class="line">  imageFeatures: "layering"</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f storageclass-ceph-rdb.yaml</span><br><span class="line">storageclass.storage.k8s.io/dynamic-ceph-rdb created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get sc</span><br><span class="line">NAME               PROVISIONER    AGE</span><br><span class="line">dynamic-ceph-rdb   ceph.com/rbd   13s</span><br></pre></td></tr></table></figure>
<h3 id="测试使用"><a href="#测试使用" class="headerlink" title="测试使用"></a>测试使用</h3><h4 id="创建-pvc-测试"><a href="#创建-pvc-测试" class="headerlink" title="创建 pvc 测试"></a>创建 pvc 测试</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-rdb-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:     </span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: dynamic-ceph-rdb</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f ceph-rdb-pvc-test.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl describe pvc ceph-rdb-claim</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pvc</span><br><span class="line">NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE</span><br><span class="line">ceph-rdb-claim   Bound    pvc-436de81b-1b3f-4fd4-ad1b-7663409d34a1   5Gi        RWO            dynamic-ceph-rdb   10s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS       REASON   AGE</span><br><span class="line">pvc-436de81b-1b3f-4fd4-ad1b-7663409d34a1   5Gi        RWO            Delete           Bound    default/ceph-rdb-claim   dynamic-ceph-rdb            25s</span><br></pre></td></tr></table></figure>
<h4 id="创建-nginx-pod-挂载测试"><a href="#创建-nginx-pod-挂载测试" class="headerlink" title="创建 nginx pod 挂载测试"></a>创建 nginx pod 挂载测试</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;nginx-pod.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod1</span><br><span class="line">  labels:</span><br><span class="line">    name: nginx-pod1</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx-pod1</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    ports:</span><br><span class="line">    - name: web</span><br><span class="line">      containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: ceph-rdb</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  volumes:</span><br><span class="line">  - name: ceph-rdb</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: ceph-rdb-claim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f nginx-pod.yaml</span><br><span class="line">pod/nginx-pod1 created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pods -o wide</span><br><span class="line">NAME         READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-pod1   1/1     Running   0          37s   10.244.2.3   k8s-master1   <span class="tag">&lt;<span class="name">none</span>&gt;</span>           <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 修改文件内容</span><br><span class="line">[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo this is from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html'</span><br><span class="line"></span><br><span class="line"># 访问测试 </span><br><span class="line">[root@k8s-master1 ceph]# curl http://10.244.2.3</span><br><span class="line">this is from Ceph RBD!!!</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 清理</span><br><span class="line">kubectl delete -f nginx-pod.yaml</span><br><span class="line">kubectl delete -f ceph-rdb-pvc-test.yaml</span><br></pre></td></tr></table></figure>
<h2 id="POD-使用-CephFS-做为持久数据卷"><a href="#POD-使用-CephFS-做为持久数据卷" class="headerlink" title="POD 使用 CephFS 做为持久数据卷"></a>POD 使用 CephFS 做为持久数据卷</h2><ol>
<li>CephFS方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany </li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">访问模式包括：</span><br><span class="line">1. ReadWriteOnce    该volume只能被单个节点以读写的方式映射</span><br><span class="line">2. ReadOnlyMany     该volume可以被多个节点以只读方式映射</span><br><span class="line">3. ReadWriteMany    该volume只能被多个节点以读写的方式映射</span><br><span class="line"></span><br><span class="line"># 注意：即使volume支持很多种访问模式，但它同时只能使用一种方式来映射。</span><br><span class="line"># 比如，GCEPersistentDisk可以被单个节点映射为ReadWriteOnce，或者多个节点映射为ReadOnlyMany，但不能同时使用这两种方式来映射。</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>Pod挂载cephfs有时候会用fuse挂载 如果宿主机没有这个工具会导致挂载失败 pod无法启动</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ceph-fuse</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_24.png" width="50%"></p>
<h3 id="Ceph-端创建-CephFS-pool"><a href="#Ceph-端创建-CephFS-pool" class="headerlink" title="Ceph 端创建 CephFS pool"></a>Ceph 端创建 CephFS pool</h3><ol>
<li>在ceph节点,CephFS需要使用两个Pool来分别存储数据和元数据</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create fs_data 16</span><br><span class="line">pool 'fs_data' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool create fs_metadata 16</span><br><span class="line">pool 'fs_metadata' created</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd lspools</span><br><span class="line">1 .rgw.root</span><br><span class="line">2 default.rgw.control</span><br><span class="line">3 default.rgw.meta</span><br><span class="line">4 default.rgw.log</span><br><span class="line">5 rbd</span><br><span class="line">6 kube</span><br><span class="line">7 cephfs-data</span><br><span class="line">8 cephfs-metadata</span><br><span class="line">9 fs_data</span><br><span class="line">10 fs_metadata</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个CephFS </span><br><span class="line"># 删除之前的</span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs volume rm cephfs --yes-i-really-mean-it</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs new cephfs fs_metadata fs_data</span><br><span class="line">new fs with metadata pool 10 and data pool 9</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph fs ls</span><br><span class="line">name: cephfs, metadata pool: fs_metadata, data pools: [fs_data ]</span><br></pre></td></tr></table></figure>
<h3 id="部署-cephfs-provisioner"><a href="#部署-cephfs-provisioner" class="headerlink" title="部署 cephfs-provisioner"></a>部署 cephfs-provisioner</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"># 使用社区提供的cephfs-provisioner</span><br><span class="line"></span><br><span class="line">cat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">   name: cephfs</span><br><span class="line">   labels:</span><br><span class="line">     name: cephfs</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["secrets"]</span><br><span class="line">    verbs: ["create", "get", "delete"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["endpoints"]</span><br><span class="line">    verbs: ["get", "list", "watch", "create", "update", "patch"]</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["persistentvolumes"]</span><br><span class="line">    verbs: ["get", "list", "watch", "create", "delete"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["persistentvolumeclaims"]</span><br><span class="line">    verbs: ["get", "list", "watch", "update"]</span><br><span class="line">  - apiGroups: ["storage.k8s.io"]</span><br><span class="line">    resources: ["storageclasses"]</span><br><span class="line">    verbs: ["get", "list", "watch"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["events"]</span><br><span class="line">    verbs: ["create", "update", "patch"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["services"]</span><br><span class="line">    resourceNames: ["kube-dns","coredns"]</span><br><span class="line">    verbs: ["list", "get"]</span><br><span class="line">  - apiGroups: [""]</span><br><span class="line">    resources: ["secrets"]</span><br><span class="line">    verbs: ["get", "create", "delete"]</span><br><span class="line">  - apiGroups: ["policy"]</span><br><span class="line">    resourceNames: ["cephfs-provisioner"]</span><br><span class="line">    resources: ["podsecuritypolicies"]</span><br><span class="line">    verbs: ["use"]</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: cephfs-provisioner</span><br><span class="line">    namespace: cephfs</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-provisioner</span><br><span class="line">  namespace: cephfs</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: cephfs-provisioner</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: cephfs-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: cephfs-provisioner</span><br><span class="line">        image: "quay.io/external_storage/cephfs-provisioner:latest"</span><br><span class="line">        env:</span><br><span class="line">        - name: PROVISIONER_NAME</span><br><span class="line">          value: ceph.com/cephfs</span><br><span class="line">        command:</span><br><span class="line">        - "/usr/local/bin/cephfs-provisioner"</span><br><span class="line">        args:</span><br><span class="line">        - "-id=cephfs-provisioner-1"</span><br><span class="line">        - "-disable-ceph-namespace-isolation=true"</span><br><span class="line">      serviceAccount: cephfs-provisioner</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f external-storage-cephfs-provisioner.yaml </span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]#  kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">cephfs                 cephfs-provisioner-847468fc-95rkg            1/1     Running   0          62s</span><br></pre></td></tr></table></figure>
<h3 id="配置-StorageClass-1"><a href="#配置-StorageClass-1" class="headerlink" title="配置 StorageClass"></a>配置 StorageClass</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 查看key 在ceph的mon或者admin节点</span><br><span class="line"># 重新配置一下</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph auth get-key client.admin</span><br><span class="line">AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw==</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl delete secret ceph-secret -n kube-system</span><br><span class="line"></span><br><span class="line">kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \</span><br><span class="line">--from-literal=key=AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw== \</span><br><span class="line">--namespace=kube-system</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get secret -n kube-system</span><br><span class="line">NAME                          TYPE                                  DATA   AGE</span><br><span class="line">ceph-secret                   kubernetes.io/rbd                     1      3s</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 配置 StorageClass</span><br><span class="line"></span><br><span class="line">cat &gt;storageclass-cephfs.yaml&lt;&lt;EOF</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: dynamic-cephfs</span><br><span class="line">provisioner: ceph.com/cephfs</span><br><span class="line">parameters:</span><br><span class="line">    monitors: 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789</span><br><span class="line">    adminId: admin</span><br><span class="line">    adminSecretName: ceph-secret</span><br><span class="line">    adminSecretNamespace: "kube-system"</span><br><span class="line">    claimRoot: /volumes/kubernetes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f storageclass-cephfs.yaml</span><br><span class="line">storageclass.storage.k8s.io/dynamic-cephfs created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get sc</span><br><span class="line">NAME               PROVISIONER       AGE</span><br><span class="line">dynamic-ceph-rdb   ceph.com/rbd      52m</span><br><span class="line">dynamic-cephfs     ceph.com/cephfs   2s</span><br></pre></td></tr></table></figure>
<h3 id="测试使用-1"><a href="#测试使用-1" class="headerlink" title="测试使用"></a>测试使用</h3><h4 id="创建pvc测试"><a href="#创建pvc测试" class="headerlink" title="创建pvc测试"></a>创建pvc测试</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:     </span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: dynamic-cephfs</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f cephfs-pvc-test.yaml</span><br><span class="line">persistentvolumeclaim/cephfs-claim created</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS     AGE</span><br><span class="line">cephfs-claim   Bound    pvc-ac8186ab-ff09-4134-9357-044ca377af25   2Gi        RWO            dynamic-cephfs   10s</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS     REASON   AGE</span><br><span class="line">pvc-ac8186ab-ff09-4134-9357-044ca377af25   2Gi        RWO            Delete           Bound    default/cephfs-claim   dynamic-cephfs            12s</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 创建 nginx pod 挂载测试</span><br><span class="line"></span><br><span class="line">cat &gt;nginx-pod2.yaml&lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod2</span><br><span class="line">  labels:</span><br><span class="line">    name: nginx-pod2</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx-pod2</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">    - name: web</span><br><span class="line">      containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: cephfs</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cephfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: cephfs-claim</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ceph]# kubectl apply -f nginx-pod2.yaml</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl get pods -o wide</span><br><span class="line">NAME         READY   STATUS    RESTARTS   AGE     IP           NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-pod2   1/1     Running   0          3m28s   10.244.1.3   k8s-node1   <span class="tag">&lt;<span class="name">none</span>&gt;</span>           <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod2 sh</span><br><span class="line"># df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">overlay          40G  4.9G   33G  13% /</span><br><span class="line">tmpfs            64M     0   64M   0% /dev</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup</span><br><span class="line">/dev/vda1        40G  4.9G   33G  13% /etc/hosts</span><br><span class="line">shm              64M     0   64M   0% /dev/shm</span><br><span class="line">ceph-fuse        18G     0   18G   0% /usr/share/nginx/html</span><br><span class="line">tmpfs           1.9G   12K  1.9G   1% /run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /proc/acpi</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /proc/scsi</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /sys/firmware</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 修改文件内容</span><br><span class="line">[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod2 -- /bin/sh -c 'echo This is from CephFS!!! &gt; /usr/share/nginx/html/index.html'</span><br><span class="line"></span><br><span class="line"># 访问pod测试</span><br><span class="line">[root@k8s-master1 ceph]# curl http://10.244.1.3</span><br><span class="line">This is from CephFS!!!</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 清理</span><br><span class="line"># pod 被删除 再拉起 数据是还在的</span><br><span class="line">kubectl delete -f nginx-pod2.yaml</span><br><span class="line"># pvc 删除 pv 数据就都不在了,storageclass自动回收 </span><br><span class="line">kubectl delete -f cephfs-pvc-test.yaml</span><br></pre></td></tr></table></figure>
<h2 id="Ceph-日常运维管理"><a href="#Ceph-日常运维管理" class="headerlink" title="Ceph 日常运维管理"></a>Ceph 日常运维管理</h2><h3 id="集群监控管理"><a href="#集群监控管理" class="headerlink" title="集群监控管理"></a>集群监控管理</h3><ol>
<li>集群整体运行状态 </li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@cephnode01 my-cluster]# ceph -s </span><br><span class="line">  cluster:</span><br><span class="line">    id:     4ed819cf-39be-4a7c-9216-effcae715c58</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 9h)</span><br><span class="line">    mgr: cephnode02(active, since 6h), standbys: cephnode03, cephnode01</span><br><span class="line">    mds: cephfs:1 &#123;0=cephnode02=up:active&#125; 2 up:standby</span><br><span class="line">    osd: 3 osds: 3 up (since 9h), 3 in (since 31h)</span><br><span class="line">    rgw: 1 daemon active (cephnode01)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   10 pools, 144 pgs</span><br><span class="line">    objects: 260 objects, 43 KiB</span><br><span class="line">    usage:   3.0 GiB used, 54 GiB / 57 GiB avail</span><br><span class="line">    pgs:     144 active+clean</span><br><span class="line"></span><br><span class="line">id:         集群ID</span><br><span class="line">health:     集群运行状态，这里有一个警告，说明是有问题，意思是pg数大于pgp数，通常此数值相等。</span><br><span class="line">mon:        Monitors运行状态。</span><br><span class="line">osd:        OSDs运行状态。</span><br><span class="line">mgr:        Managers运行状态。</span><br><span class="line">mds:        Metadatas运行状态。</span><br><span class="line">pools:      存储池与PGs的数量。</span><br><span class="line">objects:    存储对象的数量。</span><br><span class="line">usage:      存储的理论用量。</span><br><span class="line">pgs:        PGs的运行状态</span><br><span class="line"></span><br><span class="line"># 持续输出</span><br><span class="line">ceph -w</span><br><span class="line"># 健康检查</span><br><span class="line">ceph health detail</span><br></pre></td></tr></table></figure>
<h4 id="PG-状态"><a href="#PG-状态" class="headerlink" title="PG 状态"></a>PG 状态</h4><ol>
<li>查看pg状态查看通常使用下面两个命令即可，dump可以查看更详细信息，如</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump</span><br><span class="line">ceph pg stat</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph pg stat</span><br><span class="line">144 pgs: 144 active+clean; 43 KiB data, 32 MiB used, 54 GiB / 57 GiB avail</span><br><span class="line"></span><br><span class="line"># 144个pg</span><br><span class="line"># 43k数据</span><br><span class="line"># 32M使用</span><br><span class="line"># 54G空间</span><br><span class="line"></span><br><span class="line">ceph pg dump</span><br><span class="line"># pg的位置 如果主pg坏了 剩下的2个会选举出一个来在让用户访问</span><br></pre></td></tr></table></figure>
<p><img src="/img/ceph/ceph_25.png" width="50%"></p>
<h4 id="Pool状态"><a href="#Pool状态" class="headerlink" title="Pool状态"></a>Pool状态</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool stats</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd pool stats</span><br><span class="line">pool .rgw.root id 1</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool default.rgw.control id 2</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool default.rgw.meta id 3</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool default.rgw.log id 4</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool rbd id 5</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool kube id 6</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool cephfs-data id 7</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool cephfs-metadata id 8</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool fs_data id 9</span><br><span class="line">  nothing is going on</span><br><span class="line"></span><br><span class="line">pool fs_metadata id 10</span><br><span class="line">  nothing is going on</span><br></pre></td></tr></table></figure>
<h4 id="OSD-状态"><a href="#OSD-状态" class="headerlink" title="OSD 状态"></a>OSD 状态</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ceph osd stat</span><br><span class="line">ceph osd dump</span><br><span class="line">ceph osd tree</span><br><span class="line">ceph osd df</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF </span><br><span class="line">-1       0.05576 root default                                </span><br><span class="line">-3       0.01859     host cephnode01                         </span><br><span class="line"> 0   hdd 0.01859         osd.0           up  1.00000 1.00000 </span><br><span class="line">-5       0.01859     host cephnode02                         </span><br><span class="line"> 1   hdd 0.01859         osd.1           up  1.00000 1.00000 </span><br><span class="line">-7       0.01859     host cephnode03                         </span><br><span class="line"> 2   hdd 0.01859         osd.2           up  1.00000 1.00000 </span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph osd df</span><br><span class="line">ID CLASS WEIGHT  REWEIGHT SIZE   RAW USE DATA   OMAP META  AVAIL  %USE VAR  PGS STATUS </span><br><span class="line"> 0   hdd 0.01859  1.00000 19 GiB 1.0 GiB 11 MiB  0 B 1 GiB 18 GiB 5.32 1.00 144     up </span><br><span class="line"> 1   hdd 0.01859  1.00000 19 GiB 1.0 GiB 11 MiB  0 B 1 GiB 18 GiB 5.32 1.00 144     up </span><br><span class="line"> 2   hdd 0.01859  1.00000 19 GiB 1.0 GiB 11 MiB  0 B 1 GiB 18 GiB 5.32 1.00 144     up </span><br><span class="line">                    TOTAL 57 GiB 3.0 GiB 32 MiB  0 B 3 GiB 54 GiB 5.32                 </span><br><span class="line">MIN/MAX VAR: 1.00/1.00  STDDEV: 0</span><br></pre></td></tr></table></figure>
<h4 id="Monitor状态和查看仲裁状态"><a href="#Monitor状态和查看仲裁状态" class="headerlink" title="Monitor状态和查看仲裁状态"></a>Monitor状态和查看仲裁状态</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph mon stat</span><br><span class="line">ceph mon dump</span><br><span class="line">ceph quorum_status</span><br></pre></td></tr></table></figure>
<h4 id="集群空间用量"><a href="#集群空间用量" class="headerlink" title="集群空间用量"></a>集群空间用量</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ceph df 常用</span><br><span class="line">ceph df detail</span><br><span class="line"></span><br><span class="line">[root@cephnode01 my-cluster]# ceph df</span><br><span class="line">RAW STORAGE:</span><br><span class="line">    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED </span><br><span class="line">    hdd       57 GiB     54 GiB     32 MiB      3.0 GiB          5.32 </span><br><span class="line">    TOTAL     57 GiB     54 GiB     32 MiB      3.0 GiB          5.32 </span><br><span class="line"> </span><br><span class="line">POOLS:</span><br><span class="line">    POOL                    ID     STORED      OBJECTS     USED        %USED     MAX AVAIL </span><br><span class="line">    .rgw.root                1     1.2 KiB           4     768 KiB         0        17 GiB </span><br><span class="line">    default.rgw.control      2         0 B           8         0 B         0        17 GiB </span><br><span class="line">    default.rgw.meta         3         0 B           0         0 B         0        17 GiB </span><br><span class="line">    default.rgw.log          4         0 B         175         0 B         0        17 GiB </span><br><span class="line">    rbd                      5        19 B           3     192 KiB         0        17 GiB </span><br><span class="line">    kube                     6       709 B           6     576 KiB         0        17 GiB </span><br><span class="line">    cephfs-data              7         0 B           0         0 B         0        17 GiB </span><br><span class="line">    cephfs-metadata          8     6.4 KiB          41     2.6 MiB         0        17 GiB </span><br><span class="line">    fs_data                  9        23 B           1     192 KiB         0        17 GiB </span><br><span class="line">    fs_metadata             10      34 KiB          22     1.5 MiB         0        17 GiB</span><br></pre></td></tr></table></figure>
<h3 id="集群配置管理-临时和全局，服务平滑重启"><a href="#集群配置管理-临时和全局，服务平滑重启" class="headerlink" title="集群配置管理 (临时和全局，服务平滑重启)"></a>集群配置管理 (临时和全局，服务平滑重启)</h3><ol>
<li>有时候需要更改服务的配置，但不想重启服务，或者是临时修改。这时候就可以使用tell和daemon子命令来完成此需求。</li>
</ol>
<h4 id="查看运行配置"><a href="#查看运行配置" class="headerlink" title="查看运行配置"></a>查看运行配置</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config show </span><br><span class="line"></span><br><span class="line">命令举例：</span><br><span class="line"># ceph daemon osd.0 config show</span><br></pre></td></tr></table></figure>
<h4 id="tell-子命令格式"><a href="#tell-子命令格式" class="headerlink" title="tell 子命令格式"></a>tell 子命令格式</h4><ol>
<li>使用 tell 的方式适合对整个集群进行设置，使用 * 号进行匹配，就可以对整个集群的角色进行设置。</li>
<li>而出现节点异常无法设置时候，只会在命令行当中进行报错，不太便于查找。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph tell &#123;daemon-type&#125;.&#123;daemon id or *&#125; injectargs --&#123;name&#125;=&#123;value&#125; [--&#123;name&#125;=&#123;value&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 1</span><br><span class="line"></span><br><span class="line">1. daemon-type：为要操作的对象类型如osd、mon、mds等。</span><br><span class="line">2. daemon id：该对象的名称，osd通常为0、1等，mon为ceph -s显示的名称，这里可以输入*表示全部。</span><br><span class="line">3. injectargs：表示参数注入，后面必须跟一个参数，也可以跟多个</span><br></pre></td></tr></table></figure>
<h4 id="daemon子命令"><a href="#daemon子命令" class="headerlink" title="daemon子命令"></a>daemon子命令</h4><ol>
<li>使用 daemon 进行设置的方式就是一个个的去设置，这样可以比较好的反馈，此方法是需要在设置的角色所在的主机上进行设置。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">命令格式：</span><br><span class="line"># ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config set &#123;name&#125;=&#123;value&#125;</span><br><span class="line"></span><br><span class="line">命令举例：</span><br><span class="line"># ceph daemon mon.ceph-monitor-1 config set mon_allow_pool_delete false</span><br></pre></td></tr></table></figure>
<h3 id="集群操作"><a href="#集群操作" class="headerlink" title="集群操作"></a>集群操作</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 启动所有守护进程</span><br><span class="line"># systemctl start ceph.target</span><br><span class="line">2. 按类型启动守护进程</span><br><span class="line"># systemctl start ceph-mgr.target</span><br><span class="line"># systemctl start ceph-osd@id</span><br><span class="line"># systemctl start ceph-mon.target</span><br><span class="line"># systemctl start ceph-mds.target</span><br><span class="line"># systemctl start ceph-radosgw.target</span><br></pre></td></tr></table></figure>
<h3 id="添加和删除OSD"><a href="#添加和删除OSD" class="headerlink" title="添加和删除OSD"></a>添加和删除OSD</h3><h4 id="添加-OSD"><a href="#添加-OSD" class="headerlink" title="添加 OSD"></a>添加 OSD</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、格式化磁盘</span><br><span class="line">ceph-volume lvm zap /dev/sd<span class="tag">&lt;<span class="name">id</span>&gt;</span></span><br><span class="line"></span><br><span class="line">2、进入到ceph-deploy执行目录/my-cluster，添加OSD</span><br><span class="line"># ceph-deploy osd create --data /dev/sd<span class="tag">&lt;<span class="name">id</span>&gt;</span> $hostname</span><br></pre></td></tr></table></figure>
<h4 id="删除-OSD"><a href="#删除-OSD" class="headerlink" title="删除 OSD"></a>删除 OSD</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. 调整osd的crush weight为 0</span><br><span class="line">ceph osd crush reweight osd.<span class="tag">&lt;<span class="name">ID</span>&gt;</span> 0.0</span><br><span class="line">2. 将osd进程stop</span><br><span class="line">systemctl stop ceph-osd@<span class="tag">&lt;<span class="name">ID</span>&gt;</span></span><br><span class="line">3. 将osd设置out</span><br><span class="line">ceph osd out <span class="tag">&lt;<span class="name">ID</span>&gt;</span></span><br><span class="line">4. 立即执行删除OSD中数据</span><br><span class="line">ceph osd purge osd.<span class="tag">&lt;<span class="name">ID</span>&gt;</span> --yes-i-really-mean-it</span><br><span class="line">5. 卸载磁盘</span><br><span class="line">umount /var/lib/ceph/osd/ceph-？</span><br></pre></td></tr></table></figure>
<h3 id="扩容-PG"><a href="#扩容-PG" class="headerlink" title="扩容 PG"></a>扩容 PG</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set &#123;pool-name&#125; pg_num 128</span><br><span class="line">ceph osd pool set &#123;pool-name&#125; pgp_num 128 </span><br><span class="line"></span><br><span class="line">注：  </span><br><span class="line">1. 扩容大小取跟它接近的2的N次方  </span><br><span class="line">2. 在更改pool的PG数量时，需同时更改PGP的数量。PGP是为了管理placement而存在的专门的PG，它和PG的数量应该保持一致。</span><br><span class="line">如果你增加pool的pg_num，就需要同时增加pgp_num，保持它们大小一致，这样集群才能正常rebalancing。</span><br></pre></td></tr></table></figure>
<h3 id="Pool-操作"><a href="#Pool-操作" class="headerlink" title="Pool 操作"></a>Pool 操作</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 列出存储池</span><br><span class="line">ceph osd lspools</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 创建存储池</span><br><span class="line">命令格式：</span><br><span class="line"># ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; [&#123;pgp-num&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph osd pool create rbd  32 32</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置存储池配额 </span><br><span class="line">命令格式：</span><br><span class="line"># ceph osd pool set-quota &#123;pool-name&#125; [max_objects &#123;obj-count&#125;] [max_bytes &#123;bytes&#125;]</span><br><span class="line">命令举例：</span><br><span class="line"># ceph osd pool set-quota rbd max_objects 10000</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 删除存储池 </span><br><span class="line">ceph osd pool delete &#123;pool-name&#125; [&#123;pool-name&#125; --yes-i-really-really-mean-it]</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 重命名存储池 </span><br><span class="line">ceph osd pool rename &#123;current-pool-name&#125; &#123;new-pool-name&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 查看存储池统计信息 </span><br><span class="line">rados df</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 给存储池做快照 </span><br><span class="line">ceph osd pool mksnap &#123;pool-name&#125; &#123;snap-name&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 删除存储池的快照 </span><br><span class="line">ceph osd pool rmsnap &#123;pool-name&#125; &#123;snap-name&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 获取存储池选项值 </span><br><span class="line">ceph osd pool get &#123;pool-name&#125; &#123;key&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 调整存储池选项值</span><br><span class="line">ceph osd pool set &#123;pool-name&#125; &#123;key&#125; &#123;value&#125;</span><br><span class="line">size：设置存储池中的对象副本数，详情参见设置对象副本数。仅适用于副本存储池。</span><br><span class="line">min_size：设置 I/O 需要的最小副本数，详情参见设置对象副本数。仅适用于副本存储池。</span><br><span class="line">pg_num：计算数据分布时的有效 PG 数。只能大于当前 PG 数。</span><br><span class="line">pgp_num：计算数据分布时使用的有效 PGP 数量。小于等于存储池的 PG 数。</span><br><span class="line">hashpspool：给指定存储池设置/取消 HASHPSPOOL 标志。</span><br><span class="line">target_max_bytes：达到 max_bytes 阀值时会触发 Ceph 冲洗或驱逐对象。</span><br><span class="line">target_max_objects：达到 max_objects 阀值时会触发 Ceph 冲洗或驱逐对象。</span><br><span class="line">scrub_min_interval：在负载低时，洗刷存储池的最小间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_min_interval 。</span><br><span class="line">scrub_max_interval：不管集群负载如何，都要洗刷存储池的最大间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_max_interval 。</span><br><span class="line">deep_scrub_interval：“深度”洗刷存储池的间隔秒数。如果是 0 ，就按照配置文件里的 osd_deep_scrub_interval 。</span><br></pre></td></tr></table></figure>
<h3 id="获取对象副本数"><a href="#获取对象副本数" class="headerlink" title="获取对象副本数"></a>获取对象副本数</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd dump | grep 'replicated size'</span><br></pre></td></tr></table></figure>
<h3 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h3><ol>
<li>Ceph 把数据以对象的形式存于各存储池中。</li>
<li>Ceph 用户必须具有访问存储池的权限才能够读写数据。</li>
<li>另外，Ceph 用户必须具有执行权限才能够使用 Ceph 的管理命令。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 查看用户信息</span><br><span class="line">查看所有用户信息</span><br><span class="line"># ceph auth list</span><br><span class="line">获取所有用户的key与权限相关信息</span><br><span class="line"># ceph auth get client.admin</span><br><span class="line">如果只需要某个用户的key信息，可以使用pring-key子命令</span><br><span class="line"># ceph auth print-key client.admin</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 添加用户</span><br><span class="line"># ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'</span><br><span class="line"># ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'</span><br><span class="line"># ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring</span><br><span class="line"># ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 修改用户权限 </span><br><span class="line"># ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'</span><br><span class="line"># ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'</span><br><span class="line"># ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'</span><br><span class="line"># ceph auth caps client.ringo mon ' ' osd ' '</span><br></pre></td></tr></table></figure>
<h3 id="增加和删除Monitor"><a href="#增加和删除Monitor" class="headerlink" title="增加和删除Monitor"></a>增加和删除Monitor</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 新增一个monitor</span><br><span class="line"># ceph-deploy mon create $hostname</span><br><span class="line">注意：执行ceph-deploy之前要进入之前安装时候配置的目录。/my-cluster</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 删除Monitor </span><br><span class="line"># ceph-deploy mon destroy $hostname</span><br><span class="line">注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个。</span><br></pre></td></tr></table></figure>
<h3 id="Pool-配置大小"><a href="#Pool-配置大小" class="headerlink" title="Pool 配置大小"></a>Pool 配置大小</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Pool的基本配置</span><br><span class="line">若少于5个OSD   设置pg_num为128。</span><br><span class="line">5~10个OSD     设置pg_num为512。</span><br><span class="line">10~50个OSD    设置pg_num为4096。</span><br><span class="line">超过50个OSD   可以参考pgcalc计算。</span><br></pre></td></tr></table></figure>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="nearfull-osd-s-or-pool-s-nearfull"><a href="#nearfull-osd-s-or-pool-s-nearfull" class="headerlink" title="nearfull osd(s) or pool(s) nearfull"></a>nearfull osd(s) or pool(s) nearfull</h3><ol>
<li>此时说明部分osd的存储已经超过阈值，mon会监控ceph集群中OSD空间使用情况。</li>
<li>如果要消除WARN,可以修改这两个参数，提高阈值，但是通过实践发现并不能解决问题，可以通过观察osd的数据分布情况来分析原因。</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 配置文件设置阈值 </span><br><span class="line">  "mon_osd_full_ratio": "0.95",</span><br><span class="line">  "mon_osd_nearfull_ratio": "0.85"</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 自动处理 </span><br><span class="line">ceph osd reweight-by-utilization</span><br><span class="line">ceph osd reweight-by-pg 105 cephfs_data(pool_name)</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 手动处理 </span><br><span class="line">ceph osd reweight osd.2 0.8</span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 全局处理 </span><br><span class="line">ceph mgr module ls</span><br><span class="line">ceph mgr module enable balancer</span><br><span class="line">ceph balancer on</span><br><span class="line">ceph balancer mode crush-compat</span><br><span class="line">ceph config-key set "mgr/balancer/max_misplaced": "0.01"</span><br></pre></td></tr></table></figure>
<h3 id="PG-故障状态"><a href="#PG-故障状态" class="headerlink" title="PG 故障状态"></a>PG 故障状态</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">PG状态概述</span><br><span class="line">一个PG在它的生命周期的不同时刻可能会处于以下几种状态中:</span><br><span class="line"></span><br><span class="line">Creating(创建中)</span><br><span class="line">在创建POOL时,需要指定PG的数量,此时PG的状态便处于creating,意思是Ceph正在创建PG。</span><br><span class="line"></span><br><span class="line">Peering(互联中)</span><br><span class="line">peering的作用主要是在PG及其副本所在的OSD之间建立互联,并使得OSD之间就这些PG中的object及其元数据达成一致。</span><br><span class="line"></span><br><span class="line">Active(活跃的)</span><br><span class="line">处于该状态意味着数据已经完好的保存到了主PG及副本PG中,并且Ceph已经完成了peering工作。</span><br><span class="line"></span><br><span class="line">Clean(整洁的)</span><br><span class="line">当某个PG处于clean状态时,则说明对应的主OSD及副本OSD已经成功互联,并且没有偏离的PG。也意味着Ceph已经将该PG中的对象按照规定的副本数进行了复制操作。</span><br><span class="line"></span><br><span class="line">Degraded(降级的)</span><br><span class="line">当某个PG的副本数未达到规定个数时,该PG便处于degraded状态,例如:</span><br><span class="line"></span><br><span class="line">在客户端向主OSD写入object的过程,object的副本是由主OSD负责向副本OSD写入的,直到副本OSD在创建object副本完成,并向主OSD发出完成信息前,该PG的状态都会一直处于degraded状态。又或者是某个OSD的状态变成了down,那么该OSD上的所有PG都会被标记为degraded。</span><br><span class="line">当Ceph因为某些原因无法找到某个PG内的一个或多个object时,该PG也会被标记为degraded状态。此时客户端不能读写找不到的对象,但是仍然能访问位于该PG内的其他object。</span><br><span class="line"></span><br><span class="line">Recovering(恢复中)</span><br><span class="line">当某个OSD因为某些原因down了,该OSD内PG的object会落后于它所对应的PG副本。而在该OSD重新up之后,该OSD中的内容必须更新到当前状态,处于此过程中的PG状态便是recovering。</span><br><span class="line"></span><br><span class="line">Backfilling(回填)</span><br><span class="line">当有新的OSD加入集群时,CRUSH会把现有集群内的部分PG分配给它。这些被重新分配到新OSD的PG状态便处于backfilling。</span><br><span class="line"></span><br><span class="line">Remapped(重映射)</span><br><span class="line">当负责维护某个PG的acting set变更时,PG需要从原来的acting set迁移至新的acting set。这个过程需要一段时间,所以在此期间,相关PG的状态便会标记为remapped。</span><br><span class="line"></span><br><span class="line">Stale(陈旧的)</span><br><span class="line">默认情况下,OSD守护进程每半秒钟便会向Monitor报告其PG等相关状态,如果某个PG的主OSD所在acting set没能向Monitor发送报告,或者其他的Monitor已经报告该OSD为down时,该PG便会被标记为stale。</span><br></pre></td></tr></table></figure>
<h3 id="OSD-状态-1"><a href="#OSD-状态-1" class="headerlink" title="OSD 状态"></a>OSD 状态</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">单个OSD有两组状态需要关注,其中一组使用in/out标记该OSD是否在集群内,另一组使用up/down标记该OSD是否处于运行中状态。两组状态之间并不互斥,换句话说,当一个OSD处于“in”状态时,它仍然可以处于up或down的状态。</span><br><span class="line"></span><br><span class="line">OSD状态为in且up</span><br><span class="line">这是一个OSD正常的状态,说明该OSD处于集群内,并且运行正常。</span><br><span class="line"></span><br><span class="line">OSD状态为in且down</span><br><span class="line">此时该OSD尚处于集群中,但是守护进程状态已经不正常,默认在300秒后会被踢出集群,状态进而变为out且down,之后处于该OSD上的PG会迁移至其它OSD。</span><br><span class="line"></span><br><span class="line">OSD状态为out且up</span><br><span class="line">这种状态一般会出现在新增OSD时,意味着该OSD守护进程正常,但是尚未加入集群。</span><br><span class="line"></span><br><span class="line">OSD状态为out且down</span><br><span class="line">在该状态下的OSD不在集群内,并且守护进程运行不正常,CRUSH不会再分配PG到该OSD上。</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/k8s/" rel="tag"><i class="fa fa-tag"></i> k8s</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/31/k8s-15/" rel="next" title="15 k8s 全方位监控 Kubernetes">
                <i class="fa fa-chevron-left"></i> 15 k8s 全方位监控 Kubernetes
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/01/13/k8s-base13/" rel="prev" title="11 Isito 微服务治理">
                11 Isito 微服务治理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/Greeen.jpg" alt="Harris Li">
            
              <p class="site-author-name" itemprop="name">Harris Li</p>
              <p class="site-description motion-element" itemprop="description">Beijing</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">119</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-介绍"><span class="nav-number">1.</span> <span class="nav-text">Ceph 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要用-Ceph"><span class="nav-number">1.1.</span> <span class="nav-text">为什么要用 Ceph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph-架构介绍"><span class="nav-number">1.2.</span> <span class="nav-text">Ceph 架构介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-核心概念"><span class="nav-number">2.</span> <span class="nav-text">Ceph 核心概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-核心组件"><span class="nav-number">3.</span> <span class="nav-text">Ceph 核心组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-三种存储类型"><span class="nav-number">4.</span> <span class="nav-text">Ceph 三种存储类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#小总结"><span class="nav-number">4.1.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-集群部署"><span class="nav-number">5.</span> <span class="nav-text">Ceph 集群部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph-版本选择"><span class="nav-number">5.1.</span> <span class="nav-text">Ceph 版本选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Luminous-新版本特性"><span class="nav-number">5.2.</span> <span class="nav-text">Luminous 新版本特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装前准备"><span class="nav-number">5.3.</span> <span class="nav-text">安装前准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装内网yum源"><span class="nav-number">5.4.</span> <span class="nav-text">安装内网yum源</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用阿里云yum源-安装Ceph集群"><span class="nav-number">5.5.</span> <span class="nav-text">使用阿里云yum源 安装Ceph集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph-conf"><span class="nav-number">5.6.</span> <span class="nav-text">ceph.conf</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-RBD"><span class="nav-number">6.</span> <span class="nav-text">Ceph RBD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-介绍"><span class="nav-number">6.1.</span> <span class="nav-text">RBD 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-常用命令"><span class="nav-number">6.2.</span> <span class="nav-text">RBD 常用命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-配置操作"><span class="nav-number">6.3.</span> <span class="nav-text">RBD 配置操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RBD-挂载到操作系统"><span class="nav-number">6.3.1.</span> <span class="nav-text">RBD 挂载到操作系统</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-快照配置"><span class="nav-number">6.4.</span> <span class="nav-text">RBD 快照配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-镜像导出导入"><span class="nav-number">6.5.</span> <span class="nav-text">RBD 镜像导出导入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-扩容"><span class="nav-number">6.6.</span> <span class="nav-text">RBD 扩容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">6.7.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-文件系统-CephFS"><span class="nav-number">7.</span> <span class="nav-text">Ceph 文件系统 CephFS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CephFS-架构"><span class="nav-number">7.1.</span> <span class="nav-text">CephFS 架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-CephFS-MDS"><span class="nav-number">7.2.</span> <span class="nav-text">配置 CephFS MDS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-Ceph-文件系统"><span class="nav-number">7.3.</span> <span class="nav-text">部署 Ceph 文件系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建一个-Ceph-文件系统"><span class="nav-number">7.4.</span> <span class="nav-text">创建一个 Ceph 文件系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#挂载-CephFS-文件系统"><span class="nav-number">7.5.</span> <span class="nav-text">挂载 CephFS 文件系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#以-kernel-client-形式挂载-CephFS"><span class="nav-number">7.5.1.</span> <span class="nav-text">以 kernel client 形式挂载 CephFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#以-FUSE-client-形式挂载-CephFS"><span class="nav-number">7.5.2.</span> <span class="nav-text">以 FUSE client 形式挂载 CephFS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#存储文件测试并查看"><span class="nav-number">7.5.3.</span> <span class="nav-text">存储文件测试并查看</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#挂载小总结"><span class="nav-number">7.6.</span> <span class="nav-text">挂载小总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MDS主备与主主切换"><span class="nav-number">7.7.</span> <span class="nav-text">MDS主备与主主切换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置主主模式"><span class="nav-number">7.7.1.</span> <span class="nav-text">配置主主模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置备用-MDS"><span class="nav-number">7.7.2.</span> <span class="nav-text">配置备用 MDS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-Dashboard"><span class="nav-number">8.</span> <span class="nav-text">Ceph Dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-Ceph-Dashboard"><span class="nav-number">8.1.</span> <span class="nav-text">配置 Ceph Dashboard</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#修改默认配置命令"><span class="nav-number">8.2.</span> <span class="nav-text">修改默认配置命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#开启Object-Gateway管理功能"><span class="nav-number">8.3.</span> <span class="nav-text">开启Object Gateway管理功能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Promethus-Grafana-监控-Ceph"><span class="nav-number">9.</span> <span class="nav-text">Promethus+Grafana 监控 Ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-grafana"><span class="nav-number">9.1.</span> <span class="nav-text">安装 grafana</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-promethus"><span class="nav-number">9.2.</span> <span class="nav-text">安装 promethus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ceph-mgr-prometheus插件配置"><span class="nav-number">9.3.</span> <span class="nav-text">ceph mgr prometheus插件配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-promethus"><span class="nav-number">9.4.</span> <span class="nav-text">配置 promethus</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-grafana"><span class="nav-number">9.5.</span> <span class="nav-text">配置 grafana</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K8S-接入-Ceph-存储"><span class="nav-number">10.</span> <span class="nav-text">K8S 接入 Ceph 存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PV、PVC-概述"><span class="nav-number">10.1.</span> <span class="nav-text">PV、PVC 概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#POD-动态供给"><span class="nav-number">10.2.</span> <span class="nav-text">POD 动态供给</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小总结-1"><span class="nav-number">10.3.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POD-使用-RBD-做为持久数据卷"><span class="nav-number">11.</span> <span class="nav-text">POD 使用 RBD 做为持久数据卷</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装与配置"><span class="nav-number">11.1.</span> <span class="nav-text">安装与配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-rbd-provisioner"><span class="nav-number">11.1.1.</span> <span class="nav-text">配置 rbd-provisioner</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置-storageclass"><span class="nav-number">11.1.2.</span> <span class="nav-text">配置 storageclass</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-osd-pool"><span class="nav-number">11.1.3.</span> <span class="nav-text">创建 osd pool</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建k8s访问ceph的用户"><span class="nav-number">11.1.4.</span> <span class="nav-text">创建k8s访问ceph的用户</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#查看-key"><span class="nav-number">11.1.5.</span> <span class="nav-text">查看 key</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-admin-secret"><span class="nav-number">11.1.6.</span> <span class="nav-text">创建 admin secret</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在-default-命名空间创建-pvc-用于访问ceph的-secret"><span class="nav-number">11.1.7.</span> <span class="nav-text">在 default 命名空间创建 pvc 用于访问ceph的 secret</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-StorageClass"><span class="nav-number">11.2.</span> <span class="nav-text">配置 StorageClass</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试使用"><span class="nav-number">11.3.</span> <span class="nav-text">测试使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-pvc-测试"><span class="nav-number">11.3.1.</span> <span class="nav-text">创建 pvc 测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#创建-nginx-pod-挂载测试"><span class="nav-number">11.3.2.</span> <span class="nav-text">创建 nginx pod 挂载测试</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POD-使用-CephFS-做为持久数据卷"><span class="nav-number">12.</span> <span class="nav-text">POD 使用 CephFS 做为持久数据卷</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ceph-端创建-CephFS-pool"><span class="nav-number">12.1.</span> <span class="nav-text">Ceph 端创建 CephFS pool</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-cephfs-provisioner"><span class="nav-number">12.2.</span> <span class="nav-text">部署 cephfs-provisioner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置-StorageClass-1"><span class="nav-number">12.3.</span> <span class="nav-text">配置 StorageClass</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试使用-1"><span class="nav-number">12.4.</span> <span class="nav-text">测试使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建pvc测试"><span class="nav-number">12.4.1.</span> <span class="nav-text">创建pvc测试</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-日常运维管理"><span class="nav-number">13.</span> <span class="nav-text">Ceph 日常运维管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#集群监控管理"><span class="nav-number">13.1.</span> <span class="nav-text">集群监控管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PG-状态"><span class="nav-number">13.1.1.</span> <span class="nav-text">PG 状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pool状态"><span class="nav-number">13.1.2.</span> <span class="nav-text">Pool状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSD-状态"><span class="nav-number">13.1.3.</span> <span class="nav-text">OSD 状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Monitor状态和查看仲裁状态"><span class="nav-number">13.1.4.</span> <span class="nav-text">Monitor状态和查看仲裁状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#集群空间用量"><span class="nav-number">13.1.5.</span> <span class="nav-text">集群空间用量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集群配置管理-临时和全局，服务平滑重启"><span class="nav-number">13.2.</span> <span class="nav-text">集群配置管理 (临时和全局，服务平滑重启)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#查看运行配置"><span class="nav-number">13.2.1.</span> <span class="nav-text">查看运行配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tell-子命令格式"><span class="nav-number">13.2.2.</span> <span class="nav-text">tell 子命令格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#daemon子命令"><span class="nav-number">13.2.3.</span> <span class="nav-text">daemon子命令</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集群操作"><span class="nav-number">13.3.</span> <span class="nav-text">集群操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#添加和删除OSD"><span class="nav-number">13.4.</span> <span class="nav-text">添加和删除OSD</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#添加-OSD"><span class="nav-number">13.4.1.</span> <span class="nav-text">添加 OSD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#删除-OSD"><span class="nav-number">13.4.2.</span> <span class="nav-text">删除 OSD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#扩容-PG"><span class="nav-number">13.5.</span> <span class="nav-text">扩容 PG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pool-操作"><span class="nav-number">13.6.</span> <span class="nav-text">Pool 操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#获取对象副本数"><span class="nav-number">13.7.</span> <span class="nav-text">获取对象副本数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用户管理"><span class="nav-number">13.8.</span> <span class="nav-text">用户管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#增加和删除Monitor"><span class="nav-number">13.9.</span> <span class="nav-text">增加和删除Monitor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pool-配置大小"><span class="nav-number">13.10.</span> <span class="nav-text">Pool 配置大小</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常见问题"><span class="nav-number">14.</span> <span class="nav-text">常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nearfull-osd-s-or-pool-s-nearfull"><span class="nav-number">14.1.</span> <span class="nav-text">nearfull osd(s) or pool(s) nearfull</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PG-故障状态"><span class="nav-number">14.2.</span> <span class="nav-text">PG 故障状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OSD-状态-1"><span class="nav-number">14.3.</span> <span class="nav-text">OSD 状态</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Harris Li</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
