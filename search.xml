<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[21 高级调度]]></title>
    <url>%2F2020%2F03%2F24%2Fk8s-base23%2F</url>
    <content type="text"><![CDATA[污点和容忍度123451. 之前我们可以通过节点选择器,来实现Pod调度到哪一台Node节点上2. 现在可以通过 节点污点和Pod容忍度 限制 Pod 调度到指定节点3. 只有当一个Pod容忍某个Node节点的污点,这个Pod才能被调度到该节点4. 节点选择器和节点亲和性规则 都是通过再Pod中添加明确的信息 来决定Pod是否可以调度到指定节点5. 污点是再不通过修改Pod的前提下,通过再节点上添加污点信息,来拒绝Pod在指定Mode节点上部署 显示 节点 的污点信息12345678910111213141516171819202122232425262728293031323334353637383940# 默认情况下 master主节点要设置污点 # 显示节点的污点信息[root@k8s-master1 ~]# kubectl describe node k8s-masterCreationTimestamp: Mon, 09 Mar 2020 17:02:12 +0800Taints: &lt;none&gt; # 暂时没有污点信息 (二进制安装)### 显示 Pod 的污点容忍度# coredns、flannel和 nginx-ingress-controller 都是以Pod的形式不是再K8S集群当中# 如果你要求这些Pod运行在master节点上,master如果不设置任何污点信息是可以的,但是如果需要就要添加符合的污点信息# Pod的污点容忍度 需要匹配 主节点的污点信息[root@k8s-master1 ~]# kubectl describe pod coredns-6d8cfdd59d-w2xnx -n kube-system...Tolerations: CriticalAddonsOnly node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s[root@k8s-master1 ~]# kubectl describe pod kube-flannel-ds-amd64-xq7gq -n kube-system...Tolerations: :NoSchedule node.kubernetes.io/disk-pressure:NoSchedule node.kubernetes.io/memory-pressure:NoSchedule node.kubernetes.io/network-unavailable:NoSchedule node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/pid-pressure:NoSchedule node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unschedulable:NoSchedule[root@k8s-master1 ~]# kubectl describe pod nginx-ingress-controller-m8ljl -n ingress-nginx...Tolerations: node.kubernetes.io/disk-pressure:NoSchedule node.kubernetes.io/memory-pressure:NoSchedule node.kubernetes.io/network-unavailable:NoSchedule node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/pid-pressure:NoSchedule node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unschedulable:NoSchedule 了解污点效果1234567891011... node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s# 1. not-ready 和 unreachable 是描述当前节点的状态 ，如果处于该状态那么Pod可以运行在该Node上多长时间,后面跟秒数# 2. NoSchedule 和 NoExecute 都属于污点效果# 3. 每一个污点都可以关联一种效果# 4. NoSchedule : 表示如果Pod没有容忍这些污点,Pod不能调度到包含这些污点的Node节点上# 5. NoExecute : 会影响到正在运行在该Node节点上的Pod，如果在一个节点上添加了NoExecute污点，那些正在该Node节点上运行的Pod如果没有容忍这个污点,则会被从节点上去除。 在 Node 节点上添加自定义污点123456789101112131415161718192021222324252627281. 常用在一套K8S环境中,区分生产环境和测试环境2. 测试环境的Pod 不能够部署到 生产环境# 在node1添加生产环境污点信息[root@k8s-master1 ~]# kubectl taint node k8s-node1 node-type=production:NoSchedulenode/k8s-node1 tainted# 污点描述:# key = node-type# value = production# 效果 = NoSchedule[root@k8s-master1 ~]# kubectl describe node k8s-node1...Taints: node-type=production:NoSchedule...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 60m kube-proxy, k8s-node1 Starting kube-proxy. Normal Starting 60m kubelet, k8s-node1 Starting kubelet. Normal NodeHasSufficientMemory 60m kubelet, k8s-node1 Node k8s-node1 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 60m kubelet, k8s-node1 Node k8s-node1 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 60m kubelet, k8s-node1 Node k8s-node1 status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 60m kubelet, k8s-node1 Updated Node Allocatable limit across pods Warning Rebooted 60m kubelet, k8s-node1 Node k8s-node1 has been rebooted, boot id: f41c8744-b629-422b-8271-96aa873a7b73 12345678910111213# 测试部署一套没有容忍度的Pod # 看看有没有Pod部署到有污点 node-type=production:NoSchedule 的Node1上[root@k8s-master1 ~]# kubectl run test --image=busybox --replicas=5 -- sleep 99999deployment.apps/test created[root@k8s-master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-55c88c5b54-5tts2 1/1 Running 0 20s 10.244.2.103 k8s-master1 &lt;none&gt; &lt;none&gt;test-55c88c5b54-f9fgq 1/1 Running 0 20s 10.244.1.134 k8s-node2 &lt;none&gt; &lt;none&gt;test-55c88c5b54-krppr 1/1 Running 0 20s 10.244.1.136 k8s-node2 &lt;none&gt; &lt;none&gt;test-55c88c5b54-kwz8b 1/1 Running 0 20s 10.244.1.135 k8s-node2 &lt;none&gt; &lt;none&gt;test-55c88c5b54-rhxg7 1/1 Running 0 20s 10.244.2.104 k8s-master1 &lt;none&gt; &lt;none&gt; 在 Pod 上添加污点容忍度1234567891011121314151617181920212223242526272829303132333435363738# 导出 原deployment [root@k8s-master1 Taints]# kubectl get deployment test -o yaml &gt; proc-deployment.yaml# 修改 增加污点容忍度[root@k8s-master1 Taints]# vim proc-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: "1" labels: run: test name: test namespace: defaultspec: replicas: 5 selector: matchLabels: run: test template: metadata: labels: run: test spec: containers: - args: - sleep - "99999" image: busybox imagePullPolicy: Always name: test tolerations: # 添加污点容忍度 让Pod允许被调度到生产环境节点上 - key: node-type operator: Equal value: production effect: NoSchedule 12345678910111213141516171819# 部署并查看[root@k8s-master1 Taints]# kubectl delete deployment testdeployment.apps "test" deleted[root@k8s-master1 Taints]# kubectl create -f proc-deployment.yaml deployment.apps/test created[root@k8s-master1 Taints]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-5458cfd747-2p6wh 1/1 Running 0 24s 10.244.1.138 k8s-node2 &lt;none&gt; &lt;none&gt;test-5458cfd747-4xjsx 1/1 Running 0 24s 10.244.2.105 k8s-master1 &lt;none&gt; &lt;none&gt;test-5458cfd747-8tc8w 1/1 Running 0 24s 10.244.0.95 k8s-node1 &lt;none&gt; &lt;none&gt;test-5458cfd747-drm2d 1/1 Running 0 24s 10.244.1.137 k8s-node2 &lt;none&gt; &lt;none&gt;test-5458cfd747-psxtw 1/1 Running 0 24s 10.244.0.96 k8s-node1 &lt;none&gt; &lt;none&gt;# 这样就可以将Pod调度到生产环境Node1上# 如果node2是测试环境 可以增加污点信息 : node-type=test:NoSchedule # 测试环境的Pod 也要加上污点容忍度 指向测试环境 node-type=test:NoSchedule 污点和污点容忍度的使用场景1234567891011121314151617181920212223242526272829301. 区分生产环境和测试环境2. 区分不同团队部署的不同项目3. 区分硬件资源 如SSD或者其他资源 4. 配置节点失效之后的Pod重新调度最长等待时间,每个Pod都可以配置某种污点的效果等待时间5. 当k8s检测到Node出现 not-ready 或者 unreachable状态的时候,将会等待300秒,如果状态持续的话,会将该Pod调度到其他节点上6. 如果没有设置这两个状态，是会自动添加到Pod中的,如果5分钟太长,可以修改该值[root@k8s-master1 Taints]# kubectl describe pod test-5458cfd747-2p6whTolerations: node-type=production:NoSchedule node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s[root@k8s-master1 Taints]# kubectl edit pod test-5458cfd747-2p6wh... tolerations: - effect: NoSchedule key: node-type operator: Equal value: production - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300... 测试二进制安装 K8S 重启后污点是否使用1234567891011121314151617181920211. 由于二进制安装的Node都默认没有污点,测试设置污点后重启Node之前的组件Pod是否可以重建2. 看起来正常，后续应该多测试[root@k8s-master1 ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdefault nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 16 10d 10.244.0.97 k8s-node1 &lt;none&gt; &lt;none&gt;default test-5458cfd747-2p6wh 1/1 Running 1 19m 10.244.1.141 k8s-node2 &lt;none&gt; &lt;none&gt;default test-5458cfd747-4xjsx 1/1 Running 1 19m 10.244.2.108 k8s-master1 &lt;none&gt; &lt;none&gt;default test-5458cfd747-8tc8w 1/1 Running 1 19m 10.244.0.99 k8s-node1 &lt;none&gt; &lt;none&gt;default test-5458cfd747-drm2d 1/1 Running 1 19m 10.244.1.140 k8s-node2 &lt;none&gt; &lt;none&gt;default test-5458cfd747-psxtw 1/1 Running 1 19m 10.244.0.100 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-6txhq 1/1 Running 28 21d 172.31.228.70 k8s-node2 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-jqxrm 1/1 Running 27 21d 172.31.228.69 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-m8ljl 1/1 Running 27 21d 172.31.228.67 k8s-master1 &lt;none&gt; &lt;none&gt;kube-system coredns-6d8cfdd59d-w2xnx 1/1 Running 23 19d 10.244.2.107 k8s-master1 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-q29r8 1/1 Running 28 21d 172.31.228.70 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-xh7c4 1/1 Running 28 21d 172.31.228.69 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-xq7gq 1/1 Running 29 21d 172.31.228.67 k8s-master1 &lt;none&gt; &lt;none&gt;kube-system metrics-server-7dbbcf4c7-6qrsb 1/1 Running 2 2d7h 10.244.1.139 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-566cddb686-t8fw4 1/1 Running 23 19d 10.244.2.106 k8s-master1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-c4bc5bd44-4prrf 1/1 Running 27 21d 10.244.0.98 k8s-node1 &lt;none&gt; &lt;none&gt; 删除 Node上的污点信息1234567[root@k8s-master1 Taints]# kubectl taint nodes k8s-node1 node-type=test:NoSchedule-node/k8s-node1 untainted[root@k8s-master1 Taints]# kubectl describe node k8s-node1...Taints: &lt;none&gt;... 通过 节点亲和性 调度Pod到指定节点123451. 节点亲和性(node affinity) 对比 污点是更新的一种技术, 他用来告诉K8S将Pod只调度到某几个节点上2. 初始的节点亲和性 就是 NodeSelector 也就是节点选择器 必须和Node的标签Label互相匹配3. 节点亲和性功能更加强大,节点选择器将来可能被弃用4. 每个Pod都可以自定义自己的节点亲和性规则，用来指定硬性限制或者偏好5. 告诉K8S 我的Pod 更倾向于部署到哪类的Node上,K8S会尽量满足需求,如果无法满足,会将Pod调度到其他节点上。 检查默认的节点标签123456789101112131. 节点亲和性 也是根据Node节点的标签进行选择 ，和节点选择器一样2. 检查下Node里的默认标签3. 可以通过我们自定义的disk=ssd ,当然还可以增加 机房位置,服务器分区等其他标签[root@k8s-master1 Taints]# kubectl describe node k8s-node1...Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux disk=ssd # 之前学习标签时候自定义添加的 kubernetes.io/arch=amd64 kubernetes.io/hostname=k8s-node1 kubernetes.io/os=linux... 指定强制性 节点亲和性 规则1234567891011121314151617181920212223[root@k8s-master1 Taints]# vim kubia-gpu-nodeaffinity.yamlapiVersion: v1kind: Podmetadata: name: kubia-gpuspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: gpu operator: In values: - "true" containers: - image: 172.31.228.68/project/kubia name: kubia[root@k8s-master1 Taints]# kubectl create -f kubia-gpu-nodeaffinity.yaml pod/kubia-gpu created 节点亲和性 属性定义12345678910111213141. affinity / nodeAffinity2. requiredDuringScheduling... 该字段下定义的规则,为了让Pod能调度到这个节点上,明确定义了该节点必须包含的标签3. ...IgnoredDuringExecution 该字段下定义的规则,不会影响已经在Node节点上运行的Pod4. 亲和性规则影现在被调度的Pod，而不会导致之前部署的Pod被剔除的风险,需要使用IgnoredDuringExecution结尾5. nodeSelectorTerms 和 matchExpressions 定义了节点标签必须满足哪一种表达式6. 当前例子的含义: 节点中必须包含一个 gpu 的标签 ， 并且这个标签的值必须是 true[root@k8s-master1 Taints]# kubectl get nodes -L gpuNAME STATUS ROLES AGE VERSION GPUk8s-master1 Ready &lt;none&gt; 21d v1.16.0 k8s-node1 Ready &lt;none&gt; 21d v1.16.0 k8s-node2 Ready &lt;none&gt; 21d v1.16.0 true# 一个Pod的节点亲和性 指定了节点必须包含哪些标签，才能满足Pod调度的条件 调度Pod时优先考虑哪些节点1231. 大型容器平台不同机房或者地域2. 提前预留出的资源良好的服务器3. 基础的两个标签: 区域 和 独占节点还是共享节点 给 node 节点加上标签1234Region : 区域share : 共享 dedicated : 专用模式shared ： 共享模式 123456789101112131415161718192021222324252627[root@k8s-master1 Taints]# kubectl label node k8s-node1 Region=zone1node/k8s-node1 labeled[root@k8s-master1 Taints]# kubectl label node k8s-node2 Region=zone2node/k8s-node2 labeled[root@k8s-master1 Taints]# kubectl label node k8s-node1 share=dedicatednode/k8s-node1 labeled[root@k8s-master1 Taints]# kubectl label node k8s-node2 share=sharednode/k8s-node2 labeled# 删除标签[root@k8s-master1 Taints]# kubectl label node k8s-node2 Region-node/k8s-node2 labeled[root@k8s-master1 Taints]# kubectl label node k8s-node1 Region-node/k8s-node2 labeled# 重新添加[root@k8s-master1 Taints]# kubectl label node k8s-node2 region=zone2node/k8s-node2 labeled[root@k8s-master1 Taints]# kubectl label node k8s-node1 region=zone1node/k8s-node1 labeled# 查看[root@k8s-master1 Taints]# kubectl get nodes -L region -L shareNAME STATUS ROLES AGE VERSION REGION SHAREk8s-master1 Ready &lt;none&gt; 21d v1.16.0 k8s-node1 Ready &lt;none&gt; 21d v1.16.0 zone1 dedicatedk8s-node2 Ready &lt;none&gt; 21d v1.16.0 zone2 shared 指定优先级 节点亲和性 规则123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 Taints]# vim preferred-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: prefspec: replicas: 10 selector: matchLabels: app: pref template: metadata: labels: app: pref spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 preference: matchExpressions: - key: region operator: In values: - zone1 - weight: 20 preference: matchExpressions: - key: share operator: In values: - dedicated containers: - args: - sleep - "99999" image: busybox name: main 1234567891011121314151617181920211. 节点优先调度到 zone1 80权重2. 次要优先级到 dedicated 20权重3. 当前工作模式如果我们有多个节点比如4个 优先最高的是 zone1 和 dedicated 然后是 zone1 和 shared ，再然后是 其他区域的 dedicated ，最低优先级是剩下的条件[root@k8s-master1 Taints]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpref-69f48f9b59-4wk8h 1/1 Running 0 2m18s 10.244.0.111 k8s-node1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-59h24 1/1 Running 0 2m18s 10.244.2.114 k8s-master1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-77cgj 1/1 Running 0 2m18s 10.244.1.150 k8s-node2 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-bxfqs 1/1 Running 0 2m18s 10.244.0.114 k8s-node1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-gsqsc 1/1 Running 0 2m18s 10.244.0.112 k8s-node1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-klblh 1/1 Running 0 2m18s 10.244.0.110 k8s-node1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-kpbzz 1/1 Running 0 2m18s 10.244.2.115 k8s-master1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-s5mg5 1/1 Running 0 2m18s 10.244.1.149 k8s-node2 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-w4c8j 1/1 Running 0 2m18s 10.244.0.115 k8s-node1 &lt;none&gt; &lt;none&gt;pref-69f48f9b59-x4csj 1/1 Running 0 2m18s 10.244.0.113 k8s-node1 &lt;none&gt; &lt;none&gt;# 为什么 k8s-master1 和 k8s-node2 还有Pod可以分配1. 除了节点亲和性优先级函数，调度器还使用其他优先级函数来决定如何分配,其中还有Selector SpreadPriority函数2. 该函数确保属于同一个RS或者Servcie的Pod，将分散部署到不同节点上,以免出现单点问题3. 如果没有添加节点亲和性优先级配置,那么Pod会均匀的分配到3个工作节点上 使用 Pod亲和性 和 非亲和性 对Pod进行协同部署123451. 上面的操作都是在pod和node之间的亲和性来分配Pod到指定的节点上2. pod也可以与pod之间产生 Pod亲和性 3. 例如1个前端Pod和1个后端Pod，将这两个Pod部署在比较靠近的区域,可以降低延迟，提高应用的性能4. 通过节点亲和性可以将这两个Pod调度到同一个区域,或者同一个机架的Node服务器上5. K8S 还可以通过Pod亲和性,让这两个Pod部署合适的位置，并且确保他们是靠近的 使用Pod亲和性 将多个Pod部署在同一个节点上123456789# 先部署后端Pod# 添加了标签 app=backend 这个标签将在前端的pod亲和性里面使用[root@k8s-master1 Taints]# kubectl run backend -l app=backend --image busybox -- sleep 99999deployment.apps/backend created[root@k8s-master1 Taints]# kubectl get pods -L appNAME READY STATUS RESTARTS AGE APPbackend-75d54556d4-ns276 1/1 Running 0 25s backend 12345678910111213141516171819202122232425262728293031323334[root@k8s-master1 Taints]# vim frontend-podaffinity-host.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: frontendspec: replicas: 5 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend containers: - name: main image: busybox args: - sleep - "99999"# requiredDuringScheduling 强制要求# podAffinity Pod亲和性# 这个Pod必须调度到匹配 Pod选择器的节点上 app: backend # Pod亲缘性 允许 Pod 被调度到那些包含指定标签Pod所在的节点 123456789101112# 查看 全部都被调度到同一个节点[root@k8s-master1 Taints]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESbackend-75d54556d4-ns276 1/1 Running 0 8m22s 10.244.1.151 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-7bfc78d68d-46lf8 1/1 Running 0 16s 10.244.1.155 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-7bfc78d68d-6h82c 1/1 Running 0 16s 10.244.1.153 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-7bfc78d68d-m5fsz 1/1 Running 0 16s 10.244.1.154 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-7bfc78d68d-mrdxr 1/1 Running 0 16s 10.244.1.156 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-7bfc78d68d-pcd6k 1/1 Running 0 16s 10.244.1.152 k8s-node2 &lt;none&gt; &lt;none&gt;# 在调度前端Pod时,调度器最先找到所有匹配前端Pod的podAffinity中配置的 labelSelector 的 Pod # 之后将前端Pod都调度到同一节点 将Pod部署在同一个区域或者同一个机柜1234567# 前面的例子将所有Pod都部署到了同一个Node上，可能存在单点问题# 比如我们拥有20台服务器 每5台服务器 在一个机柜 那么机柜就是 1 2 3 4 # 每天Node上都设置rack=机柜(1-4) # 调度器会先去查询Pod的 podAffinity,需要定义 topologyKey 设置为 rack # 如果前端的rack=2 那么后端的Pod 就会先考虑rack=2的机柜来部署Pod # podAffinity中的topologyKey决定了Pod被调度的范围 Pod亲和性优先级12345678910111213141516171819202122232425262728293031323334353637383940414243441. 告诉K8S 优先调度到相同节点上2. 如果无法满足也可以到其他节点3. 根据调度器的其他调度函数,尽量会在同一节点上部署,也会有调度到其他节点上的以免单点[root@k8s-master1 logs]# vim frontend-podaffinity-preferred-host.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: frontendspec: replicas: 5 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend containers: - name: main image: busybox args: - sleep - "99999"[root@k8s-master1 Taints]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESbackend-75d54556d4-ns276 1/1 Running 0 35m 10.244.1.151 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-6747f764d7-7t6lx 1/1 Running 0 24s 10.244.1.157 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-6747f764d7-8kf4r 1/1 Running 0 24s 10.244.1.159 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-6747f764d7-dlkr7 1/1 Running 0 24s 10.244.1.158 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-6747f764d7-dmspl 1/1 Running 0 24s 10.244.2.116 k8s-master1 &lt;none&gt; &lt;none&gt;frontend-6747f764d7-j2cl5 1/1 Running 0 24s 10.244.0.116 k8s-node1 &lt;none&gt; &lt;none&gt; 利用 Pod 的非亲和性 分开调度Pod123451. 如果希望Pod之间远离彼此, 这种特性叫做非亲和性2. 调度器永远不会将Pod调度到 非亲和的Pod所在的节点上3. 有两个Pod业务都比较占用资源, 放在同一个Node上会影响彼此4. 按照可用区分配业务,一个可用区出现问题,另外一个可用区的服务还在5. 使用 podAntiAffinity 替换 podAffinity 1234567891011121314151617181920212223242526272829[root@k8s-master1 Taints]# vim podAntiAffinity.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: frontendspec: replicas: 5 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: frontend containers: - name: main image: busybox args: - sleep - "99999" 12345678910[root@k8s-master1 Taints]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESbackend-75d54556d4-ns276 1/1 Running 0 86m 10.244.1.151 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-b4667c656-hg26f 0/1 Pending 0 64s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;frontend-b4667c656-hzl2b 1/1 Running 0 64s 10.244.1.170 k8s-node2 &lt;none&gt; &lt;none&gt;frontend-b4667c656-mcd7j 1/1 Running 0 64s 10.244.2.120 k8s-master1 &lt;none&gt; &lt;none&gt;frontend-b4667c656-tcmlk 0/1 Pending 0 64s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;frontend-b4667c656-wmrb8 1/1 Running 0 64s 10.244.0.124 k8s-node1 &lt;none&gt; &lt;none&gt;# 出现 Pending状态 调度器不允许Pod部署在亲和性Pod的节点上]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20 计算资源管理]]></title>
    <url>%2F2020%2F03%2F24%2Fk8s-base22%2F</url>
    <content type="text"><![CDATA[为Pod中的容器申请资源12request 资源请求limits 资源限制 创建包含 requests 请求资源的 Pod123456789101112131415[root@k8s-master1 request]# vim requests-pod.yaml apiVersion: v1kind: Podmetadata: name: requests-podspec: containers: - name: main image: busybox command: ["dd","if=/dev/zero","of=/dev/null"] resources: requests: cpu: 0.5 memory: 1000Mi 1234567891011Mem: 1180484K used, 2699924K free, 1008K shrd, 36844K buff, 666140K cachedCPU0: 27.7% usr 65.0% sys 0.0% nic 6.9% idle 0.0% io 0.0% irq 0.1% sirqCPU1: 5.6% usr 7.6% sys 0.0% nic 86.1% idle 0.6% io 0.0% irq 0.0% sirqLoad average: 1.02 0.54 0.30 3/353 10 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1284 0.0 0 50.0 dd if /dev/zero of /dev/null 6 0 root R 1292 0.0 1 0.0 top# dd命令会持续消耗尽可能多的cpu 我的node主机是2核心cpu,那么它应该是跑满了1个U # 说明该Pod的实际cpu消耗 超出了 requests 申请的 0.5 个 cpu# requests 不会限制容器可以使用的cpu数量 资源 requests 如何影响调度1234567# 调度器判断一个Pod是否适合调度到某个节点1. 调度器不关注Pod的实际资源使用量,只关心节点上部署的所有Pod的资源申请量之和2. 调度器首先会对节点列表进行过滤,排除不满足需求的节点,然后根据预先设置的优先级函数对其余节点进行排序3. 有2个基于资源请求量的优先级排序函数,LeastRequestedPriority 和 MostRequestedPriority4. LeastRequestedPriority 会将Pod调度到请求量少的节点上(拥有更多未分配资源的节点) 5. MostRequestedPriority 会将Pod调度到请求量最多的节点,在云主机环境可以使资源更紧凑的使用,节省开销6. 默认只能配置一种优先级函数 查看节点资源总量1234567891011121314151617[root@k8s-master1 request]# kubectl describe node k8s-node2Capacity: # 节点的资源总量 cpu: 2 ephemeral-storage: 41147472Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 3880408Ki pods: 110Allocatable: # 可分配给Pod的资源总量 cpu: 2 ephemeral-storage: 37921510133 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 3778008Ki pods: 110 模拟超出请求资源总量123456# 模拟现在有一个申请3个CPU的Pod 超出我们的node资源总量# 事件会告诉我们没有足够的cpu Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 Insufficient cpu. 限制容器的可用资源设置容器可用资源的硬限制12# cpu是可压缩资源,内存是不可压缩资源# 如果不限制内存使用量,工作节点上的容器可能会吃掉所有内存，恶意Pod占用内存,可能导致其他Pod无法分配到该Node 创建包含 limits 限制资源的 Pod1234567891011121314151617181920212223242526[root@k8s-master1 request]# vim limits-pod.yaml apiVersion: v1kind: Podmetadata: name: limits-podspec: containers: - name: main image: busybox command: ["dd","if=/dev/zero","of=/dev/null"] resources: limits: cpu: 1 memory: 1000Mi# 如果不填写requests,那么会被设置为limits相同的值[root@k8s-master1 request]# kubectl describe pod limits-pod... Limits: cpu: 1 memory: 1000Mi Requests: cpu: 1 memory: 1000Mi... 超过 limits123451. 如果进程尝试申请比限额更多的内存时,会被杀掉(OOM)2. 如果Pod的重启策略是 Always，进程将会被重启3. 如果持续这种状态 一边超出 一边重启 会被增加下次的重启时间 4. Pod 会处于 CrashLoopBackOff 状态5. 如果不想被OOM ,内存的Limits就不应该被设置的太低 容器中的应用如何看待 limits123456789101112131415Mem: 1247588K used, 2632820K free, 1008K shrd, 77436K buff, 681992K cachedCPU0: 0.0% usr 0.0% sys 0.0% nic 100% idle 0.0% io 0.0% irq 0.0% sirqCPU1: 20.0% usr 80.0% sys 0.0% nic 0.0% idle 0.0% io 0.0% irq 0.0% sirqLoad average: 1.13 1.08 0.84 4/351 10 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1284 0.0 1 49.9 dd if /dev/zero of /dev/null 6 0 root R 1292 0.0 0 0.0 top# 在容器中看到的内存数据,始终会是节点内存,而不是容器本身的内存1. 即使你设置了最大可用内存,top命令显示的是运行该容器的节点的内存数量,而容器无法感知到此限制2. 对于Java程序 需要设置 -Xmx ，虽然设置了堆大小,但是没有管住 off-heap内存3. 新版本的JAVA会考虑到容器的limits 缓解该问题4. cpu也同样,即使我们限制了 cpu配额为1，容器中也无法只显示暴露1个核5. 如果程序通过查询CPU核数来启动线程数量，可能就不够友好，造成启动大量线程争取资源问题6. 不能依赖应用程序从系统获取CPU数量，可能需要Downward限制传递给容器,并且使用这个值给程序 了解 Pod QoS 等级123456# 当节点无法提供所有Pod指定的资源的Limis之和怎么办1. 假设有两个Pod PodA使用了节点的90% PodB突然使用到了比之前更多内存2. 这个时候节点无法提供足够的内存，那么意味着哪个Pod会被杀死3. 是PodB被杀掉 因为无法满足他的内存使用 还是释放PodA 让PodB运行4. K8S 无法直接帮我们选择,需要设定哪种Pod在该场景的优先级更高5. K8S 将Pod 划分成3个Qos等级 (服务质量管理) 1231. BestEffort (优先级最低)2. Burstable 3. Guaranteed (优先级最高) BestEffort 等级12341. 最低优先级的BestEffort 会分配给没有(为任何容器)设置任何requests和limits的Pod 2. 这个等级运行的容器没有任何资源保障,在最坏的情况下,他们不会被分配任何CPU时间3. 同时,在需要为其他Pod释放资源内存时,这些容器会被第一批杀死4. 好处是,由于他们没有设置 内存的Limits，当有充足可用内存时,这些容器可以使用任意多的内存 Guaranteed 等级123451. CPU和内存都要设置 requests和limits 2. 每个容器都需要设置资源量3. 每个容器的每种资源的requests和limits 都必须相等4. 如果只设定limits，那么requests和limits会相等,所以设置一个容器的限制量limits,就可以让这个Pod的等级为Guaranteed5. Guaranteed 的 Pod 可以使用它申请的资源的等额资源，但是无法消耗更多的资源(requests和limits相同) Burstable 等级123451. Burstable 介于前两者之间,其他的Pod都属于这个等级2. requests和limits 不相同的Pod ，3. 有一个容器定义了request但是没有定义limits的Pod 4. 在多容器的Pod中,1个容器定义了requests和limits , 另外一个确没有定义5. Burstable 的 Pod 可以获得申请的等额资源,并可以使用额外的资源(超出limits) 查看Pod的等级1234[root@k8s-master1 request]# kubectl describe pod limits-pod...QoS Class: Guaranteed... 内存不足时 哪类进程会先被杀死12341. BestEffort (优先级最低) 最先被杀掉2. Burstable 其次3. Guaranteed (优先级最高) 最后4. 如果是两个同级的Burstable，系统会根据Pod的实际使用率占内存申请量比率最高的 为命名空间的Pod设置 requests 和 limitsLimitRange1# 通过该资源为命名空间下的Pod配置资源限制 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 request]# vim limits.yamlapiVersion: v1kind: LimitRangemetadata: name: examplespec: limits: - type: Pod # 整个Pod min: # Pod中所有容器的CPU和内存的请求量之和的 最小值 cpu: 100m memory: 100Mi max: # Pod中所有容器的CPU和内存的请求量之和的 最大值 cpu: 1 memory: 1Gi - type: Container # 指定容器的资源限制 defaultRequest: # 容器没有指定CPU或内存请求量时的默认值 cpu: 100m memory: 100Mi default: # 容器没有指定 limits 时的默认值 cpu: 200m memory: 200Mi min: # 容器的cpu和内存 requests和limits 最小值和最大值 cpu: 100m memory: 100Mi max: cpu: 1 memory: 1Gi maxLimitRequestRatio: # 每种资源requests和limits的最大比值 cpu: 4 memory: 10 - type: PersistentVolumeClaim # 请求pvc容量的最大值和最小值 min: storage: 1Gi max: storage: 10Gi 1234567[root@k8s-master1 request]# vim limits.yaml[root@k8s-master1 request]# kubectl create -f limits.yaml limitrange/example created[root@k8s-master1 request]# kubectl get limitrangeNAME CREATED ATexample 2020-03-27T10:23:28Z 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 测试限制# 创建一个要求3CPU的Pod[root@k8s-master1 request]# vim limits-pod-too-big.yamlapiVersion: v1kind: Podmetadata: name: too-bigspec: containers: - image: busybox args: ["sleep", "9999999"] name: main resources: requests: cpu: 3[root@k8s-master1 request]# kubectl create -f limits-pod-too-big.yaml The Pod "too-big" is invalid: spec.containers[0].resources.requests: Invalid value: "2": must be less than or equal to cpu limit# 修改一下 不配置资源请求 看看是不是加上默认值[root@k8s-master1 request]# vim limits-pod-too-big.yamlapiVersion: v1kind: Podmetadata: name: too-bigspec: containers: - image: busybox args: ["sleep", "9999999"] name: main[root@k8s-master1 request]# kubectl describe pod too-big... Limits: cpu: 200m memory: 200Mi Requests: cpu: 100m memory: 100Mi...# QoS Class: Burstable 12345# 总结:1. Limitrange 只是限制Pod和容器的资源申请 2. ResourceQuota 限制命名空间的资源3. 可以在不同的命名空间中增加 Limitrange 限制资源的使用 比如生产环境和测试环境的Pod资源限制,但是无法限制整体使用多少资源4. ResourceQuota 可以通过限制命名空间的资源,以免大量Pod吃掉整个集群的所有资源 ResourceQuota1234567891011121314151. Limitrange应用于单独的Pod，ResourceQuota 应用于命名空间中所有的Pod [root@k8s-master1 request]# vim quota-cpu-memory.yamlapiVersion: v1kind: ResourceQuotametadata: name: cpu-and-memspec: hard: requests.cpu: 500m requests.memory: 500Mi limits.cpu: 1 limits.memory: 1024Mi 12345678910111213141516171819202122232425262728293031[root@k8s-master1 request]# kubectl create -f quota-cpu-memory.yaml resourcequota/cpu-and-mem created[root@k8s-master1 request]# kubectl describe quotaName: cpu-and-memNamespace: defaultResource Used Hard-------- ---- ----limits.cpu 0 1limits.memory 0 1Girequests.cpu 0 500mrequests.memory 0 500Mi# 重新创建默认pod看看是否增加变化[root@k8s-master1 request]# kubectl create -f limits-pod-too-big.yaml pod/too-big created[root@k8s-master1 request]# kubectl get podNAME READY STATUS RESTARTS AGEtoo-big 1/1 Running 0 17s[root@k8s-master1 request]# kubectl describe quotaName: cpu-and-memNamespace: defaultResource Used Hard-------- ---- ----limits.cpu 200m 1limits.memory 200Mi 1Girequests.cpu 100m 500mrequests.memory 100Mi 500Mi 123456# 总结1. ResourceQuota 和 LimitRange 应该一起创建,如果我们没有创建LimitRange，不指定任何资源限制的pod比如上一个，会无法创建成功2. 也就是既然有了指定的配额,就必须为Pod创建 requests和limits，否则API不会接收Pod的创建请求\3. 创建这些资源时,需要指定命名空间创建,否则会在默认的命名空间中创建4. Quota 定额5. limit 限制 监控 Pod 的资源使用量121. Kubernetes 中的组件 Metrics Server 持续采集所有 Pod 副本的指标数据。 (heapster 在k8s 1.13 版本后 被弃用了)2. cadvisor 采集容器的资源利用率: CPU、内存 的使用率 部署 Metrics Server12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master1 demo]# unzip metrics-server.zip [root@k8s-master1 demo]# cd metrics-server# 修改参数:... containers: - name: metrics-server image: lizhenliang/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-insecure-tls # 忽略证书 - --kubelet-preferred-address-types=InternalIP # 找到nodeIP，默认使用注册节点名称,修改成IP...# 部署[root@k8s-master1 metrics-server]# kubectl apply -f . [root@k8s-master1 metrics-server]# kubectl get pods -n kube-system# 检查是否正常工作[root@k8s-master1 metrics-server]# kubectl logs metrics-server-7dbbcf4c7-vzdn2 -n kube-system[root@k8s-master1 metrics-server]# kubectl get apiservice...v1beta1.metrics.k8s.io kube-system/metrics-server True 86s...# 获取资源利用率 metrics-server 工作正常 采集到了数据[root@k8s-master1 metrics-server]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master1 118m 5% 1169Mi 31% k8s-node1 81m 4% 543Mi 14% k8s-node2 83m 4% 545Mi 14% # 显示Pod的资源使用情况[root@k8s-master1 metrics-server]# kubectl top podsNAME CPU(cores) MEMORY(bytes) nfs-client-provisioner-56f4b98d47-v4nf6 1m 6Mi [root@k8s-master1 metrics-server]# kubectl top pods --all-namespacesNAMESPACE NAME CPU(cores) MEMORY(bytes) default nfs-client-provisioner-56f4b98d47-v4nf6 1m 6Mi ingress-nginx nginx-ingress-controller-6txhq 4m 111Mi ingress-nginx nginx-ingress-controller-jqxrm 4m 109Mi ingress-nginx nginx-ingress-controller-m8ljl 4m 107Mi kube-system coredns-6d8cfdd59d-w2xnx 3m 12Mi kube-system kube-flannel-ds-amd64-q29r8 2m 13Mi kube-system kube-flannel-ds-amd64-xh7c4 2m 9Mi kube-system kube-flannel-ds-amd64-xq7gq 2m 11Mi kube-system metrics-server-7dbbcf4c7-6qrsb 1m 13Mi kubernetes-dashboard dashboard-metrics-scraper-566cddb686-t8fw4 1m 11Mi kubernetes-dashboard kubernetes-dashboard-c4bc5bd44-4prrf 2m 17Mi # 如果 top pod命令拒绝，有可能是收集工作正在执行,等等再看即可]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19 Statefulset 部署有状态应用]]></title>
    <url>%2F2020%2F03%2F24%2Fk8s-base21%2F</url>
    <content type="text"><![CDATA[使用 Statefulset 部署应用12345678# 区别:1. 无状态应用 无需持久化数据，实例挂掉没有影响,重新拉起,用户无感知2. 有状态应用 每个实例都需要有自己独立的持久化存储,实例挂掉，新的实例必须有就实例保持一致的状态和标识（相同的名称和网络标识）# 有状态应用提供:1. 稳定的网络标识:Statefulset创建的每个Pod都由一个从0开始的顺序索引，在pod的名称和主机名上,还有Pod的存储上2. 提供稳定的专属存储：Statefulset的每个Pod都需要关联到不同的持久卷声明(pvc),也就是自己独自专属的持久卷3. 需要再Pod中添加 卷声明模板(volumeClaimTemplates) 12345# 持久卷的创建和删除1. 当一个声明(pvc)被删除,与之绑定的持久卷就会被回收或者删除,数据就会被丢失2. 缩容 Statefulset 只会删除一个Pod，而声明(pvc)会遗留下来3. 当你需要释放持久卷时,需要手动删除对应的持久卷声明4. 缩容时不删除pvc，扩容时可以重新挂载上 创建应用和容器镜像12# 该应用接收到POST请求时,会把Body数据写入 /var/data/kubia.txt中# 收到Get请求时，返回主机名和存储数据(文件中的内容) 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master2 kubia-pet-image]# vim app.js const http = require('http');const os = require('os');const fs = require('fs');const dataFile = "/var/data/kubia.txt";function fileExists(file) &#123; try &#123; fs.statSync(file); return true; &#125; catch (e) &#123; return false; &#125;&#125;var handler = function(request, response) &#123; if (request.method == 'POST') &#123; var file = fs.createWriteStream(dataFile); file.on('open', function (fd) &#123; request.pipe(file); console.log("New data has been received and stored."); response.writeHead(200); response.end("Data stored on pod " + os.hostname() + "\n"); &#125;); &#125; else &#123; var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet"; response.writeHead(200); response.write("You've hit " + os.hostname() + "\n"); response.end("Data stored on this pod: " + data + "\n"); &#125;&#125;;var www = http.createServer(handler);www.listen(8080); 12345678[root@k8s-master2 kubia-pet-image]# vim Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT ["node", "app.js"][root@k8s-master2 kubia-pet-image]# docker build -t 172.31.228.68/game/kubia-pet .[root@k8s-master2 kubia-pet-image]# docker push 172.31.228.68/game/kubia-pet 部署有状态应用123456# 需要创建的对象1. 存储数据的持久卷(如果不支持动态供给,还需要手动创建)2. Statefulset 的 Service3. Statefulset 控制器# 对于每一个Pod，Statefulset 都会创建一个绑定到一个持久卷上的持久卷声明 手动创建 PV 持久化存储卷12345678# nfs共享存储里创建好目录[root@k8s-master2 nfs]# mkdir -p pv-&#123;a..c&#125;[root@k8s-master2 nfs]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Mar 24 10:58 pv-adrwxr-xr-x 2 root root 4096 Mar 24 10:58 pv-bdrwxr-xr-x 2 root root 4096 Mar 24 10:58 pv-c 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@k8s-master1 statefulset]# vim pv-nfs.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv-aspec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /data/nfs/pv-a server: 172.31.228.68---apiVersion: v1kind: PersistentVolumemetadata: name: pv-bspec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /data/nfs/pv-b server: 172.31.228.68---apiVersion: v1kind: PersistentVolumemetadata: name: pv-cspec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /data/nfs/pv-c server: 172.31.228.68# 创建 pv-a pv-b pv-c 三个持久卷 # 回收策略 Recycle 当卷的声明被释放后,空间会被回收再利用[root@k8s-master1 statefulset]# kubectl create -f pv-nfs.yaml persistentvolume/pv-a createdpersistentvolume/pv-b createdpersistentvolume/pv-c created[root@k8s-master1 statefulset]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-a 2Gi RWX Recycle Available 6spv-b 2Gi RWX Recycle Available 6spv-c 2Gi RWX Recycle Available 6s 创建 Headless Service123# 常规 Service：一组POD的访问策略，提供负载均衡服务发现# Headless Service: 不需要Cluster-IP，他会直接绑定到POD IP# 没有IP 使用DNS保证网络唯一标识符 coredns 123456789101112131415161718[root@k8s-master1 statefulset]# vim kubia-service-headless.yamlapiVersion: v1kind: Servicemetadata: name: kubiaspec: clusterIP: None selector: app: kubia ports: - name: http port: 80[root@k8s-master1 statefulset]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 14dkubia ClusterIP None &lt;none&gt; 80/TCP 3s 创建 Statefulset123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 statefulset]# vim kubia-statefulset.yaml apiVersion: apps/v1kind: StatefulSetmetadata: name: kubiaspec: serviceName: kubia replicas: 2 selector: matchLabels: app: kubia # Pod 标签 app: kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: 172.31.228.68/game/kubia-pet ports: - name: http containerPort: 8080 volumeMounts: - name: data mountPath: /var/data # 容器挂载目录 volumeClaimTemplates: # 持久卷声明模板 - metadata: name: data spec: # storageClassName: "managed-nfs-storage" 自动供给，本次是手动 resources: requests: storage: 1Gi accessModes: - ReadWriteMany 123# volumeClaimTemplates 持久卷声明模板1. 定义1个 data 卷声明，根据这个模板为每个Pod都创建一个持久卷声明(pvc)2. statefulset 创建Pod时,会自动将PVC添加到Pod描述中 查看 生成的有状态 Pod12345678[root@k8s-master1 statefulset]# kubectl create -f kubia-statefulset.yaml [root@k8s-master1 statefulset]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-0 1/1 Running 0 5skubia-1 1/1 Running 0 3s1. 第二个Pod会在第一个Pod运行处于就绪状态后创建2. 依次创建启动每个Pod，以免竞争 1234567891011[root@k8s-master1 statefulset]# kubectl get pod kubia-0 -o yaml... volumeMounts: - mountPath: /var/data # 挂载点 name: data... volumes: # 创建的持久卷声明 - name: data persistentVolumeClaim: claimName: data-kubia-0 # 名称 = volumeClaimTemplates的name + pod name... 1234567891011# 查看 PVC[root@k8s-master1 statefulset]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-kubia-0 Bound pv-c 2Gi RWX 48sdata-kubia-1 Bound pv-a 2Gi RWX 47s[root@k8s-master1 statefulset]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-a 2Gi RWX Recycle Bound default/data-kubia-1 2m17spv-b 2Gi RWX Recycle Available 2m17spv-c 2Gi RWX Recycle Bound default/data-kubia-0 2m17s 使用 Pod 测试数据123456789101112131415161718192021222324# 通过API服务器与Pod通信# 创建一个新代理[root@k8s-master1 statefulset]# kubectl proxyStarting to serve on 127.0.0.1:8001[root@k8s-master1 ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/You've hit kubia-0Data stored on this pod: No data posted yet# 请求被正确收到 # 发送一个POST请求[root@k8s-master1 ~]# curl -X POST -d "Hey there! to kubia-0" localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/Data stored on pod kubia-0# 再来Get请求看看能否查看数据[root@k8s-master1 ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/You've hit kubia-0Data stored on this pod: Hey there! to kubia-0# 看看对应的存储里面的数据# data-kubia-0 Bound pv-c [root@k8s-master2 pv-c]# cat kubia.txtHey there! to kubia-0 123456789101112131415161718192021# 测试下其他集群节点数据 kubia-1# 没有数据正确[root@k8s-master1 statefulset]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/You've hit kubia-1Data stored on this pod: No data posted yet# 插入数据到 kubia-1[root@k8s-master1 statefulset]# curl -X POST -d "Hey there! to kubia-1" localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/Data stored on pod kubia-1# 每个节点的存储都保存自己的数据[root@k8s-master1 statefulset]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/You've hit kubia-1Data stored on this pod: Hey there! to kubia-1[root@k8s-master1 statefulset]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/You've hit kubia-0Data stored on this pod: Hey there! to kubia-0[root@k8s-master2 pv-a]# cat kubia.txt Hey there! to kubia-1 删除一个有状态Pod12345678910111213141516171819202122232425262728293031# 删除一个有状态Pod 查看重新调度的Pod 是否关联了之前的数据[root@k8s-master1 statefulset]# kubectl delete pod kubia-0pod "kubia-0" deleted# 当Pod成功终止后,Statefulset会立即拉起一个新的具有相同名称的Pod，有可能在其他node上,Pod地址肯定是变化了[root@k8s-master1 ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-0 0/1 Terminating 0 3m 10.244.1.104 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-1 1/1 Running 0 10m 10.244.2.83 k8s-master1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 9 4d12h 10.244.0.78 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-0 0/1 ContainerCreating 0 1s &lt;none&gt; k8s-node2 &lt;none&gt; &lt;none&gt;kubia-1 1/1 Running 0 10m 10.244.2.83 k8s-master1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 9 4d12h 10.244.0.78 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-0 1/1 Running 0 1s 10.244.1.105 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-1 1/1 Running 0 10m 10.244.2.83 k8s-master1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 9 4d12h 10.244.0.78 k8s-node1 &lt;none&gt; &lt;none&gt;# 新的Pod保留名称 主机名 存储# pod的名称是被保留的,通过访问Pod来确认[root@k8s-master1 statefulset]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/You've hit kubia-0Data stored on this pod: Hey there! to kubia-0# 说明重新拉起的Pod的主机名和持久化数据 与 之前的保持一致,即保证了服务状态,基本完全一致的新Pod 在Statefulset 中发现其他伙伴节点SRV 记录1234567891011# 运行一个DNS查询工具 dig 命令 [root@k8s-master1 ~]# kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local...;; ANSWER SECTION:kubia.default.svc.cluster.local. 5 IN SRV 0 50 80 kubia-0.kubia.default.svc.cluster.local.kubia.default.svc.cluster.local. 5 IN SRV 0 50 80 kubia-1.kubia.default.svc.cluster.local.;; ADDITIONAL SECTION:kubia-0.kubia.default.svc.cluster.local. 5 IN A 10.244.1.106kubia-1.kubia.default.svc.cluster.local. 5 IN A 10.244.2.86 通过DNS查找所有Pod节点 并获取数据123456789101112131415161718192021222324252627282930[root@k8s-master1 pub]# vim kubia-service-public.yamlapiVersion: v1kind: Servicemetadata: name: kubia-publicspec: selector: app: kubia ports: - port: 80 targetPort: 8080# 通过API服务器访问集群内部服务# 让请求随机分配到一个statefulset的pod上[root@k8s-master1 pub]# kubectl create -f kubia-service-public.yaml service/kubia-public created[root@k8s-master1 pub]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 14dkubia-public ClusterIP 10.0.0.241 &lt;none&gt; 80/TCP 8s[root@k8s-master1 pub]# curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/You've hit kubia-1Data stored on this pod: Hey there! to kubia-1[root@k8s-master1 pub]# curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/You've hit kubia-0Data stored on this pod: Hey there! to kubia-0 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[root@k8s-master2 kubia-pet-peers-image]# cat app.js const http = require('http');const os = require('os');const fs = require('fs');const dns = require('dns');const dataFile = "/var/data/kubia.txt";const serviceName = "kubia.default.svc.cluster.local";const port = 8080;function fileExists(file) &#123; try &#123; fs.statSync(file); return true; &#125; catch (e) &#123; return false; &#125;&#125;function httpGet(reqOptions, callback) &#123; return http.get(reqOptions, function(response) &#123; var body = ''; response.on('data', function(d) &#123; body += d; &#125;); response.on('end', function() &#123; callback(body); &#125;); &#125;).on('error', function(e) &#123; callback("Error: " + e.message); &#125;);&#125;var handler = function(request, response) &#123; if (request.method == 'POST') &#123; var file = fs.createWriteStream(dataFile); file.on('open', function (fd) &#123; request.pipe(file); response.writeHead(200); response.end("Data stored on pod " + os.hostname() + "\n"); &#125;); &#125; else &#123; response.writeHead(200); if (request.url == '/data') &#123; var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet"; response.end(data); &#125; else &#123; response.write("You've hit " + os.hostname() + "\n"); response.write("Data stored in the cluster:\n"); dns.resolveSrv(serviceName, function (err, addresses) &#123; if (err) &#123; response.end("Could not look up DNS SRV records: " + err); return; &#125; var numResponses = 0; if (addresses.length == 0) &#123; response.end("No peers discovered."); &#125; else &#123; addresses.forEach(function (item) &#123; var requestOptions = &#123; host: item.name, port: port, path: '/data' &#125;; httpGet(requestOptions, function (returnedData) &#123; numResponses++; response.write("- " + item.name + ": " + returnedData + "\n"); if (numResponses == addresses.length) &#123; response.end(); &#125; &#125;); &#125;); &#125; &#125;); &#125; &#125;&#125;;var www = http.createServer(handler);www.listen(port); 更新 Statefulset12345[root@k8s-master1 ~]# kubectl edit statefulset kubiareplicas: 3 # 更新副本个数...- image: 172.31.228.68/game/kubia-pet-peers # 更新镜像... 12345678910111213141516171819[root@k8s-master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-0 1/1 Running 0 5s 10.244.1.109 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-1 1/1 Running 0 45s 10.244.2.87 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-2 1/1 Running 0 80s 10.244.0.81 k8s-node1 &lt;none&gt; &lt;none&gt;# 新增的kubia-2 之前的连个Pod也被滚动更新[root@k8s-master1 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-kubia-0 Bound pv-c 2Gi RWX 3h13mdata-kubia-1 Bound pv-a 2Gi RWX 3h13mdata-kubia-2 Bound pv-b 2Gi RWX 101s[root@k8s-master1 ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-a 2Gi RWX Recycle Bound default/data-kubia-1 3h14mpv-b 2Gi RWX Recycle Bound default/data-kubia-2 3h14mpv-c 2Gi RWX Recycle Bound default/data-kubia-0 3h14m 测试集群数据12345678910[root@k8s-master1 ~]# kubectl proxyStarting to serve on 127.0.0.1:8001# 写入到不同的pod中[root@k8s-master1 pub]# curl -X POST -d "123" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/Data stored on pod kubia-2[root@k8s-master1 pub]# curl -X POST -d "567" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/Data stored on pod kubia-0 通过 DNS 获取 POD IP123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 statefulset]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 14dkubia ClusterIP None &lt;none&gt; 80/TCP 3s[root@k8s-master1 demo]# vim busybox.yaml apiVersion: v1kind: Podmetadata: name: dns-testspec: containers: - name: busybox image: busybox:1.28.4 args: - /bin/sh - -c - sleep 36000 restartPolicy: Never[root@k8s-master1 statefulset]# kubectl create -f busybox.yaml [root@k8s-master1 statefulset]# kubectl exec -it dns-test sh/ # nslookup kubiaServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubiaAddress 1: 10.244.2.89 kubia-1.kubia.default.svc.cluster.localAddress 2: 10.244.1.111 kubia-0.kubia.default.svc.cluster.localAddress 3: 10.244.0.82 kubia-2.kubia.default.svc.cluster.local/ # nslookup kubia-1.kubia.default.svc.cluster.localServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubia-1.kubia.default.svc.cluster.localAddress 1: 10.244.2.89 kubia-1.kubia.default.svc.cluster.local StatefulSet 总结1234567891011StatefulSet 与 Deployment区别: 有身份的,有网络标识,主机名,固定的存储身份三要素:域名主机名存储(PVC)statefulset 的 POD 名字 == 主机名ClusterIP A记录: &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local ClusterIP = Node A记录格式: &lt;StatefulsetName-index&gt;.&lt;namespace-name&gt;.&lt;service-name&gt;.svc.cluster.localnginx-statefulset-0.nginx.default.svc.cluster.local这个就是 statefulset 的 POD 的固定的访问地址 ，通过域名访问到后面的 POD ，通过DNS维持身份]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18 Deployment 声明式地升级应用]]></title>
    <url>%2F2020%2F03%2F19%2Fk8s-base20%2F</url>
    <content type="text"><![CDATA[使用 RC 实现自动滚动升级先定义两个容器镜像 v1 和 v2 版本1234567891011121314151617181920212223# v1 版本[root@k8s-master2 v1]# cat app.js const http = require('http');const os = require('os');console.log("Kubia server starting...");var handler = function(request, response) &#123; console.log("Received request from " + request.connection.remoteAddress); response.writeHead(200); response.end("This is v1 running in pod " + os.hostname() + "\n");&#125;;var www = http.createServer(handler);www.listen(8080);[root@k8s-master2 v1]# cat Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT ["node", "app.js"][root@k8s-master2 v1]# docker build -t 172.31.228.68/game/kubia:v1 .[root@k8s-master2 v1]# docker push 172.31.228.68/game/kubia:v1 1234567891011121314151617181920212223242526# v2 版本 区别在于返回的是 v2的信息[root@k8s-master2 v2]# docker build -t 172.31.228.68/game/kubia:v2 .[root@k8s-master2 v2]# cat app.js const http = require('http');const os = require('os');console.log("Kubia server starting...");var handler = function(request, response) &#123; console.log("Received request from " + request.connection.remoteAddress); response.writeHead(200); response.end("This is v2 running in pod " + os.hostname() + "\n");&#125;;var www = http.createServer(handler);www.listen(8080);[root@k8s-master2 v2]# cat Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT ["node", "app.js"][root@k8s-master2 v2]# docker build -t 172.31.228.68/game/kubia:v2 .[root@k8s-master2 v2]# docker push 172.31.228.68/game/kubia:v2 单个 yaml 运行 rc 和 service12345678910111213141516171819202122232425262728293031323334# 多种资源定义通过 --- 分割[root@k8s-master1 deployment]# vim kubia-rc-svc-v1.yaml apiVersion: v1kind: ReplicationControllermetadata: name: kubia-v1spec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: imagePullSecrets: - name: my-harbor-secret containers: - image: 172.31.228.68/game/kubia:v1 name: nodejs---apiVersion: v1kind: Servicemetadata: name: kubiaspec: type: NodePort selector: app: kubia ports: - port: 80 targetPort: 8080 nodePort: 30123 12345678910111213141516[root@k8s-master1 deployment]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/kubia-v1-5cgjx 1/1 Running 0 2m21spod/kubia-v1-tbhqr 1/1 Running 0 2m21spod/kubia-v1-th6rr 1/1 Running 0 2m21spod/nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 8 3d14hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 13dservice/kubia NodePort 10.0.0.119 &lt;none&gt; 80:30123/TCP 2m21s# 测试程序一直循环访问[root@k8s-master1 demo]# while true;do curl http://10.0.0.119;sleep 1; doneThis is v1 running in pod kubia-v1-tbhqrThis is v1 running in pod kubia-v1-th6rrThis is v1 running in pod kubia-v1-5cgjx RC 的滚动升级 rolling-update1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master1 deployment]# kubectl rolling-update kubia-v1 kubia-v2 --image=172.31.228.68/game/kubia:v2Command "rolling-update" is deprecated, use "rollout" insteadCreated kubia-v2Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 pods available, don't exceed 4 pods)Scaling kubia-v2 up to 1Scaling kubia-v1 down to 2Scaling kubia-v2 up to 2Scaling kubia-v1 down to 1Scaling kubia-v2 up to 3Scaling kubia-v1 down to 0Update succeeded. Deleting kubia-v1replicationcontroller/kubia-v2 rolling updated to "kubia-v2"[root@k8s-master1 ~]# kubectl describe rc kubia-v2Name: kubia-v2Namespace: defaultSelector: app=kubia,deployment=1c9b94fec2662e272187373534aef5c6Labels: app=kubia...[root@k8s-master1 ~]# kubectl describe rc kubia-v1Name: kubia-v1Namespace: defaultSelector: app=kubia,deployment=9b82b6cdfb3cd18a5e53068dda002fe1-origLabels: app=kubia...# 查看 Pod[root@k8s-master1 ~]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-v1-tbhqr 1/1 Running 0 18m app=kubia,deployment=9b82b6cdfb3cd18a5e53068dda002fe1-origkubia-v2-p8vbl 1/1 Running 0 3m15s app=kubia,deployment=1c9b94fec2662e272187373534aef5c6kubia-v2-x7g6g 1/1 Running 0 2m8s app=kubia,deployment=1c9b94fec2662e272187373534aef5c6kubia-v2-xr8hz 1/1 Running 0 62s app=kubia,deployment=1c9b94fec2662e272187373534aef5c6nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 8 3d14h app=nfs-client-provisioner,pod-template-hash=56f4b98d47# v1的pod被滚动升级到v2版本 并且多了一个标签 deployment=9b82...[root@k8s-master1 deployment]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-v2-p8vbl 1/1 Running 0 5m54s 10.244.1.75 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-v2-x7g6g 1/1 Running 0 4m47s 10.244.0.61 k8s-node1 &lt;none&gt; &lt;none&gt;kubia-v2-xr8hz 1/1 Running 0 3m41s 10.244.2.62 k8s-master1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 8 3d14h 10.244.0.59 k8s-node1 &lt;none&gt; &lt;none&gt;# 请求也都到了 v2 This is v2 running in pod kubia-v2-x7g6gThis is v2 running in pod kubia-v2-p8vblThis is v2 running in pod kubia-v2-xr8hz# rolling-update 已过时,现在使用 deployment # deployment 是否可以通过修改pod中的镜像 升级应用 而不是通过删除手法的自动伸缩,应该通过期望副本数来做升级 使用 Deployment 声明式升级应用1231. Deployment -&gt; rs -&gt; pod2. Deployment 是一种更高级资源 用于部署应用程序，并且用声明式方式更新应用3. Deployment 被创建时也会创建 ReplicaSet ，在使用Deployment时实际上Pod是由ReplicaSet创建和管理的 创建 Deployment12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master1 deployment]# kubectl explain deploymentKIND: DeploymentVERSION: apps/v1[root@k8s-master1 deployment]# vim kubia-deployment-v1.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: name: kubia labels: app: kubia spec: containers: - image: 172.31.228.68/game/kubia:v1 name: nodejs[root@k8s-master1 deployment]# kubectl delete rc --allreplicationcontroller "kubia-v2" deleted# 创建deployment并记录版本号[root@k8s-master1 deployment]# kubectl create -f kubia-deployment-v1.yaml --recorddeployment.apps/kubia created[root@k8s-master1 deployment]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-dbd9664c7-4mggm 1/1 Running 0 35s 10.244.2.64 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-dbd9664c7-8b629 1/1 Running 0 35s 10.244.1.77 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-dbd9664c7-8npt4 1/1 Running 0 35s 10.244.0.63 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 deployment]# kubectl get rsNAME DESIRED CURRENT READY AGEkubia-dbd9664c7 3 3 3 93s# 别忘记deployment是通过rs来创建pod 所有rs的名字中包含了pod模板的哈希值# Deployment可以创建多个RS来对应管理一组Pod模板# 可以通过RS这个值去区分 [root@k8s-master1 demo]# kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-dbd9664c7-4mggm 1/1 Running 0 5m36s app=kubia,pod-template-hash=dbd9664c7kubia-dbd9664c7-8b629 1/1 Running 0 5m36s app=kubia,pod-template-hash=dbd9664c7kubia-dbd9664c7-8npt4 1/1 Running 0 5m36s app=kubia,pod-template-hash=dbd9664c7 升级 Deployment123Deployment 有两种升级策略1. 默认滚动更新 渐进删除旧Pod,同时创建新Pod ，整个应用在升级中都处于可用状态 RollingUpdate2. 一次删除所有旧Pod,然后创建新Pod 应用如果不支持多版本同时对外提供服务,就需要再启动新版本之前停止旧版本，会导致短期业务无不可用 Recre-ate 减慢滚动升级速度12345678910# 用来测试查看观察升级过程 # 设置为10秒# kubectl patch 可以用来修改单个资源信息[root@k8s-master1 demo]# kubectl patch deployment kubia -p '&#123;"spec": &#123;"minReadySeconds": 10&#125;&#125;'deployment.apps/kubia patched# 查看下[root@k8s-master1 deployment]# kubectl edit deploy kubiaspec: minReadySeconds: 10 修改镜像 触发滚动升级12345678910111213141516171819202122232425262728293031[root@k8s-master1 demo]# while true;do curl http://10.0.0.119;sleep 1; doneThis is v1 running in pod kubia-dbd9664c7-8npt4This is v1 running in pod kubia-dbd9664c7-4mggmThis is v1 running in pod kubia-dbd9664c7-8b629[root@k8s-master1 deployment]# kubectl set image deployment kubia nodejs=172.31.228.68/game/kubia:v2deployment.apps/kubia image updated# 滚动升级过程 一个个的将v1版本pod 替换成 v2 pod This is v2 running in pod kubia-56fd945996-lz847This is v2 running in pod kubia-56fd945996-glj6dThis is v2 running in pod kubia-56fd945996-ccg9c[root@k8s-master1 deployment]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-56fd945996-ccg9c 1/1 Running 0 61s 10.244.1.78 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-56fd945996-glj6d 1/1 Running 0 49s 10.244.0.64 k8s-node1 &lt;none&gt; &lt;none&gt;kubia-56fd945996-lz847 1/1 Running 0 38s 10.244.2.65 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-dbd9664c7-8b629 1/1 Terminating 0 20m 10.244.1.77 k8s-node2 &lt;none&gt; &lt;none&gt;# 升级过程# 1. 创建新的的RS(v2) 并且慢慢扩容# 2. 之前版本(v1) 会缩容至0[root@k8s-master1 deployment]# kubectl get rsNAME DESIRED CURRENT READY AGEkubia-56fd945996 3 3 3 5m28skubia-dbd9664c7 0 0 0 25m# 旧版本的RS会被保留，可以用来做更多的操作 # RC 无法实现这些 比如修改镜像版本触发更新,或者是回滚 更新 ConfigMap123# 如果 deployment 中的pod 引用了1个 ConfigMap(或Secret) # 1. 更改ConfigMap资源本身不会触发升级操作# 2. 如果需要修改应用程序的配置并且想触发更新 需要创建一个新的ConfigMap 并且修改Pod模板引用新的CM 回滚 Deployment12345678910111213141516171819202122232425262728# 先创建一个错误版本 v3 # 5个请求后 会返回500错误[root@k8s-master2 v3]# cat app.js const http = require('http');const os = require('os');var requestCount = 0;console.log("Kubia server starting...");var handler = function(request, response) &#123; console.log("Received request from " + request.connection.remoteAddress); if (++requestCount &gt;= 5) &#123; response.writeHead(500); response.end("Some internal error has occurred! This is pod " + os.hostname() + "\n"); return; &#125; response.writeHead(200); response.end("This is v3 running in pod " + os.hostname() + "\n");&#125;;var www = http.createServer(handler);www.listen(8080);[root@k8s-master2 v3]# docker build -t 172.31.228.68/game/kubia:v3 .[root@k8s-master2 v3]# docker push 172.31.228.68/game/kubia:v3 1234567891011121314# 升级 然后查看错误请求[root@k8s-master1 deployment]# kubectl set image deployment kubia nodejs=172.31.228.68/game/kubia:v3deployment.apps/kubia image updated[root@k8s-master1 deployment]# kubectl set image deployment kubia nodejs=172.31.228.68/game/kubia:v3deployment.apps/kubia image updated[root@k8s-master1 ~]# while true;do curl http://10.0.0.119;sleep 1; doneThis is v3 running in pod kubia-698c869b67-z7qpwThis is v3 running in pod kubia-698c869b67-228w5Some internal error has occurred! This is pod kubia-698c869b67-vkvchSome internal error has occurred! This is pod kubia-698c869b67-z7qpwSome internal error has occurred! This is pod kubia-698c869b67-228w5Some internal error has occurred! This is pod kubia-698c869b67-vkvch 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 回滚升级 到之前的版本[root@k8s-master1 deployment]# kubectl rollout undo deployment kubiadeployment.apps/kubia rolled back...This is v2 running in pod kubia-56fd945996-t28chThis is v2 running in pod kubia-56fd945996-n7cwdThis is v2 running in pod kubia-56fd945996-jzzl4# 显示Deployment的滚动升级历史[root@k8s-master1 deployment]# kubectl rollout history deployment kubiadeployment.apps/kubia REVISION CHANGE-CAUSE1 kubectl create --filename=kubia-deployment-v1.yaml --record=true3 kubectl create --filename=kubia-deployment-v1.yaml --record=true4 kubectl create --filename=kubia-deployment-v1.yaml --record=true# 版本记录大小 [root@k8s-master1 deployment]# kubectl edit deploy kubiarevisionHistoryLimit: 10# 根据版本号回滚[root@k8s-master1 deployment]# kubectl rollout undo deployment kubia --to-revision=1deployment.apps/kubia rolled back[root@k8s-master1 deployment]# kubectl get rsNAME DESIRED CURRENT READY AGEkubia-56fd945996 0 0 0 3m26skubia-698c869b67 0 0 0 2m47skubia-dbd9664c7 3 3 3 8m52s# RS中记录着版本号[root@k8s-master1 deployment]# kubectl edit rs kubia-dbd9664c7apiVersion: apps/v1kind: ReplicaSetmetadata: annotations: deployment.kubernetes.io/desired-replicas: "3" deployment.kubernetes.io/max-replicas: "4" deployment.kubernetes.io/revision: "5" deployment.kubernetes.io/revision-history: "1" kubernetes.io/change-cause: kubectl create --filename=kubia-deployment-v1.yaml --record=true 控制滚动升级速率123456789[root@k8s-master1 deployment]# kubectl edit deploy kubia strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate# maxSurge 最多超出副本数之外 设置成1或者2 最多超出1到2个Pod # maxUnavailable 滚动升级期间,有多少个Pod成为不可用状态 暂停滚动升级 使用试探性发布1234# 假金丝雀发布# 让少数请求到升级后的服务# 之前的继续使用旧版本# 有问题回滚到旧版本,没问题全部升级 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# 先做一个最新版本[root@k8s-master2 v4]# cat app.js const http = require('http');const os = require('os');console.log("Kubia server starting...");var handler = function(request, response) &#123; console.log("Received request from " + request.connection.remoteAddress); response.writeHead(200); response.end("This is v4 running in pod " + os.hostname() + "\n");&#125;;var www = http.createServer(handler);www.listen(8080);# 暂停滚动升级[root@k8s-master1 deployment]# kubectl set image deployment kubia nodejs=172.31.228.68/game/kubia:v4deployment.apps/kubia image updated[root@k8s-master1 deployment]# kubectl rollout pause deployment kubiadeployment.apps/kubia paused[root@k8s-master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-557944968b-579xn 1/1 Running 0 71s 10.244.1.86 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-557944968b-vr6s5 1/1 Running 0 59s 10.244.2.73 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-dbd9664c7-bhpf5 1/1 Running 0 26m 10.244.0.71 k8s-node1 &lt;none&gt; &lt;none&gt;kubia-dbd9664c7-hc4kj 1/1 Running 0 27m 10.244.1.85 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# while true;do curl http://10.0.0.119;sleep 1; doneThis is v4 running in pod kubia-557944968b-579xnThis is v1 running in pod kubia-dbd9664c7-bhpf5This is v1 running in pod kubia-dbd9664c7-hc4kjThis is v4 running in pod kubia-557944968b-vr6s5This is v4 running in pod kubia-557944968b-579xn# 正常 恢复滚动升级[root@k8s-master1 deployment]# kubectl rollout resume deployment kubiadeployment.apps/kubia resumed# 如果想完全的对应pod数量 可以考虑使用两个不同的Deployment 同时数量一致,同时上线,再调整下线的Deployment[root@k8s-master1 deployment]# vim kubia-deployment-v2.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: kubia-v2spec: replicas: 3 selector: matchLabels: app: kubia template: metadata: name: kubia labels: app: kubia spec: containers: - image: 172.31.228.68/game/kubia:v2 name: nodejs[root@k8s-master1 deployment]# kubectl create -f kubia-deployment-v2.yaml deployment.apps/kubia-v2 created[root@k8s-master1 deployment]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.31.228.67:6443 13dkubia 10.244.0.72:8080,10.244.0.73:8080,10.244.1.86:8080 + 3 more... 128m# 由于svc匹配两个Pod的标签,所以被加入服务This is v4 running in pod kubia-557944968b-6bw9rThis is v4 running in pod kubia-557944968b-vr6s5This is v4 running in pod kubia-557944968b-579xnThis is v2 running in pod kubia-v2-56fd945996-t9zdbThis is v2 running in pod kubia-v2-56fd945996-76g6lThis is v2 running in pod kubia-v2-56fd945996-fgn2k 增加 就绪探针123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 先全部回滚到v2版本[root@k8s-master1 deployment]# vim kubia-deployment-v3-with-readinesscheck.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: name: kubia labels: app: kubia spec: containers: - image: 172.31.228.68/game/kubia:v3 name: nodejs readinessProbe: periodSeconds: 1 httpGet: path: / port: 8080[root@k8s-master1 deployment]# kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml This is v2 running in pod kubia-56fd945996-8zr5jThis is v2 running in pod kubia-56fd945996-6g4zlThis is v2 running in pod kubia-56fd945996-hfvk5This is v2 running in pod kubia-56fd945996-8zr5jThis is v2 running in pod kubia-56fd945996-6g4zl# 没有出现v3版本[root@k8s-master1 deployment]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-56fd945996-6g4zl 1/1 Running 0 3m59s 10.244.0.75 k8s-node1 &lt;none&gt; &lt;none&gt;kubia-56fd945996-8zr5j 1/1 Running 0 4m22s 10.244.2.75 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-56fd945996-hfvk5 1/1 Running 0 4m10s 10.244.1.89 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-58c74659b7-mf2bv 0/1 Running 0 69s 10.244.1.90 k8s-node2 &lt;none&gt; &lt;none&gt;# kubia-58c74659b7-mf2bv 为处于就绪状态# 当新的Pod启动时候,就绪探针每隔1秒去发起请求 在上面的pod里 第5个请求会出现500返回,就绪探针会报错# 因为 pod会从Svc的ep中被剔除 所以curl也请求不到新的Pod[root@k8s-master1 deployment]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.31.228.67:6443 13dkubia 10.244.0.75:8080,10.244.1.89:8080,10.244.2.75:8080 143m# 滚动升级不继续下去,新的Pod一直处于不可用状态，就绪10秒后才真可用# maxUnavailable: 0 使滚动更新正好卡在这里 除非之前的Pod准备就绪才行1 滚动升级时限12345[root@k8s-master1 deployment]# kubectl edit deploy kubiaspec:progressDeadlineSeconds: 600# 滚动超时会自动取消 取消出错版本的滚动升级123# 直接回滚到上一版本[root@k8s-master1 deployment]# kubectl rollout undo deployment kubiadeployment.apps/kubia rolled back]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[17 从应用访问 Pod 元数据]]></title>
    <url>%2F2020%2F03%2F19%2Fk8s-base19%2F</url>
    <content type="text"><![CDATA[通过Downward Api传递元数据121. 了解容器中运行的应用是如何与 k8s API 服务器进行交互2. 如何获取在集群中部署资源的信息 通过环境变量暴露可用的元数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@k8s-master1 downward]# vim downward-api-env.yaml apiVersion: v1kind: Podmetadata: name: downwardspec: containers: - name: main image: busybox command: ["sleep", "9999999"] resources: requests: memory: "100Mi" cpu: "100m" limits: memory: "200Mi" cpu: "200m" env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: CONTAINER_CPU_REQUEST_MILLICORES valueFrom: resourceFieldRef: resource: requests.cpu divisor: 1m - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Ki# 容器中的进程在运行时,可以获取容器中的环境变量# POD名称 # IP# 命名空间 这些都引用pod元数据中资源 metadata# NODE_NAME 通过 spec.nodeName暴露# 服务账户 通过 spec.serviceAccountName 服务账户是pod访问api用来身份验证的账户# 创建两个环境变量: CONTAINER_CPU_REQUEST_MILLICORES CPU请求最大值 和 CONTAINER_MEMORY_LIMIT_KIBIBYTES 内存最大限制# 暴露资源请求和使用限制的环境变量,需要设定一个基数单位,会用实际的数据 除以 这个基数单位# cpu的基数单位是 1m(千分之一核) 100m 除以 1m = 100m (0.1核) # 内存的基数单位是 1Ki 限制的值如果是200mi 就是 204800 12345678910111213141516171819[root@k8s-master1 downward]# kubectl exec downward envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=downwardPOD_NAMESPACE=defaultPOD_IP=10.244.1.66NODE_NAME=k8s-node2SERVICE_ACCOUNT=defaultCONTAINER_CPU_REQUEST_MILLICORES=100CONTAINER_MEMORY_LIMIT_KIBIBYTES=204800POD_NAME=downwardKUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1KUBERNETES_SERVICE_HOST=10.0.0.1KUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_PORT=tcp://10.0.0.1:443KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443HOME=/root 使用 downward API 卷传递元数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@k8s-master1 downward]# vim downward-api-volume.yamlapiVersion: v1kind: Podmetadata: name: downward labels: foo: bar annotations: key1: value1 key2: | multi line valuespec: containers: - name: main image: busybox command: ["sleep", "9999999"] resources: requests: memory: "100Mi" cpu: "100m" limits: memory: "200Mi" cpu: "200m" volumeMounts: - name: downward mountPath: /etc/downward volumes: - name: downward downwardAPI: items: - path: "podName" fieldRef: fieldPath: metadata.name - path: "podNamespace" fieldRef: fieldPath: metadata.namespace - path: "labels" fieldRef: fieldPath: metadata.labels - path: "annotations" fieldRef: fieldPath: metadata.annotations - path: "containerCpuRequestMilliCores" resourceFieldRef: containerName: main resource: requests.cpu divisor: 1m - path: "containerMemoryLimitBytes" resourceFieldRef: containerName: main resource: limits.memory divisor: 1[root@k8s-master1 downward]# kubectl exec downward -- ls -lL /etc/downwardtotal 24-rw-r--r-- 1 root root 139 Mar 22 23:51 annotations-rw-r--r-- 1 root root 3 Mar 22 23:51 containerCpuRequestMilliCores-rw-r--r-- 1 root root 9 Mar 22 23:51 containerMemoryLimitBytes-rw-r--r-- 1 root root 9 Mar 22 23:51 labels-rw-r--r-- 1 root root 8 Mar 22 23:51 podName-rw-r--r-- 1 root root 7 Mar 22 23:51 podNamespace[root@k8s-master1 downward]# kubectl exec downward -- cat /etc/downward/labelsfoo="bar"[root@k8s-master1 downward]# kubectl exec downward -- cat /etc/downward/annotationskey1="value1"key2="multi\nline\nvalue\n"kubernetes.io/config.seen="2020-03-23T07:51:06.149015629+08:00" 与 K8S API 服务交互12345678910111213141516171819202122232425261. 获得服务器的URL[root@k8s-master1 downward]# kubectl cluster-infoKubernetes master is running at http://localhost:8080# 获取资源的API组和版本信息[root@k8s-master1 downward]# curl http://localhost:8080/[root@k8s-master1 downward]# curl http://localhost:8080/apis/batch&#123; "kind": "APIGroup", "apiVersion": "v1", "name": "batch", # 包含两个版本 "versions": [ &#123; "groupVersion": "batch/v1", "version": "v1" &#125;, &#123; "groupVersion": "batch/v1beta1", "version": "v1beta1" &#125; ], "preferredVersion": &#123; # 客户端应该使用的版本 "groupVersion": "batch/v1", "version": "v1" &#125; 123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 downward]# curl http://localhost:8080/apis/batch/v1&#123; "kind": "APIResourceList", "apiVersion": "v1", "groupVersion": "batch/v1", "resources": [ &#123; "name": "jobs", "singularName": "", "namespaced": true, "kind": "Job", "verbs": [ "create", "delete", "deletecollection", "get", "list", "patch", "update", "watch" ], "categories": [ "all" ], "storageVersionHash": "mudhfqk/qZY=" &#125;, &#123; "name": "jobs/status", "singularName": "", "namespaced": true, "kind": "Job", "verbs": [ "get", "patch", "update" ] &#125; ]&#125; 从 Pod 内部与 API 服务器进行交互123456789apiVersion: v1kind: Podmetadata: name: curlspec: containers: - name: main image: tutum/curl command: ["sleep", "9999999"] 1234567891011121. 发现API服务器地址 [root@k8s-master1 ~]# kubectl exec -it curl bashroot@curl:/# root@curl:/# env | grep KUBERNETES_SERVICEKUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_HOST=10.0.0.1KUBERNETES_SERVICE_PORT_HTTPS=443[root@k8s-master1 ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 13d 123456789102. 验证服务器身份 CAroot@curl:/# ls -l /var/run/secrets/kubernetes.io/serviceaccount/total 0lrwxrwxrwx 1 root root 13 Mar 23 00:13 ca.crt -&gt; ..data/ca.crtlrwxrwxrwx 1 root root 16 Mar 23 00:13 namespace -&gt; ..data/namespacelrwxrwxrwx 1 root root 12 Mar 23 00:13 token -&gt; ..data/token# 增加ca证书验证root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetesroot@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 123# 3. 获得API服务器授权 TOKENroot@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) root@curl:/# echo $TOKEN 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# 如果使用的是一个带有RBAC户主的k8s集群,服务账户可能不会被授权访问API服务器# 下面的命令赋予了所有服务账户(所有pod)的集群管理员权限# 不要在生产环境执行，测试可以查看kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts root@curl:/# curl -H "Authorization: Bearer $TOKEN" https://kubernetes&#123; "paths": [ "/api", "/api/v1", "/apis", "/apis/", "/apis/admissionregistration.k8s.io", "/apis/admissionregistration.k8s.io/v1", "/apis/admissionregistration.k8s.io/v1beta1", "/apis/apiextensions.k8s.io", "/apis/apiextensions.k8s.io/v1", "/apis/apiextensions.k8s.io/v1beta1", "/apis/apiregistration.k8s.io", "/apis/apiregistration.k8s.io/v1", "/apis/apiregistration.k8s.io/v1beta1", "/apis/apps", "/apis/apps/v1", "/apis/authentication.k8s.io", "/apis/authentication.k8s.io/v1", "/apis/authentication.k8s.io/v1beta1", "/apis/authorization.k8s.io", "/apis/authorization.k8s.io/v1", "/apis/authorization.k8s.io/v1beta1", "/apis/autoscaling", "/apis/autoscaling/v1", "/apis/autoscaling/v2beta1", "/apis/autoscaling/v2beta2", "/apis/batch", "/apis/batch/v1", "/apis/batch/v1beta1", "/apis/certificates.k8s.io", "/apis/certificates.k8s.io/v1beta1", "/apis/coordination.k8s.io", "/apis/coordination.k8s.io/v1", "/apis/coordination.k8s.io/v1beta1", "/apis/events.k8s.io", "/apis/events.k8s.io/v1beta1", "/apis/extensions", "/apis/extensions/v1beta1", "/apis/networking.k8s.io", "/apis/networking.k8s.io/v1", "/apis/networking.k8s.io/v1beta1", "/apis/node.k8s.io", "/apis/node.k8s.io/v1beta1", "/apis/policy", "/apis/policy/v1beta1", "/apis/rbac.authorization.k8s.io", "/apis/rbac.authorization.k8s.io/v1", "/apis/rbac.authorization.k8s.io/v1beta1", "/apis/scheduling.k8s.io", "/apis/scheduling.k8s.io/v1", "/apis/scheduling.k8s.io/v1beta1", "/apis/storage.k8s.io", "/apis/storage.k8s.io/v1", "/apis/storage.k8s.io/v1beta1", "/healthz", "/healthz/autoregister-completion", "/healthz/etcd", "/healthz/log", "/healthz/ping", "/healthz/poststarthook/apiservice-openapi-controller", "/healthz/poststarthook/apiservice-registration-controller", "/healthz/poststarthook/apiservice-status-available-controller", "/healthz/poststarthook/bootstrap-controller", "/healthz/poststarthook/ca-registration", "/healthz/poststarthook/crd-informer-synced", "/healthz/poststarthook/generic-apiserver-start-informers", "/healthz/poststarthook/kube-apiserver-autoregistration", "/healthz/poststarthook/rbac/bootstrap-roles", "/healthz/poststarthook/scheduling/bootstrap-system-priority-classes", "/healthz/poststarthook/start-apiextensions-controllers", "/healthz/poststarthook/start-apiextensions-informers", "/healthz/poststarthook/start-kube-aggregator-informers", "/healthz/poststarthook/start-kube-apiserver-admission-initializer", "/livez", "/livez/autoregister-completion", "/livez/log", "/livez/ping", "/livez/poststarthook/apiservice-openapi-controller", "/livez/poststarthook/apiservice-registration-controller", "/livez/poststarthook/apiservice-status-available-controller", "/livez/poststarthook/bootstrap-controller", "/livez/poststarthook/ca-registration", "/livez/poststarthook/crd-informer-synced", "/livez/poststarthook/generic-apiserver-start-informers", "/livez/poststarthook/kube-apiserver-autoregistration", "/livez/poststarthook/rbac/bootstrap-roles", "/livez/poststarthook/scheduling/bootstrap-system-priority-classes", "/livez/poststarthook/start-apiextensions-controllers", "/livez/poststarthook/start-apiextensions-informers", "/livez/poststarthook/start-kube-aggregator-informers", "/livez/poststarthook/start-kube-apiserver-admission-initializer", "/logs", "/metrics", "/openapi/v2", "/readyz", "/readyz/autoregister-completion", "/readyz/log", "/readyz/ping", "/readyz/poststarthook/apiservice-openapi-controller", "/readyz/poststarthook/apiservice-registration-controller", "/readyz/poststarthook/apiservice-status-available-controller", "/readyz/poststarthook/bootstrap-controller", "/readyz/poststarthook/ca-registration", "/readyz/poststarthook/crd-informer-synced", "/readyz/poststarthook/generic-apiserver-start-informers", "/readyz/poststarthook/kube-apiserver-autoregistration", "/readyz/poststarthook/rbac/bootstrap-roles", "/readyz/poststarthook/scheduling/bootstrap-system-priority-classes", "/readyz/poststarthook/start-apiextensions-controllers", "/readyz/poststarthook/start-apiextensions-informers", "/readyz/poststarthook/start-kube-aggregator-informers", "/readyz/poststarthook/start-kube-apiserver-admission-initializer", "/readyz/shutdown", "/version" ] 123456789101112# 获取当前Pod所在的命名空间中所有的pod清单root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)curl -H "Authorization: Bearer $TOKEN" https://kubernetes/api/v1/namespaces/$NS/podsroot@curl:/# curl -H "Authorization: Bearer $TOKEN" https://kubernetes/api/v1/namespaces/$NS/pods&#123; "kind": "PodList", "apiVersion": "v1", "metadata": &#123; "selfLink": "/api/v1/namespaces/default/pods", "resourceVersion": "246651" &#125;,]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16 K8S ConfigMap 和 Secret 配置应用程序]]></title>
    <url>%2F2020%2F03%2F19%2Fk8s-base18%2F</url>
    <content type="text"><![CDATA[配置容器化应用程序1234# 应用程序 会将配置嵌入到程序本身 方式:1. 命令行参数2. 配置选项数量增多 -&gt; 配置文件3. 借助环境变量 123456789101112# 为何使用 环境变量1. 在 Docker 环境中采用 配置文件方式 有些困难，需要将配置文件打入镜像 或者是挂载包含该文件的卷 2. 打入镜像 相当于 硬编码配置,修改配置需要重新构建新的镜像 3. 挂载卷方便一些,但是在容器启动前就要先完成配置文件写入卷里 4. ConfigMap 可以做为存储配置数据的资源# 无论是否使用 ConfigMap 存储配置数据 下面访问均可以用作配置应用程序1. 向容器传递命令行参数2. 为每个容器设置自定义环境变量3. 通过特殊类型的卷 将配置文件挂载到容器里# 如果我们存在敏感数据 如证书私钥、连接数据库的账户密码等，需要保持安全的数据,可以使用Secret 容器传递命令行参数在 Docker 中定义命令与参数123456781. 容器中运行完整的指令两部分组成: 命令与参数2. Dockerfile 中的两种指令分别定义 命令与参数两个部分ENTRYPOINT: 定义容器启动时,被调用的可执行程序CMD: 指定传送给ENTRYPOINT的参数3. 正确的做法应该是使用ENTRYPOINT指令，用CMD指定所需要的默认参数，这样可以让镜像直接运行docker run &lt;image&gt;如果需要附加参数，覆盖Dockerfile中的CMD指定的默认参数值docker run &lt;image&gt; &lt;arg&gt; 了解 shell 和 exec 的区别12345678910111213141516171819202122232425262728293031shell: ENTRYPOINT node app.jsexec: ENTRYPOINT ["node","app.js"]# 区别在于 命令是否在 shell中被调用# exec 形式: [root@k8s-master2 base]# vim Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT ["node","app.js"][root@k8s-master2 ~]# docker run --name kubia-container -p 8080:8080 -d kubia[root@k8s-master2 ~]# docker exec kubia-container ps -x PID TTY STAT TIME COMMAND 1 ? Ssl 0:00 node app.js 11 ? Rs 0:00 ps -x# Shell 形式[root@k8s-master2 base]# vim Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT node app.js[root@k8s-master2 base]# docker build -t kubia-shell .[root@k8s-master2 base]# docker run --name kubia-container-shell -p 8080:8080 -d kubia-shell[root@k8s-master2 base]# docker exec kubia-container-shell ps -x PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/sh -c node app.js 6 ? Sl 0:00 node app.js 12 ? Rs 0:00 ps -x 12# 主进程(pid=1) 是 shell进程 而非 node 进程,node(pid=7)进程于shell中启动# shell 进程是多余的，因此通常可以直接采用exec形式的 ENTRYPOINT 命令 可配置化镜像中的间隔参数12345678910111213141516171819202122232425262728293031323334# 修改循环延迟间隔,使其可配置# $1 作为第一个命令行参数 传入到 脚本中,并且在Dockerfile中定义[root@k8s-master2 args]# vim fortuneloop.sh #!/bin/bashtrap "exit" SIGINTmkdir -p /var/htdocsNUM=$1echo "设定的间隔是 $&#123;NUM&#125; 秒"for i in &#123;1..5&#125;:do echo "$(date "+%H:%M:%S") $i" &gt;&gt; /var/htdocs/index.html sleep $NUM;done[root@k8s-master2 args]# vim Dockerfile FROM centos:7ADD fortuneloop.sh /bin/fortuneloop.shENTRYPOINT ["/bin/sh","/root/fortuneloop.sh"] # exec形式的 ENTRYPOINTCMD ["5"] # 可执行程序的默认参数 5秒[root@k8s-master2 args]# docker build -t 172.31.228.68/project/fortune:args .[root@k8s-master2 args]# docker push 172.31.228.68/project/fortune:args[root@k8s-master2 args]# docker run -it 172.31.228.68/project/fortune:args设定的间隔是 5 秒[root@k8s-master2 args]# docker exec 7756f62121a4 ps x PID TTY STAT TIME COMMAND 1 pts/0 Ss+ 0:00 /bin/sh /root/fortuneloop.sh 5 12 pts/0 S+ 0:00 sleep 5 13 ? Rs 0:00 ps x 123456789# 传递一个间隔参数覆盖默认值[root@k8s-master2 args]# docker run -it --rm 172.31.228.68/project/fortune:args 10设定的间隔是 10 秒[root@k8s-master2 ~]# docker exec f51246e5faaf ps x PID TTY STAT TIME COMMAND 1 pts/0 Ss+ 0:00 /bin/sh /root/fortuneloop.sh 10 12 pts/0 S+ 0:00 sleep 10 13 ? Rs 0:00 ps x 在 K8S 中覆盖命令和参数12345678910111. 在k8s定义容器时,镜像的ENTRYPOINT 和 CMD 是可以被覆盖的2. 需要在容器属性中定义 command 和 args 值3. command 和 args字段在Pod创建后无法修改spec: containers: - image: centos:7 # 容器1 写数据到 /var/htdocs/index.html name: html-generator # command: ["bash","-c","for i in &#123;1..100&#125;;do echo $i &gt;&gt; /var/htdocs/index.html;sleep 10;done"] command: ["/bin/command"] args: ["arg1","arg2","arg3"] 123Docker K8S 描述ENTRYPOINT command 容器中运行的可执行文件CMD args 传给可执行文件的参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master1 forloop]# vim fortune-pod.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: 172.31.228.68/project/fortune:args name: html-generator # command: args: ["6"] # 参数会在pod运行时传递给容器 volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: &#123;&#125;# 多个值args:- foo- bar- "10"# 字符串无需用引号标记,数值需要 # 通过命令行传参指定参数是给容器传递配置选项的其中一种方法,下面是另一种通过环境变量完成配置[root@k8s-master1 forloop]# kubectl logs fortune-t6c87 -c html-generator设定的间隔是 6 秒 容器设置环境变量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 脚本中删除 命令行传参 NUM=$1# 如果是Python编写 获取变量的方法 os.environ['NUM'][root@k8s-master2 env]# vim fortuneloop.sh #!/bin/bashtrap "exit" SIGINTmkdir -p /var/htdocsecho "设定的间隔是 $&#123;NUM&#125; 秒"for i in &#123;1..5&#125;:do echo "$(date "+%H:%M:%S") $i" &gt;&gt; /var/htdocs/index.html sleep $NUM;done[root@k8s-master2 env]# docker build -t 172.31.228.68/project/fortune:env .[root@k8s-master2 env]# docker push 172.31.228.68/project/fortune:env[root@k8s-master1 forloop-env]# vim fortune-pod.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: 172.31.228.68/project/fortune:env name: html-generator # command: # args: ["6"] env: - name: NUM value: "22" volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: &#123;&#125;[root@k8s-master1 forloop-env]# kubectl logs fortune-d82g5 -c html-generator设定的间隔是 22 秒[root@k8s-master1 forloop-env]# kubectl exec -it fortune-d82g5 bash -c html-generator[root@fortune-d82g5 /]# ps -x PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/sh /root/fortuneloop.sh 5 # dockerfile 里面传了 cmd 16 ? S 0:00 sleep 22 22 pts/0 Ss 0:00 bash 35 pts/0 R+ 0:00 ps -x 环境变量引用其他环境变量12345678env:- name: NUM value: "22"- name: Name_NUM value: "$(NUM)Leo" # Name_NUM 的值是 22Leo # command 和 args 也可以这样使用 硬编码环境变量12341. pod中定义硬编码环境变量 无法有效区分 生产环境与开发环境的pod定义2. 为了能够复用pod的定义。需要将配置从pod中解耦出来3. ConfigMap 资源对象可以完成解耦 4. 用valueFrom 字段代替 value字段,使ConfigMap成为环境变量值的来源 使用 ConfigMap 解耦配置123456781. K8S 可以将配置选项分离到 单独的资源对象 ConfigMap中2. 本质是 键值对映射3. 值可以使配置字段 也 可以是 完整的配置文件4. 应用无法直接读取 ConfigMap 5. 映射内容通过 环境变量 或者 卷文件的形式传送给容器6. pod 通过环境变量 或者 ConfigMap卷 来使用ConfigMap7. 有助于区分不同环境(开发、测试、生产)下多分不同的配置清单 8. pod 通过 ConfigMap 名称来引用,多环境下使用相同的pod定义,保持不同的ConfigMap使用不同的环境配置 创建 ConfigMap通过 kubectl 创建 ConfigMap123456789101112131415161718# 1. ConfigMap的键名 必须是一个合法的DNS子域 数字字母、破折号、下划线和圆点[root@k8s-master1 demo]# kubectl create configmap fortune-config --from-literal=sleep-interval=25configmap/fortune-config created[root@k8s-master1 demo]# kubectl get configmapNAME DATA AGEfortune-config 1 107s# 2. 创建多映射键值对的 ConfigMap [root@k8s-master1 demo]# kubectl create configmap my-config --from-literal=name=leo --from-literal=age=29configmap/my-config created[root@k8s-master1 demo]# kubectl get cmNAME DATA AGEfortune-config 1 4m22smy-config 2 10s 通过 yaml 文件创建 ConfigMap12345678910111213141516[root@k8s-master1 configmap]# kubectl get cm fortune-config -o yaml &gt; fortune-config.yaml[root@k8s-master1 configmap]# vim fortune-config.yaml apiVersion: v1kind: ConfigMapmetadata: name: fortune-config # 通过name引用configmap namespace: defaultdata: # 数据 键值对 sleep-interval: "25"[root@k8s-master1 configmap]# kubectl create -f fortune-config.yaml [root@k8s-master1 configmap]# kubectl get cmNAME DATA AGEfortune-config 1 14s 从文件内容创建 ConfigMap123456789101112131415[root@k8s-master1 configmap]# cat my.conf leo[root@k8s-master1 configmap]# kubectl create configmap my-config --from-file=my.confconfigmap/my-config created[root@k8s-master1 configmap]# kubectl describe cm my-configName: my-configNamespace: defaultData====my.conf: # 健----leo # 值 12345678910111213# 指定键名[root@k8s-master1 configmap]# kubectl create configmap my-config --from-file=name=my.confconfigmap/my-config created[root@k8s-master1 configmap]# kubectl describe cm my-configName: my-configNamespace: defaultData====name:----leo 1234567891011121314151617[root@k8s-master1 configmap]# cat redis.conf redis.hosts=127.0.0.1redis.port=6379redis.password=123456[root@k8s-master1 configmap]# kubectl describe cm reids-configName: reids-configNamespace: default...Data====redis.conf:----redis.hosts=127.0.0.1redis.port=6379redis.password=123456 使用 变量方式 传给容器123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master1 configmap]# vim fortune-pod.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortune-envspec: selector: matchLabels: app: fortune-env template: metadata: labels: app: fortune-env spec: containers: - image: 172.31.228.68/project/fortune:env name: html-generator env: - name: NUM # 设置环境变量NUM valueFrom: # valueFrom 替换 value configMapKeyRef: # 使用 configMap name: fortune-config # configMap 名称 key: sleep-interval # configMap 下的键sleep-interval的值 赋值给 NUM volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: &#123;&#125;[root@k8s-master1 configmap]# kubectl logs fortune-env-t5nfq -c html-generator设定的间隔是 25 秒 在 Pod 中引用不存在的 ConfigMap1231. k8s 正常调度尝试所有容器2. 引用不存在的ConfigMap容器会失败，其余容器正常启动，直到创建缺失的ConfigMap3. configMapKeyRef.optional: true 设置该项,即使ConfigMap不存在也不影响启动容器 使用 envFrom 一次性传入所有ConfigMap下的键值对12345678910[root@k8s-master1 configmap]# vim my.conf apiVersion: v1kind: ConfigMapmetadata: name: my-config namespace: defaultdata: name: Leo age: "28" 12345678910111213141516171819202122232425262728[root@k8s-master1 configmap]# vim my-pod.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: busybox image: busybox command: [ "/bin/sh", "-c", "echo $(CONFIG_NAME) $(CONFIG_AGE)" ] envFrom: - prefix: CONFIG_ configMapRef: name: my-config restartPolicy: Never# envFrom 代替 env# configMapRef 代替 configMapKeyRef# - prefix: CONFIG_ 所有的环境变量都增加了前缀 CONFIG_ # 引用名称为 my-config 的 configMap# k8s 不会主动转换键名 环境变量中不要带 - 破折号,如果格式不正确该键值对会被忽略，也不发送通知# 前缀设置是可选的，如果不添加前缀,就使用configmap中的键名[root@k8s-master1 configmap]# kubectl logs mypodLeo 28 使用 命令行参数 传给容器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master1 forloop]# vim fortune-pod.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: 172.31.228.68/project/fortune:args name: html-generator env: - name: NUM valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval args: ["$(NUM)"] volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: &#123;&#125;# 使用参数读取的镜像# 引入环境变量env# 在参数设置中引用环境变量[root@k8s-master1 forloop]# kubectl logs fortune-zt854 -c html-generator设定的间隔是 25 秒 使用 卷形式暴露 ConfigMap121. 环境变量和命令行传参 作为配置值通常适用于变量值较短的场景2. 如果想要暴露一个完整的配置文件 可以使用ConfigMap 卷的形式传入容器 123456789101112131415[root@k8s-master1 configmap-file]# vim my-nginx-config.conf server &#123; listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125;&#125; 12[root@k8s-master1 configmap-file]# cat sleep-interval 30 1234567[root@k8s-master1 configmap-file]# cat redis.cfgredis.hosts=127.0.0.1redis.port=6379redis.password=123456# 之前叫做 redis.conf 的时候挂载configmap到nginx目录下 结果nginx读取该文件报错了 12# 现在我有三段不同的配置文件 nginx 延迟值 和 redis配置 # 我现在想把他们打到一个 configmap里 将整个文件夹下的所有配置文件 创建 ConfigMap12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master1 configmap-file]# kubectl create configmap fortune-config --from-file=/opt/demo/configmap-file/configmap/fortune-config created# 使用yaml文件查看[root@k8s-master1 configmap-file]# kubectl get cm fortune-config -o yaml &gt; fortune-config.yaml[root@k8s-master1 configmap-file]# vim fortune-config.yaml apiVersion: v1data: my-nginx-config.conf: | server &#123; listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; &#125; redis.cfg: | redis.hosts=127.0.0.1 redis.port=6379 redis.password=123456 sleep-interval: | 30kind: ConfigMapmetadata: name: fortune-config namespace: default# 1. 三条数据 他们的键名都是各自的文件名 # 2. 所有条目数据的第一行最后的 “|” 管道符号 标识后续的健值是多行 Volume 卷形式挂载到 pod 中123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master1 configmap-file]# vim fortune-pod-cm-volume.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: 172.31.228.68/project/fortune:env name: html-generator env: - name: NUM valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: &#123;&#125; - name: config configMap: name: fortune-config 12345678910# 测试[root@k8s-master1 configmap-file]# curl -H "Accept-Encoding: gzip" -I 10.0.0.82HTTP/1.1 200 OKServer: nginx/1.16.1Date: Sun, 22 Mar 2020 02:27:26 GMTContent-Type: text/htmlLast-Modified: Sun, 22 Mar 2020 02:26:56 GMTConnection: keep-aliveETag: W/"5e76ccf0-90"Content-Encoding: gzip # 响应压缩 123456789# 查看挂载的文件[root@k8s-master1 configmap-file]# kubectl exec fortune-dncw4 -c web-server ls /etc/nginx/conf.dmy-nginx-config.confredis.cfgsleep-interval# configmap的三条数据都被作为文件挂载到指定目录下# 采用多个configmap去配置同一个pod中的不同容器的配置 不太合理# 同一个pod中的容器是紧密相联的，需要被当做一个整体单元来看 卷内暴露指定的 ConfigMap 数据12345# 想让my-nginx-config.conf作为文件暴露在 /etc/nginx/conf.d下# sleep-interval 作为 环境变量传入# 如果将来 有两个镜像 要通过cm 挂载不同路径的配置文件# 首先 两个地方都要配置 挂载点 都挂载同一个cm # 通过items 去区分挂载的文件 123456789volumes:- name: html emptyDir: &#123;&#125;- name: config configMap: name: fortune-config items: - key: my-nginx-config.conf # 要暴露的文件 path: gzip.conf # 该文件的键值对会被赋予到这个文件中 12[root@k8s-master1 configmap-file]# kubectl exec fortune-kbddc -c web-server ls /etc/nginx/conf.dgzip.conf 注意点12341. 挂载某一个文件会隐藏该文件夹中已存在的文件2. 如果我们之前镜像中 /etc/nginx/conf.d 目录下已经有了文件,通过卷挂载原本目录下的文件会被隐藏3. Linux系统挂载文件系统至非空文件夹通常如此,想想nfs和mount挂载4. 如果我们挂载到 /etc 目录就非常危险.会导致容器系统损坏，添加文件至某个文件夹 可千万不要到系统目录啊 更新应用配置且不重启应用程序1231. 环境变量和命令行传参作为配置源的弊端在于无法修改pod中的配置2. 将configmap 暴露为卷可以达到配置热更新的效果，无需重新创建Pod或者重启容器 3. configmap 更新后 卷中引用所有文件也会更新，进程发现文件更新后重新加载 123[root@k8s-master1 configmap-file]# kubectl edit cm fortune-config# 修改这行 关闭压缩gzip off; 123[root@k8s-master1 configmap-file]# kubectl exec fortune-kbddc -c web-server cat /etc/nginx/conf.d/gzip.conf# 查看容器中文件已经被修改了gzip off; 123456789101112131415# 手动通知 nginx 重启程序[root@k8s-master1 configmap-file]# kubectl exec fortune-kbddc -c web-server -- nginx -s reload[root@k8s-master1 configmap-file]# curl -H "Accept-Encoding: gzip" -I 10.0.0.82HTTP/1.1 200 OKServer: nginx/1.16.1Date: Sun, 22 Mar 2020 07:18:45 GMTContent-Type: text/htmlContent-Length: 660Last-Modified: Sun, 22 Mar 2020 07:18:44 GMTConnection: keep-aliveETag: "5e771154-294"Accept-Ranges: bytes# 无需重启容器或者重建Pod，修改了配置 Secret 给容器传递敏感数据123451. Secret 可作为环境变量传值给容器2. Secret 可作为卷 传文件给容器3. ConfigMap 存储非敏感数据4. Secret 存储敏感数据 类似 证书私钥等5. 如果一个配置文件即有敏感,又有一般数据，该文件应该被存储在Secret中 默认令牌 Secret123456789101112131415161718192021222324252627282930313233343536373839404142434445461. 默认被挂载只所有容器的 Secret # 查看任意 pod [root@k8s-master1 configmap-file]# kubectl describe pod fortune-qcvv4 default-token-r29ch: Type: Secret (a volume populated by a Secret) SecretName: default-token-r29ch Optional: false[root@k8s-master1 configmap-file]# kubectl get secretNAME TYPE DATA AGEdefault-token-r29ch kubernetes.io/service-account-token 3 12d[root@k8s-master1 configmap-file]# kubectl describe secretName: default-token-r29chNamespace: defaultLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 076d498a-c439-4148-b7c1-c727eeb441b6Type: kubernetes.io/service-account-tokenData====ca.crt: 1359 bytesnamespace: 7 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IllYeGFSNlE5RjhBR1UtX2tSanpmT0JJeUdZX05hb2pQck9pNWdmMUVvUVUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tcjI5Y2giLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjA3NmQ0OThhLWM0MzktNDE0OC1iN2MxLWM3MjdlZWI0NDFiNiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.OYVDkBJXBKmQs171S3_MdynbhrZjrGT0xOZ8T4h0DkwabOOxIhshyJlCwwD4zeR-wc3BfeWP8Pvg9YWvaY4vCqZC34khLv8GupLvW8qCGp21pbNPRL9E8EDKPuAk3YGs2lnFOsFaRIXKdJI3dqDyUzEVxKu2c5UZ0hul0wsJoabtZhyjSQUoenSX-am2vFNqYecfUxKMUye2HnRdQX_rtk71xZO_WGfjHMJhdlcPO0G4WMBwoNqzZ3fxi_zkakJvLah2Em2TpL1HmBjzJblOTGSsGHE0LiL9RgUIzyN4stEtGy2nDzwlALwIjAsBSSFbCLNavDf305st1E5SRCCnfg# 三段数据 # ca.crt# namespace# token# 他们是包含了从Pod内部安全访问K8s API服务器所需要的信息 挂载位置在Mounts:/var/run/secrets/kubernetes.io/serviceaccount from default-token-r29ch (ro)[root@k8s-master1 configmap-file]# kubectl exec fortune-qcvv4 ls /var/run/secrets/kubernetes.io/serviceaccountca.crtnamespacetoken# 应用程序可以通过他们访问API服务器 # default-token-r29ch 被自动创建，且对应的卷被自动挂载到每个Pod上# 默认令牌 Secret 创建 Secret1234567891011121314151617181920212223242526272829303132333435# 测试将https 的证书秘钥传送给Nginx容器# 自制证书# 自制证书[root@k8s-master1 secret]# openssl genrsa -out https.key 2048[root@k8s-master1 secret]# openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.kubia.example.com[root@k8s-master1 secret]# echo bar &gt; foo[root@k8s-master1 secret]# ls -ltotal 12-rw-r--r-- 1 root root 4 Mar 22 15:49 foo-rw-r--r-- 1 root root 1127 Mar 22 15:49 https.cert-rw-r--r-- 1 root root 1675 Mar 22 15:48 https.key# 创建3个文件的 Secret [root@k8s-master1 secret]# kubectl create secret generic fortune-https --from-file=https.key --from-file=https.cert --from-file=foo secret/fortune-https created[root@k8s-master1 secret]# kubectl get secret fortune-https -o yamlapiVersion: v1data: foo: YmFyCg== https.cert: LS0tLS1CRUdJTiBDRVJUSUZJ... https.key: LS0tLS1CRUdJTiBSU0E... kind: Secretmetadata: creationTimestamp: "2020-03-22T07:51:37Z" name: fortune-https namespace: default resourceVersion: "210797" selfLink: /api/v1/namespaces/default/secrets/fortune-https uid: 0ef65bf7-45e1-4dd8-9286-5634eaee6b4dtype: Opaque# Secret中的内容会被以 Base64格式编码 # ConfigMap 直接以纯文本显示 在 Pod 中读取 Secret12341. 通过secret卷 将Secret暴露给容器2. Secret中的数据会被解码并且以真实形式(纯文本或者二进制)写入对应的文件3. 通过环境变量暴露也是如此4. 应用程序无需主动解码,直接读取文件内容或者使用环境变量 在 Pod 中使用 Secret1234567891011121314151617181920212223242526272829# 开启之前nginx配置的 https选项[root@k8s-master1 secret]# kubectl get cm[root@k8s-master1 secret]# kubectl edit cm fortune-config...apiVersion: v1data: my-nginx-config.conf: | server &#123; listen 80; listen 443 ssl; server_name www.kubia-example.com; ssl_certificate certs/https.cert; ssl_certificate_key certs/https.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; gzip off; gzip_types text/plain application/xml; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; &#125;...# 证书的位置在 /etc/nginx/certs 下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 挂载 secret 到 pod里 [root@k8s-master1 secret]# vim fortune-https-pod.yaml metadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: 172.31.228.68/project/fortune:env name: html-generator env: - name: NUM valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:1.16 name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs # nginx从/etc/nginx/certs/下读取证书和秘钥文件 mountPath: /etc/nginx/certs/ # 需要将 secret卷挂载到这个目录下 readOnly: true ports: - containerPort: 80 protocol: TCP - containerPort: 443 volumes: - name: html emptyDir: &#123;&#125; - name: config configMap: name: fortune-config items: - key: my-nginx-config.conf path: gzip.conf - name: certs # 引用fortune-https Secret 来定义 secret 卷 secret: secretName: fortune-https 12345678910111213141516171819[root@k8s-master1 secret]# vim fortune-svc.yaml apiVersion: v1kind: Servicemetadata: name: fortunespec: type: NodePort ports: - name: http port: 80 targetPort: 80 nodePort: 30123 - name: https port: 443 targetPort: 443 nodePort: 30443 selector: app: fortune 1234567891011121314151617181920212223242526272829303132333435363738394041# 测试[root@k8s-master1 secret]# kubectl create -f fortune-https-pod.yaml [root@k8s-master1 secret]# kubectl create -f fortune-svc.yaml [root@k8s-master1 secret]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEfortune NodePort 10.0.0.167 &lt;none&gt; 80:30123/TCP,443:30443/TCP 93skubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 12d[root@k8s-master1 secret]# curl -k https://10.0.0.167:443[root@k8s-master1 secret]# curl -k -v https://10.0.0.167:443* About to connect() to 10.0.0.167 port 443 (#0)* Trying 10.0.0.167...* Connected to 10.0.0.167 (10.0.0.167) port 443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* skipping SSL peer certificate verification* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384* Server certificate:* subject: CN=www.kubia.example.com # 匹配证书* start date: Mar 22 07:49:14 2020 GMT* expire date: Mar 20 07:49:14 2030 GMT* common name: www.kubia.example.com* issuer: CN=www.kubia.example.com&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: 10.0.0.167&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK&lt; Server: nginx/1.16.1&lt; Date: Sun, 22 Mar 2020 08:32:47 GMT&lt; Content-Type: text/html&lt; Content-Length: 168&lt; Last-Modified: Sun, 22 Mar 2020 08:32:17 GMT&lt; Connection: keep-alive&lt; ETag: "5e772291-a8"&lt; Accept-Ranges: bytes# https://www.kubia-example.com:30443/ Secret 卷 存储于内存1234[root@k8s-master1 secret]# kubectl exec fortune-95lsq -c web-server -- mount | grep certstmpfs on /etc/nginx/certs type tmpfs (ro,relatime)# 由于是 tmpfs ,存储在secret中的数据不会写入磁盘,无法被窃取 在私有镜像仓库中 使用 Secret1234567890. 镜像仓库 设置为 私有仓库,并且上传一个镜像1. 定义一个 类型为 docker-registry 的 Secret ,需要指定 用户名 密码 邮箱2. pod 定义中 引用 Secret 镜像仓库凭据，imagePullSecrets [root@k8s-master1 secret]# kubectl create secret docker-registry my-harbor-secret --docker-username=admin --docker-password=lx@68328153 --docker-email=253911339@qq.com --docker-server=172.31.228.68[root@k8s-master1 secret]# kubectl get secret NAME TYPE DATA AGEmy-harbor-secret kubernetes.io/dockerconfigjson 1 17s 12345678910111213141516171819202122232425262728[root@k8s-master1 secret]# vim base.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: basespec: selector: matchLabels: app: base template: metadata: labels: app: base spec: imagePullSecrets: # 通过 my-harbor-secret 凭证从私有镜像中拉取镜像 - name: my-harbor-secret containers: - image: 172.31.228.68/game/base name: html-generator ports: - containerPort: 80 protocol: TCP[root@k8s-master1 secret]# kubectl get podsNAME READY STATUS RESTARTS AGEbase-z5h5v 1/1 Running 0 12s 不需要为每个 Pod 指定镜像拉取 Secret121. 如果系统中运行大量Pod，是否每个Pod都要配置镜像拉取Secret 2. 后面可以学习通过添加Secret至ServiceAccount使所有Pod都能自动添加镜像拉取Secret]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15 K8S 存储 卷]]></title>
    <url>%2F2020%2F03%2F11%2Fk8s-base17%2F</url>
    <content type="text"><![CDATA[将磁盘挂载到容器121. 容器重新启动时,卷的内容将保持不变，新容器可以识别前一个容器写入卷的所有文件2. 一个pod中有多个容器,这个卷可以被所有容器使用 emptyDir 卷12341. 创建一个空卷，挂载到Pod中的容器。2. Pod删除该卷也会被删除,随着pod的生命周期 而存在3. 应用场景：Pod中容器之间数据共享,一个pod中有多个容器,他们之间完成数据共享,不使用数据卷容器之间的文件系统是隔离的，只能看到自己的4. 使用数据卷让容器之间某个目录达到共享 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152531. emptyDir 临时将数据写入磁盘[root@k8s-master1 demo]# cat fortune-svc.yaml apiVersion: v1kind: Servicemetadata: name: fortune-nodeportspec: type: NodePort ports: - name: http port: 80 targetPort: 80 nodePort: 30123 selector: app: fortune[root@k8s-master1 demo]# cat fortune-pod.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: fortunespec: selector: matchLabels: app: fortune template: metadata: labels: app: fortune spec: containers: - image: centos:7 # 容器1 写数据到 /var/htdocs/index.html name: html-generator command: ["bash","-c","for i in &#123;1..100&#125;;do echo $i &gt;&gt; /var/htdocs/index.html;sleep 10;done"] volumeMounts: # html卷挂载到/var/htdocs - name: html mountPath: /var/htdocs - image: nginx:1.16 # 容器2 web服务 name: web-server volumeMounts: # html卷挂载到/usr/share/nginx/html - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: # 设定一个名字叫 html的emptyDir卷，挂载到上面的两个容器里 - name: html emptyDir: &#123;&#125; 1231. pod 包含两个容器和1个挂载到两个容器的共用卷，在不同的路径上2. 当html-generator 容器启动时 每10秒循环一个数据输出到 /var/htdocs/index.html 3. web-server 是 nginx服务 他的静态目录为 /usr/share/nginx/html hostPath 卷12345# 1. hostPath卷指向节点文件系统上的目录# 2. 在同一个节点上运行并且hostPath卷使用相同路径的POD 可以看到相同的文件# 3. hostPath 卷 是持久性存储# 4. emptyDir 卷 的内容会随着pod被删除时而删除# 5. hostPath 用来访问节点上的数据，常用于单节点的持久化存储,不要用它来做跨pod的持久化数据 12345678[root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-w2xnx 1/1 Running 5 7d18hkube-flannel-ds-amd64-q29r8 1/1 Running 10 9dkube-flannel-ds-amd64-xh7c4 1/1 Running 9 9dkube-flannel-ds-amd64-xq7gq 1/1 Running 10 9d[root@k8s-master1 ~]# kubectl describe pod kube-flannel-ds-amd64-q29r8 -n kube-system 持久化存储持久化存储的作用121. 当运行在一个Pod中的应用程序需要将数据保存到磁盘上,并且该Pod重新调度到另外一个节点时,也要求具有相同的数据可用。2. 数据需要可以从任何集群节点访问,因此必须将数据存储在某种类型的网络存储(NAS)中 使用NFS作为底层存储11. 集群并未运行在云服务器上 而是自建环境，需要有自己的存储集群 12345# 选择 master2 作为服务端# 关键：客户端，也就是node上也要安装[root@k8s-node1 ~]# yum install -y nfs-utils [root@k8s-node2 ~]# yum install -y nfs-utils [root@k8s-master2 ~]# yum install -y nfs-utils 12345678910# 配置服务端的访问路径 启动服务端守护进程# 得有这个目录才能挂载[root@k8s-master2 ~]# mkdir -p /data/nfs[root@k8s-master2 ~]# vim /etc/exports/data/nfs *(rw,no_root_squash)[root@k8s-master2 ~]# systemctl start nfs[root@k8s-master2 ~]# ps -ef|grep nfs 创建 mongodb 数据库 pod 测试持久化存储1234567891011121314151617181920212223# 配置启动[root@k8s-master1 demo]# vim mongodb-pod.yaml apiVersion: v1kind: Podmetadata: name: mongodbspec: volumes: # 定义卷 - name: mongodb-data # 卷名 在挂载的时候需要用到 nfs: # 使用nfs共享作为卷的外部存储 server: 172.31.228.68 # nfs 服务器地址 path: /data/nfs # nfs 服务挂载路径 containers: - image: mongo name: mongodb volumeMounts: # mongodb-data 卷挂载到 /data/db - name: mongodb-data # 挂载卷的时候引用卷名 mountPath: /data/db # mongodb 默认的数据存放路径 ports: - containerPort: 27017 # mongodb 服务端口 protocol: TCP 向 MongoDB pod 里面插入数据123456789101112[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmongodb 1/1 Running 0 73s 10.244.0.31 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.insert(&#123;name:'leo'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.doo.find()&#123; "_id" : ObjectId("5e72e43d3bb458e3186a18c5"), "name" : "leo" &#125; 测试持久化存储1234567891011121314151617181920# 删除 Pod [root@k8s-master1 demo]# kubectl delete pod mongodbpod "mongodb" deleted# 重新创建到其他节点# 如果一直在node1创建,使用节点选择器指定node上创建[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmongodb 1/1 Running 0 83s 10.244.1.40 k8s-node2 &lt;none&gt; &lt;none&gt;# 查看数据 [root@k8s-master1 demo]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.find()&#123; "_id" : ObjectId("5e72e43d3bb458e3186a18c5"), "name" : "leo" &#125;# 1. 删除了Pod并重建,数据仍然存在,后端使用持久化存储,多个Pod可以将数据写入持久化存储来保存数据# 2. 清理该Pod,但是先不清理nfs中的数据,后面继续测试 其他存储技术1234567891. 参考: 官方文档https://kubernetes.io/docs/concepts/storage/volumes/2. Kubernetes中的Volume提供了在容器中挂载外部存储的能力Pod需要设置卷来源（spec.volume）和挂载点（spec.containers.volumeMounts）两个信息后才可以使用相应的Volume3.本地数据卷hostPath 持久化 挂载node中的目录到pod中emptyDir 非持久化 随着pod删除数据也一并删除 从底层存储技术 解耦 Pod持久卷和持久卷声明1231. 开发人员需要一定数量的持久化存储 向K8S请求 类似创建Pod时请求CPU和内存 2. 系统管理员配置存储服务 让 开发人员能使用 3. 之前我们一直使用Pod常规卷,下面学习 持久卷 持久卷1234561. PersistenVolume（PV） 2. 专业的存储人员来做底层存储，然后通过k8s注册持久卷3. 创建持久卷的时候,可以指定大小和访问模式4. 集群用户需要在Pod中使用使用持久卷(PV)时,先创建持久卷声明(PVC),指定需要的容量和访问方式5. 持久卷声明 会发送给 k8s的api,k8s找到匹配的持久卷PV并绑定到PVC上6. 持久卷声明 可以当做Pod的一个卷来使用,其他用户不能使用相同的持久卷,除非先通过删除PVC绑定来释放 1234567持久卷由集群管理员创建,并被Pod通过持久卷生命消费# PV 和 PVC 流程1. 集群管理员创建某类型的网络存储(NFS或者Ceph) 2. 集群管理员向k8s api 发送请求 创建持久卷PV 3. 用户(使用者) 创建持久卷声明(PVC)4. k8s找到一个匹配足够容量的PV并设置访问模式,最终将这个PVC绑定给这个PV5. 用户创建一个Pod，并通过配置卷引用PVC 创建 PV 持久卷12341. PersistenVolume（PV）：对存储资源创建和使用的抽象，使得存储作为集群中的资源管理。(专业的存储人员来做)2. PV 可以是存储人员定义,他们会创建很多pv等待pvc来挂载静态: 手动创建资源动态: 自动创建PV 12345678# 在Kubernetes集群中，PV 作为存储资源存在。PVC 是对PV资源的请求和使用，也是对PV存储资源的”提取证”，而Pod通过PVC来使用PV。# PV 和 PVC 之间的交互过程有着自己的生命周期，这个生命周期分为5个阶段：1. 供应(Provisioning)：即PV的创建，可以直接创建PV（静态方式），也可以使用StorageClass 动态创建2. 绑定（Binding）：将PV分配给PVC3. 使用（Using）：Pod通过PVC使用该Volume4. 释放（Releasing）：Pod释放Volume并删除PVC5. 回收（Reclaiming）：回收PV，可以保留PV以便下次使用，也可以直接从云存储中删除 12345# 存储卷的存在下面的4种状态：1. Available：可用状态，处于此状态表明PV以及准备就绪了，可以被PVC使用了。2. Bound：绑定状态，表明PV已被分配给了PVC。3. Released：释放状态，表明PVC解绑PV，但还未执行回收策略。4. Failed：错误状态，表明PV发生错误。 12345678910111213141516171819202122232425[root@k8s-master1 demo]# kubectl explain PersistentVolumeKIND: PersistentVolumeVERSION: v1[root@k8s-master1 demo]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: mongodb-pvspec: capacity: # 定义pv 容量大小 storage: 5Gi accessModes: # 访问模式 - ReadWriteMany persistentVolumeReclaimPolicy: Retain # 持久化卷 回收策略 当声明释放后,pv将会被保留(不删除和释放) nfs: # 存储类型 位置 和 其他属性 path: /data/nfs server: 172.31.228.681. 访问模式的可选范围如下： ReadWriteOnce： 该卷能够以 读写模式 被加载到一个节点上。 ReadOnlyMany： 该卷能够以 只读模式 加载到多个节点上。 ReadWriteMany： 该卷能够以 读写模式 被多个节点同时加载。 123456789[root@k8s-master1 demo]# kubectl create -f nfs-pv.yaml persistentvolume/mongodb-pv created[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Retain Available 4s# Retain 默认回收策略# Available 可用状态 创建 PVC 持久卷声明12345678910111213141516[root@k8s-master1 demo]# kubectl explain PersistentVolumeClaimKIND: PersistentVolumeClaimVERSION: v1[root@k8s-master1 demo]# vim nfs-pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvc # 声明(pvc)的名称,pod中会引用到spec: resources: # 申请5G的存储空间 requests: storage: 5Gi accessModes: # 访问模式 - ReadWriteMany 123451. resources 资源; 财力2. 注意：持久卷的容量必须满足声明的需求,并且卷的访问模式必须包含申明中指定的访问模式否则pvc会无限期的处于未绑定状态,一旦存在匹配的PV，PVC绑定此PV，就算集群中存在很多的50G的PV，需要100G容量的PVC也不会匹配满足需求的PV。直到集群中有100G的PV时，PVC才会被绑定。3. 如果没有指定存储类和设置选取器，PVC会根据存储空间容量大小和访问模式匹配符合的PV 1234567891011121314151617[root@k8s-master1 demo]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound mongodb-pv 5Gi RWX 82s# 在 CLI 下，访问模式缩写为: RWO、ROX、RWX涉及工作节点数量而不是Pod数量RWO：ReadWriteOnceROX：ReadOnlyManyRWX：ReadWriteMany[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Retain Bound default/mongodb-pvc 30m# Bound 进入绑定状态 将PV分配给PVC # mongodb-pv 持久卷绑定给 default/mongodb-pvc 持久卷声明# default/mongodb-pvc default是持久卷声明的命名空间 # 持久卷PV是集群范围的，持久卷声明需要被同一命名空间的Pod创建 Pod 使用 PVC 持久卷声明12345678910111213141516171819202122[root@k8s-master1 demo]# vim mongodb-pod-pvc.yaml apiVersion: v1kind: Podmetadata: name: mongodbspec: # nodeSelector: # gpu: "true" volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc # 引用持久卷声明 pvc name containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP 12345678910111213[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmongodb 1/1 Running 0 19s 10.244.0.35 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl exec -it mongodb mongo[root@k8s-master1 demo]# kubectl create -f mongodb-pod-pvc.yaml pod/mongodb created&gt; use mystoreswitched to db mystore&gt; db.doo.find()&#123; "_id" : ObjectId("5e72e43d3bb458e3186a18c5"), "name" : "leo" &#125; 使用 PV 和 PVC 的好处121. 研发人员不用担心 底层的存储技术 只需要申请容量和访问模式2. 其他的 pod 无法使用一个 被绑定的pvc 回收持久卷12345678910111213141516171819# 删除 Pod [root@k8s-master1 demo]# kubectl delete pod mongodb# 删除 PVC[root@k8s-master1 demo]# kubectl delete pvc mongodb-pvcpersistentvolumeclaim "mongodb-pvc" deleted# 重新创建 PVC [root@k8s-master1 demo]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Pending 8s# 为什么不绑定 查看 PV [root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Retain Released default/mongodb-pvc 116m# Released 释放状态，表明PVC解绑PV，但还未执行回收策略。# 说明这个卷里面的数据 还没有被清理 手动回收持久卷121. persistentVolumeReclaimPolicy: Retain ，该设置告诉k8s，PVC和PV解绑后,保留卷和数据2. 手动回收 删除和更新创建持久卷资源,你可以自己决定是否删除底层存储中的文件,也可以不删除在下一个Pod中复用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 测试不删除底层数据,然后手动删除持久卷,重建资源[root@k8s-master1 demo]# kubectl delete pv mongodb-pvpersistentvolume "mongodb-pv" deleted# 底层数据不清理[root@k8s-master2 opt]# cd /data/nfs/[root@k8s-master2 nfs]# ls -ltotal 336-rw------- 1 polkitd input 20480 Mar 19 17:27 collection-0-485674314440757453.wt-rw------- 1 polkitd input 36864 Mar 19 17:27 collection-2-485674314440757453.wt-rw------- 1 polkitd input 4096 Mar 19 11:18 collection-4-485674314440757453.wt-rw------- 1 polkitd input 20480 Mar 19 17:27 collection-8-485674314440757453.wtdrwx------ 2 polkitd input 4096 Mar 19 17:27 diagnostic.data-rw------- 1 polkitd input 20480 Mar 19 17:27 index-1-485674314440757453.wt-rw------- 1 polkitd input 36864 Mar 19 17:27 index-3-485674314440757453.wt-rw------- 1 polkitd input 4096 Mar 19 17:27 index-5-485674314440757453.wt-rw------- 1 polkitd input 12288 Mar 19 17:27 index-6-485674314440757453.wt-rw------- 1 polkitd input 20480 Mar 19 11:18 index-9-485674314440757453.wtdrwx------ 2 polkitd input 4096 Mar 19 17:15 journal-rw------- 1 polkitd input 36864 Mar 19 17:27 _mdb_catalog.wt-rw------- 1 polkitd input 0 Mar 19 17:27 mongod.lock-rw------- 1 polkitd input 36864 Mar 19 17:27 sizeStorer.wt-rw------- 1 polkitd input 114 Mar 19 11:14 storage.bson-rw------- 1 polkitd input 47 Mar 19 11:14 WiredTiger-rw------- 1 polkitd input 4096 Mar 19 17:27 WiredTigerLAS.wt-rw------- 1 polkitd input 21 Mar 19 11:15 WiredTiger.lock-rw------- 1 polkitd input 1188 Mar 19 17:27 WiredTiger.turtle-rw------- 1 polkitd input 61440 Mar 19 17:27 WiredTiger.wt# 重建持久卷 持久卷声明 Pod[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Retain Available 3s[root@k8s-master1 demo]# kubectl create -f nfs-pvc.yaml persistentvolumeclaim/mongodb-pvc created[root@k8s-master1 demo]# kubectl create -f mongodb-pod-pvc.yaml pod/mongodb created# 数据复用[root@k8s-master1 demo]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.find()&#123; "_id" : ObjectId("5e72e43d3bb458e3186a18c5"), "name" : "leo" &#125; 测试删除 PV和底层数据123456789101112131415161718192021222324252627282930313233[root@k8s-master1 demo]# kubectl delete pod mongodb[root@k8s-master1 demo]# kubectl delete pvc mongodb-pvc[root@k8s-master1 demo]# kubectl delete pv mongodb-pv[root@k8s-master2 nfs]# rm -rf *[root@k8s-master1 demo]# kubectl create -f nfs-pv.yaml [root@k8s-master1 demo]# kubectl create -f nfs-pvc.yaml [root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Retain Bound default/mongodb-pvc 7s[root@k8s-master1 demo]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound mongodb-pv 5Gi RWX 8s[root@k8s-master1 demo]# kubectl create -f mongodb-pod-pvc.yaml pod/mongodb created# 无法复用之前的数据,已经被手动清理[root@k8s-master1 demo]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.find()&gt; db.doo.insert(&#123;name:'Lex'&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.doo.find()&#123; "_id" : ObjectId("5e733ee540bc22899dc58827"), "name" : "Lex" &#125;# 数据已重新被创建[root@k8s-master2 nfs]# ls -l... 自动回收持久卷123456789# 当前的回收策略可选值包括：Retain: 持久化卷被释放后，需要手工进行回收操作。Recycle: 基础擦除（“rm-rf /thevolume/*”） 可用于再次声明，可被不同的持久卷声明和Pod引用Delete: 相关的存储资产，例如AWSEBS或GCE PD卷一并删除。 彻底删除底层目前，只有NFS和HostPath支持Recycle策略，AWSEBS、GCE PD支持Delete策略。# 再创建持久卷PV的时候,一定要检查卷中的底层存储支持什么回收策略# 可以更改当前使用的持久卷的回收策略，如果之前的delete或者Recycle，那么可改成Retain，手动回收来保持重要数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 测试 nfs 的 Recycle 回收策略 是否删除了底层数据 和 被新的PVC复用[root@k8s-master1 demo]# vim nfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: mongodb-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle nfs: path: /data/nfs server: 172.31.228.68[root@k8s-master1 demo]# kubectl apply -f nfs-pv.yaml [root@k8s-master1 ~]# kubectl edit pv mongodb-pv[root@k8s-master1 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound mongodb-pv 5Gi RWX 11m[root@k8s-master1 ~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Recycle Bound default/mongodb-pvc 12m# 删除 pvc pv pod[root@k8s-master1 demo]# kubectl delete pod mongodb[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Recycle Released default/mongodb-pvc 13m# Released 释放状态，表明PVC解绑PV，但还未执行回收策略。# 过了一会 变成 Available 说明 pv被自动回收了 查看数据[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Recycle Available 14m# nfs 中的数据也被rm掉[root@k8s-master2 nfs]# ls -ltotal 0# 重新创建并绑定 [root@k8s-master1 demo]# kubectl create -f nfs-pvc.yaml persistentvolumeclaim/mongodb-pvc created[root@k8s-master1 demo]# kubectl create -f mongodb-pod-pvc.yaml pod/mongodb created[root@k8s-master1 demo]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound mongodb-pv 5Gi RWX 8s[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pv 5Gi RWX Recycle Bound default/mongodb-pvc 15m# 绑定成功,数据被清理,新的pv[root@k8s-master1 demo]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.find() PV 动态供给1231. 主要是针对容量问题，手动划分非常麻烦，如果pvc的容量匹配不上pv就无法绑定 2. k8s的动态供给就是可以动态划分容量 3. StorageClass声明存储插件，用于自动创建PV。 k8s 支持持久卷的存储插件1https://kubernetes.io/docs/concepts/storage/persistent-volumes/ NFS PV 动态供给1234567891011121314151617181920212223242526272829303132333435363738394041424344# 创建RBAC授权# 动态创建pv插件需要连接apiserver ，所以需要授权[root@k8s-master1 nfs]# vim rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["list", "watch", "create", "update", "patch"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io 12345678910111213141516171819202122232425262728293031323334353637383940414243# 创建nfs的nfs-client-provisioner# 该服务帮我们自动创建pv# 参考地址:https://github.com/kubernetes-incubator/external-storagehttps://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client/deploy[root@k8s-master1 nfs]# vim deployment-nfs.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: lizhenliang/nfs-client-provisioner:v2.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 172.31.228.68 - name: NFS_PATH value: /data/nfs volumes: - name: nfs-client-root nfs: server: 172.31.228.68 path: /data/nfs 123456789# StorageClass 定义[root@k8s-master1 nfs]# vim storageclass-nfs.yaml apiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifs 12345678910111213# 创建[root@k8s-master1 nfs]# kubectl create -f rbac.yaml serviceaccount/nfs-client-provisioner created[root@k8s-master1 nfs]# kubectl create -f deployment-nfs.yaml deployment.apps/nfs-client-provisioner created[root@k8s-master1 nfs]# kubectl create -f storageclass-nfs.yamlstorageclass.storage.k8s.io/managed-nfs-storage created[root@k8s-master1 nfs]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-56f4b98d47-v4nf6 1/1 Running 0 85s nfs 自动供给 机制12345678910111213141. 动态存储卷供应使用StorageClass进行实现，其允许存储卷按需被创建。2. 如果没有动态存储供应，Kubernetes集群的管理员将不得不通过手工的方式类创建新的存储卷。3. 通过动态存储卷，Kubernetes将能够按照用户的需要，自动创建其需要的存储。1）集群管理员预先创建存储类（StorageClass）；2）用户创建使用存储类的持久化存储声明(PVC：PersistentVolumeClaim)；3）存储持久化声明通知系统，它需要一个持久化存储(PV: PersistentVolume)；4）系统读取存储类的信息；5）系统基于存储类的信息，在后台自动创建PVC需要的PV；6）用户创建一个使用PVC的Pod；7）Pod中的应用通过PVC进行数据的持久化；8）而PVC使用PV进行数据的最终持久化处理。fuseim.pri/ifs 为上面deployment上创建的PROVISIONER_NAME。 创建 PVC1234567891011121314151617181920212223242526272829303132331. 在存储类被正确创建后，就可以创建PersistenetVolumeClaim来请求StorageClass，2. 而StorageClass将会为PersistenetVolumeClaim自动创建一个可用PersistentVolume。3. PersistenetVolumeClaim是对PersistenetVolume的声明，即PersistenetVolume为存储的提供者，而PersistenetVolumeClaim为存储的消费者。[root@k8s-master1 nfs]# vim nfs-pvc-dp.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: storageClassName: managed-nfs-storage resources: requests: storage: 5Gi accessModes: - ReadWriteMany# storageClassName: managed-nfs-storage # pvc 请求自定义存储类 去创建pv [root@k8s-master1 nfs]# kubectl create -f nfs-pvc-dp.yaml persistentvolumeclaim/mongodb-pvc created[root@k8s-master1 nfs]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound default-mongodb-pvc-pvc-9248dad6-b64b-48c3-a3a3-d9f576a0c412 5Gi RWX managed-nfs-storage 71s[root@k8s-master1 nfs]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEdefault-mongodb-pvc-pvc-9248dad6-b64b-48c3-a3a3-d9f576a0c412 5Gi RWX Delete Bound default/mongodb-pvc managed-nfs-storage 74s# Delete 是默认的回收策略 也就是当删除pvc与pv的绑定关系后 pv也会被删除 彻底删除底层 # 如果要修改 在 StorageClass 增加 reclaimPolicy: Retain 手动回收资源 12345678910111213141516171819202122232425262728293031[root@k8s-master1 nfs]# vim mongodb-pod-pvc.yaml apiVersion: v1kind: Podmetadata: name: mongodbspec: # nodeSelector: # gpu: "true" volumes: - name: mongodb-data persistentVolumeClaim: claimName: mongodb-pvc containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP[root@k8s-master1 nfs]# kubectl create -f mongodb-pod-pvc.yaml [root@k8s-master1 nfs]# kubectl exec -it mongodb mongo&gt; use mystoreswitched to db mystore&gt; db.doo.find()WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.doo.find()&#123; "_id" : ObjectId("5e7384172fb631dbf214e42d"), "name" : "123" &#125; 1234567891011121314151617181920212223242526272829# 删除 pod 和 pvc[root@k8s-master1 nfs]# kubectl delete pod mongodbpod "mongodb" deleted[root@k8s-master1 nfs]# kubectl delete pvc mongodb-pvcpersistentvolumeclaim "mongodb-pvc" deleted# pv 被完全删除了 [root@k8s-master1 nfs]# kubectl get pvNo resources found in default namespace.# nfs 下的数据还存在[root@k8s-master2 nfs]# ls -ltotal 4drwxrwxrwx 4 polkitd root 4096 Mar 19 22:42 archived-default-mongodb-pvc-pvc-9248dad6-b64b-48c3-a3a3-d9f576a0c412# 在重新创建一个 pvc [root@k8s-master1 nfs]# kubectl create -f mongodb-pod-pvc.yaml pod/mongodb created# 会发现绑定了一块新的 VOLUME[root@k8s-master1 nfs]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEmongodb-pvc Bound default-mongodb-pvc-pvc-9f9f3be5-8e1c-44db-bb36-fdb7219199d1 5Gi RWX managed-nfs-storage 2s[root@k8s-master2 nfs]# ls -ltotal 8drwxrwxrwx 4 polkitd root 4096 Mar 19 22:42 archived-default-mongodb-pvc-pvc-9248dad6-b64b-48c3-a3a3-d9f576a0c412drwxrwxrwx 4 polkitd root 4096 Mar 19 22:46 default-mongodb-pvc-pvc-9f9f3be5-8e1c-44db-bb36-fdb7219199d1 查看存储类12345678910111213141516[root@k8s-master1 nfs]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 26m[root@k8s-master1 nfs]# kubectl get sc managed-nfs-storage -o yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: creationTimestamp: "2020-03-19T14:22:39Z" name: managed-nfs-storage resourceVersion: "160399" selfLink: /apis/storage.k8s.io/v1/storageclasses/managed-nfs-storage uid: 08256c53-3ca7-4aa4-b0c2-620b94bd7110provisioner: fuseim.pri/ifsreclaimPolicy: DeletevolumeBindingMode: Immediate]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14 K8S 服务发现 Service]]></title>
    <url>%2F2020%2F03%2F11%2Fk8s-base16%2F</url>
    <content type="text"><![CDATA[创建服务资源利用单个地址访问一组 Pod1234567891011121314151617181920212223242526272829[root@k8s-master1 demo]# kubectl get pods -L appNAME READY STATUS RESTARTS AGE APPkubia-cq4ht 1/1 Running 0 8s kubiakubia-mwwxw 1/1 Running 0 8s kubiakubia-rf5j4 1/1 Running 0 8s kubia[root@k8s-master1 demo]# kubectl explain serviceKIND: ServiceVERSION: v1[root@k8s-master1 demo]# vim kubia-svc.yaml apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - port: 80 # 服务端口 targetPort: 8080 # 服务转发到容器的端口 selector: # 标签选择器 app: kubia 的pod都属于该服务 app: kubia[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 3d16hkubia ClusterIP 10.0.0.22 &lt;none&gt; 80/TCP 38s# CLUSTER-IP 内部集群IP 内部访问 kubuctl exec 在运行的容器中远程执行命令12345678[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.22You've hit kubia-rf5j4[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.22You've hit kubia-cq4ht[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.22You've hit kubia-mwwxw 会话亲和性1234567891011121314151617181920212223242526# sessionAffinity: ClientIP (默认为None)# 同一个Client IP的请求会被转发到同一个Pod # K8s 不支持 cookie的会话保持,因为k8s不是工作在http层上[root@k8s-master1 demo]# vim kubia-svc.yaml apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - port: 80 targetPort: 8080 selector: app: kubia sessionAffinity: ClientIP[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.62You've hit kubia-rf5j4[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.62You've hit kubia-rf5j4[root@k8s-master1 demo]# kubectl exec kubia-cq4ht -- curl -s 10.0.0.62You've hit kubia-rf5j4 指定多个端口 80 443123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 在1个容器服务中有多个服务端口 [root@k8s-master1 demo]# vim kubia-svc.yaml apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - name: http port: 80 targetPort: 8080 # svc的 80 转发到 pod 的 8080 - name: https port: 443 targetPort: 8443 # svc的 443 转发到 pod 的 8443 selector: app: kubia[root@k8s-master1 demo]# vim kubia-rs.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - image: 172.31.228.68/project/kubia name: kubia ports: - name: http containerPort: 8080 - name: https containerPort: 8443NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 3d16hservice/kubia ClusterIP 10.0.0.226 &lt;none&gt; 80/TCP,443/TCP 25s 服务发现通过环境变量发现服务1231. 客户端如何知道svc的IP和端口？2. 是否需要先创建svc，然后再手动查找IP再进行配置？3. k8s提供了svc服务发现 1234567891011121314151617181. 在pod开始运行前,k8s会完成一系列的环境变量配置2. 如果你创建的服务早于客户端Pod的创建，那么pod可以根据环境变量获得服务的IP和端口# 先删除之前创建的pod [root@k8s-master1 demo]# kubectl delete pod --all[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-55cbs 1/1 Running 0 103skubia-jpzvx 1/1 Running 0 103skubia-zkkgt 1/1 Running 0 103s[root@k8s-master1 demo]# kubectl exec kubia-55cbs env...KUBIA_SERVICE_PORT_HTTP=80KUBIA_SERVICE_HOST=10.0.0.226KUBIA_SERVICE_PORT_HTTPS=443KUBIA_SERVICE_PORT=80... 通过 DNS 发现123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 demo]# kubectl exec kubia-55cbs bash[root@k8s-master1 demo]# kubectl -it exec kubia-55cbs bashroot@kubia-55cbs:/# cat /etc/resolv.conf nameserver 10.0.0.2search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5root@kubia-55cbs:/# curl http://kubia.default.svc.cluster.localYou've hit kubia-55cbsroot@kubia-55cbs:/# curl http://kubia.default.svc.cluster.localYou've hit kubia-jpzvxroot@kubia-55cbs:/# curl http://kubia.default.svc.cluster.localYou've hit kubia-zkkgt# 解析:http://kubia.default.svc.cluster.local # kubia 服务名称# default 命名空间# svc.cluster.local 集群域名后缀# 如果前端pod和数据库pod 在同一命名空间下 可以省略 svc.cluster.local 和 命名空间root@kubia-55cbs:/# curl http://kubia.default You've hit kubia-55cbsroot@kubia-55cbs:/# curl http://kubia.defaultYou've hit kubia-jpzvxroot@kubia-55cbs:/# curl http://kubia.defaultYou've hit kubia-zkkgtroot@kubia-55cbs:/# curl http://kubia You've hit kubia-55cbsroot@kubia-55cbs:/# curl http://kubiaYou've hit kubia-jpzvxroot@kubia-55cbs:/# curl http://kubiaYou've hit kubia-zkkgtroot@kubia-55cbs:/# ping kubiaPING kubia.default.svc.cluster.local (10.0.0.246): 56 data bytes64 bytes from 10.0.0.246: icmp_seq=0 ttl=64 time=0.054 ms 暴露服务endpoint12345678910111213141516171819202122232425262728291. 服务并不是和pod直接相连2. 之间存在着endpoint 3. endpoint 就是暴露一组服务的IP和端口列表[root@k8s-master1 demo]# kubectl describe svc kubiaName: kubiaNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=kubiaType: ClusterIPIP: 10.0.0.246Port: http 80/TCPTargetPort: 8080/TCPEndpoints: 10.244.0.22:8080,10.244.1.28:8080,10.244.2.26:8080Port: https 443/TCPTargetPort: 8443/TCPEndpoints: 10.244.0.22:8443,10.244.1.28:8443,10.244.2.26:8443Session Affinity: NoneEvents: &lt;none&gt;[root@k8s-master1 demo]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.31.228.67:6443 3d17hkubia 10.244.0.22:8443,10.244.1.28:8443,10.244.2.26:8443 + 3 more... 18m[root@k8s-master1 demo]# kubectl get ep kubiaNAME ENDPOINTS AGEkubia 10.244.0.22:8443,10.244.1.28:8443,10.244.2.26:8443 + 3 more... 18m 将服务暴露到外部123456# Service 类型# 有三种方式可以在外部访问服务1. NodePort: 在每个Node上分配一个端口作为外部访问入口2. LoadBalancer: 工作在特定的Cloud Provider上，例如Google Cloud，AWS，OpenStack3. Ingress: 通过一个IP地址公开多个服务 运行在7层http svc是4层 4. ClusterIP：默认，分配一个集群内部可以访问的虚拟IP（VIP） 创建 NodePort123456789101112131415161718192021222324252627282930[root@k8s-master1 demo]# vim kubia-ndp.yaml apiVersion: v1kind: Servicemetadata: name: kubia-nodeportspec: type: NodePort ports: - name: http port: 80 # svc 端口 targetPort: 8080 # pod 端口 nodePort: 30123 # 通过集群节点的30123可以访问该svc 如果不添加,默认随机分配一个端口 selector: app: kubia[root@k8s-master1 demo]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 3d18h &lt;none&gt;kubia-nodeport NodePort 10.0.0.87 &lt;none&gt; 80:30123/TCP 55s # 访问方式1. NodePort也分配了CLUSTER-IP: 10.0.0.87:802. nodeip + 30123 [root@k8s-master1 demo]# netstat -tnlp | grep 30123tcp6 0 0 :::30123 :::* LISTEN 944/kube-proxy [root@k8s-node2 ~]# netstat -tnlp | grep 30123tcp6 0 0 :::30123 :::* LISTEN 903/kube-proxy 查看 ipvsadm 规则12345678910111213141516[root@k8s-master1 demo]# kubectl get ep kubia-nodeportNAME ENDPOINTS AGEkubia-nodeport 10.244.0.22:8080,10.244.1.28:8080,10.244.2.26:8080 9m30s[root@k8s-node1 ~]# ipvsadm -ln | grep 30123TCP 172.17.0.1:30123 rrTCP 10.244.0.0:30123 rrTCP 10.244.0.1:30123 rrTCP 127.0.0.1:30123 rrTCP 172.31.228.69:30123 rr# 都是向三台node上的podIP转发TCP 172.31.228.69:30123 rr -&gt; 10.244.0.22:8080 Masq 1 0 0 -&gt; 10.244.1.28:8080 Masq 1 0 0 -&gt; 10.244.2.26:8080 Masq 1 0 0 域名应该找哪个node12345678还得有一层负载均衡这层负载均衡负责 转发给 node集群+对外端口域名解析 -&gt; 负载均衡器nginx -&gt; nodeIP+对外端口 -&gt; ipvs -&gt; pod+端口# node1上的30123收到的请求 可以被转发到node1上的pod，也可能是node2上的# 我使用的阿里云,如果是自建可以选择修改防火墙策略开放node端口 http://47.240.12.170:30123/ Load Badancer 通过负载均衡将服务暴露出来123456781. nodeport 虽然可以解决外部暴露,但是当客户端连接的时候写了其中一个nodeip+端口,但是该node又出现问题会导致服务不可访问2. nginx+nodeport可以解决该问题,但是还有Load Badancer方式# 就是前面加一个LB# LB 转发给 每个nodeIP+port# 域名解析到LB地址# 自建LB 需要知道 生成的端口是多少 再添加到LB# 公有云 LoadBalancer LB 可以自动关联 提供访问 访问流程和NodePort一致 12345671. NodePort用户 -&gt; 域名 -&gt; 负载均衡(阿里云公网)(后端手动添加) -&gt; NodeIP(内网):Port -&gt; PodIP+Port2. LoadBalancer用户 -&gt; 域名 -&gt; 负载均衡(阿里云公网)(自动添加) -&gt; NodeIP(内网):Port -&gt; PodIP+Port1. LoadBalancer: 特定云提供商底层LB接口 例如aws,google,openstack,aliyun不知道有没有2. NodePort : NodeIP 不固定,Node Port 可以固定 通过 Ingress 暴露服务创建 Ingress123456789101112131415161718192021222324252627282930[root@k8s-master1 cfg]# kubectl explain ingressKIND: IngressVERSION: extensions/v1beta1[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4d15hkubia-nodeport NodePort 10.0.0.87 &lt;none&gt; 80:30123/TCP 21h[root@k8s-master1 demo]# vim kubia-ingress.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com # 要映射的域名 http: paths: - path: / backend: serviceName: kubia-nodeport # 转发到哪个 service servicePort: 80 # 对应 kubia-nodeport svc 的 80 端口# ingress控制器收到的 请求主机 kubia.example.com 的http请求，将被转发给 服务svc kubia-nodeport 的 80 端口[root@k8s-master1 demo]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEkubia kubia.example.com 80 16s 123456789101112131415161718# 测试增加域名解析# 不添加hosts的话 需要将域名接下到 部署控制器的 node节点上# 实际工作环境 就是将 域名 解析到 NodeIP上# windows 修改 hosts文件 # linux 修改 /etc/hosts[root@k8s-master1 demo]# cat /etc/hosts149.129.81.90 kubia.example.com 47.240.12.170 kubia.example.com 47.240.15.208 kubia.example.com [root@k8s-master1 demo]# curl kubia.example.com You've hit kubia-zkkgt[root@k8s-master1 demo]# curl kubia.example.com You've hit kubia-55cbs[root@k8s-master1 demo]# curl kubia.example.com You've hit kubia-jpzvx 通过不同路径 映射不同服务123456789101112131415161718192021# 在同一个主机(host)，不同的路径上,ingress暴露出多个服务[root@k8s-master1 demo]# vim kubia-ingress2.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com http: paths: - path: /kubia # kubia.example.com/kubia 转发给 kubia-nodeport backend: serviceName: kubia-nodeport servicePort: 80 - path: /index backend: serviceName: kubia-index # # kubia.example.com/index 转发给 kubia-index servicePort: 80 不同的服务映射到不同的主机上(host)123456789101112131415161718192021222324252627# dns本地解析149.129.81.90 kubia.example.com foo.example.com47.240.12.170 kubia.example.com foo.example.com 47.240.15.208 kubia.example.com foo.example.com [root@k8s-master1 demo]# vim kubia-ingress3.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com http: paths: - path: / backend: serviceName: kubia-nodeport servicePort: 80 - host: foo.example.com http: paths: - path: / backend: serviceName: kubia-index servicePort: 80 配置 Ingress 处理 TLS 传输1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 处理 HTTPS 流量数据# 证书和私钥需要放到 Ingress # 需要先放在 secret 中，然后再yaml文件中引用# 自制证书[root@k8s-master1 demo]# openssl genrsa -out tls.key 2048[root@k8s-master1 demo]# openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com# 创建 secret [root@k8s-master1 demo]# kubectl create secret tls tls-secret --cert=tls.cert --key=tls.keysecret/tls-secret created# 更新 yaml 文件[root@k8s-master1 demo]# vim kubia-ingress.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: kubiaspec: tls: # tls 配置 - hosts: - kubia.example.com # 主机为 kubia.example.com 的 tls 连接 secretName: tls-secret # 从 tls-secret 中获取私钥和证书 rules: - host: kubia.example.com http: paths: - path: / backend: serviceName: kubia-nodeport servicePort: 80[root@k8s-master1 demo]# kubectl apply -f kubia-ingress.yaml ingress.networking.k8s.io/kubia created[root@k8s-master1 demo]# curl -k -v https://kubia.example.com/* About to connect() to kubia.example.com port 443 (#0)* Trying 149.129.81.90...* Connected to kubia.example.com (149.129.81.90) port 443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* skipping SSL peer certificate verification* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384* Server certificate:* subject: CN=kubia.example.com* start date: Mar 14 01:40:52 2020 GMT* expire date: Mar 09 01:40:52 2021 GMT* common name: kubia.example.com* issuer: CN=kubia.example.com&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: kubia.example.com&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK&lt; Server: nginx/1.15.5&lt; Date: Sat, 14 Mar 2020 01:48:28 GMT&lt; Transfer-Encoding: chunked&lt; Connection: keep-alive&lt; Strict-Transport-Security: max-age=15724800; includeSubDomains&lt; You've hit kubia-jpzvx* Connection #0 to host kubia.example.com left intact Pod 就绪指针 就绪后发出信号1234567891011121314151617181920212223# pod 现在作为 svc 服务的后端# 新建的pod 只要具有服务的标签就会被服务代理，请求也会重定向新建的pod # 如果 pod 没有准备好 如何处理这些请求 # 不想该pod 立即接收请求，直到完全准备就绪# pod的指针 1. 存活指针2. 就绪指针# 当容器准备就绪探测返回成功时,标识容器准备好接收请求# 存活指针 三类型 1. exec 探针 判断进程退出的状态码2. httpget 探针 通过状态码3. tcp socket 探针 判断端口连接# 就绪指针操作流程:# 1. 启动容器时 k8s配置一个等待时间 经过等待时间后才执行准备继续检查# 2. 之后会周期性的调用探针,根据结果采取行动,如果pod未准备就绪，则会从服务中删除pod ，如果再次准备就绪就加入。# 3. 就绪探针 与 存活探针不同，如果容器未通过检查，不会终止或者重新启动，这是主要区别# 4. 存活探针杀死异常的容器并用心的容器替代来保持pod的正常工作# 5. 就绪探针确保只有准备好处理请求的pod，才可以接收请求# 6. 这在一个容器启动时都是必要的，当然在容器运行一段时间后也是有用的 添加就绪指针1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@k8s-master1 demo]# vim kubia-rs.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - image: 172.31.228.68/project/kubia name: kubia readinessProbe: exec: command: - ls - /var/ready ports: - containerPort: 8080# 就绪探针会定期在容器内执行 ls /var/ready ,如果文件存在,ls命令返回退出状态码0，否则非零 # 文件存在探针将成功，否则失败# 更改 rs的模板对现有pod没有影响 需要删除pod重新创建# 新建的应该处于就绪失败状态，不会作为服务的端点，直到每个pod中创建了 /var/ready 文件[root@k8s-master1 demo]# kubectl delete rs kubiareplicaset.apps "kubia" deleted[root@k8s-master1 demo]# kubectl create -f kubia-rs.yaml replicaset.apps/kubia created[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-7nwtf 0/1 Running 0 5m52skubia-cz2qh 0/1 Running 0 5m52skubia-shf8g 0/1 Running 0 5m52s[root@k8s-master1 demo]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.31.228.67:6443 4d17hkubia-nodeport 23h# 在其中一个pod 创建文件[root@k8s-master1 demo]# kubectl exec -it kubia-7nwtf -- touch /var/ready[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-7nwtf 1/1 Running 0 7mkubia-cz2qh 0/1 Running 0 7mkubia-shf8g 0/1 Running 0 7m[root@k8s-master1 demo]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.31.228.67:6443 4d17hkubia-nodeport 10.244.2.33:8080 23h[root@k8s-master1 demo]# kubectl describe pod kubia-7nwtfReadiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3# period 每10秒检查一次# 所有请求都到了这个 pod[root@k8s-master1 demo]# curl 10.0.0.87You've hit kubia-7nwtf[root@k8s-master1 demo]# curl 10.0.0.87You've hit kubia-7nwtf[root@k8s-master1 demo]# curl 10.0.0.87You've hit kubia-7nwtf 就绪探针的实际作用12341. 应该通过删除pod 或者 更改pod标签来移除pod，而不是更改探针2. enabled=true 可作为开关标签3. 务必定义就绪探针 ，不能让pod立即成为服务端点 4. 不要将停止pod的逻辑纳入就绪探针，直接删除该容器,移除 Pod 使用 headless 服务来发现独立的 Pod返回所有的 Pod IP1231. 常规 Service：执行服务的DNS时,返回服务的集群 IP(svc IP)2. Headless Service: 不需要Cluster-IP，设置为Node ,客户端通过DNS A记录查找会获取该服务所有的Pod IP3. 客户端可以使用该信息连接到其中的1个、多个或全部 创建 headless 服务1234567891011121314[root@k8s-master1 demo]# vim kubia-svc-headless.yaml apiVersion: v1kind: Servicemetadata: name: kubia-headlessspec: clusterIP: None # 设置为None ports: - name: http port: 80 targetPort: 8080 selector: app: kubia 1234[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4d18hkubia-headless ClusterIP None &lt;none&gt; 80/TCP 49s 通过DNS来发现 Pod12345678910111213141516# 临时任务 不重启 执行后销毁[root@k8s-master1 demo]# vim busybox.yaml apiVersion: v1kind: Podmetadata: name: dns-testspec: containers: - name: busybox image: busybox:1.28.4 args: - /bin/sh - -c - sleep 36000 restartPolicy: Never 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master1 demo]# kubectl apply -f busybox.yaml # 测试[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 4d18hkubia-headless ClusterIP None &lt;none&gt; 80/TCP 8m43skubia-nodeport NodePort 10.0.0.157 &lt;none&gt; 80:30123/TCP 34s[root@k8s-master1 demo]# kubectl exec dns-test nslookup kubia-nodeportServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubia-nodeportAddress 1: 10.0.0.157 kubia-nodeport.default.svc.cluster.local[root@k8s-master1 demo]# kubectl exec dns-test nslookup kubia-headlessServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubia-headlessAddress 1: 10.244.1.32 10-244-1-32.kubia-headless.default.svc.cluster.localAddress 2: 10.244.2.33 10-244-2-33.kubia-headless.default.svc.cluster.localAddress 3: 10.244.0.27 10-244-0-27.kubia-headless.default.svc.cluster.local[root@k8s-master1 demo]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdns-test 1/1 Running 0 5m37s 10.244.1.33 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-7nwtf 1/1 Running 0 47m 10.244.2.33 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-cz2qh 1/1 Running 0 47m 10.244.1.32 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-shf8g 1/1 Running 0 47m 10.244.0.27 k8s-node1 &lt;none&gt; &lt;none&gt;# headless 直接返回了 pod的IP ，在容器中可以通过 该解析访问 为后面的有状态部署提供帮助[root@k8s-master1 demo]# kubectl exec dns-test ping 10-244-0-27.kubia-headless.default.svc.cluster.localPING 10-244-0-27.kubia-headless.default.svc.cluster.local (10.244.0.27): 56 data bytes64 bytes from 10.244.0.27: seq=0 ttl=62 time=0.724 ms 排查服务故障123451. 先确保从集群内连接到服务集群IP(svc ip)2. 是否定义了 就绪探针 ，返回是否成功3. 检查端点 查看pod是否加入到 service , kubectl get ep 4. 检查svc暴露的端口5. 检查pod的服务是否正常]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13 副本机制和其他控制器]]></title>
    <url>%2F2020%2F03%2F11%2Fk8s-base15%2F</url>
    <content type="text"><![CDATA[保持 pod 的健康 在实际的应用里，希望部署的pod能自动保持运行和健康，无需手动干预 不要直接创建pod，而是创建RC或者Deployment这样的资源由他们来管理 如何托管我们的pod，k8s如何自动重启pod，node节点失败pod如何被调度。 即使进程崩溃，应用程序会停止工作，比如java进程的内存泄露 存活探针12345678# k8s三种探测容器机制 1. httpGet发送HTTP请求，返回2XX-3XX范围状态码为成功。2. exec执行Shell命令返回状态码是0为成功。3. tcpSocket发起TCP Socket建立成功。 基于HTTP的存活探针123456789101112131415161718# https://github.com/luksa/kubernetes-in-action/tree/master/Chapter04/kubia-unhealthy[root@k8s-master1 demo]# vim kubia-libeness-probe.yaml apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: 172.31.228.68/project/kubia-httpget # 出现访问问题的镜像 name: kubia livenessProbe: # 存活探针 httpGet: # httpget类型 path: / # 请求路径 port: 8080 # 请求端口# 该镜像出现2次访问后会返回 http 500 123456789101112# 大约1分半后容器会重启# RESTARTS 显示重启次数# 现在的模式是 无限循环的重启[root@k8s-master1 demo]# kubectl create -f kubia-libeness-probe.yaml pod/kubia-liveness created[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-liveness 1/1 Running 0 61s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-liveness 1/1 Running 1 2m1s 获取崩溃容器的应用日志123456789101112# 当想知道前一个容器日志,而不是当前的容器时 加上 --previous[root@k8s-master1 demo]# kubectl logs kubia-liveness --previousKubia server starting...Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1Received request from ::ffff:10.244.2.1 查看事件123456789101112131415161718192021222324252627282930[root@k8s-master1 demo]# kubectl describe pod kubia-liveness# 先前容器发生错误 返回码 137# 137 = 128+x 其中x是终止进程的信号编号,在这个例子里x=9，是sigkill的信号编号,意味着被强行终止# Restart Count 重启次数... Last State: Terminated Reason: Error Exit Code: 137 Started: Wed, 11 Mar 2020 11:39:53 +0800 Finished: Wed, 11 Mar 2020 11:41:43 +0800 Ready: True Restart Count: 3 Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/kubia-liveness to k8s-master1 Normal Created 4m36s (x3 over 8m11s) kubelet, k8s-master1 Created container kubia Normal Started 4m36s (x3 over 8m11s) kubelet, k8s-master1 Started container kubia Warning Unhealthy 3m16s (x9 over 7m16s) kubelet, k8s-master1 Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Killing 3m16s (x3 over 6m56s) kubelet, k8s-master1 Container kubia failed liveness probe, will be restarted Normal Pulling 2m46s (x4 over 8m11s) kubelet, k8s-master1 Pulling image "172.31.228.68/project/kubia-httpget" Normal Pulled 2m46s (x4 over 8m11s) kubelet, k8s-master1 Successfully pulled image "172.31.228.68/project/kubia-httpget"# 事件中告诉我们为什么会被重启# 当容器被强行终止时,会创建一个全新的容器,而不是重启原来的容器 配置存活探针的附加属性123456789101112# http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3# delay 延迟 # timeout 超时# period 周期# delay=0s 容器启动后立即探测# timeout=1s 容器需要再1秒钟内响应，否则检测失败# period=10s 每10秒探测一次# failure=3 连续3次探测后重启容器# 务必设置一个初始延迟来说明应用的启动时间# 退出代码 137表示进程被外部信号终止,128+9 如果是143对应128+15 12# initialDelaySeconds: 10 # 容器启动5秒后 开始健康检查# periodSeconds: 5 # 每隔5秒执行一次 123456789101112131415161718192021[root@k8s-master1 demo]# vim kubia-libeness-probe.yaml apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: 172.31.228.68/project/kubia-httpget name: kubia livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 10 periodSeconds: 5[root@k8s-master1 demo]# kubectl describe pod kubia-liveness... Liveness: http-get http://:8080/ delay=10s timeout=1s period=5s #success=1 #failure=3... 创建 ReplicationController1234# ReplicationController 三部分# 1. 标签选择器# 2. 副本个数# 3. pod 模板 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@k8s-master1 demo]# vim kubia-rc.yaml apiVersion: v1kind: ReplicationController # 资源对象metadata: name: kubia # rc名称spec: replicas: 3 # 副本个数 selector: app: kubia # pod 选择器 template: # pod 模板 metadata: labels: app: kubia # 模板中的pod标签 需要与pod标签选择器相同 spec: containers: - image: 172.31.228.68/project/kubia name: kubia ports: - containerPort: 8080 protocol: TCP[root@k8s-master1 demo]# kubectl create -f kubia-rc.yaml [root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-dhj7g 1/1 Running 0 33skubia-k659d 1/1 Running 0 33skubia-l4fpp 1/1 Running 0 33s# 删除pod 再查看[root@k8s-master1 demo]# kubectl delete pod kubia-dhj7g[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-dhj7g 1/1 Terminating 0 115s # 终止kubia-k659d 1/1 Running 0 115skubia-l4fpp 1/1 Running 0 115skubia-mqk8k 1/1 Running 0 28s # 重新创建# 查看 rc[root@k8s-master1 demo]# kubectl get rcNAME DESIRED CURRENT READY AGEkubia 3 3 3 2m35s# 查看rc详细信息[root@k8s-master1 demo]# kubectl describe rc kubiaName: kubiaNamespace: defaultSelector: app=kubiaLabels: app=kubiaAnnotations: &lt;none&gt;Replicas: 3 current / 3 desired # pod 实际数量和目标数量Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed # 每种状态下的pod数量Pod Template: Labels: app=kubia Containers: kubia: Image: 172.31.228.68/project/kubia Port: 8080/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 7m51s replication-controller Created pod: kubia-dhj7g Normal SuccessfulCreate 7m51s replication-controller Created pod: kubia-l4fpp Normal SuccessfulCreate 7m51s replication-controller Created pod: kubia-k659d Normal SuccessfulCreate 6m24s replication-controller Created pod: kubia-mqk8k 断开 node2 节点测试12345678910111213141516171819[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-k659d 1/1 Running 0 11m 10.244.1.16 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-l4fpp 1/1 Running 0 11m 10.244.2.16 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-mqk8k 1/1 Running 0 10m 10.244.0.14 k8s-node1 &lt;none&gt; &lt;none&gt;# 如果节点在几分钟之内无法访问,那么节点上的pod的状态会变成unknown，rc会重新创建pod[root@k8s-master1 demo]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 46h v1.16.0k8s-node1 Ready &lt;none&gt; 46h v1.16.0k8s-node2 NotReady &lt;none&gt; 46h v1.16.0[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-k659d 1/1 Terminating 0 18m 10.244.1.16 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-l4fpp 1/1 Running 0 18m 10.244.2.16 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-mqk8k 1/1 Running 0 16m 10.244.0.14 k8s-node1 &lt;none&gt; &lt;none&gt;kubia-sfhkc 1/1 Running 0 17s 10.244.2.17 k8s-master1 &lt;none&gt; &lt;none&gt; pod 迁移或迁出 rc 的作用域1234567891011121314151617181. 通过修改pod的标签 可以将它从rc的作用域中添加或者删除# 给现有rc中的pod增加新标签# rc并不关心 新增的标签 只关心pod是否有标签选择器中的标签 [root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-l4fpp 1/1 Running 0 29mkubia-mqk8k 1/1 Running 0 27mkubia-sfhkc 1/1 Running 0 11m[root@k8s-master1 demo]# kubectl label pod kubia-l4fpp team=Apod/kubia-l4fpp labeled[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-l4fpp 1/1 Running 0 29m app=kubia,team=Akubia-mqk8k 1/1 Running 0 28m app=kubiakubia-sfhkc 1/1 Running 0 11m app=kubia 12345678910111213141516# 更改已托管到rc中的pod标签[root@k8s-master1 demo]# kubectl label pod kubia-l4fpp app=foo --overwritepod/kubia-l4fpp labeled[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-l4fpp 1/1 Running 0 31m app=foo,team=A # 不再被rc管理，可以手动删除该podkubia-mqk8k 1/1 Running 0 30m app=kubiakubia-sfhkc 1/1 Running 0 13m app=kubiakubia-wsqgg 1/1 Running 0 4s app=kubia # rc新建的pod [root@k8s-master1 demo]# kubectl delete pod kubia-l4fpp# 如果修改rc控制器的标签选择器，那么rc之前管理的pod都会被脱离该rc，并且会创建3个新的pod# 不要修改rc控制器的标签选择器，可以修改pod模板 kubectl edit 修改模板123456789101112131415161718192021222324252627282930[root@k8s-master1 demo]# kubectl edit rc kubia... template: metadata: creationTimestamp: null labels: app: kubia team: C # 新增标签...# 1. 不会影响现有pod# 2. 如果删除再重新创建 则会带有新标签# 3. 如果我们修改了容器镜像，则相当于更新本次pod，后续还有更好的方法更新pod[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-mqk8k 1/1 Running 0 39mkubia-sfhkc 1/1 Running 0 22mkubia-wsqgg 1/1 Running 0 9m2s[root@k8s-master1 demo]# kubectl delete pod kubia-sfhkc kubia-wsqggpod "kubia-sfhkc" deletedpod "kubia-wsqgg" deleted[root@k8s-master1 ~]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-2ftc2 1/1 Running 0 40s app=kubia,team=Ckubia-mqk8k 1/1 Running 0 40m app=kubiakubia-x6xm2 1/1 Running 0 40s app=kubia,team=C kubectl sacle 水平扩容12345678910111213141516171819202122232425[root@k8s-master1 demo]# kubectl scale rc kubia --replicas=5replicationcontroller/kubia scaled[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-2ftc2 1/1 Running 0 4m18s app=kubia,team=Ckubia-cf627 1/1 Running 0 32s app=kubia,team=Ckubia-mqk8k 1/1 Running 0 44m app=kubiakubia-t2k4h 1/1 Running 0 32s app=kubia,team=Ckubia-x6xm2 1/1 Running 0 4m18s app=kubia,team=C# edit 编辑定义修改 rc [root@k8s-master1 demo]# kubectl edit rc kubia...spec: replicas: 3 # 副本个数...[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-2ftc2 1/1 Running 0 7m30s app=kubia,team=Ckubia-mqk8k 1/1 Running 0 47m app=kubiakubia-x6xm2 1/1 Running 0 7m30s app=kubia,team=C# 声明式的集群伸缩: 我想要运行x个实例 删除 ReplicationController1234567891011# 1. 删除rc的同时，它管理的pod也会被删除# 2. 当删除rc时可以通过 --cascade=false 来保持pod的运行# 3. 当你删除rc时,pod已经不受管理,可以新建rc管理他们,只要rc的标签管理器对应pod的标签[root@k8s-master1 demo]# kubectl delete rc kubia --cascade=false[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-2ftc2 1/1 Running 0 13m app=kubia,team=Ckubia-mqk8k 1/1 Running 0 53m app=kubiakubia-x6xm2 1/1 Running 0 13m app=kubia,team=C 使用 ReplicatSet 替换 ReplicationController123# 1. ReplicationController 是用于复制和在异常时重新调度节点的最早组件# 2. ReplicatSet 是 新一代的 ReplicationController 并且已经完全替换掉# 3. 通常不会直接创建ReplicatSet，而是创建更高层级的 Deployment时自动创建 ReplicatSet 和 ReplicationController 的区别123# 1. ReplicationController 只能匹配一组标签# 2. ReplicatSet 可以匹配多组标签# 3. ReplicatSet 可以匹配标签的键,比如名为env的标签,而不在乎值,(env=*) 定义 ReplicatSet12345678910111213141516171819202122232425262728293031323334353637383940# 参考[root@k8s-master1 demo]# kubectl explain rsKIND: ReplicaSetVERSION: apps/v1[root@k8s-master1 demo]# kubectl explain rcKIND: ReplicationControllerVERSION: v1# 创建 ReplicatSet 并且接管之前三个被移除rc的pod[root@k8s-master1 demo]# vim kubia-rs.yamlapiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: # 选择器里增加了 matchLabels选择器 app: kubia # 标签选择器 template: metadata: labels: # pod中的标签对应rs的标签选择器 app: kubia spec: containers: - image: 172.31.228.68/project/kubia name: kubia ports: - containerPort: 8080[root@k8s-master1 demo]# kubectl create -f kubia-rs.yaml replicaset.apps/kubia created[root@k8s-master1 demo]# kubectl get rsNAME DESIRED CURRENT READY AGEkubia 3 3 3 26s ReplicatSet 的标签选择器123456789101112# ReplicatSet 对于 rc的主要改进就是标签选择器selector: matchExpressions: - key: app operator: In values: - kubia# 会有四种 In NotIn Exists DoesNotExist # 如果是多个表达式 则所有条件必须都为true# 如果同时指定 matchExpressions 和 matchLabels 则所有标签条件都必须为true# 后续应该始终使用rs,当然也有其他人的部署里看到rc 删除 ReplicatSet123# 删除rs,则下面的pod同时删除[root@k8s-master1 ~]# kubectl delete rs kubiareplicaset.apps "kubia" deleted DaemonSet 在每个 node节点上运行1个pod12341. 如果 node节点下线 DaemonSet不会再其他节点上重新部署pod2. 如果一个新的node节点上线,DaemonSet会立即在这个新节点上创建一个pod 3. 除非指定pod在某个node上运行 nodeSelector 4. DaemonSet 会绕过调度器，即使节点被设置为不可调度 创建 DaemonSet12345678910111213141516171819202122232425262728293031323334[root@k8s-master2 ssd]# vim DockerfileFROM busyboxENTRYPOINT while true; do echo 'SSD OK'; sleep 5; donedocker build -t ssd-monitor .docker imagesdocker tag ssd-monitor 172.31.228.68/project/ssd-monitordocker push 172.31.228.68/project/ssd-monitor[root@k8s-master1 demo]# kubectl explain dsKIND: DaemonSetVERSION: apps/v1[root@k8s-master1 demo]# vim kubia-ds.yaml apiVersion: apps/v1kind: DaemonSetmetadata: name: ssd-monitorspec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: # 节点选择器 选择标签有disk: ssd的node disk: ssd containers: - image: 172.31.228.68/project/ssd-monitor name: main 1234567891011121314151617181920212223[root@k8s-master1 demo]# kubectl create -f kubia-ds.yaml # 还没有给node 打上 disk=ssd标签 所有没有pod产生[root@k8s-master1 demo]# kubectl get dsNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEssd-monitor 0 0 0 0 0 disk=ssd 33s# 给两个node打上标签[root@k8s-master1 demo]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 2d v1.16.0k8s-node1 Ready &lt;none&gt; 2d v1.16.0k8s-node2 Ready &lt;none&gt; 2d v1.16.0[root@k8s-master1 demo]# kubectl label node k8s-node1 k8s-node2 disk=ssdnode/k8s-node1 labelednode/k8s-node2 labeled[root@k8s-master1 demo]# kubectl get node -L diskNAME STATUS ROLES AGE VERSION DISKk8s-master1 Ready &lt;none&gt; 2d v1.16.0 k8s-node1 Ready &lt;none&gt; 2d v1.16.0 ssdk8s-node2 Ready &lt;none&gt; 2d v1.16.0 ssd 12345678910111213141516171819202122232425262728# 查看[root@k8s-master1 demo]# kubectl get dsNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEssd-monitor 2 2 2 2 2 disk=ssd 3m29s[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESssd-monitor-9twms 1/1 Running 0 4s 10.244.1.22 k8s-node2 &lt;none&gt; &lt;none&gt;ssd-monitor-tlrfq 1/1 Running 0 4s 10.244.0.16 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl logs ssd-monitor-9twmsSSD OKSSD OK...# 更换一个node标签 合理的下线了 ds[root@k8s-master1 demo]# kubectl label node k8s-node2 disk=hdd --overwritenode/k8s-node2 labeled[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESssd-monitor-9twms 1/1 Terminating 0 74s 10.244.1.22 k8s-node2 &lt;none&gt; &lt;none&gt;ssd-monitor-tlrfq 1/1 Running 0 74s 10.244.0.16 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl get dsNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEssd-monitor 1 1 1 1 1 disk=ssd 77s 123# 删除 ds[root@k8s-master1 demo]# kubectl delete ds ssd-monitordaemonset.apps "ssd-monitor" deleted Job 运行单个任务的 pod1231. jod 一旦运行完成不会重启容器2. Job 分为 普通任务(Job) 和 定时任务 (CronJob) 3. 应用场景: 离线数据处理，视频解码等业务 创建 Job12345678[root@k8s-master2 job]# vim Dockerfile FROM busyboxENTRYPOINT echo "$(date) Batch job starting"; sleep 120; echo "$(date) Finished succesfully"[root@k8s-master2 job]# docker build -t batch-job .[root@k8s-master2 job]# docker tag batch-job 172.31.228.68/project/batch-job[root@k8s-master2 job]# docker push 172.31.228.68/project/batch-job 12345678910111213141516171819202122[root@k8s-master1 demo]# kubectl explain jobKIND: JobVERSION: batch/v1[root@k8s-master1 demo]# vim kubia-job.yaml apiVersion: batch/v1kind: Jobmetadata: name: batch-jobspec: template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure containers: - image: 172.31.228.68/project/batch-job name: main[root@k8s-master1 demo]# kubectl create -f kubia-job.yaml 123• Always： 当容器终止退出后，总是重启容器，默认策略。• OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。• Never:： 当容器终止推出，从不重启容器。 查看 Job 运行1234567891011121314151617181920212223[root@k8s-master1 demo]# kubectl get jobsNAME COMPLETIONS DURATION AGEbatch-job 0/1 50s 50s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEbatch-job-5tr5g 1/1 Running 0 70s[root@k8s-master1 demo]# kubectl describe jobs batch-job[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEbatch-job-5tr5g 0/1 Completed 0 2m31s# 读取 job 日志[root@k8s-master1 demo]# kubectl logs batch-job-5tr5g Wed Mar 11 10:00:51 UTC 2020 Batch job startingWed Mar 11 10:02:51 UTC 2020 Finished succesfully# 删除 job[root@k8s-master1 demo]# kubectl delete job batch-jobjob.batch "batch-job" deleted 计划任务 CronJob创建 CronJob12345678[root@k8s-master1 demo]# kubectl explain cronjobKIND: CronJobVERSION: batch/v1beta1定时任务，像Linux的Crontab一样应用场景: 通知,备份crontab的格式如下：分 时 日 月 周 要运行的命令: 第1列分钟0～59 第2列小时0～23） 第3列日1～31 第4列月1～12 第5列星期0～7（0和7表示星期天） 第6列要运行的命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master1 demo]# vim cronjob.yaml apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: "*/1 * * * *" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure[root@k8s-master1 demo]# kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello */1 * * * * False 0 &lt;none&gt; 12s[root@k8s-master1 demo]# kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello */1 * * * * False 0 31s 58s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEhello-1583921700-s4rwp 0/1 Completed 0 28s[root@k8s-master1 demo]# kubectl logs hello-1583921700-s4rwpWed Mar 11 10:15:16 UTC 2020Hello from the Kubernetes cluster# 到达定时后 会再次执行并产生pod[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEhello-1583921700-s4rwp 0/1 Completed 0 61shello-1583921760-sxz8q 0/1 ContainerCreating 0 1s# 删除 cronjob[root@k8s-master1 demo]# kubectl delete cronjob hello]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12 K8S Pod 合理管理容器]]></title>
    <url>%2F2020%2F03%2F11%2Fk8s-base14%2F</url>
    <content type="text"><![CDATA[通过 Pod 合理管理容器准备工作123456789101112131415# 容器准备 [root@k8s-master2 src]# cat app.js const http = require ('http');const os = require ('os');console.log("Kubia server starting...");var handler = function(request,response)&#123; console.log("Received request from" + request.connection.remoteAddress); response.writeHead(200); response.end("You've hit " + os.hostname() + "\n");&#125;;var www = http.createServer(handler);www.listen(8080); 1234[root@k8s-master2 src]# cat Dockerfile FROM node:7ADD app.js /app.jsENTRYPOINT ["node","app.js"] 12345678# 上传到镜像仓库docker build -t kubia .docker imagesdocker run --name kubia-container -p 8080:8080 -d kubiadocker login 172.31.228.68docker tag kubia 172.31.228.68/project/kubiadocker imagesdocker push 172.31.228.68/project/kubia kubectl run 命令创建资源1234# run 命令 可以创建所有必要的组件 无需yaml文件 [root@k8s-master1 ~]# kubectl run kubia --image=172.31.228.68/project/kubia --port=8080 --generator=run/v1kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.replicationcontroller/kubia created 1234567891011# 查看 replicationcontroller [root@k8s-master1 ~]# kubectl get replicationcontrollersNAME DESIRED CURRENT READY AGEkubia 1 1 1 7m46s[root@k8s-master1 ~]# kubectl get rcNAME DESIRED CURRENT READY AGEkubia 1 1 1 7m50sDESIRED: 希望保持的pod副本数CURRENT: 当前运行的pod副本数 1234567# 增加期望的副本数 [root@k8s-master1 ~]# kubectl scale rc kubia --replicas=3replicationcontroller/kubia scaled[root@k8s-master1 ~]# kubectl get rcNAME DESIRED CURRENT READY AGEkubia 3 3 3 11m 123# 一步增加副本数[root@k8s-master1 ~]# kubectl delete rc kubia[root@k8s-master1 ~]# kubectl run kubia --image=172.31.228.68/project/kubia --port=8080 --generator=run/v1 --replicas=3 导出现有 pod 的 yaml 描述文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# 参考: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/ # 导出一个正在运行的Pod[root@k8s-master1 demo]# kubectl get pod kubia-manual -o yamlapiVersion: v1 # k8s api 版本kind: Pod # k8s 对象 资源类型 # POD 元数据 (名称、标签、注解)metadata: creationTimestamp: "2020-03-10T23:59:10Z" labels: env: prod team: A name: kubia-manual namespace: default resourceVersion: "28257" selfLink: /api/v1/namespaces/default/pods/kubia-manual uid: f56b37e8-70d7-47c4-9299-a8841f574205 # POD 规格 | 内容 (POD的容器列表、volume等)spec: containers: - image: 172.31.228.68/project/kubia imagePullPolicy: Always name: kubia ports: - containerPort: 8080 protocol: TCP resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-r29ch readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: k8s-master1 priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-r29ch secret: defaultMode: 420 secretName: default-token-r29ch # POD 内部容器的详细状态status: conditions: - lastProbeTime: null lastTransitionTime: "2020-03-10T23:59:10Z" status: "True" type: Initialized - lastProbeTime: null lastTransitionTime: "2020-03-10T23:59:12Z" status: "True" type: Ready - lastProbeTime: null lastTransitionTime: "2020-03-10T23:59:12Z" status: "True" type: ContainersReady - lastProbeTime: null lastTransitionTime: "2020-03-10T23:59:10Z" status: "True" type: PodScheduled containerStatuses: - containerID: docker://b210eb2b042f79873db515d39c87081277fed8529754c0cae82cd69984ff51a9 image: 172.31.228.68/project/kubia:latest imageID: docker-pullable://172.31.228.68/project/kubia@sha256:6b5f96aa6c2376e394301b83c57c551c84ae800415a53f668f6042bd1b76fcea lastState: &#123;&#125; name: kubia ready: true restartCount: 0 started: true state: running: startedAt: "2020-03-10T23:59:11Z" hostIP: 172.31.228.67 phase: Running podIP: 10.244.2.11 podIPs: - ip: 10.244.2.11 qosClass: BestEffort startTime: "2020-03-10T23:59:10Z" 创建一个简单的yaml描述文件12345678910111213141516171819[root@k8s-master1 demo]# vim kubia-manual.yaml# k8s api 版本apiVersion: v1 # 资源对象 kind: Pod# 元数据 pod名称metadata: name: kubia-manual# POD 内容spec: containers # 容器使用的镜像 - image: 172.31.228.68/project/kubia # 容器名称 name: kubia ports: # 监听端口 - containerPort: 8080 protocol: TCP 123# kubectl explain 查看支持的api对象[root@k8s-master1 demo]# kubectl explain pod[root@k8s-master1 demo]# kubectl explain pod.spec 使用 kubectl create 来创建 pod12345# kubectl create -f 命令用于从YAML或者JSON文件创建任何资源(不只是POD)[root@k8s-master1 demo]# kubectl create -f kubia-manual.yaml pod/kubia-manual created[root@k8s-master1 demo]# kubectl get pod -o wide 123# kubectl logs 查看应用程序日志[root@k8s-master1 demo]# kubectl logs kubia-manualKubia server starting... 12# 获取多容器pod的日志 -c 指定容器名称[root@k8s-master1 demo]# kubectl logs kubia-manual -c kubia 使用标签组织 pod12345# 常用的标签app: 指向应用 rel: 指向版本 stable beta canary(金丝雀) team: 指向组 TeamA TeamBenv: 指向环境 test prod dev 123456789101112131415161718[root@k8s-master1 demo]# vim kubia-manual-with-labels.yamlapiVersion: v1kind: Podmetadata: name: kubia-manual-v2 labels: team: A env: prodspec: containers: - image: 172.31.228.68/project/kubia name: kubia ports: - containerPort: 8080 protocol: TCP[root@k8s-master1 demo]# kubectl create -f kubia-manual-with-labels.yaml 12345# 查看 pod 并列出所有标签[root@k8s-master1 demo]# kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-manual 1/1 Running 0 33m env=prod,team=Akubia-manual-v2 1/1 Running 0 22m env=prod,team=A 12345# 根据标签名列出 pod [root@k8s-master1 demo]# kubectl get pod -L envNAME READY STATUS RESTARTS AGE ENVkubia-manual 1/1 Running 0 33m prodkubia-manual-v2 1/1 Running 0 23m prod 12345678# 修改pod的标签[root@k8s-master1 demo]# kubectl label pod kubia-manual-v2 env=debug --overwritepod/kubia-manual-v2 labeled[root@k8s-master1 demo]# kubectl get pod -L envNAME READY STATUS RESTARTS AGE ENVkubia-manual 1/1 Running 0 58m prodkubia-manual-v2 1/1 Running 0 48m debug 12345678910111213# 给pod添加一个新标签[root@k8s-master1 demo]# kubectl label pod kubia-manual app=ospod/kubia-manual labeled[root@k8s-master1 demo]# kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-manual 1/1 Running 0 60m app=os,env=prod,team=Akubia-manual-v2 1/1 Running 0 49m env=debug,team=A[root@k8s-master1 demo]# kubectl get pod -L appNAME READY STATUS RESTARTS AGE APPkubia-manual 1/1 Running 0 60m oskubia-manual-v2 1/1 Running 0 50m 通过标签选择器列出pod子集1234# 标签选择器条件:1. 包含或不包含2. 包含具有指定的键和值的标签3. 包含具有指定键的标签，值不指定 12345# 列出所有Team=A组的标签的pod[root@k8s-master1 demo]# kubectl get pod -l team=ANAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 64mkubia-manual-v2 1/1 Running 0 53m 1234# 列出所有 有env 标签的app[root@k8s-master1 demo]# kubectl get pod -l appNAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 65m 1234# 列出 没有app 标签的pod[root@k8s-master1 demo]# kubectl get pod -l '!app'NAME READY STATUS RESTARTS AGEkubia-manual-v2 1/1 Running 0 55m 标签选择器匹配1234567891011# 反向条件选择 [root@k8s-master1 demo]# kubectl label pod kubia-manual team=B --overwrite[root@k8s-master1 demo]# kubectl get pod -L teamNAME READY STATUS RESTARTS AGE TEAMkubia-manual 1/1 Running 0 70m Bkubia-manual-v2 1/1 Running 0 59m A[root@k8s-master1 demo]# kubectl get pod -l 'team!=A'NAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 70m 12345678910# in 或[root@k8s-master1 demo]# kubectl get pod -L envNAME READY STATUS RESTARTS AGE ENVkubia-manual 1/1 Running 0 72m prodkubia-manual-v2 1/1 Running 0 61m debug[root@k8s-master1 demo]# kubectl get pod -l 'env in (prod,debug)'NAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 71mkubia-manual-v2 1/1 Running 0 61m 1234# not in [root@k8s-master1 demo]# kubectl get pod -l 'env notin (prod)' -L envNAME READY STATUS RESTARTS AGE ENVkubia-manual-v2 1/1 Running 0 63m debug 多条件标签选择器1234# ,分割条件[root@k8s-master1 demo]# kubectl get pod -l team=B,app=osNAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 79m 使用标签选择器来约束pod调度1234# 解决pod随机调度到工作节点# 根据基建基础,比如机械硬盘或者固态硬盘 将一组pod调度到ssd硬盘的工作节点# 又或者是GPU节点# 给node加上标签,然后在使用标签选择器 1234567891011121314151617181920[root@k8s-master1 demo]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 40h v1.16.0k8s-node1 Ready &lt;none&gt; 40h v1.16.0k8s-node2 Ready &lt;none&gt; 40h v1.16.0# 比如node2 是 gpu计算节点[root@k8s-master1 demo]# kubectl label node k8s-node2 gpu=truenode/k8s-node2 labeled# 列出gpu node节点[root@k8s-master1 demo]# kubectl get node -l gpu=trueNAME STATUS ROLES AGE VERSIONk8s-node2 Ready &lt;none&gt; 40h v1.16.0[root@k8s-master1 demo]# kubectl get node -L gpuNAME STATUS ROLES AGE VERSION GPUk8s-master1 Ready &lt;none&gt; 40h v1.16.0 k8s-node1 Ready &lt;none&gt; 40h v1.16.0 k8s-node2 Ready &lt;none&gt; 40h v1.16.0 true 将pod调度到特定节点12345678910111213141516171819202122232425# 在yaml文件中添加 节点选择器# 节点选择器将pod部署到包含gpu=true的节点上[root@k8s-master1 demo]# vim kubia-gpu.yaml apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: nodeSelector: gpu: "true" containers: - image: 172.31.228.68/project/kubia name: kubia ports: - containerPort: 8080 protocol: TCP[root@k8s-master1 demo]# kubectl create -f kubia-gpu.yaml [root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkubia-gpu 1/1 Running 0 6s 10.244.1.13 k8s-node2 &lt;none&gt; &lt;none&gt;kubia-manual 1/1 Running 0 101m 10.244.2.11 k8s-master1 &lt;none&gt; &lt;none&gt;kubia-manual-v2 1/1 Running 0 90m 10.244.0.10 k8s-node1 &lt;none&gt; &lt;none&gt; 调度到一个特定节点1234567891. 每个机器都有一个通有标签 即主机名2. 如果该node处于离线状态，会导致该pod不可调度3. 不应该考虑单点调度[root@k8s-master1 demo]# kubectl get node -L kubernetes.io/hostnameNAME STATUS ROLES AGE VERSION HOSTNAMEk8s-master1 Ready &lt;none&gt; 40h v1.16.0 k8s-master1k8s-node1 Ready &lt;none&gt; 40h v1.16.0 k8s-node1k8s-node2 Ready &lt;none&gt; 40h v1.16.0 k8s-node2 pod 的注解123456789101112131415# 指定创建pod对象的人员姓名# 注解不能超过256KB[root@k8s-master1 demo]# kubectl annotate pod kubia-gpu MAINTAINER="leo"pod/kubia-gpu annotated# 查看注解[root@k8s-master1 demo]# kubectl describe pod kubia-gpuName: kubia-gpuNamespace: defaultPriority: 0Node: k8s-node2/172.31.228.70Start Time: Wed, 11 Mar 2020 09:40:37 +0800Labels: &lt;none&gt;Annotations: MAINTAINER: leo 使用命名空间对资源进行分组12345678910# 列出集群中的所有命名空间# 默认 default[root@k8s-master1 demo]# kubectl get nsNAME STATUS AGEdefault Active 41hingress-nginx Active 40hkube-node-lease Active 41hkube-public Active 41hkube-system Active 41hkubernetes-dashboard Active 40h 1234567891011121314# 根据命名空间查找 pod[root@k8s-master1 demo]# kubectl get pod --namespace kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-6b75f 1/1 Running 3 41hkube-flannel-ds-amd64-q29r8 1/1 Running 3 41hkube-flannel-ds-amd64-xh7c4 1/1 Running 3 41hkube-flannel-ds-amd64-xq7gq 1/1 Running 4 41h[root@k8s-master1 demo]# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-6b75f 1/1 Running 3 41hkube-flannel-ds-amd64-q29r8 1/1 Running 3 41hkube-flannel-ds-amd64-xh7c4 1/1 Running 3 41hkube-flannel-ds-amd64-xq7gq 1/1 Running 4 41h 123# 使用命名空间做k8s集群的用户隔离，多租户环境# 允许某些用户访问指定资源# 限制用户可用的计算资源数量 创建命名空间123456789# 从yaml文件创建 [root@k8s-master1 demo]# vim leo-com-namespace.yaml apiVersion: v1kind: Namespace # 资源类型metadata: name: leo-com # 命名空间名称 [root@k8s-master1 demo]# kubectl create -f leo-com-namespace.yaml namespace/leo-com created 123456789# 使用命令创建[root@k8s-master1 demo]# kubectl create namespace lex-com# 命名空间不允许包含点号[root@k8s-master1 demo]# kubectl get nsNAME STATUS AGE...leo-com Active 101slex-com Active 26s 在命名空间中创建资源123456789101112131415161718# 在yaml文件中指定[root@k8s-master1 demo]# vim kubia-manual.yaml apiVersion: v1kind: Podmetadata: name: kubia-manual namespace: leo-com # 指定[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEkubia-gpu 1/1 Running 0 42mkubia-manual 1/1 Running 0 144mkubia-manual-v2 1/1 Running 0 133m[root@k8s-master1 demo]# kubectl get pods -n leo-comNAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 8s 12345678# 创建资源的时候指定# 如果yaml文件中有指定，创建的时候会报错,所以要统一# 尽量在yaml中指定，强化api对象概念# 如果不指定会以当前上下文默认的命名空间操作，kubectl config命名修改[root@k8s-master1 demo]# kubectl create -f kubia-manual.yaml -n lex-com[root@k8s-master1 demo]# kubectl get pods --all-namespaces 命名空间提供的隔离12341. 命名空间将对象分隔到不同的组2. 命名空间不提供对象的任何隔离3. 命名空间是否提供网络隔离取决于k8s的网络方案4. 如果网络方案不提供命名空间的网络隔离，那么两个不同namespace中的两个pod之间是可以相互通信 停止和移除 pod按 pod 名称删除123456# k8s终止该pod中的所有容器# k8s发送sigterm信号并等待一定的秒数(默认30秒),使其正常关闭[root@k8s-master1 demo]# kubectl delete pod kubia-gpu# 删除多个pod 用空格分割[root@k8s-master1 demo]# kubectl delete pod kubia-gpu kubia-manual 使用标签选择器删除 pod1234567891011# 先找到他们共有的标签条件# 可以一次性删除所有标签的pod,使用前需要确认[root@k8s-master1 demo]# kubectl label pod kubia-manual env=debug --overwrite[root@k8s-master1 demo]# kubectl get pod --show-labelsNAME READY STATUS RESTARTS AGE LABELSkubia-manual 1/1 Running 0 162m app=os,env=debug,team=Bkubia-manual-v2 1/1 Running 0 151m env=debug,team=A[root@k8s-master1 demo]# kubectl delete pod -l env=debug 通过删除命名空间 来删除 pod12345678910111213141. 不需要命名空间下的pod2. 不需要该命名空间3. pod会伴随命名空间自动删除[root@k8s-master1 demo]# kubectl get pod -n leo-comNAME READY STATUS RESTARTS AGEkubia-manual 1/1 Running 0 21m[root@k8s-master1 demo]# kubectl delete ns leo-com[root@k8s-master1 demo]# kubectl get ns[root@k8s-master1 demo]# kubectl get pod -n leo-comNo resources found in leo-com namespace. 删除命名空间中的pod 保留命名空间1234[root@k8s-master1 demo]# kubectl delete pod --all -n lex-com[root@k8s-master1 demo]# kubectl get nslex-com Active 34m 注意 删除 pod 和 其他资源123# 如果我们通过run命令创建的rc，他不会直接创建pod，而是先创建一个rc，再由rc创建pod# 如果我们删除rc创建的pod，他会立即创建自定个数的新pod# 如果想要完整删除,还需要删除这个rc控制器，和deployment其他资源对象一样。 删除命名空间中的所有资源1234567# 可以删除rc和pod，也可以删除service# all 指定删除所有的资源对象类型# --all 删除所有资源实例# 并不是真的完整删除素有内容,有一些会被保留，比如secret会包保留# 这种命名不要不经考虑就使用# 他会删除kubernetes的service,需要看看是否会自动创建[root@k8s-master1 demo]# kubectl delete all --all]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 生态工具]]></title>
    <url>%2F2020%2F02%2F24%2Fpython-tools%2F</url>
    <content type="text"><![CDATA[Python 内置小工具下载服务器 在当前目录执行启动web服务，默认打开8000端口 123456# Python 2[root@data]# python -m SimpleHTTPServerServing HTTP on 0.0.0.0 port 8000 ...# Python 3[root@data]# python -m http.server JSON 格式化工具 提高可读性 123456789101112131415161718[root@k8s-master2 data]# echo '&#123;"name":"leo","age":29,"sex":"male"&#125;' | python -m json.tool&#123; "age": 29, "name": "leo", "sex": "male"&#125;# 自动对齐和格式化 [root@k8s-master2 data]# echo '&#123;"address":&#123;"city":"beijing","district":"xicheng"&#125;,"name":"leo","age":29,"sex":"male"&#125;' | python -m json.tool&#123; "address": &#123; "city": "beijing", "district": "xicheng" &#125;, "age": 29, "name": "leo", "sex": "male"&#125; 检查第三方库是否安装正确1[root@k8s-master2 data]# python -c "import paramiko" PIP 的使用12345678910111213141516171819202122232425262728293031323334353637383940# 升级 pip[root@k8s-master2 data]# pip install --upgrade pip# 查找安装包 [root@k8s-master2 data]# pip search flask# 安装指定版本的软件包[root@k8s-master2 data]# pip install flask==0.12# 查看安装包信息[root@k8s-master2 data]# pip show flaskVersion: 0.12Summary: A microframework based on Werkzeug, Jinja2 and good intentionsHome-page: http://github.com/pallets/flask/Author: Armin RonacherAuthor-email: armin.ronacher@active-4.comLicense: BSDLocation: /usr/lib64/python2.7/site-packagesRequires: Jinja2, click, itsdangerous, WerkzeugRequired-by: # 检查依赖是否完整[root@k8s-master2 data]# pip check flask# 删除软件包[root@k8s-master2 data]# pip uninstall Werkzeug[root@k8s-master2 data]# pip check flaskflask 0.12 requires werkzeug, which is not installed.[root@k8s-master2 data]# pip install flask==0.12# 查看安装包列表[root@k8s-master2 data]# pip list# 导出系统已安装的软件环境到 requirements 文件[root@k8s-master2 data]# pip freeze &gt; requirements.txt# 从 requirements 文件安装[root@k8s-master2 data]# scp requirements.txt root@172.31.228.70:/root[root@k8s-node2 ~]# pip install -r requirements.txt [root@k8s-master2 data]# pip list pip 加速 和 离线安装1234567891011121314# 如果网络不稳定 可选择国内的源地址[root@k8s-node2 ~]# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ flask==0.12# 修改pip的配置文件 阿里云主机已经是本地[root@k8s-node2 ~]# vim ~/.pip/pip.conf ## Note, this file is written by cloud-init on first boot of an instance## modifications made here will not survive a re-bundle.###[global]index-url=http://mirrors.cloud.aliyuncs.com/pypi/simple/[install]trusted-host=mirrors.cloud.aliyuncs.com 离线安装123456789101112131415# 安装包比较大的情况可以先下载到本地在部署# 下载到本地 [root@k8s-master2 tmp]# pip download -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com -r requirements.txt -d /tmp/packages[root@k8s-master2 packages]# ls -l...Click-7.0-py2.py3-none-any.whlFlask-0.12-py2.py3-none-any.whlitsdangerous-1.1.0-py2.py3-none-any.whlJinja2-2.11.1-py2.py3-none-any.whlMarkupSafe-1.1.1-cp27-cp27mu-manylinux1_x86_64.whlWerkzeug-1.0.0-py2.py3-none-any.whl# 将requirements.txt和packages文件夹拷贝到需要离线安装的电脑，通过以下命令进行安装：pip install --no-index --find-links=D:\packages -r requirements.txt根据pip版本不同，里面的参数有可能是--find-link。 Python 工作环境管理pyenv 全局的Python版本切换，也可以为单个项目提供对应的Python版本 使用pyenv可以在服务器上安装多个不同的Python版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@k8s]# echo 'export PYENV_ROOT="$HOME/.pyenv"' &gt;&gt; ~/.bash_profile [root@k8s]# echo 'export PATH="$PYENV_ROOT/bin:$PATH"' &gt;&gt; ~/.bash_profile [root@k8s]# echo 'eval "$(pyenv init -)"' &gt;&gt; ~/.bash_profile[root@k8s]# [root@k8s]# source ~/.bash_profile # 查看支持的版本[root@k8s-master2 packages]# pyenv install --list# 安装不同的python版本[root@k8s-master2 packages]# yum -y install zlib*[root@k8s-master2 packages]# pyenv install -v 3.6.0# 查看版本[root@k8s-master2 packages]# pyenv versions* system (set by /root/.pyenv/version) 2.7.13 3.6.0# 切换版本[root@k8s-master2 packages]# pyenv global 3.6.0[root@k8s-master2 packages]# pyenv versions system 2.7.13* 3.6.0 (set by /root/.pyenv/version)[root@k8s-master2 packages]# python Python 3.6.0 (default, Feb 25 2020, 10:07:56) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linuxType "help", "copyright", "credits" or "license" for more information.[root@k8s-master2 packages]# pip --versionpip 9.0.1 from /root/.pyenv/versions/3.6.0/lib/python3.6/site-packages (python 3.6)# 删除版本[root@k8s-master2 packages]# pyenv uninstall 2.7.13[root@k8s-master2 packages]# pyenv versions system* 3.6.0 (set by /root/.pyenv/version)# 常见问题及解决方案： 在使用pyenv install安装python，可能会比较慢，甚至下载安装不成功1. 这时最好使用是下载好python源码包，然后通过pyenv进行安装（可以下载到境外云服务器） 2. 将python源码包放置在~/.pyenv/cache/目录中（不要解压 ; 如没有cache就创建一个） 3. 执行pyenv install 3.6.4 virtualenv 用于隔离不同项目的工作环境 如一个应用的环境是flask0.8，另外一个事flask0.9 pyenv管理不同的python版本,如生产是2.7,开发是3.6,virtualenv隔离项目的工作环境,A和B项目都使用python2.7但是A项目的flask是0.8,B项目是0.9 如果使用了 pyenv 就必须使用 pyenv-virtualenv插件 12345678910111213141516171819# 安装 pyenv-virtualenv[root@k8s-master2 packages]# git clone https://github.com/yyuu/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv [root@k8s-master2 packages]# echo 'eval "$(pyenv virtualenv-init -)"' &gt;&gt; ~/.bash_profile [root@k8s-master2 packages]# pyenv help virtualenv[root@k8s-master2 packages]# pyenv virtualenv 3.6.0 A_project[root@k8s-master2 packages]# pyenv virtualenv 3.6.0 B_project# 查看环境[root@k8s-master2 packages]# pyenv virtualenvs 3.6.0/envs/A_project (created from /root/.pyenv/versions/3.6.0) 3.6.0/envs/B_project (created from /root/.pyenv/versions/3.6.0) A_project (created from /root/.pyenv/versions/3.6.0) B_project (created from /root/.pyenv/versions/3.6.0)# 进入和退出环境[root@k8s-master2 packages]# pyenv activate A_project(A_project) [root@k8s-master2 packages]# pyenv deactivate A_project]]></content>
      <categories>
        <category>Python 运维</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11 Isito 微服务治理]]></title>
    <url>%2F2020%2F01%2F13%2Fk8s-base13%2F</url>
    <content type="text"><![CDATA[Service Mesh Service Mesh 的中文译为 “服务网格” ，是一个用于处理服务和服务之间通信的基础设施层，它负责为构建复杂的云原生应用传递可靠的网络请求，并为服务通信实现了微服务所需的基本组件功能。 例如服务发现、负载均衡、监控、流量管理、访问控制等。 在实践中，服务网格通常实现为一组和应用程序部署在一起的轻量级的网络代理，但对应用程序来说是透明的。 12341. 基于通信2. 传递可靠的网络请求 -&gt; 网络转发3. 实现微服务的通信服务,提供组件 -&gt; 服务发现、负载均衡、监控、流量管理、访问控制4. 网络代理 12345# 图中:1. 绿色代表应用2. 蓝色代表服务网络代理3. 所有访问绿色应用的流量 都要被蓝色的网络代理转发 4. 访问绿色应用都要进出这个网络代理，他们组成的这个大型的服务网络叫做 “服务网格” 123456789101112# 传统的网络代理 -&gt; node1用户 -&gt; Nginx -&gt; node2 -&gt; node3 # 用户访问的流量都会经过Nginx,Nginx可以实现 WAF 白名单 限流# Nginx 拿到了所有的访问流量，所有的进出都可以看到 实现的功能比较简单# 如果有多套服务的情况下 Nginx就要不断的更新配置 # 微服务的组件更多 那么Nginx的集中管理就会出现较为复杂的问题# Service Mesh # 与这条代理模式类似,相当于一个分布式代理，管理代理策略,每一个服务都会绑定一个代理# 从一对多改为一对一,1个代理服务1组应用,就成为一个 "服务网格" 12341. 治理能力独立（Sidecar） 2. 应用程序无感知3. 服务通信的基础设施层 4. 解耦应用程序的重试/超时、监控、追踪和服务发现 1234# Sidecar 就是一个网络代理,如果ServiceA 要访问 ServiceB ，都需要经过 SidecarA -&gt; SidecarB -&gt; ServiceB# 需要集中的对所有 Sidecar代理 下发策略并管理的控制面板# 服务通信的基础设施层:服务发现、负载均衡、监控、流量管理、访问控制# Sidecar 独立服务，让应用服务之间通过Sidecar对外，拿到链路追踪 Istio 概述 Isito是Service Mesh的产品化落地，是目前最受欢迎的服务网格，功能丰富、成熟度高。 Linkerd是世界上第一个服务网格类的产品。 12官网:https://istio.io/ 12345678910111213141516171819202122231. 连接（Connect） - 流量管理- 负载均衡- 灰度发布# 控制数据流量传输2. 安全（Secure） - 认证- 鉴权# 用户到服务之间访问 # 服务到服务之间传输3. 控制（Control） - 限流- ACL(访问控制列表)# 控制每秒最大请求# 可以访问谁,不可以访问谁4. 观察（Observe） - 监控- 调用链# 收集流量信息、日志、qps# 服务之间相互调用链 Istio 与 K8S 12345# 微服务链路监控系统 Pinpoint 可以拿到微服务产生的一些流量数据，但是比较有限# Istio 全面的接管了 k8s与微服务内部之间的流量数据收集和更多的组件功能(流量管控waf,限流,链路追踪)# 动态路由:灰度发布,按照特定用户,按照流量可以转发到不同的实例版本# 链路追踪:下一个订单都经过哪些服务,都可以有拓扑图展示# k8s作为服务部署的基础组件,istio作为服务之间内部的管理组件，两者完美结合 Isito 架构与组件 数据平面： 由一组代理组成，这些代理微服务所有网络通信，并接收和实施来自Mixer的策略。 Proxy：负责高效转发与策略实现。实现:Envoy(与nginx类似) 控制平面： 管理和配置代理来路由流量。此外，通过mixer实施策略与收集来自边车代理的数据。 Mixer：适配组件，数据平面与控制平面通过它交互，为Proxy提供策略和数据上报。 Pilot：策略配置组件，为Proxy提供服务发现、智能路由、错误处理等。 Citadel：安全组件，提供证书生成下发、加密通信、访问控制。 Galley：配置管理、验证、分发。 Istio 基本概念 Istio 有 4 个配置资源，落地所有流量管理需求(实现上面Istio的功能)： 12341. VirtualService： 实现服务请求路由规则的功能。 转发规则2. DestinationRule： 实现目标服务的负载均衡、服务发现、故障处理和故障注入的功能。 3. Gateway： 让服务网格内的服务，可以被全世界看到。 暴露应用4. ServiceEntry ： 让服务网格内的服务，可以看到外面的世界。 默认情况下服务网格的服务只能相互访问,可以与没有安装Sidecar服务通信。 在 Kubernetes 部署 Istio123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# istioctl 暂时不支持 helm v3的部署[root@k8s-master1 opt]# wget https://github.com/istio/istio/releases/download/1.4.2/istio-1.4.2-linux.tar.gz[root@k8s-master1 opt]# tar zxvf istio-1.4.2-linux.tar.gz[root@k8s-master1 opt]# cd istio-1.4.2[root@k8s-master1 istio-1.4.2]# mv bin/istioctl /usr/bin# 部署类型[root@k8s-master1 istio-1.4.2]# istioctl profile listIstio configuration profiles: sds # 更全面 default # 默认,不指定任何类型 基本核心组件都有 demo # 所有的都有,还有附加组件,promethues、grafana、网格可视化、链路跟踪等 minimal # 最小化 remote # 部署[root@k8s-master1 istio-1.4.2]# istioctl manifest apply --set profile=demo # 形成资源清单Preparing manifests for these components:- Policy- Galley- CoreDNS- Grafana- EgressGateway- Kiali- PrometheusOperator- IngressGateway- Cni- Pilot- Injector- Citadel- Tracing- Telemetry- Prometheus- NodeAgent- Base- CertManagerApplying manifest for component BaseFinished applying manifest for component BaseApplying manifest for component IngressGatewayApplying manifest for component GalleyApplying manifest for component CitadelApplying manifest for component EgressGatewayApplying manifest for component PrometheusApplying manifest for component PolicyApplying manifest for component PilotApplying manifest for component TelemetryApplying manifest for component TracingApplying manifest for component KialiApplying manifest for component InjectorApplying manifest for component GrafanaFinished applying manifest for component CitadelFinished applying manifest for component PrometheusFinished applying manifest for component GalleyFinished applying manifest for component PolicyFinished applying manifest for component InjectorFinished applying manifest for component KialiFinished applying manifest for component TracingFinished applying manifest for component PilotFinished applying manifest for component EgressGatewayFinished applying manifest for component IngressGatewayFinished applying manifest for component GrafanaFinished applying manifest for component TelemetryComponent Grafana installed successfully:=========================================Component EgressGateway installed successfully:===============================================Component Policy installed successfully:========================================Component Galley installed successfully:========================================Component CoreDNS installed successfully:=========================================Component IngressGateway installed successfully:================================================Component Cni installed successfully:=====================================Component Kiali installed successfully:=======================================Component PrometheusOperator installed successfully:====================================================Component Citadel installed successfully:=========================================Component Tracing installed successfully:=========================================Component Pilot installed successfully:=======================================Component Injector installed successfully:==========================================Component Base installed successfully:======================================Component CertManager installed successfully:=============================================Component Telemetry installed successfully:===========================================Component Prometheus installed successfully:============================================Component NodeAgent installed successfully:=========================================== 12345678910111213141516171819202122# 在k8s里面查看 , 如果网络不好的话 就需要自己下载好镜像,在各个节点上提前准备好[root@k8s-master1 istio-1.4.2]# kubectl get deploy -n istio-system -o yaml|grep image[root@k8s-master1 istio-1.4.2]# kubectl get pod -n istio-systemNAME READY STATUS RESTARTS AGEgrafana-6b65874977-942p9 1/1 Running 0 45mistio-citadel-86dcf4c6b-8vg54 1/1 Running 0 45m # 安全istio-egressgateway-68f754ccdd-znhsf 1/1 Running 0 45m # 出的 gatewayistio-galley-5fc6d6c45b-86vdv 1/1 Running 0 45mistio-ingressgateway-6d759478d8-66hfx 1/1 Running 0 45m # 进的 gatewayistio-pilot-5c4995d687-z74wt 1/1 Running 0 45m # 策略分发istio-policy-57b99968f-k6klc 1/1 Running 8 45mistio-sidecar-injector-746f7c7bbb-2xqz2 1/1 Running 0 45m # 为每个pod 注册代理istio-telemetry-854d8556d5-t8ztd 1/1 Running 8 45mistio-tracing-c66d67cd9-pf9mw 1/1 Running 0 45mkiali-8559969566-vvdtv 1/1 Running 0 45m # 可视化网格prometheus-66c5887c86-qm6l2 1/1 Running 0 45m[root@k8s-master1 istio-1.4.2]# kubectl get svc -n istio-system卸载：[root@k8s-master1 istio-1.4.2]# istioctl manifest generate --set profile=demo | kubectl delete -f - Sidercar 注入1234567891011121314151617181920212223242526272829303132# 查看实例[root@k8s-master1 istio-1.4.2]# cd samples/[root@k8s-master1 samples]# ls -ltotal 72drwxr-xr-x 7 root root 4096 Dec 7 04:54 bookinfodrwxr-xr-x 2 root root 4096 Dec 7 04:54 certsdrwxr-xr-x 2 root root 4096 Dec 7 04:54 custom-bootstrapdrwxr-xr-x 2 root root 4096 Dec 7 04:54 externaldrwxr-xr-x 2 root root 4096 Dec 7 04:54 fortiodrwxr-xr-x 2 root root 4096 Dec 7 04:54 health-checkdrwxr-xr-x 3 root root 4096 Dec 7 04:54 helloworlddrwxr-xr-x 4 root root 4096 Dec 7 04:54 httpbin # http web 小实例drwxr-xr-x 2 root root 4096 Dec 7 04:54 httpsdrwxr-xr-x 2 root root 4096 Dec 7 04:54 kubernetes-blogdrwxr-xr-x 2 root root 4096 Dec 7 04:54 multiclusterdrwxr-xr-x 2 root root 4096 Dec 7 04:54 operatordrwxr-xr-x 2 root root 4096 Dec 7 04:54 rawvm-rw-r--r-- 1 root root 98 Dec 7 04:54 README.mddrwxr-xr-x 3 root root 4096 Dec 7 04:54 securitydrwxr-xr-x 4 root root 4096 Dec 7 04:54 sleepdrwxr-xr-x 3 root root 4096 Dec 7 04:54 tcp-echodrwxr-xr-x 2 root root 4096 Dec 7 04:54 websockets[root@k8s-master1 samples]# cd httpbin/[root@k8s-master1 httpbin]# ls -l-rw-r--r-- 1 root root 474 Dec 7 04:54 httpbin-gateway.yaml -rw-r--r-- 1 root root 1415 Dec 7 04:54 httpbin-nodeport.yaml-rw-r--r-- 1 root root 1445 Dec 7 04:54 httpbin-vault.yaml-rw-r--r-- 1 root root 1498 Dec 7 04:54 httpbin.yamldrwxr-xr-x 2 root root 4096 Dec 7 04:54 policy-rw-r--r-- 1 root root 1726 Dec 7 04:54 README.mddrwxr-xr-x 2 root root 4096 Dec 7 04:54 sample-client 部署 httpbin Web示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@k8s-master1 httpbin]# vim httpbin-nodeport.yaml apiVersion: v1kind: Servicemetadata: name: httpbin labels: app: httpbinspec: type: NodePort ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin---apiVersion: apps/v1kind: Deploymentmetadata: name: httpbinspec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - image: docker.io/kennethreitz/httpbin imagePullPolicy: IfNotPresent name: httpbin ports: - containerPort: 80[root@k8s-master1 httpbin]# kubectl apply -f httpbin-nodeport.yaml service/httpbin createddeployment.apps/httpbin created# 和之前一样的访问web[root@k8s-master1 httpbin]# kubectl get pods,svc,epNAME READY STATUS RESTARTS AGEpod/httpbin-768b999cb5-pfp8j 1/1 Running 0 99sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/httpbin NodePort 10.0.0.124 &lt;none&gt; 8000:30991/TCP 99sservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 3h27mNAME ENDPOINTS AGEendpoints/httpbin 10.244.1.6:80 99sendpoints/kubernetes 172.31.228.67:6443 3h27m# http://47.240.12.170:30991/ # nodeip+nodeport 手动注入 Sidercar12345678910111213141516171819# Sidercar 就是 istio 的 proxy [root@k8s-master1 httpbin]# kubectl apply -f &lt;(istioctl kube-inject -f httpbin-nodeport.yaml)service/httpbin unchangeddeployment.apps/httpbin configured[root@k8s-master1 httpbin]# kubectl get podsNAME READY STATUS RESTARTS AGEhttpbin-5c8ff7878b-h7fzf 2/2 Running 0 4m2s# 或者istioctl kube-inject -f httpbin-nodeport.yaml |kubectl apply -f -# 自动注入# 为命名空间打标签 这个命名空间就会被 istio感知 这个命名空间下的pod都会被注入kubectl label namespace default istio-injection=enabledkubectl apply -f httpbin-gateway.yamlNodePort访问地址：http://nodeip:nodeport 123456789101112131415161718192021222324252627282930313233343536373839# istio 自身的管控进出流量Gateway根据流入流出方向分为：1. IngressGateway：接收外部访问，并将流量转发到网格内的服务。 2. EgressGateway：网格内服务访问外部应用。# 使用istio自身网关访问# VirtualService 定义路由规则 类似nginx的虚拟主机概念[root@k8s-master1 httpbin]# vim httpbin-gateway.yaml apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: httpbin-gatewayspec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - "*"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - "*" gateways: - httpbin-gateway http: - route: - destination: host: httpbin port: number: 8000 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master1 httpbin]# kubectl apply -f httpbin-gateway.yaml gateway.networking.istio.io/httpbin-gateway createdvirtualservice.networking.istio.io/httpbin created[root@k8s-master1 httpbin]# kubectl get gatewayNAME AGEhttpbin-gateway 18s# 找到端口和方式 [root@k8s-master1 httpbin]# kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEgrafana-6b65874977-942p9 1/1 Running 0 136mistio-citadel-86dcf4c6b-8vg54 1/1 Running 0 137mistio-egressgateway-68f754ccdd-znhsf 1/1 Running 0 137m # 内部访问外部 istio-galley-5fc6d6c45b-86vdv 1/1 Running 0 137mistio-ingressgateway-6d759478d8-66hfx 1/1 Running 0 137m # 外部访问内部istio-pilot-5c4995d687-z74wt 1/1 Running 0 137mistio-policy-57b99968f-k6klc 1/1 Running 8 137mistio-sidecar-injector-746f7c7bbb-2xqz2 1/1 Running 0 137mistio-telemetry-854d8556d5-t8ztd 1/1 Running 8 137mistio-tracing-c66d67cd9-pf9mw 1/1 Running 0 137mkiali-8559969566-vvdtv 1/1 Running 0 [root@k8s-master1 httpbin]# vim httpbin-gateway.yaml host: httpbin # k8s service名字# ingressgateway 也是个svc [root@k8s-master1 httpbin]# kubectl get svc -n istio-systemistio-ingressgateway LoadBalancer 10.0.0.248 &lt;pending&gt; 15020:30088/TCP,80:31713/TCP,443:31409/TCP,15029:30683/TCP,15030:32202/TCP,15031:30249/TCP,15032:30552/TCP,15443:31498/TCP 139m# LoadBalancer 对接公有云的LB 类型 # 没人为这个LoadBalancer 创建LB 但是生成了SVC 所以这个的暴露端口就是 80:31713 443:31409# istio-ingressgateway 相当于 istio自己的一套 ingress# istio-ingressgateway -&gt; istio-proxy 帮我们拿到流量去做处理# 请求: nodeip + 31713 -&gt; ingressgateway(负载) -&gt; istio-proxy -&gt; httpbin -&gt; pod# 请求: nodeip + 30991 -&gt; k8s svc nodeport -&gt; pod# 日志[root@k8s-master1 httpbin]# kubectl logs istio-ingressgateway-6d759478d8-66hfx -n istio-system 服务网关 Gateway Gateway为网格内服务提供负载均衡器，提供以下功能： Envoy L4-L7的负载均衡 对外的mTLS Gateway根据流入流出方向分为： IngressGateway：接收外部访问，并将流量转发到网格内的服务。 EgressGateway：网格内服务访问外部应用。 部署 bookinfo 微服务示例 Bookinfo 应用分为四个单独的微服务 productpage ：productpage 微服务会调用 details 和reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本 v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 5个黑色五角星 来显示评分信息。 v3 版本会调用 ratings 服务，并使用 5个红色五角星 来显示评分信息。 部署 bookinfo12345678910111213141516171819202122232425262728293031323334353637383940414243# 创建命名空间[root@k8s-master1 httpbin]# kubectl create ns bookinfo # 自动注入 Sidercar# 为命名空间打标签 这个命名空间就会被 istio感知 这个命名空间下的pod都会被注入[root@k8s-master1 httpbin]# kubectl label namespace bookinfo istio-injection=enabled# 创建 bookinfo 一定要确保每个pod的2个镜像都创建成功 Running状态[root@k8s-master1 httpbin]# cd /opt/istio-1.4.2/samples/bookinfo/[root@k8s-master1 httpbin]# kubectl apply -f platform/kube/bookinfo.yaml -n bookinfo[root@k8s-master1 bookinfo]# kubectl get pod -n bookinfo -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdetails-v1-78d78fbddf-k242n 2/2 Running 0 7m1s 10.244.1.8 k8s-node1 &lt;none&gt; &lt;none&gt;productpage-v1-596598f447-fw8qn 2/2 Running 0 7m 10.244.0.8 k8s-node2 &lt;none&gt; &lt;none&gt;ratings-v1-6c9dbf6b45-v2nrz 2/2 Running 0 7m1s 10.244.2.7 k8s-master1 &lt;none&gt; &lt;none&gt;reviews-v1-7bb8ffd9b6-d26cg 2/2 Running 0 7m1s 10.244.2.8 k8s-master1 &lt;none&gt; &lt;none&gt;reviews-v2-d7d75fff8-lfwdj 2/2 Running 0 7m1s 10.244.1.9 k8s-node1 &lt;none&gt; &lt;none&gt;reviews-v3-68964bc4c8-zvtwf 2/2 Running 0 7m1s 10.244.2.9 k8s-master1 &lt;none&gt; &lt;none&gt;# gateway网关,需要替换微服务的gateway,需要开发支持,这里面定义了跳转# 通过ingressgateway 暴露服务[root@k8s-master1 bookinfo]# kubectl apply -f networking/bookinfo-gateway.yaml -n bookinfo[root@k8s-master1 bookinfo]# kubectl get svc -n istio-system|grep istio-ingressgatewayistio-ingressgateway LoadBalancer 10.0.0.248 &lt;pending&gt; 15020:30088/TCP,80:31713/TCP,443:31409/TCP,15029:30683/TCP,15030:32202/TCP,15031:30249/TCP,15032:30552/TCP,15443:31498/TCP 5h27m[root@k8s-master1 bookinfo]# kubectl get svc -n bookinfoNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdetails ClusterIP 10.0.0.149 &lt;none&gt; 9080/TCP 13mproductpage ClusterIP 10.0.0.62 &lt;none&gt; 9080/TCP 13mratings ClusterIP 10.0.0.118 &lt;none&gt; 9080/TCP 13mreviews ClusterIP 10.0.0.184 &lt;none&gt; 9080/TCP 13m# 访问地址：http://47.240.12.170:31713/productpage# 刷新会轮旋3个版本的reviews,都在工作...reviews-v1-7bb8ffd9b6-d26cg 2/2 Running 0 7m1s 10.244.2.8 k8s-master1 &lt;none&gt; &lt;none&gt;reviews-v2-d7d75fff8-lfwdj 2/2 Running 0 7m1s 10.244.1.9 k8s-node1 &lt;none&gt; &lt;none&gt;reviews-v3-68964bc4c8-zvtwf 2/2 Running 0 7m1s 10.244.2.9 k8s-master1 &lt;none&gt; &lt;none&gt; 通过域名分流12配置hosts47.240.15.208 grafana.ctnrs.com kiali.ctnrs.com tracing.ctnrs.com httpbin.ctnrs.com bookinfo.ctnrs.com 12345678910111213141516171819202122232425# 增加LB nginx 作为流量的统一入口 -&gt; ingressgateway # 找其中一个node节点安装# 当前的ingressgateway 的svc是以 nodeport方式暴露的 那么现在所有的node节点都可以访问到[root@k8s-node2 ~]# yum install nginx -y upstream ingressgateway &#123; # nodeip+inggateway_svc_port server 172.31.228.70:31713; server 172.31.228.69:31713; &#125;... # 所有域名都解析到这台机器的 80端口或者使用公有云7层LB # 设置请求头 必须加 # 指定 http版本1.1 location / &#123; proxy_pass http://ingressgateway; proxy_set_header Host $host; proxy_http_version 1.1; &#125;[root@k8s-node2 ~]# systemctl restart nginx# 访问 http://httpbin.ctnrs.com -&gt; 47.240.15.208 -&gt; 172.31.228.70 nginx -&gt; server 172.31.228.69/70:31713 # 但是这个时候 不管访问哪个域名 都会到 httpbin的页面# 必须匹配url bookinfo.ctnrs.com/productpage# 这样并不是用域名做分流,默认还是 httpbin的页面 1234567891011# 修改 ingressgateway的 host # 如果是用域名区分,以后host那就不能用* 必须用域名区分[root@k8s-master1 samples]# vim httpbin/httpbin-gateway.yaml ... hosts: - "httpbin.ctnrs.com" [root@k8s-master1 samples]# kubectl apply -f httpbin/httpbin-gateway.yaml http://httpbin.ctnrs.com 只有这个能访问到了 httpbinhttp://bookinfo.ctnrs.com 无法再默认访问到 httpbin 1234567891011121314151617# 修改 bookinfo [root@k8s-master1 samples]# vim bookinfo/networking/bookinfo-gateway.yaml ...apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - "bookinfo.ctnrs.com"... [root@k8s-master1 samples]# kubectl apply -f bookinfo/networking/bookinfo-gateway.yaml -n bookinfo# web访问:http://bookinfo.ctnrs.com/productpage Istio 实现灰度发布1. 主流发布方案： 12341. 蓝绿发布2. 灰度发布（金丝雀发布）3. A/B Test4. 滚动发布 11. 都是为新旧业务切换,保证平滑切换 蓝绿发布 蓝绿发布 项目逻辑上分为AB组，在项目升级时，首先把A组从负 载均衡中摘除，进行新版本的部署。B组仍然继续提供服务。 A组升级完成上线，B组从负载均衡中摘除。 特点： - 策略简单 - 升级/回滚速度快 - 用户无感知，平滑过渡 缺点： - 需要两倍以上服务器资源 - 短时间内浪费一定资源成本 滚动发布 滚动发布 每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版升级新版本。 Kubernetes的默认发布策略。 特点： 用户无感知，平滑过渡 缺点： 部署周期长 发布策略较复杂 不易回滚 有问题影响范围大 灰度发布（金丝雀发布） 灰度发布（金丝雀发布） 只升级部分服务，即让一部分用户继续用老版本，一部分用户开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来。 特点： 保证整体系统稳定性 用户无感知，平滑过渡 缺点： 自动化要求高 灰度发布 A/B Test A/B Test 灰度发布的一种方式，主要对特定用户采样后，对收集到的反馈数据做相关对比，然后根据比对结果作出决策。 用来测试应用功能表现的方法，侧重应用的可用性，受欢迎程度等，最后决定是否升级。 基于权重的路由（金丝雀发布） 任务 流量全部发送到reviews v1版本（不带五角星） 将90%的流量发送到reviews v1版本，另外10%的流量发送到reviews v2版本（5个黑色五角星），最后完全切换到v2版本 将50%的流量发送到v2版本，另外50%的流量发送到v3版本（5个红色五角星） 123456789101112131415161718192021222324252627kubectl apply -f networking/virtual-service-all-v1.yaml -n bookinfokubectl apply -f networking/destination-rule-all.yaml -n bookinfokubectl apply -f networking/virtual-service-reviews-90-10.yaml -n bookinfokubectl apply -f networking/virtual-service-reviews-v2-v3.yaml -n bookinfo# 完全切换 v3[root@k8s-master1 bookinfo]# vim networking/virtual-service-reviews-v2-v3.yamlapiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 weight: 0 - destination: host: reviews subset: v3 weight: 100 基于请求内容的路由（A/B Test） 任务： 将特定用户的请求发送到reviews v2版本（5个黑色五角星），其他用户则不受影响（v3） 这个实例是根据请求的用户做区分 jason登录的 都是v2版本 1kubectl apply -f networking/virtual-service-reviews-jason-v2-v3.yaml -n bookinfo Istio 实现灰度发布流程 可视化监控 监控指标（Grafana） 网格可视化（Kiali） 调用链跟踪（Jaeger） 1234567891011121314151617# 通过 istio-ingressgateway 暴露应用[root@k8s-master1 istio-1.4.2]# ls -l monitor-gateway.yaml -rw-r--r-- 1 root root 1529 Jan 17 18:24 monitor-gateway.yaml[root@k8s-master1 istio-1.4.2]# kubectl apply -f monitor-gateway.yaml -n istio-systemgateway.networking.istio.io/grafana-gateway createdvirtualservice.networking.istio.io/grafana createdgateway.networking.istio.io/kiali-gateway createdvirtualservice.networking.istio.io/kiali createdgateway.networking.istio.io/tracing-gateway createdvirtualservice.networking.istio.io/tracing created# web访问grafana.ctnrs.com [root@k8s-master1 istio-1.4.2]# for i in &#123;1..100&#125;;do curl -I http://172.31.228.70 -H "Host: http://bookinfo.ctnrs.com/productpage";sleep 1;donekiali.ctnrs.com admin/admin tracing.ctnrs.com 链路跟踪 jaeger 1234567891011121314151617grafana1、请求错误率2、请求时延（响应时间）kiali3、链路调用拓扑图4、RPS（每秒请求），也有请求错误率5、请求/响应数据包大小6、查看Pod日志7、istio配置资源在线编辑jeager8、一个服务涉及的调用情况9、分析数据包中具体请求/响应信息10、也有响应时间主要针对流量获取。 小总结123用户 -&gt; LB搭理多套ingress -&gt; ingress -&gt; pod 用户 -&gt; LVS -&gt; dep1 -&gt; svc1 Pod -&gt; dep2 -&gt; svc1 Pod]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10 K8S 持久化存储 Ceph]]></title>
    <url>%2F2020%2F01%2F13%2Fk8s-base12%2F</url>
    <content type="text"><![CDATA[Ceph 介绍为什么要用 Ceph Ceph是当前非常流行的开源分布式存储系统，具有高扩展性、高性能、高可靠性等优点，同时提供块存储服务(rbd)、对象存储服务(rgw)以及文件系统存储服务(cephfs)。 Ceph在存储的时候充分利用存储节点的计算能力，在存储每一个数据时都会通过计算得出该数据的位置，尽量的分布均衡。 目前也是OpenStack的主流后端存储，随着OpenStack在云计算领域的广泛使用，ceph也变得更加炙手可热。 国内目前使用ceph搭建分布式存储系统较为成功的企业有x-sky,深圳元核云，上海UCloud等三家企业。 Ceph设计思想：集群可靠性、集群可扩展性、数据安全性、接口统一性、充分发挥存储设备自身的计算能力、去除中心化。 Ceph 架构介绍 Ceph使用RADOS提供对象存储，通过librados封装库提供多种存储方式的文件和对象转换。 外层通过RGW（Object，有原生的API，而且也兼容Swift和S3的API，适合单客户端使用）、RBD（Block，支持精简配置、快照、克隆，适合多客户端有目录结构）、CephFS（File，Posix接口，支持快照，适合更新变动少的数据，没有目录结构不能直接打开）将数据写入存储。 1234567891011- 高性能 1. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高 2. 考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等 3. 能够支持上千个存储节点的规模，支持TB到PB级的数据 - 高可扩展性 1. 去中心化 2. 扩展灵活 3. 随着节点增加而线性增长 - 特性丰富 1. 支持三种存储接口：块存储、文件存储、对象存储 2. 支持自定义接口，支持多种语言驱动 Ceph 核心概念RADOS 全称Reliable Autonomic Distributed Object Store，即可靠的、自动化的、分布式对象存储系统。 RADOS是Ceph集群的精华，用户实现数据分配、Failover等集群操作。 Librados Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的。 目前提供PHP、Ruby、Java、Python、C和C++支持。 Crush Crush算法是Ceph的两大创新之一，通过Crush算法的寻址操作，Ceph得以摒弃了传统的集中式存储元数据寻址方案。 而Crush算法在一致性哈希基础上很好的考虑了容灾域的隔离，使得Ceph能够实现各类负载的副本放置规则，例如跨机房、机架感知等。 同时，Crush算法有相当强大的扩展性，理论上可以支持数千个存储节点，这为Ceph在大规模云环境中的应用提供了先天的便利。 121. Crush 算法 数据平均分配到各个节点上,访问的时候把数据拼起来获取 2. Crush 类似索引 有地址记录数据存放在哪里 Pool Pool是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略(默认3副本 )。 支持两种类型：副本（replicated）和 纠删码（ Erasure Code） PG PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略。 简单点说就是相同PG内的对象都会放到相同的硬盘上，PG是ceph的逻辑概念，服务端数据均衡和恢复的最小粒度就是PG。 一个PG包含多个OSD,引入PG这一层其实是为了更好的分配数据和定位数据； Object 简单来说块存储读写快，不利于共享，文件存储读写慢，利于共享。 能否弄一个读写快，利于共享的出来呢。于是就有了对象存储。最底层的存储单元，包含元数据和原始数据。 Ceph 核心组件OSD OSD是负责物理存储的进程，一般配置成和磁盘一一对应，一块磁盘启动一个OSD进程。 主要功能是存储数据、复制数据、平衡数据、恢复数据，以及与其它OSD间进行心跳检查，负责响应客户端请求返回具体数据的进程等； 1234# Pool、PG和OSD的关系：1. 1个Pool里有很多PG； 2. 1个PG里包含一堆对象，1个对象只能属于一个PG； 3. PG有主从之分，一个PG分布在不同的OSD上（针对三副本类型）; Monitor 1个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。 负责坚实整个Ceph集群运行的Map视图（如OSD Map、Monitor Map、PG Map和CRUSH Map），维护集群的健康状态，维护展示集群状态的各种图表，管理集群客户端认证与授权； Monitor生产上至少3个组成高可用。 定期探测组件的健康状态。 11. osd想要什么数据 去 Monitor 里的 OSD Map 里面获取 MDS MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。 负责保存文件系统的元数据，管理目录结构。 对象存储和块设备存储不需要元数据服务； Mgr ceph 官方开发了 ceph-mgr，主要目标实现 ceph 集群的管理，为外界提供统一的入口。 例如 cephmetrics、zabbix、calamari、promethus。 Mgr可以作为主从模式,挂了不影响集群使用。 RGW RGW 全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。 Admin Ceph常用管理接口通常都是命令行工具，如rados、ceph、rbd等命令 。 另外Ceph还有可以有一个专用的管理节点，在此节点上面部署专用的管理工具来实现近乎集群的一些管理工作，如集群部署，集群组件管理等。 Ceph 三种存储类型块存储（RBD） 12345678910111213- 优点： * 通过Raid与LVM等手段，对数据提供了保护； * 多块廉价的硬盘组合起来，提高容量； * 多块磁盘组合出来的逻辑盘，提升读写效率； - 缺点： * 采用SAN架构组网时，光纤交换机，造价成本高； * 主机之间无法共享数据；- 使用场景 * docker容器、虚拟机磁盘存储分配； * 日志存储； * 文件存储； 文件存储（CephFS） 123456789101112- 优点： * 造价低，随便一台机器就可以了； * 方便文件共享；- 缺点： * 读写速率低； * 传输速率慢；- 使用场景 * 日志存储； * FTP、NFS； * 其它有目录结构的文件存储 对象存储（Object）(适合更新变动较少的数据) 1234567- 优点： * 具备块存储的读写高速； * 具备文件存储的共享等特性；- 使用场景 * 图片存储； * 视频存储； io ceph 流程 client 访问 Monitor map 拿数据 然后去OSD去找文件,OSD里面有主从概念,主提供服务,副本不做改动,通过盘符标识(SSD/HDD),分配主从节点。 读数据 流程到返回 写数据 强一致性,主副本先写,写完后同步从副本，三个副本之间通信数据一致后,数据才可以继续读取 小总结 为什么用ceph： 可扩展,节省成本,支持接口多 架构：分布式架构,多接口,RADOS -&gt; Librados -&gt; RGW,RBD,CephFS ,每个组件分布式,数据也是分布式 三种存储类型: RBD(块存储),CephFS(文件存储),对象存储(object) Ceph 集群部署Ceph 版本选择 官网安装 最新版本 手动安装 内网yum源 安装指定版本 Ceph版本来源介绍 Ceph 社区最新版本是 14，而 Ceph 12 是市面用的最广的稳定版本。 第一个 Ceph 版本是 0.1 ，要回溯到 2008 年 1 月。 多年来，版本号方案一直没变，直到 2015 年 4 月 0.94.1 （ Hammer 的第一个修正版）发布后，为了避免 0.99 （以及 0.100 或 1.00 ？），制定了新策略。 123456789x.0.z - 开发版（给早期测试者和勇士们）x.1.z - 候选版（用于测试集群、高手们）x.2.z - 稳定、修正版（给用户们）x 将从 9 算起，它代表 Infernalis （ I 是第九个字母），这样第九个发布周期的第一个开发版就是 9.0.0 ；后续的开发版依次是 9.0.1 、 9.0.2 等等。ceph不好招人,对数据存在敬畏之心,小心操作,数据的安全性和一致性太重要了 版本名称 版本号 发布时间 Argonaut 0.48版本(LTS) 2012年6月3日 Bobtail 0.56版本(LTS) 2013年5月7日 Cuttlefish 0.61版本 2013年1月1日 Dumpling 0.67版本(LTS) 2013年8月14日 Emperor 0.72版本 2013年11月9 Firefly 0.80版本(LTS) 2014年5月 Giant Giant October 2014 - April 2015 Hammer Hammer April 2015 - November 2016 Infernalis Infernalis November 2015 - June 2016 Jewel 10.2.9 2016年4月 Kraken 11.2.1 2017年10月 Luminous 12.2.12 2017年10月 mimic 13.2.7 2018年5月 nautilus 14.2.5 2019年2月 12# 本次实验 nautilus# 成熟版本 Luminous Luminous 新版本特性1. Bluestore 12345678910111213141516171. ceph-osd的新后端存储BlueStore已经稳定，是新创建的OSD的默认设置。# Ceph BlueStore 与 FileStore # FileStore 先格式化系统盘 把数据写入磁盘形成文件 再写入到 RADOS # BlueStore 直接转化，无需格式化,直接管理裸盘# BlueStore通过直接管理物理HDD或SSD而不使用诸如XFS的中间文件系统，来管理每个OSD存储的数据，这提供了更大的性能和功能。# BlueStore支持Ceph存储的所有的完整的数据和元数据校验。# BlueStore内嵌支持使用zlib，snappy或LZ4进行压缩。（Ceph还支持zstd进行RGW压缩，但由于性能原因，不为BlueStore推荐使用zstd）2. 集群的总体可扩展性有所提高。已经成功测试了多达10,000个OSD的集群。按实际走100个osd应该稳定。3. ceph-mgr# ceph-mgr是一个新的后台进程，这是任何Ceph部署的必须部分。虽然当ceph-mgr停止时，IO可以继续，但是度量不会刷新，并且某些与度量相关的请求（例如，ceph df）可能会被阻止。# 我们建议您多部署ceph-mgr的几个实例来实现可靠性。# ceph-mgr守护进程daemon包括基于REST的API管理。注：API仍然是实验性质的，目前有一些限制，但未来会成为API管理的基础。# ceph-mgr还包括一个Prometheus插件。# ceph-mgr现在有一个Zabbix插件。使用zabbix_sender，它可以将集群故障事件发送到Zabbix Server主机。这样可以方便地监视Ceph群集的状态，并在发生故障时发送通知。 安装前准备1234# 我自己重新创建了3台 阿里云主机# cephnode01 172.31.228.59# cephnode02 172.31.228.60# cephnode03 172.31.228.61 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455（1）关闭防火墙：systemctl stop firewalldsystemctl disable firewalld（2）关闭selinux：sed -i 's/enforcing/disabled/' /etc/selinux/configsetenforce 0（3）关闭NetworkManagersystemctl disable NetworkManagersystemctl stop NetworkManager（4）添加主机名与IP对应关系：vim /etc/hosts172.31.228.59 cephnode01172.31.228.60 cephnode02 172.31.228.61 cephnode03（5）设置主机名：[root@ceph ~]# hostnamectl set-hostname cephnode01[root@ceph ~]# hostnamectl set-hostname cephnode02[root@ceph ~]# hostnamectl set-hostname cephnode03（6）同步网络时间和修改时区systemctl restart chronyd.service &amp;&amp; systemctl enable chronyd.servicecp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime（7）设置文件描述符echo "ulimit -SHn 102400" &gt;&gt; /etc/rc.localcat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 65535* hard nofile 65535（8）内核参数优化cat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOFkernel.pid_max = 4194303echo "vm.swappiness = 0" /etc/sysctl.conf EOFsysctl -p（9）在cephnode01上配置免密登录到cephnode02、cephnode03ssh-keygen ls -l .ssh/ssh-copy-id root@cephnode02ssh-copy-id root@cephnode03ssh root@cephnode03ssh root@cephnode02(10)read_ahead,通过数据预读并且记载到随机访问内存方式提高磁盘读操作echo "8192" &gt; /sys/block/sda/queue/read_ahead_kb(11) I/O Scheduler，SSD要用noop，SATA/SAS使用deadlineecho "deadline" &gt;/sys/block/sd[x]/queue/schedulerecho "noop" &gt;/sys/block/sd[x]/queue/scheduler 安装内网yum源这一步有问题 直接用下面的 阿里云源 1. 安装httpd、createrepo 和 epel源 1yum install httpd createrepo epel-release -y 2. 编辑yum源文件 123456789101112131415161718192021222324252627[root@cephnode01 ~]# vim /etc/yum.repos.d/ceph.repo [Ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.asc 3. 下载Ceph 相关rpm包 1[root@cephnode01 ~]# yum --downloadonly --downloaddir=/var/www/html/ceph/rpm-nautilus/el7/x86_64/ install ceph ceph-radosgw 4. 下载Ceph依赖文件 12345678910111213141516171819202122232425262728# 自作源的时候 要下载这些依赖wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-14.2.4-0.el7.src.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-deploy-2.0.1-0.src.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-deploy-2.0.1-0.noarch.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-cloud-14.2.4-0.el7.noarch.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-diskprediction-local-14.2.4-0.el7.noarch.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-rook-14.2.4-0.el7.noarch.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-mgr-ssh-14.2.4-0.el7.noarch.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-release-1-1.el7.src.rpm wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/ceph-medic-1.0.4-16.g60cf7e9.el7.src.rpmwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/repomd.xml wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/repomd.xmlwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/a4bf0ee38cd4e64fae2d2c493e5b5eeeab6cf758beb7af4eec0bc4046b595faf-filelists.sqlite.bz2wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/183278bb826f5b8853656a306258643384a1547c497dd8b601ed6af73907bb22-other.sqlite.bz2 wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/52bf459e39c76b2ea2cff2c5340ac1d7b5e17a105270f5f01b454d5a058adbd2-filelists.sqlite.bz2wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/4f3141aec1132a9187ff5d1b4a017685e2f83a761880884d451a288fcedb154e-primary.sqlite.bz2wget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/0c554884aa5600b1311cd8f616aa40d036c1dfc0922e36bcce7fd84e297c5357-other.sqlite.bz2 wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/597468b64cddfc386937869f88c2930c8e5fda3dd54977c052bab068d7438fcb-primary.sqlite.bz2wget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/9a25e39d0c5038776cd0385513e275c6a4fd22c4a420a2097f03bac8d20fc2ab-primary.xml.gzwget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/d65e1c530f969af5458c89e0933adbee9a5192ccab209cbeb8f0887c887aee75-primary.xml.gzwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/b98aa90d4606e473035f3cc0fa28dcb68cac506fea2ac1b62ffe5b4f9dcf6c73-filelists.xml.gzwget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/eb9fad1d84b372046f07f437219700b5ae5872e65eae396f88b7eaf05da89646-other.xml.gzwget -P /var/www/html/ceph/rpm-nautilus/el7/srpms/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/SRPMS/repodata/9334c7e361a2e2ec768c1df34e386f15223abee6227b0fd4c5ff953bab4c3ba2-filelists.xml.gzwget -P /var/www/html/ceph/rpm-nautilus/el7/noarch/repodata/ http://mirrors.163.com/ceph/rpm-nautilus/el7/noarch/repodata/af3ec697842e36c456331b347577225f288124b407d87a173a9e08bf9a482164-other.xml.gz 更新yum源 12createrepo --update /var/www/html/ceph/rpm-nautilus[root@cephnode01 rpm-nautilus]# cp -a repodata el7/x86_64/ 123[root@cephnode01 rpm-nautilus]# systemctl start httpd[root@cephnode01 rpm-nautilus]# netstat -tnlp|grep 80tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 11639/httpd 使用阿里云yum源 安装Ceph集群1. 编辑yum源 将yum源同步到其它节点并提前做好 yum makecache 1234567891011121314151617181920# 三台都操作[root@cephnode01 html]# vim /etc/yum.repos.d/ceph.repo [ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMSgpgcheck=0priority=1 12yum clean allyum makecache 2. 安装ceph-deploy(确认ceph-deploy版本是否为2.0.1) 12345[root@cephnode01 rpm-nautilus]# yum list|grep ceph[root@cephnode01 rpm-nautilus]# yum install ceph-deploy -Y[root@cephnode01 rpm-nautilus]# ceph-deploy --version2.0.1 3. 创建一个my-cluster目录 所有命令在此目录下进行（文件位置和名字可以随意） 生产上按照项目启目录名字也可以，这个是测试用的 12mkdir /my-clustercd /my-cluster 4. 创建一个Ceph集群 123456789[root@cephnode01 my-cluster]# ceph-deploy new cephnode01 cephnode02 cephnode03 ...[ceph_deploy.new][DEBUG ] Monitor cephnode03 at 172.31.228.61[ceph_deploy.new][DEBUG ] Monitor initial members are ['cephnode01', 'cephnode02', 'cephnode03'][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.228.59', '172.31.228.60', '172.31.228.61'] # Monitor地址[ceph_deploy.new][DEBUG ] Creating a random mon key...[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... 5. 安装Ceph软件（每个节点执行） 1234[root@cephnode01 my-cluster]# yum -y install epel-release[root@cephnode01 my-cluster]# yum install -y ceph[root@cephnode01 my-cluster]# ceph -vceph version 14.2.6 (f0aa067ac7a02ee46ea48aa26c6e298b5ea272e9) nautilus (stable) 6. 生成monitor检测集群所使用的的秘钥 1[root@cephnode01 my-cluster]# ceph-deploy mon create-initial 7. 查看基本配置 123456789101112131415161718192021[root@cephnode01 my-cluster]# ls -ltotal 16-rw-r--r-- 1 root root 253 Jan 15 09:38 ceph.conf # 基础配置-rw-r--r-- 1 root root 5096 Jan 15 09:38 ceph-deploy-ceph.log # 记录安装过程-rw------- 1 root root 73 Jan 15 09:38 ceph.mon.keyring-rw------- 1 root root 113 Jan 15 10:25 ceph.bootstrap-mds.keyring # 秘钥 用于交互-rw------- 1 root root 113 Jan 15 10:25 ceph.bootstrap-mgr.keyring-rw------- 1 root root 113 Jan 15 10:25 ceph.bootstrap-osd.keyring-rw------- 1 root root 113 Jan 15 10:25 ceph.bootstrap-rgw.keyring-rw------- 1 root root 151 Jan 15 10:25 ceph.client.admin.keyring[root@cephnode01 my-cluster]# cat ceph.conf [global]fsid = 4ed819cf-39be-4a7c-9216-effcae715c58 # 集群编号 mon_initial_members = cephnode01, cephnode02, cephnode03mon_host = 172.31.228.59,172.31.228.60,172.31.228.61 # 集群成员auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 8. 安装Ceph CLI，方便执行一些管理命令 1[root@cephnode01 my-cluster]# ceph-deploy admin cephnode01 cephnode02 cephnode03 12345678# 其他节点下会生成[root@cephnode02 ceph]# cd /etc/ceph/[root@cephnode02 ceph]# ls -ltotal 12-rw------- 1 root root 151 Jan 15 10:28 ceph.client.admin.keyring # 管理员的秘钥-rw-r--r-- 1 root root 253 Jan 15 10:28 ceph.conf-rw-r--r-- 1 root root 92 Jan 9 03:44 rbdmap-rw------- 1 root root 0 Jan 15 10:25 tmpk0SexV 9. 配置mgr，用于管理集群 12345[root@cephnode01 my-cluster]# ceph-deploy mgr create cephnode01 cephnode02 cephnode03...[cephnode03][INFO ] Running command: systemctl start ceph-mgr@cephnode03 # 启动并设置成开机启动[cephnode03][INFO ] Running command: systemctl enable ceph.target 10. 部署rgw 1234# 生产上多机器安装 rgw 对象存储 用nginx做负载均衡代理[root@cephnode01 my-cluster]# yum install -y ceph-radosgw# 加到集群里[root@cephnode01 my-cluster]# ceph-deploy rgw create cephnode01 11. 部署MDS（CephFS） 1234[root@cephnode01 my-cluster]# ceph-deploy mds create cephnode01 cephnode02 cephnode03 ...[cephnode03][INFO ] Running command: systemctl start ceph-mds@cephnode03 # 开机自启动[cephnode03][INFO ] Running command: systemctl enable ceph.target 12. 阿里云购买云盘 为每台实例购买云盘,按照分区购买后,挂载到实例上 高效云盘,20G,可用区C 12345[root@cephnode01 my-cluster]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part /vdb 253:16 0 20G 0 disk 裸盘 13. 添加osd 12345678910# 该命令可以将 裸盘 vdb 自动格式化成ceph BlueStore 认识的格式 [root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode01...[cephnode01][WARNIN] --&gt; ceph-volume lvm activate successful for osd ID: 0 # osd编号0,之前的版本都要手动执行[cephnode01][WARNIN] --&gt; ceph-volume lvm create successful for: /dev/vdb[cephnode01][INFO ] checking OSD status...[cephnode01][DEBUG ] find the location of an executable[cephnode01][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host cephnode01 is now ready for osd use. 12345[root@cephnode01 my-cluster]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01859 root default -3 0.01859 host cephnode01 0 hdd 0.01859 osd.0 up 1.00000 1.00000 # 加入,标识hdd sata盘 123456789101112131415161718192021# 查看集群状态[root@cephnode01 my-cluster]# ceph -s cluster: id: 4ed819cf-39be-4a7c-9216-effcae715c58 health: HEALTH_WARN # 当前是 WARN状态 pg太少了 Reduced data availability: 8 pgs inactive Degraded data redundancy: 8 pgs undersized OSD count 1 &lt; osd_pool_default_size 3 too few PGs per OSD (8 &lt; min 30) services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 34m) mgr: cephnode01(active, since 29m), standbys: cephnode03, cephnode02 osd: 1 osds: 1 up (since 2m), 1 in (since 2m) data: pools: 1 pools, 8 pgs objects: 0 objects, 0 B usage: 1.0 GiB used, 18 GiB / 19 GiB avail pgs: 100.000% pgs not active 8 undersized+peered 把其他实例上的盘也加入到集群里 1234567891011121314151617181920212223# 别忘记购买啊...[root@cephnode02 ceph]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part /vdb 253:16 0 20G 0 disk [root@cephnode03 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part /vdb 253:16 0 20G 0 disk # 添加osd 加入集群[root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode02[root@cephnode01 my-cluster]# ceph-deploy osd create --data /dev/vdb cephnode03[cephnode02][WARNIN] --&gt; ceph-volume lvm activate successful for osd ID: 1[cephnode02][WARNIN] --&gt; ceph-volume lvm create successful for: /dev/vdb[cephnode02][INFO ] checking OSD status...[cephnode02][DEBUG ] find the location of an executable[cephnode02][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host cephnode02 is now ready for osd use. 12345678910# 三块盘 每台实例加入一块[root@cephnode01 my-cluster]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.05576 root default -3 0.01859 host cephnode01 0 hdd 0.01859 osd.0 up 1.00000 1.00000 -5 0.01859 host cephnode02 1 hdd 0.01859 osd.1 up 1.00000 1.00000 -7 0.01859 host cephnode03 2 hdd 0.01859 osd.2 up 1.00000 1.00000 123456789101112131415161718192021# 在磁盘不平衡或者新加盘的时候 会出现 WARN状态# 等数据填满平衡时 会变成OK [root@cephnode01 my-cluster]# ceph -s cluster: id: 4ed819cf-39be-4a7c-9216-effcae715c58 health: HEALTH_OK services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 42m) mgr: cephnode01(active, since 37m), standbys: cephnode03, cephnode02 osd: 3 osds: 3 up (since 33s), 3 in (since 33s) rgw: 1 daemon active (cephnode01) data: pools: 4 pools, 32 pgs objects: 187 objects, 1.2 KiB usage: 3.0 GiB used, 54 GiB / 57 GiB avail pgs: 32 active+clean io: recovery: 67 B/s, 4 objects/s 12345678910111213141516# 有报错的话 会看到详细报错[root@cephnode01 my-cluster]# ceph health detailHEALTH_OK# 查看实时日志[root@cephnode01 my-cluster]# ceph -w# 查看每块盘使用的空间[root@cephnode01 my-cluster]# ceph osd dfID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 0 hdd 0.01859 1.00000 19 GiB 1.0 GiB 3.6 MiB 0 B 1 GiB 18 GiB 5.28 1.00 32 up 1 hdd 0.01859 1.00000 19 GiB 1.0 GiB 3.6 MiB 0 B 1 GiB 18 GiB 5.28 1.00 32 up 2 hdd 0.01859 1.00000 19 GiB 1.0 GiB 3.6 MiB 0 B 1 GiB 18 GiB 5.28 1.00 32 up TOTAL 57 GiB 3.0 GiB 11 MiB 0 B 3 GiB 54 GiB 5.28 # AVAIL 可用 里面有元数据信息 123456# 查看pool[root@cephnode01 my-cluster]# ceph osd lspools1 .rgw.root2 default.rgw.control3 default.rgw.meta4 default.rgw.log 12# 查看pg,pg是逻辑概念,磁盘规置[root@cephnode01 my-cluster]# ceph pg dump 添加硬盘业务无感知 如果新加盘需要清空再键入 最好直接加入裸盘 ceph.conf1234567891011# 原始配置文件 # 修改配置也在这个文件中# 完成修改后 通过 ceph-deploy 推送[root@cephnode01 my-cluster]# cat /my-cluster/ceph.conf [global]fsid = 4ed819cf-39be-4a7c-9216-effcae715c58mon_initial_members = cephnode01, cephnode02, cephnode03mon_host = 172.31.228.59,172.31.228.60,172.31.228.61auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 书写格式 该配置文件采用init文件语法，#和;为注释，ceph集群在启动的时候会按照顺序加载所有的conf配置文件。 配置文件分为以下几大块配置。 123456789101112131415161718global： 全局配置。osd： osd专用配置，可以使用osd.N，来表示某一个OSD专用配置，N为osd的编号，如0、2、1等。mon： mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、ceph-monitor-1等。使用命令 ceph mon dump 可以获取节点的名称。client： 客户端专用配置。[root@cephnode01 my-cluster]# ceph mon dumpdumped monmap epoch 1epoch 1fsid 4ed819cf-39be-4a7c-9216-effcae715c58last_changed 2020-01-15 10:25:08.611321created 2020-01-15 10:25:08.611321min_mon_release 14 (nautilus)0: [v2:172.31.228.59:3300/0,v1:172.31.228.59:6789/0] mon.cephnode011: [v2:172.31.228.60:3300/0,v1:172.31.228.60:6789/0] mon.cephnode022: [v2:172.31.228.61:3300/0,v1:172.31.228.61:6789/0] mon.cephnode03[root@cephnode01 my-cluster]# ceph mgr dump[root@cephnode01 my-cluster]# ceph osd dump 配置文件可以从多个地方进行顺序加载，如果冲突将使用最新加载的配置，其加载顺序为。 123456# 不需要改动$CEPH_CONF环境变量-c 指定的位置/etc/ceph/ceph.conf~/.ceph/ceph.conf./ceph.conf 配置文件还可以使用一些元变量应用到配置文件，如 12345$cluster： 当前集群名。$type： 当前服务类型。$id： 进程的标识符。$host： 守护进程所在的主机名。$name： 值为$type.$id。 ceph.conf 详细参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 全局设置 默认接口 不用调整太多[global]fsid = xxxxxxxxxxxxxxx # 集群标识ID mon host = 10.0.1.1,10.0.1.2,10.0.1.3 # monitor IP 地址auth cluster required = cephx # 集群认证auth service required = cephx # 服务认证auth client required = cephx # 客户端认证osd pool default size = 3 # 最小副本数 默认是3osd pool default min size = 1 # 当副本只剩下1个的时候,数据就无法使用 PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数public network = 10.0.1.0/24 # 公共网络(monitorIP段) 用户使用的网络 万M网络 cluster network = 10.0.2.0/24 # 集群网络 集群数据同步走的网络 万M网络 两个网段隔离用户网络,走的更快max open files = 131072 # 默认0#如果设置了该选项，Ceph会设置系统的max open fdsmon initial members = node1, node2, node3 # 初始monitor (由创建monitor命令而定)##############################################################[mon]mon data = /var/lib/ceph/mon/ceph-$idmon clock drift allowed = 1 # 默认值0.05 #monitor间的clock driftmon osd min down reporters = 13 # 默认值1 向monitor报告down的最小OSD数 超过该数 数据无法使用mon osd down out interval = 600 # 默认值300 标记一个OSD状态为down和out之前ceph等待的秒数 自动踢出集群##############################################################[osd]osd data = /var/lib/ceph/osd/ceph-$idosd mkfs type = xfs # 格式化系统类型 默认即可osd max write size = 512 # 默认值90 #OSD一次可写入的最大值(MB)osd client message size cap = 2147483648 # 默认值100 #客户端允许在内存中的最大数据(bytes)osd deep scrub stride = 131072 # 默认值524288 健康检查 扫描数据一致 在Deep Scrub时候允许读取的字节数(bytes)osd op threads = 16 # 默认值2 并发文件系统操作数osd disk threads = 4 # 默认值1 #OSD密集型操作例如恢复和Scrubbing时的线程osd map cache size = 1024 # 默认值500 # 保留OSD Map的缓存(MB)osd map cache bl size = 128 # 默认值50 # OSD进程在内存中的OSD Map缓存(MB)osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier" # 默认值rw,noatime,inode64 #Ceph OSD xfs Mount选项osd recovery op priority = 2 # 默认值10 恢复操作优先级，取值1-63，值越高占用资源越高osd recovery max active = 10 # 默认值15 同一时间内活跃的恢复请求数 osd max backfills = 4 # 默认值10 一个OSD允许的最大backfills数osd min pg log entries = 30000 # 默认值3000 修建PGLog是保留的最小PGLog数osd max pg log entries = 100000 # 默认值10000 修建PGLog是保留的最大PGLog数osd mon heartbeat interval = 40 # 默认值30 OSD ping一个monitor的时间间隔（默认30s）ms dispatch throttle bytes = 1048576000 # 默认值 104857600 #等待派遣的最大消息数objecter inflight ops = 819200 # 默认值1024 客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限osd op log threshold = 50 # 默认值5 一次显示多少操作的logosd crush chooseleaf type = 0 # 默认值为1 CRUSH规则用到chooseleaf时的bucket的类型##############################################################[client]rbd cache = true # 默认值 true RBD缓存rbd cache size = 335544320 # 默认值33554432 RBD缓存大小(bytes)rbd cache max dirty = 134217728 # 默认值25165824 缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-throughrbd cache max dirty age = 30 # 默认值1 在被刷新到存储盘前dirty数据存在缓存的时间(seconds)rbd cache writethrough until flush = false # 默认值true 该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写 # 设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。rbd cache max dirty object = 2 # 默认值0 最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分 # 每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能rbd cache target dirty = 235544320 # 默认值16777216 开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty Ceph RBDRBD 介绍 RBD即RADOS Block Device的简称，RBD块存储是最稳定且最常用的存储类型。 RBD块设备类似磁盘可以被挂载。 RBD块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。 如下是对Ceph RBD的理解。 123456789101. RBD: 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；2. resizable: 这个块可大可小；3. data striped: 这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；4. thin-provisioned: 精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间 ；# 总结1. 快设备就好似一块盘,拥有快照备份的功能。2. 这个块可以调整3. 创建出来的块是不占用空间的，用多少使用占用多少 块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。 1231. ceph可以通过内核模块和librbd库提供块设备支持。2. 客户端可以通过内核模块挂在rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；3. 客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。 使用场景： 12341. 云平台（OpenStack做为云的存储后端提供镜像存储）2. K8s容器3. map成块设备直接使用4. ISCIS，安装Ceph客户 RBD 常用命令 命令 功能 rbd create 创建块设备映像 rbd ls 列出 rbd 存储池中的块设备 rbd info 查看块设备信息 rbd diff 可以统计 rbd 使用量 rbd map 映射块设备 rbd showmapped 查看已映射块设备 rbd remove 删除块设备 rbd resize 更改块设备的大小 RBD 配置操作RBD 挂载到操作系统创建rbd使用的pool 12345# 32 pg_num 32 pgp_num 随着容量在增加扩容 生产要做规划# pgp_num 描述pg位置# osd 与 pg 算法 ? ceph osd pool create rbd 32 32# 创建标记 rbd 123456789[root@cephnode01 my-cluster]# ceph osd pool create rbd 32 32pool 'rbd' created[root@cephnode01 my-cluster]# ceph osd pool ls detail...pool 5 'rbd' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 29 flags hashpspool stripe_width 0[root@cephnode01 my-cluster]# ceph osd pool application enable rbd rbd enabled application 'rbd' on pool 'rbd' 创建一个块设备 1[root@cephnode01 my-cluster]# rbd create --size 10240 image01 查看快设备 123456789101112131415161718192021222324[root@cephnode01 my-cluster]# rbd lsimage01[root@cephnode01 my-cluster]# rbd info image01rbd image 'image01': size 10 GiB in 2560 objects # 大小 放多少个对象 order 22 (4 MiB objects) snapshot_count: 0 id: 11797bd74618 block_name_prefix: rbd_data.11797bd74618 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed Jan 15 15:20:46 2020 access_timestamp: Wed Jan 15 15:20:46 2020 modify_timestamp: Wed Jan 15 15:20:46 2020[root@cephnode01 my-cluster]# rados -p rbd ls --all rbd_id.image01 rbd_directory rbd_info rbd_object_map.11797bd74618 rbd_header.11797bd74618 禁用当前系统内核不支持的feature 1[root@cephnode01 my-cluster]# rbd feature disable image01 exclusive-lock, object-map, fast-diff, deep-flatten 将块设备映射到系统内核 12345[root@cephnode01 my-cluster]# rbd map image01 /dev/rbd0[root@cephnode01 my-cluster]# ls -l /dev/rbd0 brw-rw---- 1 root disk 251, 0 Jan 15 15:26 /dev/rbd0 格式化块设备镜像 1[root@cephnode01 my-cluster]# mkfs.xfs /dev/rbd0 mount到本地 12345678910111213141516[root@cephnode01 my-cluster]# mount /dev/rbd0 /mnt[root@cephnode01 my-cluster]# umount /mnt[root@cephnode01 my-cluster]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 2.3G 36G 6% //dev/rbd0 10G 33M 10G 1% /mnt...# 在/mnt下写数据相当于存放到 这个rbd里了[root@cephnode01 my-cluster]# cd /mnt/[root@cephnode01 mnt]# touch a[root@cephnode01 mnt]# ls -l-rw-r--r-- 1 root root 0 Jan 15 15:30 a 取消挂载 12345678910111213141516171819202122232425262728[root@cephnode01 mnt]# rbd showmappedid pool namespace image snap device 0 rbd image01 - /dev/rbd0 [root@cephnode01 /]# umount /mnt# 取消块设备和内核映射[root@cephnode01 /]# rbd unmap image01 [root@cephnode01 /]# rbd showmapped[root@cephnode01 /]# rbd lsimage01[root@cephnode01 /]# rbd info image01rbd image 'image01': size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count: 0 id: 11797bd74618 block_name_prefix: rbd_data.11797bd74618 format: 2 features: layering op_features: flags: create_timestamp: Wed Jan 15 15:20:46 2020 access_timestamp: Wed Jan 15 15:20:46 2020 modify_timestamp: Wed Jan 15 15:20:46 2020# 虽然取消了挂载,但是里面的数据还在 删除RBD块设备 1234[root@cephnode01 /]# rbd rm image01Removing image: 100% complete...done.[root@cephnode01 /]# rbd ls 其他机器需要创建 12ceph common k8s会讲到创建快需要调用rbd命令 ， 需要再机器上安装ceph common 才可以创建块 RBD 快照配置123[root@cephnode01 my-cluster]# rbd create --size 10240 image02[root@cephnode01 my-cluster]# rbd lsimage02 创建快照 1[root@cephnode01 my-cluster]# rbd snap create image02@image02_snap01 列出创建的快照 12345678[root@cephnode01 my-cluster]# rbd snap list image02SNAPID NAME SIZE PROTECTED TIMESTAMP 4 image02_snap01 10 GiB Wed Jan 15 15:58:41 2020 [root@cephnode01 my-cluster]# rbd ls -lNAME SIZE PARENT FMT PROT LOCK image02 10 GiB 2 image02@image02_snap01 10 GiB 2 # 快照 用于很重要的数据 可以用于回滚 查看快照详细信息 123456789101112131415161718192021222324252627282930[root@cephnode01 my-cluster]# rbd info image02@image02_snap01rbd image 'image02': size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count: 1 id: 11b2d62f36b7 block_name_prefix: rbd_data.11b2d62f36b7 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed Jan 15 15:57:55 2020 access_timestamp: Wed Jan 15 15:57:55 2020 modify_timestamp: Wed Jan 15 15:57:55 2020 protected: False[root@cephnode01 my-cluster]# rbd info image02rbd image 'image02': size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count: 1 id: 11b2d62f36b7 block_name_prefix: rbd_data.11b2d62f36b7 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed Jan 15 15:57:55 2020 access_timestamp: Wed Jan 15 15:57:55 2020 modify_timestamp: Wed Jan 15 15:57:55 2020 克隆快照（快照必须处于被保护状态才能被克隆） 1234567891011# 保护状态就无法操作了[root@cephnode01 my-cluster]# rbd snap protect image02@image02_snap01# 创建一个新pool kube[root@cephnode01 my-cluster]# ceph osd pool create kube 16 16pool 'kube' created[root@cephnode01 my-cluster]# rbd clone rbd/image02@image02_snap01 kube/image02_clone01[root@cephnode01 my-cluster]# rbd ls -p kubeimage02_clone01 查看快照的children 123# 查看快照的子快照[root@cephnode01 my-cluster]# rbd children image02kube/image02_clone01 去掉快照的parent 12345# 取消克隆关系[root@cephnode01 my-cluster]# rbd flatten kube/image02_clone01Image flatten: 100% complete...done.[root@cephnode01 my-cluster]# rbd children image02# 这就成为了 独立的快照 恢复快照 123# 回滚 一开始有5个文件 一直写 想回滚[root@cephnode01 my-cluster]# rbd snap rollback image02@image02_snap01Rolling back to snapshot: 100% complete...done. 删除快照 123456# 取消保护[root@cephnode01 my-cluster]# rbd snap unprotect image02@image02_snap01[root@cephnode01 my-cluster]# rbd snap remove image02@image02_snap01Removing snap: 100% complete...done.[root@cephnode01 my-cluster]# rbd snap list image02 RBD 镜像导出导入导出RBD镜像 12345678910[root@cephnode01 my-cluster]# rbd export image02 /tmp/image02Exporting image: 100% complete...done.[root@cephnode01 tmp]# ls -lh /tmp/image02 -rw-r--r-- 1 root root 10G Jan 15 16:11 /tmp/image02[root@cephnode01 tmp]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 2.3G 36G 6% /... 导入RBD镜像 12345678910[root@cephnode01 tmp]# rbd lsimage02[root@cephnode01 tmp]# rbd remove image02Removing image: 100% complete...done.[root@cephnode01 tmp]# rbd ls[root@cephnode01 tmp]# rbd import /tmp/image02 rbd/image02 --image-format 2 Importing image: 100% complete...done.[root@cephnode01 tmp]# rbd lsimage02 RBD 扩容12345678[root@cephnode01 tmp]# rbd info image02|grep size size 10 GiB in 2560 objects[root@cephnode01 tmp]# rbd --image image02 resize --size 15240Resizing image: 100% complete...done.[root@cephnode01 tmp]# rbd info image02|grep size size 15 GiB in 3810 objects 总结123451. 把rbd映射到文件系统挂载2. 再通过nfs /etc/exports 暴露,映射共享到其他机器上3. 快照 备份回滚，快照占用空间,可以定期拍摄快照，对重要的块进行快照,如果太大了 就导出备份4. 克隆 到另外的集群 5. 数据分成3份 分到3个pg上 再分到3个osd上 Ceph 文件系统 CephFS Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server - MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。 对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。 CephFS 架构 底层是核心集群所依赖的, 包括: 12341. OSDs (ceph-osd): CephFS 的数据和元数据就存储在 OSDs 上2. MDS (ceph-mds): Metadata Servers, 管理着 CephFS 的元数据3. Mons (ceph-mon): Monitors 管理着集群 Map 的主副本Ceph 存储集群的协议层是 Ceph 原生的 librados 库, 与核心集群交互. 数据访问流程 1231. client 访问MDS 获取数据的元数据(文件名,大小等信息) 2. client 去 RADOS(OSD) 获取数据3. RADOS(OSD) 和 MDS 通过 Journal Metadata 记录文件写入的操作，数据最终都存入RADOS中。 配置 CephFS MDS1234567891011121314151617# 当前集群状态[root@cephnode01 my-cluster]# ceph -s cluster: id: 4ed819cf-39be-4a7c-9216-effcae715c58 health: HEALTH_OK services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 15m) mgr: cephnode02(active, since 15m), standbys: cephnode03, cephnode01 osd: 3 osds: 3 up (since 15m), 3 in (since 21h) rgw: 1 daemon active (cephnode01) data: pools: 6 pools, 80 pgs objects: 196 objects, 1.9 KiB usage: 3.0 GiB used, 54 GiB / 57 GiB avail pgs: 80 active+clean 123456789101112131415161. 要使用 CephFS， 至少就需要一个 metadata server 进程。2. 可以手动创建一个 MDS， 也可以使用 ceph-deploy 或者 ceph-ansible 来部署 MDS。 3. 登录到ceph-deploy工作目录执行[root@cephnode01 my-cluster]# cd /my-cluster/[root@cephnode01 my-cluster]# ceph-deploy mds create cephnode01 cephnode02 cephnode03...[cephnode03][INFO ] Running command: systemctl enable ceph-mds@cephnode03[cephnode03][INFO ] Running command: systemctl start ceph-mds@cephnode03[cephnode03][INFO ] Running command: systemctl enable ceph.target[root@cephnode01 my-cluster]# ps -ef|grep mdsceph 957 1 0 08:34 ? 00:00:00 /usr/bin/ceph-mds -f --cluster ceph --id cephnode01 --setuser ceph --setgroup cephroot 2640 2102 0 08:56 pts/0 00:00:00 grep --color=auto mds[root@cephnode01 my-cluster]# tail -200 /var/log/ceph/ceph-mds.cephnode01.log 部署 Ceph 文件系统 部署一个 CephFS, 步骤如下: 123451. 在一个 Mon 节点上创建 Ceph 文件系统.2. 若使用 CephX 认证,需要创建一个访问 CephFS 的客户端3. 挂载 CephFS 到一个专用的节点. - 以 kernel client 形式挂载 CephFS - 以 FUSE client 形式挂载 CephFS 创建一个 Ceph 文件系统1. CephFS 需要两个 Pools 123# cephfs-data 和 cephfs-metadata, 分别存储文件数据和文件元数据 # ceph osd pool create cephfs-data 256 256# ceph osd pool create cephfs-metadata 64 64 123456789101112131415[root@cephnode01 my-cluster]# ceph osd pool create cephfs-data 16 16pool 'cephfs-data' created[root@cephnode01 my-cluster]# ceph osd pool create cephfs-metadata 16 16 pool 'cephfs-metadata' created[root@cephnode01 my-cluster]# ceph osd lspools1 .rgw.root2 default.rgw.control3 default.rgw.meta4 default.rgw.log5 rbd6 kube7 cephfs-data8 cephfs-metadata 123# 注意1. 一般 metadata pool 可以从相对较少的 PGs 启动, 2. 之后可以根据需要增加 PGs. 因为 metadata pool 存储着 CephFS 文件的元数据, 为了保证安全, 最好有较多的副本数. 为了能有较低的延迟, 可以考虑将 metadata 存储在 SSDs 上. 2. 创建一个 CephFS, 名字为 cephfs 12[root@cephnode01 my-cluster]# ceph fs new cephfs cephfs-metadata cephfs-datanew fs with metadata pool 8 and data pool 7 3. 验证至少有一个 MDS 已经进入 Active 状态 123[root@cephnode01 my-cluster]# ceph fs status cephfs# 现在源数据的进程跑在 cephnode03上 4. 在 Monitor 上, 创建一个用户，用于访问CephFs 123[root@cephnode01 my-cluster]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs-data, allow rw pool=cephfs-metadata'[client.cephfs] key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg== 5. 验证key是否生效 1234567[root@cephnode01 my-cluster]# ceph auth get client.cephfsexported keyring for client.cephfs[client.cephfs] key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg== caps mds = "allow rw" # 读写 caps mon = "allow r" # 读 caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata" # 读写 6. 检查CephFs和mds状态 1234567891011[root@cephnode01 my-cluster]# ceph -s[root@cephnode01 my-cluster]# ceph mds statcephfs:1 &#123;0=cephnode03=up:active&#125; 2 up:standby[root@cephnode01 my-cluster]# ceph fs lsname: cephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]# 文件系统名称 cephfs [root@cephnode01 my-cluster]# ceph fs status# 以上正常说明 cephfs 文件系统创建成功 挂载 CephFS 文件系统以 kernel client 形式挂载 CephFS12345# 找一台不在集群中的服务器测试挂载 # 联系系统内核挂载文件系统，和挂载文件一样# 缺点: 读写速度有限1. 创建挂载目录 cephfs[root@cephnode04 ~]# mkdir /cephfs 123# secret 就是刚生成的秘钥 key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg==2. 挂载目录 mount -t ceph 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789:/ /cephfs/ -o name=cephfs,secret=AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg== 123# 开机自动挂载3. 自动挂载echo "mon1:6789,mon2:6789,mon3:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,_netdev,noatime 0 0" | sudo tee -a /etc/fstab 124. 验证是否挂载成功stat -f /cephfs 以 FUSE client 形式挂载 CephFS12345678910111213141516171819202122232425261. 安装ceph-common[root@cephnode04 cephfs]# vim /etc/yum.repos.d/ceph.repo [ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMSgpgcheck=0priority=1[root@cephnode04 cephfs]# yum install -y ceph-common# 安装完成后 客户端就会有ceph和rbd命令[root@cephnode04 cephfs]# ceph[root@cephnode04 cephfs]# rbd 1232. 安装ceph-fuse # 客户端工具，用ceph的方式挂载yum install -y ceph-fuse 12345673. 将集群的ceph.conf拷贝到客户端[root@cephnode04 ceph]# scp root@172.31.228.59:/etc/ceph/ceph.conf /etc/ceph/[root@cephnode04 ceph]# scp root@172.31.228.59:/etc/ceph/ceph.client.admin.keyring /etc/ceph/[root@cephnode04 ceph]# chmod 644 /etc/ceph/ceph.conf[root@cephnode04 ceph]# lsceph.conf rbdmap 12345678910111213141516171819202122232425264. 使用 ceph-fuse 挂载 CephFS # 挂载之前把刚才的挂载umount [root@cephnode04 ~]# umount /cephfs/# 先去获取一下认证信息[root@cephnode01 my-cluster]# ceph auth get client.cephfsexported keyring for client.cephfs[client.cephfs] key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg== caps mds = "allow rw" caps mon = "allow r" caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"# 写入到keyring 文件中 [root@cephnode04 ~]# vim /etc/ceph/ceph.client.cephfs.keyring[client.cephfs] key = AQCluh9eDsknLBAAjxABEQpF8vJeUt8Buk22Dg== caps mds = "allow rw" caps mon = "allow r" caps osd = "allow rw pool=cephfs-data, allow rw pool=cephfs-metadata"# 挂载# ceph-fuse 自己的文件系统格式 [root@cephnode04 ~]# ceph-fuse --keyring /etc/ceph/ceph.client.cephfs.keyring --name client.cephfs -m 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789 /cephfs/ 12345678# 验证 CephFS 已经成功挂载[root@cephnode04 ~]# stat -f /cephfs File: "/cephfs" ID: 0 Namelen: 255 Type: fuseblkBlock size: 4194304 Fundamental block size: 4194304Blocks: Total: 4362 Free: 4362 Available: 4362Inodes: Total: 1 Free: 0 1234# 自动挂载 echo "none /cephfs fuse.ceph ceph.id=cephfs[,ceph.conf=/etc/ceph/ceph.conf],_netdev,defaults 0 0"| sudo tee -a /etc/fstab或echo "id=cephfs,conf=/etc/ceph/ceph.conf /mnt/ceph2 fuse.ceph _netdev,defaults 0 0"| sudo tee -a /etc/fstab 12# 卸载 fusermount -u /cephfs 存储文件测试并查看123456789101112131415161718192021222324252627282930313233# 在目录下写点东西[root@cephnode04 ~]# touch test# 看看存在pool里是什么样子[root@cephnode01 my-cluster]# rados lspools# 创建的是空文件 没有数据[root@cephnode01 my-cluster]# rados -p cephfs-data ls --all# 源文件数据[root@cephnode01 my-cluster]# rados -p cephfs-metadata ls --all 601.00000000 602.00000000 600.00000000 603.00000000 1.00000000.inode 200.00000000 200.00000001 606.00000000 607.00000000 mds0_openfiles.0 608.00000000 604.00000000 500.00000000 mds_snaptable 605.00000000 mds0_inotable 100.00000000 mds0_sessionmap 609.00000000 400.00000000 100.00000000.inode 1.00000000 挂载小总结123# 推荐ceph-fuse 效率更高# 实际应用中 其实也没什么区别 只不过ceph-fuse 是源生的可能更好# 生产上如果追求少安装软件的话 用内核模式也可以 MDS主备与主主切换配置主主模式 当cephfs的性能出现问题时，就应该配置多个活动的MDS。 通常是多个客户机应用程序并行的执行大量元数据操作，并且它们分别有自己单独的工作目录。这种情况下很适合使用多主MDS模式。 配置MDS多主模式 每个cephfs文件系统都有一个max_mds设置，可以理解为它将控制创建多少个主MDS。注意只有当实际的MDS个数大于或等于max_mds设置的值时，mdx_mds设置才会生效。 例如，如果只有一个MDS守护进程在运行，并且max_mds被设置为两个，则不会创建第二个主MDS。 12345# 设置完成后 存在两个主[root@cephnode01 my-cluster]# ceph fs set cephfs max_mds 2 # 还原单主MDS[root@cephnode01 my-cluster]# ceph fs set cephfs max_mds 1 配置备用 MDS 即使有多个活动的MDS，如果其中一个MDS出现故障，仍然需要备用守护进程来接管。因此，对于高可用性系统，实际配置max_mds时，最好比系统中MDS的总数少一个。 但如果你确信你的MDS不会出现故障，可以通过以下设置来通知ceph不需要备用MDS，否则会出现insufficient standby daemons available告警信息： 12# 设置不需要备用MDSceph fs set &lt;fsname&gt; standby_count_wanted 0 Ceph Dashboard Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能 通过Dashboard可以获取Ceph集群的各种基本状态信息。 mimic版 (nautilus版) dashboard 安装。如果是 (nautilus版) 需要安装 ceph-mgr-dashboard 配置 Ceph Dashboard123456789101. 在每个mgr节点安装# yum install ceph-mgr-dashboard 2. 开启mgr功能# ceph mgr module enable dashboard3. 生成并安装自签名的证书# ceph dashboard create-self-signed-cert 4. 创建一个dashboard登录用户名密码# ceph dashboard ac-user-create guest 1q2w3e4r administrator 5. 查看服务访问方式# ceph mgr services 12345678910111213141516171819202122232425262728293031323334# 出现问题 ImportError: cannot import name UnrewindableBodyError[root@cephnode01 my-cluster]# tail -200 /var/log/ceph/ceph-mgr.cephnode01.log # 需要重新安装 urllib3pip uninstall urllib3yum install python-urllib3 -yyum remove ceph-mgr-dashboardyum install -y ceph-mgr-dashboard# 在所有的mgr上都要装这个包 ,有1个的话只装1个[root@cephnode01 my-cluster]# ceph mgr module enable dashboard[root@cephnode01 my-cluster]# ceph mgr module ls &#123; "enabled_modules": [ "dashboard", "iostat", "restful" ],...[root@cephnode01 my-cluster]# ceph dashboard create-self-signed-cert Self-signed certificate created[root@cephnode01 my-cluster]# ceph dashboard ac-user-create guest 1q2w3e4r administrator &#123;"username": "guest", "lastUpdate": 1579144411, "name": null, "roles": ["administrator"], "password": "$2b$12$ZwdxYdVcpKI7FCz2IZyKs.qR.fJrkGnxlNttZN8eoRbNHcqGdRZry", "email": null&#125;[root@cephnode01 my-cluster]# ceph mgr services&#123; "dashboard": "https://cephnode02:8443/"&#125;# web访问https://47.240.15.208:8443 修改默认配置命令12345指定集群dashboard的访问端口# ceph config-key set mgr/dashboard/server_port 7000指定集群 dashboard的访问IP# ceph config-key set mgr/dashboard/server_addr $IP 开启Object Gateway管理功能1234567891011# 这块可能有问题 需要用到对象存储的时候 再网上查查# 可以参考官方给出的开启方式开启1、创建rgw用户# radosgw-admin user create --uid=user01 --display-name=user012、提供Dashboard证书# ceph dashboard set-rgw-api-access-key $access_key# ceph dashboard set-rgw-api-secret-key $secret_key3、配置rgw主机名和端口# ceph dashboard set-rgw-api-host 10.151.30.1254、刷新web页面 Promethus+Grafana 监控 Ceph安装 grafana123456789101112131415161718192021222324252627281、配置yum源文件# vim /etc/yum.repos.d/grafana.repo[grafana]name=grafanabaseurl=https://packages.grafana.com/oss/rpmrepo_gpgcheck=1enabled=1gpgcheck=1gpgkey=https://packages.grafana.com/gpg.keysslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crt2.通过yum命令安装grafana# yum -y install grafana3.启动grafana并设为开机自启[root@cephnode04 ~]# yum list | grep grafanaceph-grafana-dashboards.noarch 2:14.2.6-0.el7 @ceph-noarchgrafana.x86_64 6.5.3-1 @grafana pcp-webapp-grafana.noarch 4.3.2-3.el7_7 updates # systemctl start grafana-server.service # systemctl enable grafana-server.service[root@cephnode04 ~]# netstat -antlp|grep grafana# web访问:http://47.240.10.142:3000/login admin/admin 安装 promethus1234567891011121314151617181920212223242526272829303132333435361、下载安装包，下载地址https://prometheus.io/download/wget https://github.com/prometheus/prometheus/releases/download/v2.14.0/prometheus-2.14.0.linux-amd64.tar.gz2、解压压缩包# tar fvxz prometheus-2.14.0.linux-amd64.tar.gz3、将解压后的目录改名# mv prometheus-2.14.0.linux-amd64 /opt/prometheus4、查看promethus版本# ./prometheus --version5、配置系统服务启动# vim /etc/systemd/system/prometheus.service[Unit]Description=Prometheus Monitoring SystemDocumentation=Prometheus Monitoring System[Service]ExecStart=/opt/prometheus/prometheus \ --config.file /opt/prometheus/prometheus.yml \ --web.listen-address=:9090[Install]WantedBy=multi-user.target6、加载系统服务# systemctl daemon-reload7、启动服务和添加开机自启动# systemctl start prometheus# systemctl enable prometheus# web访问http://47.240.10.142:9090/graph ceph mgr prometheus插件配置123456789101112# ceph mgr module enable prometheus# netstat -nltp | grep mgr 检查端口# curl 127.0.0.1:9283/metrics 测试返回值[root@cephnode01 my-cluster]# ceph mgr services&#123; "dashboard": "https://cephnode02:8443/", "prometheus": "http://cephnode02:9283/"&#125;# web访问 采集数据http://47.240.15.208:9283/metrics 配置 promethus1234567891011121314151617181920[root@cephnode04 prometheus]# vim /opt/prometheus/prometheus.yml ...scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'ceph_cluster' honor_labels: true scrape_interval: 5s static_configs: - targets: ['172.31.228.60:9283'] labels: instance: ceph... 123# 重启promethus服务[root@cephnode04 prometheus]# systemctl restart prometheus[root@cephnode04 prometheus]# ps -ef|grep prometheus 12# 检查prometheus服务器中是否添加成功http://47.240.10.142:9090/targets 配置 grafana1234561. 浏览器登录 grafana 管理界面 2. 添加 data sources，点击 configuration --&gt; data sources 3. 添加 dashboard，点击HOME--》find dashboard on grafana.com 4. 搜索ceph的dashboard 5. 点击HOME--&gt;Import dashboard, 选择合适的dashboard,模板编号: 28426. avg(ceph_osd_apply_latency_ms) 关注延迟,如果到秒级,集群就变慢了,正常都是毫秒 K8S 接入 Ceph 存储PV、PVC 概述 管理存储是管理计算的一个明显问题。PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。 于是引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim 123456789101112131415161. PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象包含存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 2. PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。3. 虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 管理员需要能够提供多种不同于PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。4. StorageClass为集群提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的,这个概念有时在其他存储系统中称为“配置文件” POD 动态供给 动态供给主要是能够自动帮你创建pv，需要多大的空间就创建多大的pv。 k8s帮助创建pv，创建pvc就直接api调用存储类来寻找pv。 如果是存储静态供给的话，会需要我们手动去创建pv，如果没有足够的资源，找不到合适的pv，那么pod就会处于pending等待的状态。 而动态供给主要的一个实现就是StorageClass存储对象，其实它就是声明你使用哪个存储，然后帮你去连接，再帮你去自动创建pv。 小总结 pv去存储里申请的一块空间,映射到pv上,作为物理卷存活在容器里 POD使用pvc联系pv拿到存储 动态供给,省却手动创建pv,而是调用StorageClass帮我们去创建 POD 使用 RBD 做为持久数据卷安装与配置 RBD支持ReadWriteOnce，ReadOnlyMany两种模式 1234# 访问模式包括：ReadWriteOnce 该volume只能被单个节点以读写的方式映射ReadOnlyMany 该volume可以被多个节点以只读方式映射ReadWriteMany 该volume只能被多个节点以读写的方式映射 配置 rbd-provisioner12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# 配置rbd-provisioner cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: rbd-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] - apiGroups: [""] resources: ["services"] resourceNames: ["kube-dns"] verbs: ["list", "get"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionersubjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: rbd-provisioner namespace: kube-systemrules:- apiGroups: [""] resources: ["secrets"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbd-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisionersubjects:- kind: ServiceAccount name: rbd-provisioner namespace: kube-system---apiVersion: apps/v1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: selector: matchLabels: app: rbd-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: "quay.io/external_storage/rbd-provisioner:v2.0.0-k8s1.11" env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisionerEOF 1234[root@k8s-master1 ceph]# kubectl apply -f external-storage-rbd-provisioner.yaml[root@k8s-master1 ceph]# kubectl get pod --all-namespaceskube-system rbd-provisioner-578dc7dd99-5w7lr 1/1 Running 0 70s... 配置 storageclass 创建pod时，kubelet需要使用rbd命令去检测和挂载pv对应的ceph image，所以要在所有的worker节点安装ceph客户端ceph-common。 将ceph的ceph.client.admin.keyring和ceph.conf文件拷贝到所有工作节点的/etc/ceph目录下 12345678910111213141516171819[root@k8s-master1 ceph]# vim /etc/yum.repos.d/ceph.repo [ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMSgpgcheck=0priority=1 123456yum -y install ceph-commonmkdir -p /etc/cephcd /etc/cephscp root@172.31.228.59:/etc/ceph/ceph.client.admin.keyring .scp root@172.31.228.59:/etc/ceph/ceph.conf .ceph -s 创建 osd pool1234567891011[root@cephnode01 my-cluster]# ceph osd pool create kube 16 16pool 'kube' already exists[root@cephnode01 my-cluster]# ceph osd lspools1 .rgw.root2 default.rgw.control3 default.rgw.meta4 default.rgw.log5 rbd6 kube7 cephfs-data8 cephfs-metadata 创建k8s访问ceph的用户1ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring 查看 key12345[root@cephnode01 my-cluster]# ceph auth get-key client.adminAQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw==[root@cephnode01 my-cluster]# ceph auth get-key client.kubeAQCQGyBeHjByOhAAj8oEic0LQOOX3MT6O5FVSg== 创建 admin secret123kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw== \--namespace=kube-system 在 default 命名空间创建 pvc 用于访问ceph的 secret123kubectl create secret generic ceph-user-secret --type="kubernetes.io/rbd" \--from-literal=key=AQCQGyBeHjByOhAAj8oEic0LQOOX3MT6O5FVSg== \--namespace=default 1[root@k8s-master1 ceph]# kubectl get secret --all-namespaces 配置 StorageClass 源码中，monitors需要k8s dns解析，我这里使用外部ceph，肯定没有相关解析。 所以手动添加解析。 而且storageclass配置默认不支持直接修改（只能删除再添加），维护解析比维护storageclass配置要好些。 1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master1 ceph]# kubectl create ns ceph[root@k8s-master1 ceph]# vim rbd-monitor-dns.yamlkind: ServiceapiVersion: v1metadata: name: ceph-mon-1 namespace: cephspec: type: ExternalName externalName: 172.31.228.59.xip.io---kind: ServiceapiVersion: v1metadata: name: ceph-mon-2 namespace: cephspec: type: ExternalName externalName: 172.31.228.60.xip.io---kind: ServiceapiVersion: v1metadata: name: ceph-mon-3 namespace: cephspec: type: ExternalName externalName: 172.31.228.61.xip.io[root@k8s-master1 ceph]# kubectl create -f rbd-monitor-dns.yaml[root@k8s-master1 ceph]# kubectl get svc -n cephNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEceph-mon-1 ExternalName &lt;none&gt; 172.31.228.59.xip.io &lt;none&gt; 9sceph-mon-2 ExternalName &lt;none&gt; 172.31.228.60.xip.io &lt;none&gt; 9sceph-mon-3 ExternalName &lt;none&gt; 172.31.228.61.xip.io &lt;none&gt; 9s 123456789101112131415161718[root@k8s-master1 ceph]# vim storageclass-ceph-rdb.yaml kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-ceph-rdbprovisioner: ceph.com/rbdparameters: monitors: ceph-mon-1.ceph.svc.cluster.local.:6789,ceph-mon-2.ceph.svc.cluster.local.:6789,ceph-mon-3.ceph.svc.cluster.local.:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-user-secret fsType: ext4 imageFormat: "2" imageFeatures: "layering" 123456[root@k8s-master1 ceph]# kubectl apply -f storageclass-ceph-rdb.yamlstorageclass.storage.k8s.io/dynamic-ceph-rdb created[root@k8s-master1 ceph]# kubectl get scNAME PROVISIONER AGEdynamic-ceph-rdb ceph.com/rbd 13s 测试使用创建 pvc 测试12345678910111213cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: ceph-rdb-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-ceph-rdb resources: requests: storage: 5GiEOF 1[root@k8s-master1 ceph]# kubectl apply -f ceph-rdb-pvc-test.yaml 123456789[root@k8s-master1 ceph]# kubectl describe pvc ceph-rdb-claim[root@k8s-master1 ceph]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEceph-rdb-claim Bound pvc-436de81b-1b3f-4fd4-ad1b-7663409d34a1 5Gi RWO dynamic-ceph-rdb 10s[root@k8s-master1 ceph]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-436de81b-1b3f-4fd4-ad1b-7663409d34a1 5Gi RWO Delete Bound default/ceph-rdb-claim dynamic-ceph-rdb 25s 创建 nginx pod 挂载测试12345678910111213141516171819202122cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: ceph-rdb mountPath: /usr/share/nginx/html volumes: - name: ceph-rdb persistentVolumeClaim: claimName: ceph-rdb-claimEOF 123456[root@k8s-master1 ceph]# kubectl apply -f nginx-pod.yamlpod/nginx-pod1 created[root@k8s-master1 ceph]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-pod1 1/1 Running 0 37s 10.244.2.3 k8s-master1 &lt;none&gt; &lt;none&gt; 123456# 修改文件内容[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo this is from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html'# 访问测试 [root@k8s-master1 ceph]# curl http://10.244.2.3this is from Ceph RBD!!! 123# 清理kubectl delete -f nginx-pod.yamlkubectl delete -f ceph-rdb-pvc-test.yaml POD 使用 CephFS 做为持久数据卷 CephFS方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany 1234567访问模式包括：1. ReadWriteOnce 该volume只能被单个节点以读写的方式映射2. ReadOnlyMany 该volume可以被多个节点以只读方式映射3. ReadWriteMany 该volume只能被多个节点以读写的方式映射# 注意：即使volume支持很多种访问模式，但它同时只能使用一种方式来映射。# 比如，GCEPersistentDisk可以被单个节点映射为ReadWriteOnce，或者多个节点映射为ReadOnlyMany，但不能同时使用这两种方式来映射。 Pod挂载cephfs有时候会用fuse挂载 如果宿主机没有这个工具会导致挂载失败 pod无法启动 1yum install -y ceph-fuse Ceph 端创建 CephFS pool 在ceph节点,CephFS需要使用两个Pool来分别存储数据和元数据 1234567891011121314151617[root@cephnode01 my-cluster]# ceph osd pool create fs_data 16pool 'fs_data' created[root@cephnode01 my-cluster]# ceph osd pool create fs_metadata 16pool 'fs_metadata' created[root@cephnode01 my-cluster]# ceph osd lspools1 .rgw.root2 default.rgw.control3 default.rgw.meta4 default.rgw.log5 rbd6 kube7 cephfs-data8 cephfs-metadata9 fs_data10 fs_metadata 123456789# 创建一个CephFS # 删除之前的[root@cephnode01 my-cluster]# ceph fs volume rm cephfs --yes-i-really-mean-it[root@cephnode01 my-cluster]# ceph fs new cephfs fs_metadata fs_datanew fs with metadata pool 10 and data pool 9[root@cephnode01 my-cluster]# ceph fs lsname: cephfs, metadata pool: fs_metadata, data pools: [fs_data ] 部署 cephfs-provisioner123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121# 使用社区提供的cephfs-provisionercat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: Namespacemetadata: name: cephfs labels: name: cephfs ---apiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: cephfs ---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] ---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisioner namespace: cephfsrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["services"] resourceNames: ["kube-dns","coredns"] verbs: ["list", "get"] - apiGroups: [""] resources: ["secrets"] verbs: ["get", "create", "delete"] - apiGroups: ["policy"] resourceNames: ["cephfs-provisioner"] resources: ["podsecuritypolicies"] verbs: ["use"] ---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: cephfsroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner ---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfsroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io ---apiVersion: apps/v1kind: Deploymentmetadata: name: cephfs-provisioner namespace: cephfsspec: selector: matchLabels: app: cephfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: "quay.io/external_storage/cephfs-provisioner:latest" env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - "/usr/local/bin/cephfs-provisioner" args: - "-id=cephfs-provisioner-1" - "-disable-ceph-namespace-isolation=true" serviceAccount: cephfs-provisionerEOF 12345[root@k8s-master1 ceph]# kubectl apply -f external-storage-cephfs-provisioner.yaml [root@k8s-master1 ceph]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEcephfs cephfs-provisioner-847468fc-95rkg 1/1 Running 0 62s 配置 StorageClass123456789101112131415# 查看key 在ceph的mon或者admin节点# 重新配置一下[root@cephnode01 my-cluster]# ceph auth get-key client.adminAQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw==[root@k8s-master1 ceph]# kubectl delete secret ceph-secret -n kube-systemkubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=AQAReB5ebROqLBAAdMpq/uEqUBSxZeUVxxTAvw== \--namespace=kube-system[root@k8s-master1 ceph]# kubectl get secret -n kube-systemNAME TYPE DATA AGEceph-secret kubernetes.io/rbd 1 3s 123456789101112131415# 配置 StorageClasscat &gt;storageclass-cephfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-cephfsprovisioner: ceph.com/cephfsparameters: monitors: 172.31.228.59:6789,172.31.228.60:6789,172.31.228.61:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: "kube-system" claimRoot: /volumes/kubernetesEOF 1234567[root@k8s-master1 ceph]# kubectl apply -f storageclass-cephfs.yamlstorageclass.storage.k8s.io/dynamic-cephfs created[root@k8s-master1 ceph]# kubectl get scNAME PROVISIONER AGEdynamic-ceph-rdb ceph.com/rbd 52mdynamic-cephfs ceph.com/cephfs 2s 测试使用创建pvc测试12345678910111213cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-cephfs resources: requests: storage: 2GiEOF 12345678910[root@k8s-master1 ceph]# kubectl apply -f cephfs-pvc-test.yamlpersistentvolumeclaim/cephfs-claim created[root@k8s-master1 ceph]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEcephfs-claim Bound pvc-ac8186ab-ff09-4134-9357-044ca377af25 2Gi RWO dynamic-cephfs 10s[root@k8s-master1 ceph]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-ac8186ab-ff09-4134-9357-044ca377af25 2Gi RWO Delete Bound default/cephfs-claim dynamic-cephfs 12s 123456789101112131415161718192021222324# 创建 nginx pod 挂载测试cat &gt;nginx-pod2.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod2 labels: name: nginx-pod2spec: containers: - name: nginx-pod2 image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: cephfs mountPath: /usr/share/nginx/html volumes: - name: cephfs persistentVolumeClaim: claimName: cephfs-claimEOF 12345678910111213141516171819[root@k8s-master1 ceph]# kubectl apply -f nginx-pod2.yaml[root@k8s-master1 ceph]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-pod2 1/1 Running 0 3m28s 10.244.1.3 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod2 sh# df -hFilesystem Size Used Avail Use% Mounted onoverlay 40G 4.9G 33G 13% /tmpfs 64M 0 64M 0% /devtmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/vda1 40G 4.9G 33G 13% /etc/hostsshm 64M 0 64M 0% /dev/shmceph-fuse 18G 0 18G 0% /usr/share/nginx/htmltmpfs 1.9G 12K 1.9G 1% /run/secrets/kubernetes.io/serviceaccounttmpfs 1.9G 0 1.9G 0% /proc/acpitmpfs 1.9G 0 1.9G 0% /proc/scsitmpfs 1.9G 0 1.9G 0% /sys/firmware 123456# 修改文件内容[root@k8s-master1 ceph]# kubectl exec -ti nginx-pod2 -- /bin/sh -c 'echo This is from CephFS!!! &gt; /usr/share/nginx/html/index.html'# 访问pod测试[root@k8s-master1 ceph]# curl http://10.244.1.3This is from CephFS!!! 12345# 清理# pod 被删除 再拉起 数据是还在的kubectl delete -f nginx-pod2.yaml# pvc 删除 pv 数据就都不在了,storageclass自动回收 kubectl delete -f cephfs-pvc-test.yaml Ceph 日常运维管理集群监控管理 集群整体运行状态 123456789101112131415161718192021222324252627282930313233[root@cephnode01 my-cluster]# ceph -s cluster: id: 4ed819cf-39be-4a7c-9216-effcae715c58 health: HEALTH_OK services: mon: 3 daemons, quorum cephnode01,cephnode02,cephnode03 (age 9h) mgr: cephnode02(active, since 6h), standbys: cephnode03, cephnode01 mds: cephfs:1 &#123;0=cephnode02=up:active&#125; 2 up:standby osd: 3 osds: 3 up (since 9h), 3 in (since 31h) rgw: 1 daemon active (cephnode01) data: pools: 10 pools, 144 pgs objects: 260 objects, 43 KiB usage: 3.0 GiB used, 54 GiB / 57 GiB avail pgs: 144 active+cleanid: 集群IDhealth: 集群运行状态，这里有一个警告，说明是有问题，意思是pg数大于pgp数，通常此数值相等。mon: Monitors运行状态。osd: OSDs运行状态。mgr: Managers运行状态。mds: Metadatas运行状态。pools: 存储池与PGs的数量。objects: 存储对象的数量。usage: 存储的理论用量。pgs: PGs的运行状态# 持续输出ceph -w# 健康检查ceph health detail PG 状态 查看pg状态查看通常使用下面两个命令即可，dump可以查看更详细信息，如 12345678910111213ceph pg dumpceph pg stat[root@cephnode01 my-cluster]# ceph pg stat144 pgs: 144 active+clean; 43 KiB data, 32 MiB used, 54 GiB / 57 GiB avail# 144个pg# 43k数据# 32M使用# 54G空间ceph pg dump# pg的位置 如果主pg坏了 剩下的2个会选举出一个来在让用户访问 Pool状态1234567891011121314151617181920212223242526272829303132ceph osd pool stats[root@cephnode01 my-cluster]# ceph osd pool statspool .rgw.root id 1 nothing is going onpool default.rgw.control id 2 nothing is going onpool default.rgw.meta id 3 nothing is going onpool default.rgw.log id 4 nothing is going onpool rbd id 5 nothing is going onpool kube id 6 nothing is going onpool cephfs-data id 7 nothing is going onpool cephfs-metadata id 8 nothing is going onpool fs_data id 9 nothing is going onpool fs_metadata id 10 nothing is going on OSD 状态12345678910111213141516171819202122ceph osd statceph osd dumpceph osd treeceph osd df[root@cephnode01 my-cluster]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.05576 root default -3 0.01859 host cephnode01 0 hdd 0.01859 osd.0 up 1.00000 1.00000 -5 0.01859 host cephnode02 1 hdd 0.01859 osd.1 up 1.00000 1.00000 -7 0.01859 host cephnode03 2 hdd 0.01859 osd.2 up 1.00000 1.00000 [root@cephnode01 my-cluster]# ceph osd dfID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 0 hdd 0.01859 1.00000 19 GiB 1.0 GiB 11 MiB 0 B 1 GiB 18 GiB 5.32 1.00 144 up 1 hdd 0.01859 1.00000 19 GiB 1.0 GiB 11 MiB 0 B 1 GiB 18 GiB 5.32 1.00 144 up 2 hdd 0.01859 1.00000 19 GiB 1.0 GiB 11 MiB 0 B 1 GiB 18 GiB 5.32 1.00 144 up TOTAL 57 GiB 3.0 GiB 32 MiB 0 B 3 GiB 54 GiB 5.32 MIN/MAX VAR: 1.00/1.00 STDDEV: 0 Monitor状态和查看仲裁状态123ceph mon statceph mon dumpceph quorum_status 集群空间用量123456789101112131415161718192021ceph df 常用ceph df detail[root@cephnode01 my-cluster]# ceph dfRAW STORAGE: CLASS SIZE AVAIL USED RAW USED %RAW USED hdd 57 GiB 54 GiB 32 MiB 3.0 GiB 5.32 TOTAL 57 GiB 54 GiB 32 MiB 3.0 GiB 5.32 POOLS: POOL ID STORED OBJECTS USED %USED MAX AVAIL .rgw.root 1 1.2 KiB 4 768 KiB 0 17 GiB default.rgw.control 2 0 B 8 0 B 0 17 GiB default.rgw.meta 3 0 B 0 0 B 0 17 GiB default.rgw.log 4 0 B 175 0 B 0 17 GiB rbd 5 19 B 3 192 KiB 0 17 GiB kube 6 709 B 6 576 KiB 0 17 GiB cephfs-data 7 0 B 0 0 B 0 17 GiB cephfs-metadata 8 6.4 KiB 41 2.6 MiB 0 17 GiB fs_data 9 23 B 1 192 KiB 0 17 GiB fs_metadata 10 34 KiB 22 1.5 MiB 0 17 GiB 集群配置管理 (临时和全局，服务平滑重启) 有时候需要更改服务的配置，但不想重启服务，或者是临时修改。这时候就可以使用tell和daemon子命令来完成此需求。 查看运行配置12345命令格式：# ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config show 命令举例：# ceph daemon osd.0 config show tell 子命令格式 使用 tell 的方式适合对整个集群进行设置，使用 * 号进行匹配，就可以对整个集群的角色进行设置。 而出现节点异常无法设置时候，只会在命令行当中进行报错，不太便于查找。 12345678命令格式：# ceph tell &#123;daemon-type&#125;.&#123;daemon id or *&#125; injectargs --&#123;name&#125;=&#123;value&#125; [--&#123;name&#125;=&#123;value&#125;]命令举例：# ceph tell osd.0 injectargs --debug-osd 20 --debug-ms 11. daemon-type：为要操作的对象类型如osd、mon、mds等。2. daemon id：该对象的名称，osd通常为0、1等，mon为ceph -s显示的名称，这里可以输入*表示全部。3. injectargs：表示参数注入，后面必须跟一个参数，也可以跟多个 daemon子命令 使用 daemon 进行设置的方式就是一个个的去设置，这样可以比较好的反馈，此方法是需要在设置的角色所在的主机上进行设置。 12345命令格式：# ceph daemon &#123;daemon-type&#125;.&#123;id&#125; config set &#123;name&#125;=&#123;value&#125;命令举例：# ceph daemon mon.ceph-monitor-1 config set mon_allow_pool_delete false 集群操作123456781. 启动所有守护进程# systemctl start ceph.target2. 按类型启动守护进程# systemctl start ceph-mgr.target# systemctl start ceph-osd@id# systemctl start ceph-mon.target# systemctl start ceph-mds.target# systemctl start ceph-radosgw.target 添加和删除OSD添加 OSD123451、格式化磁盘ceph-volume lvm zap /dev/sd&lt;id&gt;2、进入到ceph-deploy执行目录/my-cluster，添加OSD# ceph-deploy osd create --data /dev/sd&lt;id&gt; $hostname 删除 OSD123456789101. 调整osd的crush weight为 0ceph osd crush reweight osd.&lt;ID&gt; 0.02. 将osd进程stopsystemctl stop ceph-osd@&lt;ID&gt;3. 将osd设置outceph osd out &lt;ID&gt;4. 立即执行删除OSD中数据ceph osd purge osd.&lt;ID&gt; --yes-i-really-mean-it5. 卸载磁盘umount /var/lib/ceph/osd/ceph-？ 扩容 PG1234567ceph osd pool set &#123;pool-name&#125; pg_num 128ceph osd pool set &#123;pool-name&#125; pgp_num 128 注： 1. 扩容大小取跟它接近的2的N次方 2. 在更改pool的PG数量时，需同时更改PGP的数量。PGP是为了管理placement而存在的专门的PG，它和PG的数量应该保持一致。如果你增加pool的pg_num，就需要同时增加pgp_num，保持它们大小一致，这样集群才能正常rebalancing。 Pool 操作12# 列出存储池ceph osd lspools 12345# 创建存储池命令格式：# ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; [&#123;pgp-num&#125;]命令举例：# ceph osd pool create rbd 32 32 12345# 设置存储池配额 命令格式：# ceph osd pool set-quota &#123;pool-name&#125; [max_objects &#123;obj-count&#125;] [max_bytes &#123;bytes&#125;]命令举例：# ceph osd pool set-quota rbd max_objects 10000 12# 删除存储池 ceph osd pool delete &#123;pool-name&#125; [&#123;pool-name&#125; --yes-i-really-really-mean-it] 12# 重命名存储池 ceph osd pool rename &#123;current-pool-name&#125; &#123;new-pool-name&#125; 12# 查看存储池统计信息 rados df 12# 给存储池做快照 ceph osd pool mksnap &#123;pool-name&#125; &#123;snap-name&#125; 12# 删除存储池的快照 ceph osd pool rmsnap &#123;pool-name&#125; &#123;snap-name&#125; 12# 获取存储池选项值 ceph osd pool get &#123;pool-name&#125; &#123;key&#125; 123456789101112# 调整存储池选项值ceph osd pool set &#123;pool-name&#125; &#123;key&#125; &#123;value&#125;size：设置存储池中的对象副本数，详情参见设置对象副本数。仅适用于副本存储池。min_size：设置 I/O 需要的最小副本数，详情参见设置对象副本数。仅适用于副本存储池。pg_num：计算数据分布时的有效 PG 数。只能大于当前 PG 数。pgp_num：计算数据分布时使用的有效 PGP 数量。小于等于存储池的 PG 数。hashpspool：给指定存储池设置/取消 HASHPSPOOL 标志。target_max_bytes：达到 max_bytes 阀值时会触发 Ceph 冲洗或驱逐对象。target_max_objects：达到 max_objects 阀值时会触发 Ceph 冲洗或驱逐对象。scrub_min_interval：在负载低时，洗刷存储池的最小间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_min_interval 。scrub_max_interval：不管集群负载如何，都要洗刷存储池的最大间隔秒数。如果是 0 ，就按照配置文件里的 osd_scrub_max_interval 。deep_scrub_interval：“深度”洗刷存储池的间隔秒数。如果是 0 ，就按照配置文件里的 osd_deep_scrub_interval 。 获取对象副本数1ceph osd dump | grep 'replicated size' 用户管理 Ceph 把数据以对象的形式存于各存储池中。 Ceph 用户必须具有访问存储池的权限才能够读写数据。 另外，Ceph 用户必须具有执行权限才能够使用 Ceph 的管理命令。 1234567# 查看用户信息查看所有用户信息# ceph auth list获取所有用户的key与权限相关信息# ceph auth get client.admin如果只需要某个用户的key信息，可以使用pring-key子命令# ceph auth print-key client.admin 12345# 添加用户# ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'# ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'# ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring# ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key 12345# 修改用户权限 # ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'# ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'# ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'# ceph auth caps client.ringo mon ' ' osd ' ' 增加和删除Monitor123# 新增一个monitor# ceph-deploy mon create $hostname注意：执行ceph-deploy之前要进入之前安装时候配置的目录。/my-cluster 123# 删除Monitor # ceph-deploy mon destroy $hostname注意： 确保你删除某个 Mon 后，其余 Mon 仍能达成一致。如果不可能，删除它之前可能需要先增加一个。 Pool 配置大小12345# Pool的基本配置若少于5个OSD 设置pg_num为128。5~10个OSD 设置pg_num为512。10~50个OSD 设置pg_num为4096。超过50个OSD 可以参考pgcalc计算。 常见问题nearfull osd(s) or pool(s) nearfull 此时说明部分osd的存储已经超过阈值，mon会监控ceph集群中OSD空间使用情况。 如果要消除WARN,可以修改这两个参数，提高阈值，但是通过实践发现并不能解决问题，可以通过观察osd的数据分布情况来分析原因。 123# 配置文件设置阈值 "mon_osd_full_ratio": "0.95", "mon_osd_nearfull_ratio": "0.85" 123# 自动处理 ceph osd reweight-by-utilizationceph osd reweight-by-pg 105 cephfs_data(pool_name) 12# 手动处理 ceph osd reweight osd.2 0.8 123456# 全局处理 ceph mgr module lsceph mgr module enable balancerceph balancer onceph balancer mode crush-compatceph config-key set "mgr/balancer/max_misplaced": "0.01" PG 故障状态1234567891011121314151617181920212223242526272829303132PG状态概述一个PG在它的生命周期的不同时刻可能会处于以下几种状态中:Creating(创建中)在创建POOL时,需要指定PG的数量,此时PG的状态便处于creating,意思是Ceph正在创建PG。Peering(互联中)peering的作用主要是在PG及其副本所在的OSD之间建立互联,并使得OSD之间就这些PG中的object及其元数据达成一致。Active(活跃的)处于该状态意味着数据已经完好的保存到了主PG及副本PG中,并且Ceph已经完成了peering工作。Clean(整洁的)当某个PG处于clean状态时,则说明对应的主OSD及副本OSD已经成功互联,并且没有偏离的PG。也意味着Ceph已经将该PG中的对象按照规定的副本数进行了复制操作。Degraded(降级的)当某个PG的副本数未达到规定个数时,该PG便处于degraded状态,例如:在客户端向主OSD写入object的过程,object的副本是由主OSD负责向副本OSD写入的,直到副本OSD在创建object副本完成,并向主OSD发出完成信息前,该PG的状态都会一直处于degraded状态。又或者是某个OSD的状态变成了down,那么该OSD上的所有PG都会被标记为degraded。当Ceph因为某些原因无法找到某个PG内的一个或多个object时,该PG也会被标记为degraded状态。此时客户端不能读写找不到的对象,但是仍然能访问位于该PG内的其他object。Recovering(恢复中)当某个OSD因为某些原因down了,该OSD内PG的object会落后于它所对应的PG副本。而在该OSD重新up之后,该OSD中的内容必须更新到当前状态,处于此过程中的PG状态便是recovering。Backfilling(回填)当有新的OSD加入集群时,CRUSH会把现有集群内的部分PG分配给它。这些被重新分配到新OSD的PG状态便处于backfilling。Remapped(重映射)当负责维护某个PG的acting set变更时,PG需要从原来的acting set迁移至新的acting set。这个过程需要一段时间,所以在此期间,相关PG的状态便会标记为remapped。Stale(陈旧的)默认情况下,OSD守护进程每半秒钟便会向Monitor报告其PG等相关状态,如果某个PG的主OSD所在acting set没能向Monitor发送报告,或者其他的Monitor已经报告该OSD为down时,该PG便会被标记为stale。 OSD 状态12345678910111213单个OSD有两组状态需要关注,其中一组使用in/out标记该OSD是否在集群内,另一组使用up/down标记该OSD是否处于运行中状态。两组状态之间并不互斥,换句话说,当一个OSD处于“in”状态时,它仍然可以处于up或down的状态。OSD状态为in且up这是一个OSD正常的状态,说明该OSD处于集群内,并且运行正常。OSD状态为in且down此时该OSD尚处于集群中,但是守护进程状态已经不正常,默认在300秒后会被踢出集群,状态进而变为out且down,之后处于该OSD上的PG会迁移至其它OSD。OSD状态为out且up这种状态一般会出现在新增OSD时,意味着该OSD守护进程正常,但是尚未加入集群。OSD状态为out且down在该状态下的OSD不在集群内,并且守护进程运行不正常,CRUSH不会再分配PG到该OSD上。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15 k8s 全方位监控 Kubernetes]]></title>
    <url>%2F2019%2F12%2F31%2Fk8s-15%2F</url>
    <content type="text"><![CDATA[K8S 监控方案两种监控方案 121. cAdvisor+Heapster+InfluxDB+Grafana 2. cAdvisor/exporter+Prometheus+Grafana 对于非容器化业务来说，像Zabbix，open-falcon已经在企业深入使用。 而Prometheus新型的监控系统的兴起来源容器领域，所以重点是放在怎么监控容器。随着容器化大势所趋，如果传统技术不随着改变，将会被淘汰，基础架构也会发生新的技术栈组合。 cAdvisor+Heapster+InfluxDB+Grafana 1234567891. cAdvisor 采集所有容器的性能指标,与kubelet集成2. Heapster 汇总数据3. InfluxDB 时间序列数据库4. Grafana 展示数据缺点:1. 无法对业务进行监控 Heapster 2. 扩展性差3. Heapster 被淘汰 替代者是 Metrics Server cAdvisor/exporter+Prometheus+Grafana 123451. cAdvisor 采集容器性能指标 2. node_exporter 对node监控3. Prometheus 汇总数据4. Grafana 展示数据5. kube-state-metrics -&gt; apiserver -&gt; etcd K8S 监控指标Kubernetes本身监控 Node资源利用率 Node数量 Pods数量（Node） • 资源对象状态• Node资源利用率 Node数量 Pods数量（Node） • 资源对象状态 Pod监控 Pod数量（项目） 容器资源利用率 应用程序 实现思路 k8s中的pod都是动态创建的,不能每次都在Prometheus配置文件中去写 所有要使用服务发现 k8s的服务发现是从 k8sapi中发现目标 ，并且获取当前状态，随着pod的生命周期采集数据 123456789服务发现：https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config支持监控的维度:1. node 自动发现集群中的node节点2. pod 自动发现运行的容器和端口3. service 自动发现创建的serviceIP、端口4. endpoints 自动发现pod中的容器5. ingress 自动发现创建的访问入口和规则 在 K8S 中部署 Prometheus准备工作12341. k8s集群2. 存储: nfs 自动供给 3. 部署参考：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus 1234567891011121314151617181920212223[root@k8s-master1 opt]# unzip prometheus-k8s\ .zip [root@k8s-master1 prometheus-k8s]# ls -l-rw-r--r-- 1 root root 652 Feb 9 2019 alertmanager-configmap.yaml-rw-r--r-- 1 root root 2183 Feb 7 2019 alertmanager-deployment.yaml-rw-r--r-- 1 root root 331 Feb 9 2019 alertmanager-pvc.yaml-rw-r--r-- 1 root root 372 Feb 7 2019 alertmanager-service.yaml-rw-r--r-- 1 root root 1198 Feb 8 2019 grafana.yaml-rw-r--r-- 1 root root 2377 Feb 8 2019 kube-state-metrics-deployment.yaml-rw-r--r-- 1 root root 2240 Feb 7 2019 kube-state-metrics-rbac.yaml-rw-r--r-- 1 root root 506 Feb 7 2019 kube-state-metrics-service.yaml-rw-r--r-- 1 root root 1495 Feb 7 2019 node-exporter-ds.yml-rw-r--r-- 1 root root 425 Feb 7 2019 node-exporter-service.yaml-rw-r--r-- 1 root root 646 Feb 2 2019 node_exporter.sh-rw-r--r-- 1 root root 99 Feb 7 2019 OWNERS-rw-r--r-- 1 root root 5131 Feb 10 2019 prometheus-configmap.yaml-rw-r--r-- 1 root root 1080 Feb 7 2019 prometheus-rbac.yaml-rw-r--r-- 1 root root 1802 Feb 9 2019 prometheus-rules.yaml-rw-r--r-- 1 root root 370 Feb 7 2019 prometheus-service.yaml-rw-r--r-- 1 root root 3539 Feb 10 2019 prometheus-statefulset-static-pv.yaml-rw-r--r-- 1 root root 3259 Feb 10 2019 prometheus-statefulset.yaml-rw-r--r-- 1 root root 349 Feb 7 2019 README.md 123456# k8s 基础组件[root@k8s-master1 prometheus-k8s]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 8d v1.16.0k8s-node1 Ready &lt;none&gt; 8d v1.16.0k8s-node2 Ready &lt;none&gt; 8d v1.16.0 12345# nfs pv自动供给[root@k8s-master1 prometheus-k8s]# kubectl get podsNAME READY STATUS RESTARTS AGEjenkins-6459665769-2dzp7 1/1 Running 1 25hnfs-client-provisioner-769f87c8f6-sdk9q 1/1 Running 1 25h 123[root@k8s-master1 prometheus-k8s]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 43h k8s 部署 prometheus12345678910111213# rbac 访问kubeapi授权,无需变动# configmap 配置文件 修改node地址[root@k8s-master1 prometheus-k8s]# vim prometheus-configmap.yaml ... - job_name: kubernetes-nodes scrape_interval: 30s static_configs: - targets: - 172.31.228.50:9100 - 172.31.228.52:9100 - 172.31.228.53:9100... 1234567891011121314151617181920212223242526# prometheus-statefulset.yaml 有状态部署# 修改 storageClassName volumeClaimTemplates: - metadata: name: prometheus-data spec: storageClassName: managed-nfs-storage # 初始化安装没有rules,先注释掉 volumeMounts: - name: config-volume mountPath: /etc/config - name: prometheus-data mountPath: /data subPath: "" #- name: prometheus-rules # mountPath: /etc/config/rules terminationGracePeriodSeconds: 300 volumes: - name: config-volume configMap: name: prometheus-config #- name: prometheus-rules # configMap: # name: prometheus-rules 1234567891011121314151617181920# 部署:[root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-rbac.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-configmap.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-statefulset.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-service.yaml [root@k8s-master1 prometheus-k8s]# kubectl get pods,svc -n kube-systemNAME READY STATUS RESTARTS AGEpod/coredns-6d8cfdd59d-k5wl9 1/1 Running 15 8dpod/kube-flannel-ds-amd64-2k5kz 1/1 Running 8 8dpod/kube-flannel-ds-amd64-gvs6b 1/1 Running 15 8dpod/kube-flannel-ds-amd64-hwglz 1/1 Running 15 8dpod/prometheus-0 2/2 Running 0 20mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 8dservice/prometheus NodePort 10.0.0.254 &lt;none&gt; 9090:32037/TCP 29m# web访问http://47.240.12.8:32037/graph 基于 K8S 服务发现的配置解析1234567891011121314151617181920# 用于修改 configmap 后 重新加载 containers: - name: prometheus-server-configmap-reload image: "jimmidyson/configmap-reload:v0.1" imagePullPolicy: "IfNotPresent" args: - --volume-dir=/etc/config - --webhook-url=http://localhost:9090/-/reload volumeMounts: - name: config-volume mountPath: /etc/config readOnly: true resources: limits: cpu: 10m memory: 10Mi requests: cpu: 10m memory: 10Mi 12345678910111213141516171819202122232425262728293031323334# 进入容器 查看配置文件# 需要掌握 重命名标签[root@k8s-master1 prometheus-k8s]# kubectl exec -it prometheus-0 sh -c prometheus-server -n kube-system/prometheus $ vi /etc/config/prometheus.yml # 采集自己scrape_configs:- job_name: prometheus static_configs: - targets: - localhost:9090- job_name: kubernetes-apiservers # k8s服务发现 kubernetes_sd_configs: - role: endpoints # 重新标记 relabel_configs: # 保留正则匹配的标签 - action: keep regex: default;kubernetes;https source_labels: - __meta_kubernetes_namespace - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name scheme: https tls_config: # 证书默认生 ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # 跳过证书认证 insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token# 可以通过 ip+端口6443访问 自身支持 123456789101112# kubelet的服务端口是 10250- job_name: kubernetes-nodes-kubelet kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 1234567891011121314151617181920212223242526272829303132333435- job_name: kubernetes-service-endpoints kubernetes_sd_configs: - role: endpoints relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scrape - action: replace regex: (https?) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_service_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_service_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_service_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: kubernetes_namespace - action: replace source_labels: - __meta_kubernetes_service_name target_label: kubernetes_name 监控 K8S 集群中Pod kubelet的节点使用cAdvisor提供的metrics接口获取该节点所有容器相关的性能指标数据 cAdvisor 已经集成到 kubelet 中 1234暴露接口地址：https://NodeIP:10255/metrics/cadvisor # cadvisor 自身端口https://NodeIP:10250/metrics/cadvisor # kubelet 方式 建议使用# https://172.31.228.50:10250/metrics/cadvisor 在 K8S中部署 Grafana Grafana是一个开源的度量分析和可视化系统。 Grafana 也部署在 k8s 集群内 123456官网：https://grafana.com/grafana/download推荐模板：1. 集群资源监控：31192. 资源状态监控 ：64173. Node监控 ：9276 12345678910111213141516171819202122232425[root@k8s-master1 prometheus-k8s]# kubectl apply -f grafana.yaml [root@k8s-master1 prometheus-k8s]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-k5wl9 1/1 Running 15 8dgrafana-0 1/1 Running 0 62skube-flannel-ds-amd64-2k5kz 1/1 Running 8 8dkube-flannel-ds-amd64-gvs6b 1/1 Running 15 8dkube-flannel-ds-amd64-hwglz 1/1 Running 15 8dprometheus-0 2/2 Running 0 4h11m[root@k8s-master1 prometheus-k8s]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgrafana NodePort 10.0.0.220 &lt;none&gt; 80:30007/TCP 5m41skube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 8dprometheus NodePort 10.0.0.254 &lt;none&gt; 9090:32037/TCP 4h24m# web访问 http://nodeip:30007http://47.240.12.8:30007/login admin/admin 修改密码# 添加数据源 # Prometheus url = http://prometheus:9090# 资源监控模板 3119# 硬盘资源 之前查询的是 xvda 虚拟磁盘 根据自己的磁盘情况df -h 查看 修改成vda 即可查询到数据,修改后保存 监控K8S集群Node node_exporter：用于*NIX系统监控，使用Go语言编写的收集器。 node_exporter 在k8s中使用DaemonSet方式部署,每个node都会启动一个收集器 本次收集不通过yaml方式部署 yaml方式局限于数据获取不到,挂载麻烦 每台node节点都需要部署123使用文档：https://prometheus.io/docs/guides/node-exporter/GitHub：https://github.com/prometheus/node_exporterexporter列表：https://prometheus.io/docs/instrumenting/exporters/ 12345678910111213141516171819202122232425# 部署脚本[root@k8s-master1 prometheus-k8s]# vim node_exporter.sh #!/bin/bashwget https://github.com/prometheus/node_exporter/releases/download/v0.17.0/node_exporter-0.17.0.linux-amd64.tar.gztar zxf node_exporter-0.17.0.linux-amd64.tar.gzmv node_exporter-0.17.0.linux-amd64 /usr/local/node_exportercat &lt;&lt;EOF &gt;/usr/lib/systemd/system/node_exporter.service[Unit]Description=https://prometheus.io[Service]Restart=on-failureExecStart=/usr/local/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|kubelet|kube-proxy|flanneld).service[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable node_exportersystemctl restart node_exporter 12# 测试访问http://47.240.12.8:9100/metrics 123456789101112131415161718192021222324# 接入监控 修改配置文件[root@k8s-master1 prometheus-k8s]# vim prometheus-configmap.yaml ...data: prometheus.yml: | scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # node 手动指定 scrape_configs: - job_name: kubernetes-nodes static_configs: - targets: - 172.31.228.50:9100 - 172.31.228.52:9100 - 172.31.228.53:9100...# 手动生效[root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-configmap.yaml configmap/prometheus-config configured 1# 导入模板: K8S Node监控 ：9276 100 - (avg(irate(node_cpu_seconds_total{instance=~”$node”,mode=”idle”}[1m])) * 100) 监控 K8S 资源对象与Grafana可视化123456789101112131415kube-state-metrics采集了k8s中各种资源对象的状态信息：kube_daemonset_*kube_deployment_*kube_job_*kube_namespace_*kube_node_*kube_persistentvolumeclaim_*kube_pod_container_*kube_pod_*kube_replicaset_*kube_service_*kube_statefulset_*# 官方文档https://github.com/kubernetes/kube-state-metrics 123[root@k8s-master1 prometheus-k8s]# kubectl apply -f kube-state-metrics-rbac.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f kube-state-metrics-deployment.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f kube-state-metrics-service.yaml 1# 导入模板 k8s 资源对象状态监控 ：6417 在 K8S中部署 Alertmanager 部署Alertmanager 配置Prometheus与Alertmanager通信 配置告警 12341. prometheus指定rules目录2. configmap存储告警规则3. configmap挂载到容器rules目录4. 增加alertmanager告警配置 1234[root@k8s-master1 prometheus-k8s]# kubectl apply -f alertmanager-configmap.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f alertmanager-pvc.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f alertmanager-deployment.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f alertmanager-service.yaml 1234567891011121314151617[root@k8s-master1 prometheus-k8s]# kubectl get pods,svc -n kube-systemNAME READY STATUS RESTARTS AGEpod/alertmanager-7866dbb64c-z2kcg 2/2 Running 0 112spod/coredns-6d8cfdd59d-k5wl9 1/1 Running 15 8dpod/grafana-0 1/1 Running 0 87mpod/kube-flannel-ds-amd64-2k5kz 1/1 Running 8 8dpod/kube-flannel-ds-amd64-gvs6b 1/1 Running 15 8dpod/kube-flannel-ds-amd64-hwglz 1/1 Running 15 8dpod/kube-state-metrics-5c656f9944-cng55 2/2 Running 0 21mpod/prometheus-0 2/2 Running 0 93mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager ClusterIP 10.0.0.110 &lt;none&gt; 80/TCP 56sservice/grafana NodePort 10.0.0.132 &lt;none&gt; 80:30007/TCP 87mservice/kube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 8dservice/kube-state-metrics ClusterIP 10.0.0.251 &lt;none&gt; 8080/TCP,8081/TCP 20mservice/prometheus NodePort 10.0.0.66 &lt;none&gt; 9090:31078/TCP 96m 配置 Prometheus 与 Alertmanager 通信[root@k8s-master1 prometheus-k8s]# vim prometheus-configmap.yaml 1234alerting: alertmanagers: - static_configs: - targets: ["alertmanager:80"] 配置告警 prometheus指定rules目录 1234567891011121314[root@k8s-master1 prometheus-k8s]# vim prometheus-configmap.yaml data: prometheus.yml: | rule_files: - /etc/config/rules/*.rules scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090...[root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-configmap.yaml configmap存储告警规则12# 测试告警规则 无需修改[root@k8s-master1 prometheus-k8s]# vim prometheus-rules.yaml configmap挂载到容器rules目录12345678910111213[root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-rules.yaml [root@k8s-master1 prometheus-k8s]# kubectl apply -f prometheus-statefulset.yaml [root@k8s-master1 prometheus-k8s]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEalertmanager-7866dbb64c-z2kcg 2/2 Running 0 22mcoredns-6d8cfdd59d-k5wl9 1/1 Running 15 8dgrafana-0 1/1 Running 0 107mkube-flannel-ds-amd64-2k5kz 1/1 Running 8 8dkube-flannel-ds-amd64-gvs6b 1/1 Running 15 8dkube-flannel-ds-amd64-hwglz 1/1 Running 15 8dkube-state-metrics-5c656f9944-cng55 2/2 Running 0 41mprometheus-0 2/2 Running 0 53s 增加alertmanager告警配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@k8s-master1 prometheus-k8s]# vim alertmanager-configmap.yaml apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-config namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: EnsureExistsdata: alertmanager.yml: | global: resolve_timeout: 5m smtp_smarthost: 'smtp.qq.com:465' smtp_from: '365042337@qq.com' smtp_auth_username: '365042337@qq.com' smtp_auth_password: 'jtebrxpgcuyjcafi' # smtp_require_tls: false receivers: - name: default-receiver email_configs: - to: "365042337@qq.com" route: group_interval: 1m group_wait: 10s receiver: default-receiver repeat_interval: 1m[root@k8s-master1 prometheus-k8s]# kubectl apply -f alertmanager-configmap.yaml[root@k8s-master1 prometheus-k8s]# kubectl exec -it prometheus-0 sh -c prometheus-server -n kube-system/prometheus $ ls /etc/config/rules/general.rules node.rules/prometheus $ cat /etc/config/rules/general.rules groups:- name: general.rules rules: - alert: InstanceDown expr: up == 0 for: 1m labels: severity: error annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; 停止工作" description: "&#123;&#123; $labels.instance &#125;&#125; job &#123;&#123; $labels.job &#125;&#125; 已经停止5分钟以上."/prometheus $ [root@k8s-master1 prometheus-k8s]# kubectl exec -it alertmanager-7866dbb64c-z2kcg sh -n kube-system/alertmanager # cat /etc/config/alertmanager.yml global: resolve_timeout: 5m smtp_smarthost: 'smtp.qq.com:465' smtp_from: '365042337@qq.com' smtp_auth_username: '365042337@qq.com' smtp_auth_password: 'jtebrxpgcuyjcafi' # smtp_require_tls: falsereceivers:- name: default-receiver email_configs: - to: "365042337@qq.com"route: group_interval: 1m group_wait: 10s receiver: default-receiver repeat_interval: 1m 12# 关闭一个node_exporter 看看是否告警[root@k8s-node1 ~]# systemctl stop node_exporter 小结 标签重要性（环境，部门，项目，管理者） Grafana灵活 PromSQL 利用服务发现动态加入目标 下一步计划：Prometheus集群， PromSQL， Grafana，对业务监控]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14 Prometheus + Gafana 全方位立体式监控系统]]></title>
    <url>%2F2019%2F12%2F31%2Fk8s-14%2F</url>
    <content type="text"><![CDATA[聊聊监控为什么要监控 监视应用程序的当前状态是预测问题和发现生产环境中瓶颈的最有效方法之一。 监控是整个产品周期中最重要的一环，及时预警减少故障影响免扩大，而且能根据历史数据追溯问题。 对系统不间断实时监控 实时反馈系统当前状态 保证业务持续性运行 怎么来监控监控工具 适合临时性分析性能及排查故障。少量的服务器。 目前业内有很多不错的开源产品可供选择，利用开源产品很容易能做到这一点。 free vmstat df top ss iftop … 监控系统 Zabbix 不适用容器 Open-Falcon 小米开源 功能强大 非容器 Prometheus 适用容器 监控方案 告警 特点 适用 Zabbix Y 大量定制工作 大部分的互联网公司 open-falcon Y 功能模块分解比较细，显得更复杂 系统和应用监控 Prometheus+Grafana Y 扩展性好，容器，应用 主机全方面监控 开源监控系统基本流程 数据采集：对监控数据采集 数据存储：将监控数据持久化存储，用于历时查询 数据分析：数据按需处理，例如阈值对比、聚合 数据展示：Web页面展示 监控告警：电话，邮件，微信，短信 要监控什么 可以从其他监控产品公司了解到要监控什么。 可参考的网站 https://www.jiankongbao.com 准备工作 熟悉被监控对象 整理监控指标 告警阈值定义 故障处理流程 Prometheus 监控范畴 Prometheus 最擅长 api监控、应用监控 Prometheus 不擅长 日志监控、安全监控 Prometheus 需要数据源传来数据 Prometheus 概述Prometheus 介绍及特点Prometheus 是什么 Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。 Prometheus是由SoundCloud开发的开源监控报警系统和时序列数据库。从字面上理解，Prometheus由两个部分组成，一个是监控报警系统，另一个是自带的时序数据库（TSDB） 官方网站: 12https://prometheus.iohttps://github.com/prometheus Prometheus 特点 多维数据模型：由度量名称(metric名称)和键/值对标识的时间序列数据 PromSQL：一种灵活的查询语言,可以利用多维数据完成复杂的查询 不依赖分布式存储,单个服务器节点可直接工作,数据被存储在本地磁盘上 基于HTTP的pull方式采集时间序列数据 推送时间序列数据通过PushGateway组件支持 通过服务发现或静态配置发现目标 多种图形和仪表板支持 Prometheus 组成及架构Prometheus 架构图 定时任务和短任务,他们会向Pushgateway传送数据,主动push,Prometheus Server 会定时拉取(pull) Pushgateway 里面的数据 jobs/Exporters 长期任务,jobs采集项目提供的metrics接口指标，Exporters是已经制造好的监控,是官方提供的现成的采集方法,它负责采集数据,Prometheus Server来定时拉取 Prometheus Server 的 Retrieval 模块负责定时拉取数据,存储时间序列数据(TSDB) 服务发现功能,动态的发现监控对象 提供查询接口 http server 数据展现除了 Prometheus 自带的 webui,还可以通过 grafana 等组件查询 Prometheus 监控数据 PromQL 为 Prometheus 提供的查询语法,PromQL 模块通过解析语法树,调用存储模块,查询获取监控数据 Prometheus 将告警推送到 alertmanger,然后通过 alertmanger 对告警进行处 理并执行相应动作，有多钟告警类型，微信、邮件、钉钉(需要第三方服务支持) Prometheus 组成 Prometheus Server：收集指标和存储时间序列数据,并提供查询接口 ClientLibrary：客户端库,为各种语言提供客户端工具,让用户自己编写暴露metrics Push Gateway：短期存储指标数据。主要用于临时性的任务 Exporters：采集已有的第三方服务监控指标并暴露metrics Alertmanager：告警 Web UI：简单的Web控制台 Prometheus 监控指标数据模型监控指标 数据模型 Prometheus将所有数据存储为时间序列；具有相同度量名称以及标签属于同一个指标。 每个时间序列都由度量标准名称和一组键值对（也称为标签）唯一标识 时间序列格式: 123度量标准名称&#123;标签(健值对),标签2(健值对),...&#125; 值&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;api_http_requests_total&#123;method="POST", handler="/messages"&#125; 123451. 指标格式分为两个部分:一份是指标名称,另一个是指标标签。2. 标签可体现指标的维度特征,用于过滤和聚合。它通过标签名(label name)和标签值(label value) 这种键值对的形式,形成多种维度。3. 例如:对于指标 http_request_total , 可以有 &#123;status="200", method="POST"&#125; 和 &#123;status="200", method="GET"&#125;这两个标签。4. 在需要分别获取 GET 和 POST 返回 200 的请求时,可分别使用上述两种指标; 5. 在需要获取所有返回 200 的请求时,可以通过 http_request_total&#123;status="200"&#125;完成数据的聚合,非常便捷和通用。 监控指标 数据类型 Counter：递增的计数器 Gauge：可以任意变化的数值 Histogram：对一段时间范围内数据进行采样，并对所有数值求和与统计数量 Summary：与Histogram类似 12345671. 数据类型是提现在客户端的,在暴露数据的时候要指定数据类型2. 开发自己写的项目,想要采集的指标需要暴露出来,使用客户端引用接口,将数据和数据类型填写上去3. Counter: 可以选择接口请求次数,适用于一直递增的数据4. Gauge: 波浪线数据5. Histogram： web上某一个时间阶段的数据 求和和统计数量 , 比如一个时间段内http响应的大小，耗时多长时间等6. Summary：与Histogram类似7. 服务端不会区分这些数据类型,只有客户端写 作业和实例监控目标的两个概念 实例：可以抓取的目标称为实例（Instances）, 比如监控的某一台服务器,它就是一个实例 作业：具有相同目标的实例集合称为作业（Job）, 类型一样的实例的集合。 在配置文件里面定义,是监控目标的概念术语 12345678910scrape_configs: # 作业 - job_name: 'prometheus' static_configs: # 实例 - targets: ['localhost:9090'] - job_name: 'node' static_configs: - targets: ['192.168.1.10:9090'] Prometheus 数据采集 Prometheus 通过 HTTP 接口的方式从各种客户端获取数据,这些客户端必须符合 Prometheus 监控数据格式,通常由两种方式。 一是侵入式埋点监控,通过在客户端集成,如果 kubernetes API 直接通过引入 Prometheus go client,提供/metrics 接口查询 kubernetes API 各种指标, 另一种是通过 exporter 方式,在外部将原来各种中间件的监控支持转化为 Prometheus 的监控数据格式,如 redis exporter 将 reids 指标转化为 Prometheus 能够识别的 HTTP 请求。 Prometheus 并没有采用 json 的数据格式,而是采用 text/plain 纯文本的方式 ,这是它的特殊之处。 HTTP 返回 Header 和 Body 如上图所示,指标前面两行#是注释,标识指标的含义和类型。指标和指标的值 通过空格分割,开发者通常不需要自己拼接这种个数的数据, Prometheus 提供了各种语言的 SDK 支持。 Prometheus exporter Prometheus 为了支持各种中间件以及第三方的监控提供了 exporter,大家可以把它理解成监控适配器, 将不同指标类型和格式的数据统一转化为 Prometheus 能够识别的指标类型。 譬如 Node exporter 主要通过读取 linux 的/proc 以及/sys 目录下的系统文件获取操作系统运行状态 reids exporter 通过 reids 命令行获取指标,mysql exporter 通过读取数据库监控表获取 mysql 的性能数据。 他们将这些异构的数据转化为标准的 Prometheus 格式,并提供 HTTP 查询接口。 Prometheus 数据存储 Prometheus 提供了两种数据持久化方式:一种是本地存储,通过 Prometheus 自带的tsdb(时序数据库),将数据保存到本地磁盘,为了性能考虑,建议使用 SSD。 但本地存储的容量毕竟有限,建议不要保存超过一个月的数据。 Prometheus 本地存储经过多年改进,自Prometheus2.0 后提供的V3版本tsdb性能已经非常高,可以支持单机每秒 1000w 个指标的收集。 Prometheus 本地数据存储能力一直为大家诟病,但 Prometheus 本地存储设计的初衷就是为了监控数据的查询。 Facebook 发现85%的查询是针对 26 小时内的数据。所以 Prometheus 本地时序数据库的设计更多考虑的是高性能而非分布式大容量。 另一种是远端存储,适用于大量历史监控数据的存储和查询。通过中间层的适配器的转化,Prometheus 将数据保存到远端存储。 适配器实现 Prometheus 存储的 remote write 和 remote read 接口并把数据转化为 远端存储支持的数据格式。 目前,远端存储主要包括 OpenTSDB、InfluxDB、Elasticsearch、M3db 等,其中 M3db 是目前非常受欢迎的后端存储。 Prometheus 部署Prometheus 二进制部署二进制部署: 12https://prometheus.io/docs/prometheus/latest/getting_started/https://prometheus.io/download/ 123456789101112131415161718# 下载安装包[root@k8s-master2 opt]# wget https://github.com/prometheus/prometheus/releases/download/v2.15.1/prometheus-2.15.1.linux-amd64.tar.gz# 解压[root@k8s-master2 opt]# tar -zxvf prometheus-2.15.1.linux-amd64.tar.gz [root@k8s-master2 prometheus-2.15.1.linux-amd64]# ls -ltotal 140132drwxr-xr-x 2 3434 3434 4096 Dec 25 10:59 console_librariesdrwxr-xr-x 2 3434 3434 4096 Dec 25 10:59 consoles-rw-r--r-- 1 3434 3434 11357 Dec 25 10:59 LICENSE-rw-r--r-- 1 3434 3434 3184 Dec 25 10:59 NOTICE-rwxr-xr-x 1 3434 3434 81905918 Dec 25 09:06 prometheus # 服务端-rw-r--r-- 1 3434 3434 926 Dec 25 10:59 prometheus.yml # 配置文件-rwxr-xr-x 1 3434 3434 47954507 Dec 25 09:07 promtool # 客户端工具-rwxr-xr-x 1 3434 3434 13600661 Dec 25 09:08 tsdb[root@k8s-master2 opt]# mv prometheus-2.15.1.linux-amd64 /usr/local/prometheus[root@k8s-master2 opt]# cd /usr/local/prometheus 配置监控本机 1234567891011121314[root@k8s-master2 prometheus]# cat prometheus.yml...# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090']... 启动参数 123[root@k8s-master2 prometheus]# ./prometheus --help--storage.tsdb.path="data/" # 数据存储目录,默认当前目录data 可以指定--storage.tsdb.retention.time=STORAGE.TSDB.RETENTION.TIME # 数据过期清理时间，默认保存15天 启动服务 123[root@k8s-master2 prometheus]# ./prometheus --config.file="prometheus.yml" ...level=info ts=2020-01-02T06:53:28.186Z caller=main.go:617 msg="Server is ready to receive web requests." 12# 查看页面http://47.240.13.206:9090/graph 123456789alert: 管理告警规则graph: 通过PromQL查询数据 status: 查看当前运行环境Runtime Information: 运行状态Command-Line Flags: 命令行日志Configuration: 配置信息rules: 角色,定义监控指标的告警规则Targets: 当前被纳入监控的主机信息Service Discovery: 服务发现,动态配置监控目标 配置系统服务管理systemd服务管理配置文件 123456789101112131415[root@k8s-master2 ~]# cd /usr/lib/systemd/system[root@k8s-master2 system]# cp sshd.service prometheus.service[root@k8s-master2 system]# vim prometheus.service [Unit]Description=prometheus server daemon[Service]Restart=on-failureExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml[Install]WantedBy=multi-user.target~ 12345678910[root@k8s-master2 system]# systemctl start prometheus[root@k8s-master2 system]# ps -ef|grep prometheusroot 8472 1 1 15:22 ? 00:00:00 /usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.ymlroot 8670 23429 0 15:22 pts/1 00:00:00 grep --color=auto prometheus[root@k8s-master2 system]# systemctl stop prometheus[root@k8s-master2 system]# ps -ef|grep prometheusroot 8788 23429 0 15:22 pts/1 00:00:00 grep --color=auto prometheus Prometheus Docker部署Docker部署: 12https://prometheus.io/docs/prometheus/latest/installation/1. 确保服务端主机安装了docker服务 123456789# 使用刚才二进制部署的配置文件挂载到容器中 docker run \ -p 9090:9090 \ -v /usr/local/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \ prom/prometheus [root@k8s-master2 system]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf71fadc4fbdb prom/prometheus "/bin/prometheus --c…" 4 seconds ago Up 3 seconds 0.0.0.0:9090-&gt;9090/tcp mystifying_zhukovsky 123# 容器放在后台运行 [root@k8s-master2 prometheus]# docker run -d -p 9090:9090 -v /usr/local/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus02941abc1d318c20965121803d096168a4846994733b9641b2e99c942f016b25 访问本机的监控指标1http://47.240.13.206:9090/metrics Prometheus 配置文件与核心功能全局配置文件介绍12# 官方说明https://prometheus.io/docs/prometheus/latest/configuration/configuration/ 123456789101112131415161718192021222324252627282930313233343536global: # 采集被监控端数据的时间周期(间隔),默认1分钟 [ scrape_interval: &lt;duration&gt; | default = 1m ] # 请求metrics的超时时间,默认10秒 [ scrape_timeout: &lt;duration&gt; | default = 10s ] # 告警评估周期,默认1分钟，执行 rules 的时间间隔。 [ evaluation_interval: &lt;duration&gt; | default = 1m ] # 额外的属性，会添加到拉取的数据并存到数据库中。 external_labels: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ] # 监控告警规则 rule_files: [ - &lt;filepath_glob&gt; ... ]# 配置被监控端指标scrape_configs: [ - &lt;scrape_config&gt; ... ]# 告警配置alerting: alert_relabel_configs: [ - &lt;relabel_config&gt; ... ] alertmanagers: [ - &lt;alertmanager_config&gt; ... ]# 远程存储 writeremote_write: [ - &lt;remote_write&gt; ... ]# 远程存储 readremote_read: [ - &lt;remote_read&gt; ... ] 配置采集目标 scrape_configs123参考:https://www.jianshu.com/p/fb5c82de935dhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/#job_name 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 作业job_name: &lt;job_name&gt;- job_name: 'prometheus'# job作业的采集时间间隔,默认使用全局配置[ scrape_interval: &lt;duration&gt; | default = &lt;global_config.scrape_interval&gt; ]# 抓取此job时，每次抓取超时时间,默认使用全局配置[ scrape_timeout: &lt;duration&gt; | default = &lt;global_config.scrape_timeout&gt; ]# 从目标获取指标的HTTP资源路径， 默认是/metrics[ metrics_path: &lt;path&gt; | default = /metrics ]# 不覆盖采集标签[ honor_labels: &lt;boolean&gt; | default = false ]# 配置用于请求的协议方案,采集目标 是 http 还是https[ scheme: &lt;scheme&gt; | default = http ]# 可选的HTTP URL参数.params: [ &lt;string&gt;: [&lt;string&gt;, ...] ]# 基础认证,访问监控端的用户名和密码， password和password_file是互斥的。basic_auth: [ username: &lt;string&gt; ] [ password: &lt;secret&gt; ] [ password_file: &lt;string&gt; ]# token 认证 [ bearer_token: &lt;secret&gt; ]# 配置scrape请求的TLS设置 https证书tls_config: [ &lt;tls_config&gt; ]# 可选的代理URL.可选的代理URL.[ proxy_url: &lt;string&gt; ]# 服务发现配置列表 consul consul_sd_configs: [ - &lt;consul_sd_config&gt; ... ]# 服务发现配置列表 DNSdns_sd_configs: [ - &lt;dns_sd_config&gt; ... ]# 服务发现配置列表 k8s kubernetes_sd_configs: [ - &lt;kubernetes_sd_config&gt; ... ]# 服务发现配置列表 文件file_sd_configs: [ - &lt;file_sd_config&gt; ... ]...# 静态配置被监控端 static_configs: [ - &lt;static_config&gt; ... ]# 在数据采集之前对标签进行重新标记relabel_configs: [ - &lt;relabel_config&gt; ... ]# 在数据采集之后重新标记 metric_relabel_configs: [ - &lt;relabel_config&gt; ... ] # 对每个将被接受的样本数量的每次抓取限制。# 如果在度量重新标记后存在超过此数量的样本，则整个抓取将被视为失败。 0表示没有限制。[ sample_limit: &lt;int&gt; | default = 0 ]... 重新标记 relabel_configs relabel_configs ：允许在采集之前对任何目标及其标签进行修改 重新标签的意义? 优化标签 重命名标签名 删除标签 过滤目标 relabel_configs 配置选项 123456789101112131415161718192021# 源标签[ source_labels: '[' &lt;labelname&gt; [, ...] ']' ]# 分隔符放置在连接的源标签值之间。[ separator: &lt;string&gt; | default = ; ]# 重新标记的标签# 替换操作是强制性的。 正则表达式捕获组可用。[ target_label: &lt;labelname&gt; ]# 正则表达式匹配源标签的值[ regex: &lt;regex&gt; | default = (.*) ]# 采用源标签值的散列的模数。[ modulus: &lt;uint64&gt; ]# 如果正则表达式匹配，则执行正则表达式替换的替换值。 正则表达式捕获组可用。[ replacement: &lt;string&gt; | default = $1 ]# 基于正则表达式匹配执行的操作。[ action: &lt;relabel_action&gt; | default = replace ]# 实例,查看一组服务器的cpu使用率根据标签聚合123# 用户和系统CPU花费的时间 # 度量指标名称process_cpu_seconds_total 增加一组服务器机房标签 12345678910111213141516171819202122232425262728293031# 比如通过idc区分机房# 增加标签[root@k8s-master2 prometheus]# vim prometheus.yml scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090'] # 实例新增标签 labels: idc: bj# 检查配置文件语法[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml Checking prometheus.yml SUCCESS: 0 rule files found# 热更新[root@k8s-master2 prometheus]# ps -ef|grep prometheusroot 15513 1 0 17:03 ? 00:00:02 /usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.ymlroot 15573 2714 0 18:01 pts/0 00:00:00 grep --color=auto prometheus[root@k8s-master2 prometheus]# kill -hup 15513# 有了这标签之后,不同机房的服务器就可以按照IDC分组查询数据了,相当于mysql表中的一个字段,用来增加维度 通过聚合函数 + 标签查询数据 12# 就可以聚合所有idc=bj标签的实例的cpu数据,然后在相加得到总数sum(process_cpu_seconds_total&#123;idc="bj"&#125;) 重命名标签1234567891011121314# 修改job_name的值# prometheus 默认标记 instance="localhost:9090"(targets的值),job="prometheus"(job_name的值)scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'bj' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090'] [root@k8s-master2 prometheus]# kill -hup 15513# 过一会采集生效 1234567891011121314151617181920212223242526272829# 把job修改成idc # relabel重命名[root@k8s-master2 prometheus]# vim prometheus.yml scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'bj' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090'] relabel_configs: - action: replace source_labels: ['job'] # 正则默认匹配所有 regex: (.*) replacement: $1 # 赋予新的标签 target_label: idc[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml [root@k8s-master2 prometheus]# kill -hup 15513# job 标签并没有被删除,所以还是存在的# 聚合查询sum(process_cpu_seconds_total&#123;idc="bj"&#125;) 根据标签过滤目标123456789# &lt;relabel_action&gt;确定要采取的重新签名行动：replace： 将regex与连接的source_labels匹配。 然后，将target_label设置为replacement，将匹配组引用（$&#123;1&#125;，$&#123;2&#125;，...）替换为其值。 如果正则表达式不匹配，则不进行替换。keep： 删除regex与连接的source_labels不匹配的目标。drop： 删除regex与连接的source_labels匹配的目标。hashmod： 将target_label设置为连接的source_labels的哈希模数。labelmap： 将regex与所有标签名称匹配。 然后将匹配标签的值复制到替换时给出的标签名称，替换为匹配组引用（$&#123;1&#125;，&#123;2&#125;，...）替换为其值。labeldrop： 将regex与所有标签名称匹配。匹配的任何标签都将从标签集中删除。labelkeep： 将regex与所有标签名称匹配。任何不匹配的标签都将从标签集中删除。# 必须小心使用labeldrop和labelkeep，以确保在删除标签后仍然对指标进行唯一标记。 123456789101112131415161718192021# drop 过滤匹配的标签,如果有这个标签,就不采集这个数据scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'bj' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090'] relabel_configs: - action: replace source_labels: ['job'] # 正则默认匹配所有 regex: (.*) replacement: $1 # 赋予新的标签 target_label: idc # 过滤标签,有job标签的实例就不采集数据 - action: drop source_labels: ['job'] 1234# keep 过滤匹配的标签,如果有这个标签,可以采集到数据 # 过滤标签,有job标签的实例采集数据 - action: keep source_labels: ['job'] 删除标签123labeldrop： 将regex与所有标签名称匹配。匹配的任何标签都将从标签集中删除。labelkeep： 将regex与所有标签名称匹配。任何不匹配的标签都将从标签集中删除。# 必须小心使用labeldrop和labelkeep，以确保在删除标签后仍然对指标进行唯一标记。 1234# 查询你到的数据将不会再有jop标签 - action: labeldrop regex: job 1234567891011121314151617181920212223# 测试的配置信息scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'bj' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 static_configs: - targets: ['localhost:9090'] relabel_configs: - action: replace source_labels: ['job'] # 正则默认匹配所有 regex: (.*) replacement: $1 # 赋予新的标签 target_label: idc # 过滤标签,有job标签的实例采集数据 - action: keep source_labels: ['job'] - action: labeldrop regex: job 基于文件的服务发现 123456# 注释下我们的监控配置# static_configs:# - targets: ['localhost:9090'][root@k8s-master2 prometheus]# kill -hup 1566# 再查看监控目标,当前没有被监控的目标 123456789101112131415# 通过服务发现的形式 加入scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 file_sd_configs: - files: ['/usr/local/prometheus/sd_config/*.yml'] # 检查刷新周期 5s读取配置文件 refresh_interval: 5s# static_configs:# - targets: ['localhost:9090'] 12345678# 创建目录和yml文件,*.yml可以匹配所有的yml文件[root@k8s-master2 prometheus]# mkdir -p /usr/local/prometheus/sd_config/[root@k8s-master2 prometheus]# cd /usr/local/prometheus/sd_config/[root@k8s-master2 sd_config]# vim test.yml- targets: ['localhost:9090'] labels: group: app01 123456# 检查语法并热加载[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml Checking prometheus.yml SUCCESS: 0 rule files found[root@k8s-master2 prometheus]# kill -hup 1566 监控案例监控 Linux 服务器 node_exporter：用于*NIX系统监控，使用Go语言编写的收集器。 1234使用文档：https://prometheus.io/docs/guides/node-exporter/GitHub：https://github.com/prometheus/node_exporterexporter列表：https://prometheus.io/docs/instrumenting/exporters/收集指标信息: https://github.com/prometheus/node_exporter 下载解压 1234567891011121314# 在被监控的linux主机 下载 node_exporter[root@k8s-node2 opt]# https://github.com/prometheus/node_exporter/releases/download/v0.18.1/node_exporter-0.18.1.linux-amd64.tar.gz[root@k8s-node2 opt]# tar -zxvf node_exporter-0.18.1.linux-amd64.tar.gz [root@k8s-node2 opt]# mv node_exporter-0.18.1.linux-amd64 /usr/local/node_exporter[root@k8s-node2 opt]# cd /usr/local/node_exporter[root@k8s-node2 node_exporter]# ls -ltotal 16500-rw-r--r-- 1 3434 3434 11357 Jun 5 2019 LICENSE-rwxr-xr-x 1 3434 3434 16878582 Jun 5 2019 node_exporter # 收集器 go语言编写-rw-r--r-- 1 3434 3434 463 Jun 5 2019 NOTICE[root@k8s-node2 node_exporter]# ./node_exporter --help 配置系统服务 123456789101112131415161718[root@k8s-node2 node_exporter]# vim /usr/lib/systemd/system/node_exporter.service[Unit]Description=node_exporter server daemon[Service]Restart=on-failureExecStart=/usr/local/node_exporter/node_exporter[Install]WantedBy=multi-user.target[root@k8s-node2 node_exporter]# systemctl daemon-reload[root@k8s-node2 node_exporter]# ps -ef|grep node_exporterroot 9809 1 0 09:17 ? 00:00:00 /usr/local/node_exporter/node_exporter[root@k8s-node2 node_exporter]# netstat -tnlp|grep 9100tcp6 0 0 :::9100 :::* LISTEN 9809/node_exporter 查看暴露的接口 12# node_exporter服务会暴露metrics http://47.240.1.178:9100/metrics 配置prometheus监控该实例 12345678910111213141516171819202122# 配置一个新的job,并使用服务发现scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 监控实例 本机:9090 启动服务后监听9090端口 file_sd_configs: - files: ['/usr/local/prometheus/sd_config/*.yml'] # 检查周期 refresh_interval: 5s# static_configs:# - targets: ['localhost:9090'] - job_name: 'node' # 服务发现 file_sd_configs: - files: ['/usr/local/prometheus/sd_config/*.yml'] refresh_interval: 5s 1234567891011[root@k8s-master2 prometheus]# vim sd_config/node.yml- targets: - 172.31.228.53:9100[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml [root@k8s-master2 prometheus]# ps -ef|grep prometheusroot 1574 1 0 08:56 ? 00:00:03 /usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.ymlroot 1658 1549 0 09:27 pts/0 00:00:00 grep --color=auto prometheus[root@k8s-master2 prometheus]# kill -hup 1574 总结 123# 增加一个node_exporter监控linux服务器：1. 在被监控端安装node_exporter 并启动2. 在服务端配置job和服务发现，配置新实例 PromSQL 获取CPU,内存,硬盘使用率123456CPU使用率：100 - (avg(irate(node_cpu_seconds_total&#123;mode="idle"&#125;[5m])) by (instance) * 100)内存使用率：100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100磁盘使用率：100 - (node_filesystem_free_bytes&#123;mountpoint="/",fstype=~"ext4|xfs"&#125; / node_filesystem_size_bytes&#123;mountpoint="/",fstype=~"ext4|xfs"&#125; * 100) CPU使用率 12345678910111213141516100 - 服务器所有cpu的空闲率 = cpu的使用率# 语句编写1 先收集 172.31.228.53 的所有cpu idle值 node_cpu_seconds_total&#123;instance="172.31.228.53:9100",job="node",mode="idle"&#125;# 不统计瞬间,而是使用范围区间去统计 # 统计变化大的数据 需要加时间范围# 范围向量选择器 []# 语句编写2 5分钟之内的数据node_cpu_seconds_total&#123;instance="172.31.228.53:9100",job="node",mode="idle"&#125;[5m]# 语句编写3 irate 取的是在指定时间范围内的最近两个数据点 来算速率 * 100 得到百分比irate(node_cpu_seconds_total&#123;instance="172.31.228.53:9100",job="node",mode="idle"&#125;[5m]) * 100# 语句编写4 100 - 剩余的 就是使用的 # 这个实例在5分钟之内cpu 使用率的 百分比100 - irate(node_cpu_seconds_total&#123;instance="172.31.228.53:9100",job="node",mode="idle"&#125;[5m]) * 100 内存使用率 12345678# 剩余内存的统计 = 剩余内存free + 缓存buff/cache # node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes# 剩余内存百分比 = (剩余内存free + 缓存buff/cache) / 内存总值 * 100 (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100# 内存使用率 100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100 磁盘使用率 12345678# 使用正则排除不想统计的分区node_filesystem_free_bytes&#123;mountpoint="/",fstype=~"ext4|xfs"&#125; # 剩余百分比 = 磁盘剩余 / 磁盘总数# 使用百分比 = 100 - 剩余百分比100 - (node_filesystem_free_bytes&#123;mountpoint="/",fstype=~"ext4|xfs"&#125; / node_filesystem_size_bytes&#123;mountpoint="/",fstype=~"ext4|xfs"&#125; * 100)# 后续这些语句要接入 Grafana 中使用图形展示 数据 PromSQL 获取系统服务运行状态123使用systemd收集器，需要再服务启动时候增加参数 --collector.systemd.unit-whitelist=".+" 从systemd中循环正则匹配单元--collector.systemd.unit-whitelist="(docker|sshd|nginx).service" 白名单，收集目标 1234567891011121314[root@k8s-node2 node_exporter]# vim /usr/lib/systemd/system/node_exporter.service [Unit]Description=node_exporter server daemon[Service]Restart=on-failureExecStart=/usr/local/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|kubelet|kube-proxy).service[Install]WantedBy=multi-user.target[root@k8s-node2 node_exporter]# systemctl daemon-reload[root@k8s-node2 node_exporter]# systemctl restart node_exporter 123node_systemd_unit_state&#123;name=“docker.service”&#125; # 只查询docker服务node_systemd_unit_state&#123;name=“docker.service”,state=“active”&#125; # 返回活动状态，值是1node_systemd_unit_state&#123;name=“docker.service”&#125; == 1 # 当前服务状态 123node_systemd_unit_state&#123;instance="172.31.228.53:9100",job="node",name="docker.service"&#125;node_systemd_unit_state&#123;instance="172.31.228.53:9100",job="node",name="docker.service"&#125; ==1node_systemd_unit_state&#123;instance="172.31.228.53:9100",job="node"&#125; == 1 Grafana 与 Prometheus 集成 Grafana是一个开源的度量分析和可视化系统 它本身不存储数据,只会展示数据库里面的数据 官网资料 12https://grafana.com/grafana/downloadhttps://grafana.com/dashboards/9276 Grafana 安装12# 容器安装[root@k8s-master2 prometheus]# docker run -d --name=grafana -p 3000:3000 grafana/grafana 12# web 默认用户名和密码 admin/admin 进去自己修改http://47.240.13.206:3000/login Grafana 添加数据源 Grafana 可视化展示Linux资源使用率自己添加数据 使用 grafana dashboard1234# 别人绘制好的 我们拿来用https://grafana.com/grafana/dashboards# 要使用数据源相应的图表版本 比如含有 prometheus node_exporter# 使用ID号码导入 监控 Docker 服务器 与 Grafana 可视化 cAdvisor（Container Advisor）用于收集正在运行的容器资源使用和性能信息。 和 node_exporter 一样暴露接口地址 官方资料: 12https://github.com/google/cadvisorhttps://grafana.com/dashboards/193 部署 cAdvisor12345678910111213141516# 部署镜像在本地 docker run \ --volume=/:/rootfs:ro \ --volume=/var/run:/var/run:ro \ --volume=/sys:/sys:ro \ --volume=/var/lib/docker/:/var/lib/docker:ro \ --volume=/dev/disk/:/dev/disk:ro \ --publish=8080:8080 \ --detach=true \ --name=cadvisor \ gcr.io/google-containers/cadvisor:latest [root@k8s-master2 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES05e96381c5d6 gcr.io/google-containers/cadvisor:latest "/usr/bin/cadvisor -…" 6 seconds ago Up 5 seconds (health: starting) 0.0.0.0:8080-&gt;8080/tcp cadvisorc439fe2d68aa grafana/grafana "/run.sh" 3 hours ago Up 39 minutes 0.0.0.0:3000-&gt;3000/tcp grafana 123# 访问接口 ip:8080# 会去采集这台服务器上所有docker容器的指标 http://47.240.13.206:8080/metrics cAdvisor 对接 prometheus12345678910[root@k8s-master2 ~]# vim /usr/local/prometheus/prometheus.yml # 把监控的url加入进来scrape_configs:... - job_name: 'docker' static_configs: - targets: ['172.31.228.51:8080'] [root@k8s-master2 prometheus]# ./promtool check config prometheus.yml [root@k8s-master2 prometheus]# kill -hup 1559 Grafana 数据展示1# 采集的都是容器的资源指标 123# 再启动一个 nginx 容器 [root@k8s-master2 prometheus]# docker run -d --name my-nginx nginx:1.165ac9c98f64a6103c8a44e0c3bc616c82ec0751e47649866f3513352177b2c883 监控 MySQL 服务器与 Grafana 可视化 mysql_exporter：用于收集MySQL性能信息 官方资料 12https://github.com/prometheus/mysqld_exporterhttps://grafana.com/dashboards/7362 下载 mysql_exporter1234567891011121314# 需要在要被监控的mysql服务主机上下载部署# 每个应用都由不同的采集exporter,暴露的端口也不同https://prometheus.io/download/# 版本要求Prometheus exporter for MySQL server metrics.Supported MySQL &amp; MariaDB versions: 5.5 and up.# 不是所有的采集都支持 小于 5.6版本NOTE: Not all collection methods are supported on MySQL/MariaDB &lt; 5.6[root@k8s-node2 opt]# wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.12.1/mysqld_exporter-0.12.1.linux-amd64.tar.gz[root@k8s-node2 opt]# tar -zxvf mysqld_exporter-0.12.1.linux-amd64.tar.gz [root@k8s-node2 opt]# mv mysqld_exporter-0.12.1.linux-amd64 /usr/local/mysqld_exporter[root@k8s-node2 opt]# cd /usr/local/mysqld_exporter/ 配置连接本地实例123456789101112131415161718192021[root@k8s-node2 mysqld_exporter]# mysqlWelcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 2Server version: 5.5.64-MariaDB MariaDB ServerCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; # 创建用户并授权MariaDB [(none)]&gt; CREATE USER 'exporter'@'localhost' IDENTIFIED BY '123456' ;MariaDB [(none)]&gt; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost';# 创建用户配置文件[root@k8s-node2 mysqld_exporter]# vim .my.cnf[client]user=exporterpassword=123456 启动 mysqld_exporter 服务12345678910111213141516171819202122./mysqld_exporter --config.my-cnf=.my.cnf # 增加系统服务配置[root@k8s-node2 local]# vim /usr/lib/systemd/system/mysqld_exporter.service [Unit]Description=mysqld_exporter server daemon[Service]Restart=on-failureExecStart=/usr/local/mysqld_exporter/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter/.my.cnf[Install]WantedBy=multi-user.target[root@k8s-node2 local]# ps -ef|grep mysqld_exporter[root@k8s-node2 local]# netstat -tnlp | grep 9104 tcp6 0 0 :::9104 :::* LISTEN 4286/mysqld_exporte # web http://47.240.1.178:9104/metrics 配置 prometheus 监控mysqld实例12345678[root@k8s-master2 prometheus]# vim prometheus.yml - job_name: 'mysql' static_configs: - targets: ['172.31.228.53:9104'][root@k8s-master2 prometheus]# ./promtool check config prometheus.yml [root@k8s-master2 prometheus]# kill -hup 1559 Grafana 数据展示12345https://grafana.com/dashboards/7362# 经常关注的点# 1. mysql连接数# 2. 增删改查次数# 3. 内存的使用环境 小总结1234561. 不管是监控node资源 还是 mysql服务,都需要先下载对应的 exporter2. 然后去 prometheus 配置文件中增加配置3. 使用模板接入到 Grafana4. 在Grafana中再去筛选想要的数据,和自定义监控点5. 进程监控 Process-exporter https://www.cnblogs.com/bigberg/p/10174222.html 报警神器 AlertManager部署 AlertManager 官网资料 1234地址1：https://prometheus.io/download/地址2：https://github.com/prometheus/alertmanager/releases1. 可以不在 prometheus 服务端继承 AlertManager2. 只要他们之间可以通信即可 12[root@k8s-master2 opt]# wget https://github.com/prometheus/alertmanager/releases/download/v0.20.0/alertmanager-0.20.0.linux-amd64.tar.gz[root@k8s-master2 opt]# tar -zxvf alertmanager-0.20.0.linux-amd64.tar.gz 12管理-报警媒介类型-Email1. QQ邮箱开启SNMP和一个授权码，填写发件人密码时需要设置授权码为密码 配置 Alertmanager 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# vim alertmanager.yml # 学习的时候 先把抑制 关闭了global: # 解析超时时间 resolve_timeout: 5m # 邮件客户端 smtp_smarthost: 'smtp.qq.com:465' smtp_from: '365042337@qq.com' smtp_auth_username: '365042337@qq.com' smtp_auth_password: 'jtebrxpgcuyjcafi' smtp_require_tls: falseroute: # 告警分配 group_by: ['alertname'] group_wait: 10s group_interval: 10s # 重复告警间隔 默认1小时 实际应用20-30分钟最好 repeat_interval: 1m # 接收着 receiver: 'mail'receivers: # 告警发送给谁- name: 'mail' email_configs: - to: '365042337@qq.com'#inhibit_rules:# 告警抑制# - source_match:# severity: 'critical'# target_match:# severity: 'warning'# equal: ['alertname', 'dev', 'instance'] 123456789# 检查语法[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# ./amtool check-config alertmanager.yml Checking 'alertmanager.yml' SUCCESSFound: - global config - route - 0 inhibit rules - 1 receivers - 0 templates 启动 alertmanager1[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# ./alertmanager --config.file=alertmanager.yml 配置 Prometheus 与 Alertmanager 通信1234567891011121314# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - 127.0.0.1:9093rule_files: # 告警规则文件 - "rules/*.yml" # - "second_rules.yml"# 检查配置文件语法[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml 在 Prometheus 中创建告警规则1234567891011121314151617181920212223242526[root@k8s-master2 prometheus]# mkdir rules# up 指标 判断所有job的状态# up = 1 是正常 = 0 是失败# [root@k8s-node2 local]# systemctl stop mysqld_exporter 故意停止一个mysql的 agent [root@k8s-master2 rules]# vim test.ymlgroups: # 一组监控- name: general.rules rules: # Alert for any instance that is unreachable for &gt;5 minutes. # 告警名称 - alert: InstanceDown expr: up == 0 # 告警持续时间 for: 1m labels: # 告警级别 severity: error # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; down 停止工作" description: "&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; 已经停止5分钟." 1234567# 重新加载配置 启动[root@k8s-master2 prometheus]# ./promtool check config prometheus.ymlChecking prometheus.yml SUCCESS: 1 rule files foundChecking rules/test.yml SUCCESS: 1 rules found 验证告警1# 我们已经配置了告警规则,而且有一个mysql的服务我们设置了错误状态 等待告警发送过来 再来关闭docker的客户端监控12345[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES05e96381c5d6 gcr.io/google-containers/cadvisor:latest "/usr/bin/cadvisor -…" 26 hours ago Up About an hour (healthy) 0.0.0.0:8080-&gt;8080/tcp cadvisorc439fe2d68aa grafana/grafana "/run.sh" 29 hours ago Up About an hour 0.0.0.0:3000-&gt;3000/tcp grafana[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# docker stop 05e96381c5d6 123# 新触发了一个告警# 监控状态 会随着告警时间 改变# 两个同类告警 会被合并 告警方式支持1https://prometheus.io/docs/alerting/configuration/ 告警状态 告警分配11. 告警分配 到指定接收组 告警收敛（分组，抑制，静默）123分组（group）： 将类似性质的警报分类为单个通知抑制（Inhibition）： 当警报发出后，停止重复发送由此警报引发的其他警报静默（Silences）： 是一种简单的特定时间静音提醒的机制 分组123456789101112131415161718192021222324252627# 10台服务器 如果都发生故障 通过分组 只发送一条通知# 分组的依据: [root@k8s-master2 alertmanager-0.20.0.linux-amd64]# vim /opt/alertmanager-0.20.0.linux-amd64/alertmanager.yml route: # 告警分配 默认 group_by: ['alertname'][root@k8s-master2 rules]# vim /usr/local/prometheus/rules/test.yml - alert: InstanceDown # 1. 减少告警消息的数量# 2. 同类告警聚合,方便排查问题route: # 告警分组依据 group_by: ['alertname'] # 分组发送等待时间 group_wait: 10s # 发送告警间隔 group_interval: 10s # 重复告警间隔 默认1小时 实际应用20-30分钟最好 repeat_interval: 20m # 接收者 receiver: 'mail' 抑制1231. 一个告警产生,其他告警无需再发2. 比如mysql的主从挂了 mysql服务器挂了 ,从层次来看, 服务器挂了的优先级更高,就不用去排查主从了3. 直接发送服务器挂了的通知,不发送主从的问题通知了 123456789101112[root@k8s-master2 alertmanager-0.20.0.linux-amd64]# vim /opt/alertmanager-0.20.0.linux-amd64/alertmanager.ymlinhibit_rules: 告警抑制 - source_match: # 告警的级别 critical 危险 severity: 'critical' target_match: # 被抑制的告警级别 warning severity: 'warning' # 必须 有这三个标签 equal: ['alertname', 'dev', 'instance'] 123456789101112131415161718# 磁盘 80%的使用率 warning # 服务器访问不到 critical # equal: ['alertname', 'dev', 'instance'] 标签, instance=服务器 id=mysql-a # 在这样一套机器上 出现了 两个状态 那么会发送 critical ,不会触发 warning的告警inhibit_rules: 告警抑制 - source_match: # 告警的级别 critical 危险 severity: 'critical' target_match: # 被抑制的告警级别 warning severity: 'warning' # 必须 有这三个标签 equal: ['id', 'instance']1. 消除冗余的告警2. 收到关键告警,方便排查 静默123451. 比如服务升级,一个时间内不用监控告警,可以配置静默减少,报警邮件2. 静默的配置页面 端口是 9093http://47.240.13.206:9093/#/alerts# 在这个页面添加 http://47.240.13.206:9093/#/silences 按照标签匹配,在一段时间内(维护时间),不设定告警邮件的发送 Prometheus一条告警怎么触发的 ？ 报警处理流程如下： Prometheus Server 监控目标主机上暴露的http接口（这里假设接口A），通过上述Promethes配置的’scrape_interval’定义的时间间隔，定期采集目标主机上监控数据。 当接口A不可用的时候，Server端会持续的尝试从接口中取数据，直到”scrape_timeout”时间后停止尝试。这时候把接口的状态变为“DOWN”。 Prometheus同时根据配置的”evaluation_interval”的时间间隔，定期（默认1min）的对Alert Rule进行评估；当到达评估周期的时候，发现接口A为DOWN，即UP=0为真，激活Alert，进入“PENDING”状态，并记录当前active的时间； 当下一个alert rule的评估周期到来的时候，发现UP=0继续为真，然后判断警报Active的时间是否已经超出rule里的‘for’ 持续时间，如果未超出，则进入下一个评估周期；如果时间超出，则alert的状态变为“FIRING”；同时调用Alertmanager接口，发送相关报警数据。 AlertManager收到报警数据后，会将警报信息进行分组，然后根据alertmanager配置的“group_wait”时间先进行等待。等wait时间过后再发送报警信息。 属于同一个Alert Group的警报，在等待的过程中可能进入新的alert，如果之前的报警已经成功发出，那么间隔“group_interval”的时间间隔后再重新发送报警信息。比如配置的是邮件报警，那么同属一个group的报警信息会汇总在一个邮件里进行发送。 如果Alert Group里的警报一直没发生变化并且已经成功发送，等待‘repeat_interval’时间间隔之后再重复发送相同的报警邮件；如果之前的警报没有成功发送，则相当于触发第6条条件，则需要等待group_interval时间间隔后重复发送。同时最后至于警报信息具体发给谁，满足什么样的条件下指定警报接收人，设置不同报警发送频率，这里有alertmanager的route路由规则进行配置。 1234567891011121314151617181920212223242526272829303132333435361. 采集数据# 全局配置 也 可以在 job中单独配置# my global configglobal: scrape_interval: 15s 采集周期 evaluation_interval: 15s 评估告警周期,看是否到达阈值 scrape_timeout is set to the global default (10s). 采集超时时间 2. prometheus 判断采集的数据 是否到达阈值 ，有判断时间[root@k8s-master2 rules]# vim /usr/local/prometheus/rules/test.yml - alert: InstanceDown expr: up == 0 # 告警持续时间 for: 1m labels: # 告警级别 severity: error # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; down 停止工作" description: "&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; 已经停止5分钟." 3. 如果1分钟持续=0 就发送给 alertmanager 先找分组: 这个也有等待时间 route: # 告警分组依据 group_by: ['alertname'] # 分组发送等待时间 group_wait: 10s # 发送告警间隔 group_interval: 10s # 重复告警间隔 默认1小时 实际应用20-30分钟最好 repeat_interval: 20m # 接收者 receiver: 'mail' 编写告警规则案例123# cpu使用率 要对所有的分区做统计# 先在浏览器测试通过再使用100 - (node_filesystem_free_bytes&#123;fstype=~"ext4|xfs"&#125; / node_filesystem_size_bytes&#123;fstype=~"ext4|xfs"&#125; * 100) 12345678910111213141516171819202122232425262728293031# 当前值: &#123;&#123; value &#125;&#125; [root@k8s-master2 etc]# cd /usr/local/prometheus/rules/[root@k8s-master2 rules]# cp test.yml node.yml[root@k8s-master2 prometheus]# vim rules/node.yml groups: # 一组监控- name: node.rules rules: # Alert for any instance that is unreachable for &gt;5 minutes. # 告警名称 - alert: NodeFilesystemUsage expr: 100 - (node_filesystem_free_bytes&#123;fstype=~"ext4|xfs"&#125; / node_filesystem_size_bytes&#123;fstype=~"ext4|xfs"&#125; * 100) &gt; 10 # 告警持续时间 for: 1m labels: # 告警级别 severity: warning # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; : &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用率过高" description: "&#123;&#123; $labels.instance &#125;&#125;: &#123;&#123; $labels.mountpoint &#125;&#125; 使用率大于80% (当前值: &#123;&#123; $value &#125;&#125;) "[root@k8s-master2 prometheus]# ./promtool check config prometheus.yml[root@k8s-master2 prometheus]# ps -ef|grep promeroot 1584 1 0 08:41 ? 00:00:04 /usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.ymlroot 3636 1624 0 09:02 pts/1 00:00:00 grep --color=auto prome[root@k8s-master2 prometheus]# kill -hup 1584 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 加上cpu和内存[root@k8s-master2 prometheus]# vi rules/node.yml groups: # 一组监控- name: node.rules rules: # Alert for any instance that is unreachable for &gt;5 minutes. # 告警名称 - alert: NodeFilesystemUsage expr: 100 - (node_filesystem_free_bytes&#123;fstype=~"ext4|xfs"&#125; / node_filesystem_size_bytes&#123;fstype=~"ext4|xfs"&#125; * 100) &gt; 10 # 告警持续时间 for: 1m labels: # 告警级别 severity: warning # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; : &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用率过高" description: "&#123;&#123; $labels.instance &#125;&#125;: &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用率大于80% (当前值: &#123;&#123; $value &#125;&#125;)" - alert: NodeCpuUsage expr: 100 - (avg(irate(node_cpu_seconds_total&#123;mode="idle"&#125;[5m])) by (instance) * 100) &gt; 1 # 告警持续时间 for: 1m labels: # 告警级别 severity: warning # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; : CPU使用过高 " description: "&#123;&#123; $labels.instance &#125;&#125;: CPU使用率大于60% (当前值: &#123;&#123; $value &#125;&#125;)" - alert: NodeMemUsage expr: 100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100 &gt; 10 # 告警持续时间 for: 1m labels: # 告警级别 severity: warning # 告警内容 annotations: summary: "Instance &#123;&#123; $labels.instance &#125;&#125; : 内存使用过高 " description: "&#123;&#123; $labels.instance &#125;&#125;: 内存使用率大于80% (当前值: &#123;&#123; $value &#125;&#125;)"]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13 k8s 配置管理]]></title>
    <url>%2F2019%2F12%2F31%2Fk8s-13%2F</url>
    <content type="text"><![CDATA[Secret 机密存储 加密数据并存放Etcd中，让Pod的容器以挂载Volume方式访问。 应用场景：凭据 官方文档: 1https://kubernetes.io/docs/concepts/configuration/secret/ 创建 Secret123456789101112131415161718192021222324[root@k8s-master1 demo]# echo -n 'admin' &gt; ./username.txt[root@k8s-master1 demo]# echo -n '1f2d1e2e67df' &gt; ./password.txt[root@k8s-master1 demo]# kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txtsecret/db-user-pass created[root@k8s-master1 demo]# kubectl get secretNAME TYPE DATA AGEdb-user-pass Opaque 2 27sdefault-token-h6969 kubernetes.io/service-account-token 3 36h# 数据安全[root@k8s-master1 demo]# kubectl describe secret db-user-passName: db-user-passNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====password.txt: 12 bytesusername.txt: 5 bytes 通过 yaml 文件创建1234567891011121314151617181920212223242526# 数据需要base64位编码[root@k8s-master1 demo]# echo -n 'admin' | base64YWRtaW4=[root@k8s-master1 demo]# echo -n '1f2d1e2e67df' | base64MWYyZDFlMmU2N2Rm[root@k8s-master1 demo]# vim secret.yamlapiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm[root@k8s-master1 demo]# kubectl create -f secret.yaml secret/mysecret created[root@k8s-master1 demo]# kubectl get secretNAME TYPE DATA AGEdb-user-pass Opaque 2 5m20smysecret Opaque 2 6s 使用 Secret通过变量传入 将secret里面的数据通过变量导入pod中 12345678910111213141516171819202122232425[root@k8s-master1 demo]# vim secret-var.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: nginx image: nginx env: - name: SECRET_USERNAME valueFrom: # 引用哪个secret，username的值传给SECRET_USERNAME，password的值传给SECRET_PASSWORD secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password[root@k8s-master1 demo]# kubectl apply -f secret-var.yaml pod/mypod created 1234567891011# 验证数据[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEmypod 1/1 Running 0 38s[root@k8s-master1 demo]# kubectl exec -it mypod bashroot@mypod:/# echo $SECRET_USERNAMEadminroot@mypod:/# echo $SECRET_PASSWORD1f2d1e2e67df Volume 形式挂载到pod目录下 会将secret里面的健值以文件挂载到目录下,key作为文件名，value作为内容 可以将敏感的数据创建secret交给k8s管理,等应用的使用再去使用 12345678910111213141516171819[root@k8s-master1 demo]# vim secret-vol.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: nginx image: nginx volumeMounts: - name: foo # 挂载到的路径 mountPath: "/etc/foo" readOnly: true volumes: - name: foo secret: secretName: mysecret 123456789101112[root@k8s-master1 demo]# kubectl delete -f secret-var.yaml[root@k8s-master1 demo]# kubectl apply -f secret-vol.yaml pod/mypod createdroot@mypod:/# cd /etc/foo/root@mypod:/etc/foo# lspassword usernameroot@mypod:/etc/foo# cat password 1f2d1e2e67dfroot@mypod:/etc/foo# cat username admin ConfigMap 配置文件存储 与Secret类似,区别在于ConfigMap保存的是不需要加密配置信息。 应用场景：应用配置 官方文档 1https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ 创建 ConfigMap1234[root@k8s-master1 demo]# vim redis.propertiesredis.hosts=127.0.0.1redis.port=6379redis.password=123456 12345678910111213141516171819[root@k8s-master1 demo]# kubectl create configmap redis-config --from-file=redis.properties configmap/redis-config created[root@k8s-master1 demo]# kubectl get configmap[root@k8s-master1 demo]# kubectl get cmNAME DATA AGEredis-config 1 5s# configmap 存储的是不需要加密的数据[root@k8s-master1 demo]# kubectl deseribe cm redis-config...Data====redis.properties:----redis.hosts=127.0.0.1redis.port=6379redis.password=123456 Volume 形式挂载到pod目录下12345678910111213141516171819202122[root@k8s-master1 demo]# vim cm-vol.yaml apiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: busybox image: busybox # redis.properties 名字 = 创建configmap的文件名 deseribe 可以看到 command: [ "/bin/sh","-c","cat /etc/config/redis.properties" ] volumeMounts: # 挂载到的目录 - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: # 指定挂载哪个cm name: redis-config restartPolicy: Never 12345678910111213[root@k8s-master1 demo]# kubectl apply -f cm-vol.yaml pod/mypod created[root@k8s-master1 demo]# kubectl get podNAME READY STATUS RESTARTS AGEmypod 0/1 Completed 0 19s# pod状态是执行完成，查看日志即可# 生产的时候 将文件 挂载到指定的目录下 让程序读取即可[root@k8s-master1 demo]# kubectl logs mypodredis.hosts=127.0.0.1redis.port=6379redis.password=123456 通过 yaml创建configmap123456789101112131415161718[root@k8s-master1 demo]# vim myconfig.yamlapiVersion: v1kind: ConfigMapmetadata: name: myconfig namespace: defaultdata: special.level: info special.type: hello[root@k8s-master1 demo]# kubectl apply -f myconfig.yaml configmap/myconfig created[root@k8s-master1 demo]# kubectl get cmNAME DATA AGEmyconfig 2 3sredis-config 1 15m 使用变量方式传入pod123456789101112131415161718192021222324[root@k8s-master1 demo]# vim cm-val.yamlapiVersion: v1kind: Podmetadata: name: mypodspec: containers: - name: busybox image: busybox # LEVEL 和 TYPE 是传入后的变量名,下面指定 command: [ "/bin/sh", "-c", "echo $(LEVEL) $(TYPE)" ] env: - name: LEVEL valueFrom: configMapKeyRef: name: myconfig key: special.level - name: TYPE valueFrom: configMapKeyRef: name: myconfig key: special.type restartPolicy: Never 123456789[root@k8s-master1 demo]# kubectl apply -f cm-val.yaml pod/mypod created[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEmypod 0/1 Completed 0 9s[root@k8s-master1 demo]# kubectl logs mypodinfo hello 总结 secret 保存机密数据,数据是加密的 configmap 保存配置数据,数据不加密 secret 可以保存连接harbor的配置 configmap 使用最多的是,管理应用程序的配置文件,通过Volume挂载到指定的程序目录下 configmap 可以代替配置中心]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09 基于 K8S 构建 Jenkins 微服务发布平台]]></title>
    <url>%2F2019%2F12%2F30%2Fk8s-base11%2F</url>
    <content type="text"><![CDATA[发布流程设计 拉取代码 编译 把jar包打到镜像里 部署到k8s平台,写yaml文件 暴露你的应用 准备基础环境 K8S（Ingress Controller，CoreDNS） Helm v3 Gitlab Harbor 并启用Chart存储功能 MySQL（微服务数据库） eureka 注册中心 K8S 环境coredns123456[root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-k5wl9 1/1 Running 12 6d18hkube-flannel-ds-amd64-2k5kz 1/1 Running 6 6d18hkube-flannel-ds-amd64-gvs6b 1/1 Running 12 6d18hkube-flannel-ds-amd64-hwglz 1/1 Running 13 6d18h ingress12345[root@k8s-master1 ~]# kubectl get pods -n ingress-nginxNAME READY STATUS RESTARTS AGEnginx-ingress-controller-2zvq5 1/1 Running 6 6d18hnginx-ingress-controller-9hq5n 1/1 Running 13 6d18hnginx-ingress-controller-k5vsm 1/1 Running 12 6d18h PV自动供给 NFS12345678910# NFS服务器,每个Node上安装nfs-utils包，用于mount挂载时用[root@k8s-node2 ~]# yum install nfs-utils[root@k8s-node2 ~]# vi /etc/exports/ifs/kubernetes *(rw,no_root_squash)[root@k8s-node2 ~]# mkdir -p /ifs/kubernetes[root@k8s-node2 ~]# systemctl daemon-reload[root@k8s-node2 ~]# systemctl start nfs[root@k8s-node2 ~]# systemctl enable nfs 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 由于K8S不支持NFS动态供给，还需要先安装上图中的nfs-client-provisioner插件[root@k8s-master1 opt]# unzip nfs-client.zip [root@k8s-master1 opt]# cd nfs-client[root@k8s-master1 nfs-client]# ls -ltotal 12-rw-r--r-- 1 root root 225 Jan 6 14:48 class.yaml # StorageClass 声明使用哪种存储插件，它来对接存储,设置好 StorageClass 名称-rw-r--r-- 1 root root 981 Jan 5 10:36 deployment.yaml # 该服务帮我们自动创建 pv-rw-r--r-- 1 root root 1526 Jan 5 10:36 rbac.yaml # 动态创建pv插件需要连接apiserver ，所以需要授权[root@k8s-master1 nfs-client]# vim deployment.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: apps/v1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: lizhenliang/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 172.31.228.53 - name: NFS_PATH value: /ifs/kubernetes volumes: - name: nfs-client-root nfs: server: 172.31.228.53 path: /ifs/kubernetes 123456789101112131415# 创建 pv 动态供给 [root@k8s-master1 nfs-client]# kubectl apply -f class.yaml [root@k8s-master1 nfs-client]# kubectl apply -f rbac.yaml [root@k8s-master1 nfs-client]# kubectl apply -f deployment.yaml [root@k8s-master1 nfs-client]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 47s# 查看自动创建pv的pod,当申请资源的时候这个pod会自动去创建pv[root@k8s-master1 nfs-client]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-769f87c8f6-7hn29 1/1 Running 0 51s 应用包管理器 Helm V3123[root@k8s-master1 opt]# wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz[root@k8s-master1 opt]# tar zxvf helm-v3.0.0-linux-amd64.tar.gz [root@k8s-master1 opt]# mv linux-amd64/helm /usr/bin/ 12345678910# 配置国内Chart仓库 [root@k8s-master1 opt]# helm repo add stable http://mirror.azure.cn/kubernetes/charts[root@k8s-master1 opt]# helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts [root@k8s-master1 opt]# helm repo listNAME URL stable http://mirror.azure.cn/kubernetes/charts aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts[root@k8s-master1 opt]# helm search repo mysql 12345678# 安装 push 插件helm plugin install https://github.com/chartmuseum/helm-push# 如果网络下载不了，也可以直接解压课件里包：tar zxvf helm-push_0.7.1_linux_amd64.tar.gzmkdir -p /root/.local/share/helm/plugins/helm-pushchmod +x bin/*mv bin plugin.yaml /root/.local/share/helm/plugins/helm-push 微服务数据库 MySQL12345678[root@k8s-master2]# yum -y install mariadb mariadb-server[root@k8s-master2]# systemctl start mariadb[root@k8s-master2]# systemctl enable mariadb[root@k8s-master2]# mysql_secure_installation# 授权访问用户[root@k8s-master2]# mysql -uroot -pMariaDB [(none)]&gt; grant all on *.* to 'root'@'%' identified by '123456'; 1234567891011121314151617181920212223242526272829# 将微服务数据库导入 [root@k8s-master1 db]# scp *.sql root@172.17.70.252:/tmpMariaDB [(none)]&gt; create database tb_order CHARACTER SET utf8mb4;MariaDB [(none)]&gt; create database tb_stock CHARACTER SET utf8mb4;MariaDB [(none)]&gt; create database tb_product CHARACTER SET utf8mb4; MariaDB [(none)]&gt; use tb_order;MariaDB [tb_order]&gt; source /tmp/order.sql;MariaDB [tb_order]&gt; use tb_stock;MariaDB [tb_stock]&gt; source /tmp/stock.sql;MariaDB [tb_stock]&gt; use tb_product;MariaDB [tb_product]&gt; source /tmp/product.sql;MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || tb_order || tb_product || tb_stock || test |+--------------------+7 rows in set (0.03 sec) 代码版本仓库 Gitlabdocker 部署 gitlab1234567891011121314151617[root@k8s-master2 pinpoint-docker]# mkdir -p /opt/gitlab[root@k8s-master2 pinpoint-docker]# cd /opt/gitlab# 数据目录挂载在本机,之后备份该目录即可docker run -d \ --name gitlab \ -p 8443:443 \ -p 9999:80 \ -p 9998:22 \ -v $PWD/config:/etc/gitlab \ -v $PWD/logs:/var/log/gitlab \ -v $PWD/data:/var/opt/gitlab \ -v /etc/localtime:/etc/localtime \ lizhenliang/gitlab-ce-zh:latest# 稍微等待几分钟启动后# 访问地址：http://IP:9999# 初次会先设置管理员密码 ，然后登陆，默认管理员用户名root，密码就是刚设置的。 镜像仓库 Harbor1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 之前部署的步骤,按照实际IP填写 # 该主机也需要二进制安装docker [root@k8s-master2]# scp root@172.17.70.254:/opt/src/docker-18.09.6.tgz .[root@k8s-master2]# scp root@172.17.70.254:/usr/lib/systemd/system/docker.service .[root@k8s-master2]# scp root@172.17.70.254:/etc/docker/daemon.json .[root@k8s-master2]# tar -zxvf docker-18.09.6.tgz [root@k8s-master2]# mv docker/* /usr/bin/[root@k8s-master2]# mv docker.service /usr/lib/systemd/system[root@k8s-master2]# mkdir -p /etc/docker[root@k8s-master2]# mv daemon.json /etc/docker/[root@k8s-master2]# cat /etc/docker/daemon.json &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], # 镜像加速 "insecure-registries": ["172.17.70.252"] # 可信任IP 回头换成harbor地址&#125;[root@k8s-node1 src]# systemctl start docker[root@k8s-node1 src]# systemctl enable docker[root@k8s-node1 src]# docker info# 上传文件[root@k8s-master2 src]# ls -l-rw-r--r-- 1 root root 17237024 Nov 13 14:33 docker-compose-Linux-x86_64-rw-r--r-- 1 root root 580462944 Nov 13 14:34 harbor-offline-installer-v1.8.4.tgz# 部署 compose[root@k8s-master2 src]# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose[root@k8s-master2 src]# chmod +x /usr/local/bin/docker-compose [root@k8s-master2 src]# docker-compose -versiondocker-compose version 1.25.0dev, build bc57a1bd# 部署 harbor[root@k8s-master2 src]# tar -xf harbor-offline-installer-v1.8.4.tgz -C /opt/[root@k8s-master2 src]# cd /opt/harbor/# 修改主机名和管理员密码、数据库密码hostname: 172.17.70.252 # httpharbor_admin_password: 123456 # 访问密码database: password: 123456# 准备[root@k8s-master2 src]# ./prepare# 安装 --with-chartmuseum 参数表示启用Charts存储功能。[root@k8s-master2 src]# ./install.sh --with-chartmuseum# web访问http://123.56.14.192# 列出docker-compose ps# 配置Docker可信任 由于habor未配置https，还需要在docker配置可信[root@Docker harbor]# vim /etc/docker/daemon.json # 写入进项仓库 IP+port&#123; "registry-mirrors": ["http://f1361db2.m.daocloud.io"], "insecure-registries": ["172.17.70.252"]&#125;# 重启docker systemctl daemon-reloadsystemctl restart docker.service[root@Docker nginx]# docker infoInsecure Registries: 172.17.70.252 127.0.0.0/8 在 Kubernetes 中部署 Jenkins 12参考：https://github.com/jenkinsci/kubernetes-plugin/tree/fc40c869edfd9e3904a9a56b0f80c5a25e988fa1/src/main/kubernetes Jenkins 需要持久化存储 K8S的pod不固定,所以需要持久化存储 pv,pvc 1234567891011121314151617[root@k8s-master1 opt]# unzip jenkins.zip Archive: jenkins.zip creating: jenkins/ inflating: jenkins/deployment.yml inflating: jenkins/ingress.yml inflating: jenkins/rbac.yml inflating: jenkins/service-account.yml inflating: jenkins/service.yml [root@k8s-master1 opt]# cd jenkins[root@k8s-master1 jenkins]# ls -l-rw-r--r-- 1 root root 1953 Jan 5 10:36 deployment.yml-rw-r--r-- 1 root root 349 Jan 5 10:36 ingress.yml-rw-r--r-- 1 root root 908 Jan 5 10:36 rbac.yml # 授权 k8s-apiserver -rw-r--r-- 1 root root 914 Jan 5 10:36 service-account.yml # 访问 k8s-apiserver 调度创建pod-rw-r--r-- 1 root root 270 Jan 5 10:36 service.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@k8s-master1 jenkins]# cat deployment.yml apiVersion: apps/v1kind: Deployment metadata: name: jenkins labels: name: jenkinsspec: replicas: 1 selector: matchLabels: name: jenkins template: metadata: name: jenkins labels: name: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins containers: - name: jenkins # 官方长期维护版本 image: jenkins/jenkins:lts imagePullPolicy: Always ports: # 8080 ui # 50000 slave访问master端口 - containerPort: 8080 - containerPort: 50000 resources: limits: cpu: 1 memory: 1Gi requests: cpu: 0.5 memory: 500Mi env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 volumeMounts: - name: jenkins-home # 挂载点 mountPath: /var/jenkins_home livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 securityContext: fsGroup: 1000 volumes: # 数据卷 - name: jenkins-home persistentVolumeClaim: # pvc claimName: jenkins-home---apiVersion: v1kind: PersistentVolumeClaimmetadata: # 自动供给 name: jenkins-homespec: storageClassName: "managed-nfs-storage" accessModes: ["ReadWriteOnce"] resources: requests: storage: 5Gi 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 jenkins]# cat service.yml apiVersion: v1kind: Servicemetadata: name: jenkinsspec: selector: name: jenkins type: NodePort ports: - name: http port: 80 targetPort: 8080 protocol: TCP nodePort: 30006 - name: agent port: 50000 protocol: TCP[root@k8s-master1 jenkins]# cat ingress.yml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jenkins annotations: nginx.ingress.kubernetes.io/ssl-redirect: "true" nginx.ingress.kubernetes.io/proxy-body-size: 100mspec: rules: - host: jenkins.ctnrs.com http: paths: - path: / backend: serviceName: jenkins servicePort: 80 1234567891011121314151617181920212223242526# 创建[root@k8s-master1 jenkins]# kubectl apply -f .[root@k8s-master1 jenkins]# kubectl get pods,svc,pv,pvc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/jenkins-557cf6fd7f-nvwbp 1/1 Running 0 10m 10.244.1.38 k8s-node1 &lt;none&gt; &lt;none&gt;pod/nfs-client-provisioner-769f87c8f6-7hn29 1/1 Running 0 99m 10.244.2.26 k8s-master1 &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/jenkins NodePort 10.0.0.47 &lt;none&gt; 80:30006/TCP,50000:31324/TCP 10m name=jenkinsservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 6d20h &lt;none&gt;NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpersistentvolume/pvc-ffe668d2-1fd1-403d-87c6-739ccf6bd6ee 5Gi RWO Delete Bound default/jenkins-home managed-nfs-storage 10m FilesystemNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODEpersistentvolumeclaim/jenkins-home Bound pvc-ffe668d2-1fd1-403d-87c6-739ccf6bd6ee 5Gi RWO managed-nfs-storage 10m Filesystem# 查看日志[root@k8s-master1 jenkins]# kubectl logs jenkins-557cf6fd7f-nvwbp -f# 初始化密码 启动日志中查看到 0791683090e046489635805ade39f81c# web访问http://47.240.14.16:30006/ 123# 修改时区 # 打开 【系统管理】-&gt;【脚本命令行】运行下面的命令System.setProperty('org.apache.commons.jelly.tags.fmt.timeZone', 'Asia/Shanghai') Jenkins PipelineJenkins Pipeline 及参数化构建 Jenkins Pipeline是一套插件，支持在Jenkins中实现集成和持续交付管道； Pipeline通过特定语法对简单到复杂的传输管道进行建模； 声明式：遵循与Groovy相同语法。pipeline { } 脚本式：支持Groovy大部分功能，也是非常表达和灵活的工具。node { } Jenkins Pipeline的定义被写入一个文本文件，称为Jenkinsfile。 参考 1https://jenkins.io/doc/book/pipeline/syntax/ 安装插件 Jenkins Pipeline1234567891011121314# 修改国内插件源# 修改实际下载插件地址# 在持久化目录中 [root@k8s-node2 kubernetes]# cd default-jenkins-home-pvc-ffe668d2-1fd1-403d-87c6-739ccf6bd6ee/[root@k8s-node2 default-jenkins-home-pvc-ffe668d2-1fd1-403d-87c6-739ccf6bd6ee]# cd updates # sed 批量替换# 替换清华源下载插件地址sed -i 's/http:\/\/updates.jenkins-ci.org\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g' default.json# 替换URL检查地址sed -i 's/http:\/\/www.google.com/https:\/\/www.baidu.com/g' default.json# 页面重启 jenkins 服务http://47.240.14.16:30006/restart YES 12345# 安装插件 http://47.240.14.16:30006/pluginManager/# 选择可用插件 搜索 Pipelinehttp://47.240.14.16:30006/pluginManager/available# 安装插件完成后 创建job出现 流水线 为正常 创建测试 Pipeline Pipeline 的脚本语法为 Groovy 选择一个声明式的用例粘贴出来去修改 1234567891011121314151617181920212223242526272829# 基础语法# 1. stages: 大步骤,拉取代码 到编译、发布 每一步都为步骤# 2. stage: 小步骤,卸载一个大步骤里# 3. steps: 小步骤里面的小单元，具体任务的分节pipeline &#123; agent any stages &#123; stage('1. 拉取代码') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('2. 代码编译') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('3. 单元测试') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; 参数化构建1234567# 多个项目部署, 套用一个pipeline，找出不同点1. git地址2. 分支名3. 部署的机器4. 打出的包名# 知道不同点,那么可以使用参数化构建 流水线生成 git 项目地址12341. 测试生成 git项目地址 选择框2. 复制代码到 pipeline 3. 保存后先构建一次,再进入项目会看到 构建+参数选项4. 在这里就可以选择 发布项目的git地址 123456789pipeline &#123; agent any parameters &#123; choice choices: ['172.31.228.51:9999/root/a.git', '172.31.228.51:9999/root/b.git', '172.31.228.51:9999/root/c.git'], description: '请选择要发布的项目git地址', name: 'git' &#125; stages &#123; ...&#125; 流水线生成 分支名12# 动态参数化构建# 分支名需要动态的从选择的 git地址里面 后去所有分支 流水线生成 部署的机器1234567pipeline &#123; agent any parameters &#123; choice choices: ['172.31.228.51:9999/root/a.git', '172.31.228.51:9999/root/b.git', '172.31.228.51:9999/root/c.git'], description: '请选择要发布的项目git地址', name: 'git' choice choices: ['172.31.228.50', '172.31.228.52', '172.31.228.53'], description: '请选择要发布到的服务器', name: 'host' &#125; ... 脚本中获取参数12# 通过设置的Name 就是变量名获取值# pipeline 脚本中直接获取 12345678910111213141516171819202122232425262728293031323334pipeline &#123; agent any parameters &#123; choice choices: ['172.31.228.51:9999/root/a.git', '172.31.228.51:9999/root/b.git', '172.31.228.51:9999/root/c.git'], description: '请选择要发布的项目git地址', name: 'git' choice choices: ['172.31.228.50', '172.31.228.52', '172.31.228.53'], description: '请选择要发布到的服务器', name: 'host' &#125; stages &#123; stage('1. 拉取代码') &#123; steps &#123; echo "$&#123;git&#125;" &#125; &#125; stage('2. 代码编译') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('3. 单元测试') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('4. 部署') &#123; steps &#123; echo "$&#123;host&#125;" &#125; &#125; &#125;&#125; Jenkins 在 Kubernetes 中动态创建代理Jenkins Master/Slave架构 上面的例子中,所有的动态创建都是在Jenkins服务器中处理完成 我们的Jenkins部署在了k8s里,所以是pod里的Jenkins服务 处理的编辑构建等操作 如果我们的项目很多,每天都做持续继承的频率也很高,面对大批量的任务处理,1个pod是很难支持的 所以我们需要使用 Jenkins Master/Slave架构 , Master负责任务的分配,Slave完成job任务 Slave 之前是作为一个虚拟机或者物理机存在,它与Master保持通信获取任务,解决 Jenkins 服务问题 1234[root@k8s-master1 jenkins]# kubectl get podsNAME READY STATUS RESTARTS AGEjenkins-6459665769-2dzp7 1/1 Running 0 97mnfs-client-provisioner-769f87c8f6-sdk9q 1/1 Running 0 118m 架构说明 传统的Jenkins部署完成后, 再在其他的虚拟机上部署几个Jenkins 做为Slave ，在页面的管理节点中配置好 当点击job构建的时候 Master将任务分配给Slave去完成,Master 本身没有太大工作压力 当用k8s环境的时候,Slave也应该作为pod运行在k8s中，Master的任务交给pod中的Slave去完成 考虑到两种运行方式,预先启动和动态创建，预先创建也没有问题,但是会一直消耗资源 动态创建Slave，当执行任务的时候,master来组织创建Slave pod ,然后在把任务交给Slave去完成,完成后销毁pod 即开即用，节省资源 动态创建代理 Kubernetes插件：Jenkins在Kubernetes集群中运行动态代理 1插件介绍：https://github.com/jenkinsci/kubernetes-plugin 安装插件 如果想动态的在k8s中创建Slave pod 需要配置连接参数 连接的k8s地址 连接的Jenkins地址 配置插件1234561. 系统配置最下面2. 首先 Jenkins Master 是运行在k8s里面的,作为一个pod 他本身就可以访问到k8s下面的pod资源3. 他连接k8s地址可以用svc名字访问,dns可以解析到4. Kubernetes 地址: https://kubernetes.default 5. 如果 Jenkins 是部署在外面 需要连接k8s的外部地址 https://172.31.228.50:6443 并且需要添加CA证书和凭据6. Jenkins 地址 ：http://jenkins.default 同样适用svc地址 也可以使用当前页面的地址+端口 1234[root@k8s-master1 jenkins]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEjenkins NodePort 10.0.0.51 &lt;none&gt; 80:30006/TCP,50000:30523/TCP 144mkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 7d14h 自定义构建 Jenkins Slave 镜像12341. 现在 jenkins 已经知道连接 k8s了，现在要考虑如何创建pod代理2. 创建pod需要 jenkins Slave 镜像 3. k8s通过这个镜像拉起 jenkins Slave参考：https://github.com/jenkinsci/docker-jnlp-slave 123456# 制作镜像需要考虑的事# 镜像中要完成: 代码拉取 代码编译 单元测试 构建镜像 1. 什么开发语言,不同的开发环境 java - mvn ， go语言编译 和 python环境 2. 构建镜像 docker3. 部署到k8s helm4. 推送镜像 12345# java 为例1. 代码编译: maven jdk 2. 打包镜像: docker 3. 持续部署: helm4. slave的agent：slave.jar 1234# 镜像 如何 作为slave存在 1. 传统方式就是在 jenkins 管理页面添加,添加后就可以连接到slave节点,启动agent与master交互2. 每个slave上有个agent jar包 ，会与master实时通信，kill jar包就会不可用3. 镜像中必须包含 slave的agent ，传统方式会自动安装的，镜像里面需要自己安装，然后才能连接master，得到下发任务 12345678910[root@k8s-master1 opt]# unzip jenkins-slave.zip [root@k8s-master1 opt]# cd jenkins-slave[root@k8s-master1 jenkins-slave]# ls -l-rw-r--r-- 1 root root 435 Dec 26 15:49 Dockerfile-rwxr-xr-x 1 root root 37818368 Nov 13 21:39 helm-rw-r--r-- 1 root root 1980 Nov 24 14:56 jenkins-slave-rwxr-xr-x 1 root root 46677376 Dec 26 15:43 kubectl-rw-r--r-- 1 root root 10409 Nov 24 14:56 settings.xml-rw-r--r-- 1 root root 770802 Nov 24 14:56 slave.jar 123456789101112131415161718192021# docker环境,使用数据卷挂载即可，pod在node上启动,node上都有docker ，docker in docker# kubectl 连接 k8s 需要认证信息 也就是客户端访问# 参考：https://github.com/jenkinsci/docker-jnlp-slave[root@k8s-master1 jenkins-slave]# vim Dockerfile FROM centos:7LABEL maintainer lizhenliangRUN yum install -y java-1.8.0-openjdk maven curl git libtool-ltdl-devel &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/* &amp;&amp; \ mkdir -p /usr/share/jenkinsCOPY slave.jar /usr/share/jenkins/slave.jar # agent jar包COPY jenkins-slave /usr/bin/jenkins-slave # shell脚本 管理启动 slave.jar 他们都是官方提供的COPY settings.xml /etc/maven/settings.xml # RUN chmod +x /usr/bin/jenkins-slaveCOPY helm kubectl /usr/bin/ # helm 二进制程序 用来部署k8s,kubectl 用来 连接k8s 查看资源状态等 操作k8s部署删除ENTRYPOINT ["jenkins-slave"] 1234567# 构建镜像[root@k8s-master1 jenkins-slave]# docker build -t jenkins-slave:jdk-1.8 .# 推送到harbor公共仓库中[root@k8s-master1 jenkins-slave]# docker tag jenkins-slave:jdk-1.8 172.31.228.51/library/jenkins-slave:jdk-1.8[root@k8s-master1 jenkins-slave]# docker login 172.31.228.51[root@k8s-master1 jenkins-slave]# docker push 172.31.228.51/library/jenkins-slave:jdk-1.8 测试动态创建 slave1234567891. 生成创建pod 2. jenkins pipeline 创建pod slave 3. - name: jnlp 是固定的4. 打印hostname 查看job是不是在pod中的slave运行# 参考: https://plugins.jenkins.io/kubernetes -&gt; Declarative Pipeline# 会拉取镜像 jenkins/jnlp-slave 3.35-5-alpine ccf03f44972a 3 months ago 141MB# 同时运行两个构建 在不同的node上创建了pod去执行任务# label 'jenkins-slave' 需要变化,不能同一时间运行同一个 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748pipeline &#123; agent &#123; kubernetes &#123; label 'jenkins-slave' yaml """kind: Podmetadata: name: jenkins-slavespec: containers: - name: jnlp image: 172.31.228.51/library/jenkins-slave:jdk-1.8""" &#125;&#125; parameters &#123; choice choices: ['172.31.228.51:9999/root/a.git', '172.31.228.51:9999/root/b.git', '172.31.228.51:9999/root/c.git'], description: '请选择要发布的项目git地址', name: 'git' choice choices: ['172.31.228.50', '172.31.228.52', '172.31.228.53'], description: '请选择要发布到的服务器', name: 'host' &#125; stages &#123; stage('1. 拉取代码') &#123; steps &#123; echo "$&#123;git&#125;" &#125; &#125; stage('2. 代码编译') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('3. 单元测试') &#123; steps &#123; echo 'Hello World' &#125; &#125; stage('4. 部署') &#123; steps &#123; echo "$&#123;host&#125;" sh "hostname" &#125; &#125; &#125;&#125; Pipeline 集成 Helm 发布微服务项目 k8s 动态创建 jenkins slave pod jenkins slave pod 完成 拉取代码 -&gt; 代码编译 -&gt; 单元测试 -&gt; 构建镜像 -&gt; Helm部署到k8s 需要的插件 Git Parameter # 动态从git中后去所有分支 Git # 拉取代码 Pipeline # Pipeline Config File Provider # 将配置文件存放在 jenkins 里 让Pipeline引用,放到slave pod 中 (kubectl 配置文件 kubeconfig), 直接放到镜像中不太安全 kubernetes # 动态创建代理 Extended Choice Parameter # 扩展参数可选配置 (多选) 12345678910# 微服务 需要选择的点1. 分支 2. 微服务名称3. 端口4. 命名空间5. 副本数6. chart模板 # 一般方法 : dp.yaml svc.yaml ing.yaml configmap.yaml 一套,使用sed 's###g' xx.yaml 替换# helm : 一键部署/卸载 模板化部署 传参不同的环境 准备chart目录1234567[root@k8s-master1 opt]# tar -xf ms-0.1.0.tgz [root@k8s-master1 opt]# cd ms[root@k8s-master1 ms]# ls -ltotal 12-rw-r--r-- 1 root root 101 Jan 1 1970 Chart.yamldrwxr-xr-x 2 root root 4096 Jan 7 15:16 templates-rw-r--r-- 1 root root 638 Jan 1 1970 values.yaml 12345678# 添加 repo [root@k8s-master1 ms]# helm repo add --username admin --password lx@68328153 myrepo http://172.31.228.51/chartrepo/library[root@k8s-master1 ms]# helm repo listNAME URL stable http://mirror.azure.cn/kubernetes/charts aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartsmyrepo http://172.31.228.51/chartrepo/library 1234# 推送 与 安装Chart[root@k8s-master1 opt]# helm push ms-0.1.0.tgz --username=admin --password=lx@68328153 http://172.31.228.51/chartrepo/microservicePushing ms-0.1.0.tgz to http://172.31.228.51/chartrepo/microservice...Done. 准备 git 项目 和 源代码1gitlab创建ms项目 12345678910111213141516171819202122232425262728# 拉取项目到本地 [root@k8s-master1 microservic-code]# cd /root/microservic-code[root@k8s-master1 microservic-code]# git clone http://47.240.13.206:9999/root/ms.git# 将dev3分支的代码 拷贝到ms下[root@k8s-master1 microservic-code]# cp -rf simple-microservice-dev3/* ms/[root@k8s-master1 microservic-code]# ls -l ms# 上传代码 提交缓存区[root@k8s-master1 microservic-code]# cd ms/[root@k8s-master1 ms]# git add .[root@k8s-master1 ms]# git config --global user.email "365042337@.qq.com"[root@k8s-master1 ms]# git config --global user.name "touchlixiang"[root@k8s-master1 ms]# git commit -m 'all'[root@k8s-master1 ms]# git push origin masterUsername for 'http://47.240.13.206:9999': rootPassword for 'http://root@47.240.13.206:9999': Counting objects: 416, done.Delta compression using up to 2 threads.Compressing objects: 100% (312/312), done.Writing objects: 100% (416/416), 758.09 KiB | 0 bytes/s, done.Total 416 (delta 55), reused 0 (delta 0)remote: Resolving deltas: 100% (55/55), done.To http://47.240.13.206:9999/root/ms.git * [new branch] master -&gt; master 修改 Pipeline 默认变量12def registry = "172.31.228.51"def git_url = "http://172.31.228.51:9999/root/ms.git" 创建凭据连接 harbor 连接 gitlab 修改 Pipeline 凭据变量1231. 这两个凭证 点击更新 复制ID def harbor_registry_auth = "e2666836-7e4a-4d25-a4f0-f97a7711cda7"def git_auth = "70ab1d82-b8cb-4dcd-a520-9b257daaf44f" 把所有的插件 安装好1所需插件: Git Parameter/Git/Pipeline/Config File Provider/kubernetes/Extended Choice Parameter kubeconfig 凭据 12345678910111213141516171819202122232425262728# 修改 Pipeline 凭据变量 def k8s_auth = "c3e46862-a03a-4f8b-91ff-7a0c8f86b796"# 需要kubeconfi配置文件# 二进制部署 需要单独生成 使用admin证书[root@k8s-master1 k8s]# cd /root/ansible-k8s-deploy/ssl/k8s/[root@k8s-master1 k8s]# vim kubeconfig.sh# 设置集群参数 kubectl config set-cluster kubernetes --server=https://172.31.228.50:6443 --embed-certs=true --certificate-authority=ca.pem --kubeconfig=config# 设置客户端认证参数kubectl config set-credentials cluster-admin --certificate-authority=ca.pem --embed-certs=true --client-key=admin-key.pem --client-certificate=admin.pem --kubeconfig=config# 设置上下文参数kubectl config set-context default --cluster=kubernetes --user=cluster-admin --kubeconfig=config# 设置当前环境的defaultkubectl config use-context default --kubeconfig=config[root@k8s-master1 k8s]# bash kubeconfig.sh Cluster "kubernetes" set.User "cluster-admin" set.Context "default" created.Switched to context "default".# 将生成的 config里面的所有内容 复制到 k8s_auth 的 Configuration File[root@k8s-master1 k8s]# cat config 创建新的流水线 ms1234567891011121314151617# 别忘记修改配置文件的变量 再确认一次#!/usr/bin/env groovy// 所需插件: Git Parameter/Git/Pipeline/Config File Provider/kubernetes/Extended Choice Parameter// 公共def registry = "172.31.228.51"// 项目def project = "microservice"def git_url = "http://172.31.228.51:9999/root/ms.git"def gateway_domain_name = "gateway.ctnrs.com"def portal_domain_name = "portal.ctnrs.com"// 认证def image_pull_secret = "registry-pull-secret"def harbor_registry_auth = "e2666836-7e4a-4d25-a4f0-f97a7711cda7"def git_auth = "70ab1d82-b8cb-4dcd-a520-9b257daaf44f"// ConfigFileProvider IDdef k8s_auth = "c3e46862-a03a-4f8b-91ff-7a0c8f86b796"... 12345# 将文件内容粘贴进入流水线# 第一次构建预配置# 第二次通过参数化构建 什么都不选 再让构建跑一次 看看能不能获取到所有分支# 纯写 Pipeline 前1、2次会有些问题,要读取参数# 第二次 我没有选择任何要构建的服务,运行的时候也走到了编译步骤，不会有任何部署，出现编译问题再重新试试 12345[root@k8s-master1 k8s]# kubectl get podsNAME READY STATUS RESTARTS AGEjenkins-6459665769-2dzp7 1/1 Running 0 8hjenkins-slave-8810d-rffzp 1/1 Running 0 10snfs-client-provisioner-769f87c8f6-sdk9q 1/1 Running 0 8h 创建新的分支 1# 修改下镜像的地址 安装时间服务太慢使用老师的镜像仓库下的镜像 部署一个服务测试 12345678[root@k8s-master1 k8s]# kubectl get pods -n ms -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESeureka-0 1/1 Running 0 9h 10.244.2.28 k8s-master1 &lt;none&gt; &lt;none&gt;eureka-1 1/1 Running 3 30h 10.244.1.42 k8s-node1 &lt;none&gt; &lt;none&gt;eureka-2 1/1 Running 2 30h 10.244.0.52 k8s-node2 &lt;none&gt; &lt;none&gt;ms-gateway-service-6bb588dbdd-tbsrb 1/1 Running 0 91s 10.244.0.55 k8s-node2 &lt;none&gt; &lt;none&gt;# eureka 查看注册 123[root@k8s-master1 k8s]# helm ls -n msNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONgateway-service ms 1 2020-01-07 09:19:28.140796979 +0000 UTC deployed ms-0.1.0 0.1.0 1# 没问题的话 把剩下的服务 一起部署 12345678910[root@k8s-master1 templates]# kubectl get pods -n ms -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESeureka-0 1/1 Running 0 9h 10.244.2.28 k8s-master1 &lt;none&gt; &lt;none&gt;eureka-1 1/1 Running 3 30h 10.244.1.42 k8s-node1 &lt;none&gt; &lt;none&gt;eureka-2 1/1 Running 2 30h 10.244.0.52 k8s-node2 &lt;none&gt; &lt;none&gt;ms-gateway-service-6bb588dbdd-tbsrb 1/1 Running 0 17m 10.244.0.55 k8s-node2 &lt;none&gt; &lt;none&gt;ms-order-service-565fb84987-pzgt9 1/1 Running 0 2m12s 10.244.0.56 k8s-node2 &lt;none&gt; &lt;none&gt;ms-portal-service-777596f65-hhcjv 1/1 Running 0 2m16s 10.244.2.45 k8s-master1 &lt;none&gt; &lt;none&gt;ms-product-service-6987b969-s6xbm 1/1 Running 0 2m16s 10.244.2.46 k8s-master1 &lt;none&gt; &lt;none&gt;ms-stock-service-5b5b88865d-r9kjv 1/1 Running 0 2m2s 10.244.2.47 k8s-master1 &lt;none&gt; &lt;none&gt; 手动删除所有服务12345678910# helm 删除 helm ls -n mshelm uninstall gateway-service -n mshelm uninstall portal-service -n mshelm uninstall order-service -n mshelm uninstall stock-service -n mshelm uninstall product-service -n ms[root@k8s-master1 ms]# kubectl get pods,svc,ing -n ms 12345678# 我修改了数据库连接地址 ms项目中的 重新Git提交 再打包一次[root@k8s-master1 ms]# git add .[root@k8s-master1 ms]# git commit -m 'all 2'[root@k8s-master1 ms]# git push origin master# 再把master的容器镜像修改一下 再重新部署看看# 我刚才页面的问题是我代码里的 连接数据库地址没有改 再上线前一定要做好检查工作 回滚思路1231. 重新部署镜像 2. jenkins 中获取 镜像仓库的镜像 tag , 脚本动态从harbor api 获取某个API下的所有镜像列表,3. 拿到镜像列表选择 要回滚的镜像版本 重新部署]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08 微服务链路监控系统]]></title>
    <url>%2F2019%2F12%2F30%2Fk8s-base10%2F</url>
    <content type="text"><![CDATA[全链路监控是什么 全链路监控解决什么问题 请求链路追踪：通过分析服务调用关系，绘制运行时拓扑信息，可视化展示 调用情况衡量：各个调用环节的性能分析，例如吞吐量、响应时间、错误次数 容器规划参考：扩容/缩容、服务降级、流量控制 运行情况反馈：告警，通过调用链结合业务日志快速定位错误信息 全链路监控选择依据全链路监控系统有很多，应从这几方面选择： 探针的性能消耗APM组件服务的影响应该做到足够小，数据分析要快，性能占用小。 代码的侵入性即也作为业务组件，应当尽可能少入侵或者无入侵其他业务系统，对于使用方透明，减少开发人员的负担。 监控维度分析的维度尽可能多。 • 可扩展性一个优秀的调用跟踪系统必须支持分布式部署，具备良好的可扩展性。能够支持的组件越多当然越好。 主流系统：zipkin、skywalking、pinpoint Pinpoint 介绍 Pinpoint是一个APM（应用程序性能管理）工具，适用于用Java/PHP编写的大型分布式系统。 特性: 服务器地图（ServerMap）通过可视化分布式系统的模块和他们之间的相互联系来理解系统拓扑。点击某个节点会,展示这个模块的详情，比如它当前的状态和请求数量。 实时活动线程图 （Realtime Active Thread Chart）: 实时监控应用内部的活动线程。 请求/响应分布图（ Request/Response Scatter Chart ）: 长期可视化请求数量和应答模式来定位潜在问题。通过在图表上拉拽可以选择请求查看 更多的详细信息 调用栈（ CallStack :在分布式环境中为每个调用生成代码级别的可视图，在单个视图中定位瓶颈和失败点。 检查器（ Inspector ）: 查看应用上的其他详细信息，比如CPU使用率，内存/垃圾回收，TPS，和JVM参数。 Docker 部署 Pinpoint1234567git clone https://github.com/naver/pinpoint-docker.gitcd pinpoint-dockerdocker-compose pull &amp;&amp; docker-compose up -d# 官方文档https://github.com/naver/pinpoint-docker# 网络太慢的话 可以先下载好包 1234567891011[root@k8s-master2 opt]# yum install git[root@k8s-master2 opt]# git clone https://github.com/naver/pinpoint-docker.git[root@k8s-master2 opt]# cd pinpoint-docker/[root@k8s-master2 pinpoint-docker]# docker-compose pull[root@k8s-master2 pinpoint-docker]# docker-compose up -d[root@k8s-master2 pinpoint-docker]# docker-compose ps# 查看web 服务端可以访问到界面即正常http://47.240.13.206:8079/ http://47.240.13.206:8000/ quickstart 访问后会有测试数据 Pinpoint Agent 部署探针 随着服务一起启动 12345# Tomcat:# catalina.shCATALINA_OPTS="$CATALINA_OPTS -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar" # AGENT_PATH 路径CATALINA_OPTS="$CATALINA_OPTS -Dpinpoint.agentId=$AGENT_ID" # 区分实例ID 不唯一CATALINA_OPTS="$CATALINA_OPTS -Dpinpoint.applicationName=$APPLICATION_NAME“ # app 名称 哪个应用 12# Jar:java -jar -javaagent:$AGENT_PATH/pinpoint-bootstrap-$VERSION.jar -Dpinpoint.agentId=$AGENT_ID -Dpinpoint.applicationName=$APPLICATION_NAME xxx.jar JAR包下载地址 微服务接入Pinpoint服务启动时指定 Pinpoint1234567891011121314151617181920# 每个组件 都需要配置指定 [root@k8s-master1 microservic-code]# cd simple-microservice-dev4[root@k8s-master1 simple-microservice-dev4]# cd portal-service/# pinpoint 下载好后和项目放在同一目录,Dockerfile会copy到容器中[root@k8s-master1 portal-service]# lsDockerfile pinpoint pom.xml src# 启动在Dockerfile里面指定[root@k8s-master1 portal-service]# vim Dockerfile FROM java:8-jdk-alpineLABEL maintainer lizhenliang/www.ctnrs.comRUN apk add -U tzdata &amp;&amp; \ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/portal-service.jar ./COPY pinpoint /pinpointEXPOSE 8080CMD java -jar -javaagent:/pinpoint/pinpoint-bootstrap-1.8.3.jar -Dpinpoint.agentId=$&#123;HOSTNAME&#125; -Dpinpoint.applicationName=ms-protal /portal-service.jar 1234567CMD java -jar -javaagent:/pinpoint/pinpoint-bootstrap-1.8.3.jar # 路径 /pinpoint -Dpinpoint.agentId=$(echo $HOSTNAME | awk -F- '&#123;print "portal-"$NF&#125;') # portal + 容器名截取最后一位 与POD对应上 -Dpinpoint.applicationName=ms-portal # 项目名 $JAVA_ARGS $JAVA_OPTS /portal-service.jar 12345# 修改配置文件的 服务端地址[root@k8s-master1 portal-service]# vim pinpoint/pinpoint.config ...profiler.collector.ip=172.31.228.51... 重新部署 查看采集情况12# eureka 属于基础组件 无需每次都重新启动配置 一般都是手动部署 发布的时候不要重新部署# 本次重新部署 是因为增加了 链路监控 1234567891011121314151617181920212223242526272829[root@k8s-master1 k8s]# cat docker_build.sh #!/bin/bashdocker_registry=172.31.228.51# kubectl create secret docker-registry registry-pull-secret --docker-server=$docker_registry --docker-username=admin --docker-password=Harbor12345 --docker-email=admin@ctnrs.com -n ms# kubectl create secret docker-registry registry-pull-secret --docker-server=172.31.228.51 --docker-username=admin --docker-password=lx@68328153 --docker-email=admin@ctnrs.com -n msservice_list="eureka-service gateway-service order-service product-service stock-service portal-service"service_list=$&#123;1:-$&#123;service_list&#125;&#125;work_dir=$(dirname $PWD)current_dir=$PWDcd $work_dirmvn clean package -Dmaven.test.skip=truefor service in $service_list; do cd $work_dir/$service if ls |grep biz &amp;&gt;/dev/null; then cd $&#123;service&#125;-biz fi service=$&#123;service%-*&#125; image_name=$docker_registry/microservice/$&#123;service&#125;:$(date +%F-%H-%M-%S) docker build -t $&#123;image_name&#125; . docker push $&#123;image_name&#125; # 替换镜像名称 直接部署 sed -i -r "s#(image: )(.*)#\1$image_name#" $&#123;current_dir&#125;/$&#123;service&#125;.yaml kubectl apply -f $&#123;current_dir&#125;/$&#123;service&#125;.yamldone 12# 使用脚本重新构建项目# 本次构建所有微服务加入了 pinpoint 链路监控 查看采集 12345# 启动日志中是否含有 pinpoint 加载[root@k8s-master1 k8s]# kubectl logs order-564b46f676-fb9gh -n ms# 模拟请求http://portal.ctnrs.com/ 监控JAVA应用 都需要监控哪些指标 JVM相关,堆内存:年轻代,老年代(持久常用对象 不断gcc),非堆内存(持久代,1.9后没有了) 线程数量 GCC(垃圾回收) ,G1,CMS CPU利用率 堆栈跟踪 哪段代码堆内存使用高,cpu使用高 接口状态 吞吐量 堆栈信息 有利于开发定位问题 响应时间 和 load 传入事务 (并发) 1个请求5秒 堆内存 id对应pod Non-Heap Usage : 非堆内存(持久代) Heap Usage : 堆内存:年轻代+老年代 , 堆内存的使用率快到达了堆内存大小,那么要重新分配 JVM/System Cpu Usage : CPU消耗 Transactions Per Second ： 每秒事务传输,相当于并发平均 Active Thread : 活动线程 Response Time : 请求响应时间 Open File Descriptor : 打开文件描述符的数量]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07 K8S 微服务容器化迁移]]></title>
    <url>%2F2019%2F12%2F30%2Fk8s-base09%2F</url>
    <content type="text"><![CDATA[从运维角度看微服务微服务特点 服务组件化每个服务独立开发、部署，有效避免一个服务的修改引起整个系统重新部署。哪个服务出现问题,修改后重新部署,不影响整个业务。 技术栈灵活约定通信方式，使得服务本身功能实现对技术要求不再那么敏感。不再采用一种语言开发，通过对接接口互相访问。 独立部署每个微服务独立部署，加快部署速度，方便扩展。 扩展性强每个微服务可以部署多个，并且有负载均衡能力。 独立数据每个微服务有独立的基本组件，例如数据库、缓存等。 微服务不足 沟通成本 数据一致性 运维成本 内部架构复杂性 大量服务如何治理 如何部署 如何监控 数据和事务 单体应用 vs 微服务 将一套业务 按功能拆分 每个服务处理自己的业务 每个业务都由独立的数据库 都通过 apigateway 网关 分发到独立的服务 ，路由转发 如何互相找到对方,通过注册中心，请求会去询问注册中心向哪里转发 单体架构优势： 易于部署 易于测试 单体架构不足： 代码膨胀，难以维护 构建、部署成本大 新人上手难 k8s治理微服务： 产品迭代 无状态 有状态很少应用 把每个微服务作为一个镜像去部署管理、迭代 SOA面向服务： 把每个功能做成api 每次增加功能服务都要让其他服务去感知 微服务可以通过 注册中心 感知 SOA可能需要人工干预，细腻度比微服务大一些 Java 微服务框架 Spring Boot Spring Cloud (Spring生态技术栈,Spring Boot的升级完善，应用最多) Dubbo (阿里云开源 ) 在K8S平台部署微服务考虑的问题微服务架构图 用户通过手机或者浏览器访问域名 访问域名到达前端(前后端分离),访问功能调用对应业务逻辑处理 访问功能的时候,通过负载均衡到达网关，网关作为微服务的入口,认证过滤降级都在这个层面实现,类似nginx的location 网关根据用户请求的url匹配，去注册中心去找提供相应的服务 注册中心保存了所有微服务的信息 每个微服务的数据都保存在自己的数据库中 配置中心存储每个微服务的配置数据，集中化管理 微服务架构的理解 微服务间如何通信？ REST API，RPC，MQ 微服务如何发现彼此？ 注册中心 组件之间怎么个调用关系？ 查看商品 -&gt; 订单 -&gt; 库存 -&gt; 支付 链路跟踪 打点 判断一个请求经过哪些环节 哪个服务作为整个网站入口？ 网关 作为 服务端入口 前端 作为 用户访问入口 哪些微服务需要对外访问？ 前端程序调用服务端接口api，前端和网关都需要暴露 微服务怎么部署？更新？扩容？ java 应用 ，微服务jar包,更新jar包,再启动一个jar包服务 区分有状态应用与无状态应用 网关,微服务都是无状态的,包括注册中心 有状态的都进行持久化,数据库,redis,MQ,分布式存储都按照传统部署 为什么要用注册中心 微服务太多面临的问题： 怎么记录一个微服务多个副本接口地址？ 新加入的服务如何让其他业务动态感知到 怎么实现一个微服务多个副本负载均衡？ 怎么判断一个微服务副本是否可用？ SOA的需要监控http,lv做健康检查 主流注册中心：Eureka，Nacos 123# 注册中心主要解决: 每个微服务之间如何交互# Eureka 去年年中闭源# Nacos 阿里开源 部署维护基本一样 12345# SOA用户 -&gt; 前端 -&gt; api -&gt; lv -&gt; DB数据 -&gt; api1 -&gt; api2 Eureka注册中心 每启动一个微服务都会被注册到Eureka中 Eureka不单存储信息,还提供负载均衡功能，检查接口是否可用 请求访问注册中心的服务,响应给请求的客户端,客户端再去请求微服务 客户端都集成在程序中 项目迁移到K8S平台是怎样的流程 制作镜像 镜像作为交付物 1231. 基础镜像 CentOS2. 运行环境 基础镜像+ java、php、python 3. 项目环境 基础镜像+运行环境+项目代码 控制器管理Pod Deployment：无状态部署 StatefulSet：有状态部署 DaemonSet：守护进程部署 Job &amp; CronJob：批处理 暴露应用 微服务之间都是通过注册中心通信,而不是走service,所以不大多数微服务不需要service 网关和前端需要ingress暴露 对外发布应用 Pod 数据持久化 容器部署过程中一般有以下三种数据： 启动时需要的初始数据，可以是配置文件 configmap,secret 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据 日志与监控主流方案： Filebeat+ELK Prometheus+Grafana 传统部署与K8S部署区别 在K8S平台部署 Spring Cloud 微服务项目熟悉Spring Cloud微服务项目12# 项目代码地址 https://github.com/lizhenliang/simple-microservice portal 域名 让用户访问 portal 访问 ingress 暴露的域名 域名指向网关 代码编译构建12# 将代码拉取到部署服务器git clone https://github.com/lizhenliang/simple-microservice.git 12345678910111213141516[root@k8s-master1 simple-microservice-dev1]# pwd/opt/microservic-code/simple-microservice-dev1[root@k8s-master1 simple-microservice-dev1]# ls -l# 源码目录 drwxr-xr-x 4 root root 4096 Jul 28 11:56 basic-common # 公共依赖drwxr-xr-x 2 root root 4096 Jul 28 11:56 db # 数据库sql drwxr-xr-x 3 root root 4096 Jul 28 11:56 eureka-service # 配置中心drwxr-xr-x 3 root root 4096 Jul 28 11:56 gateway-service # zuul 网关 微服务统一入口认证-rw-r--r-- 1 root root 11357 Jul 28 11:56 LICENSE-rw-r--r-- 1 root root 420 Jul 28 11:56 lombok.configdrwxr-xr-x 4 root root 4096 Jul 28 11:56 order-service # order 订单服务-rw-r--r-- 1 root root 5419 Jul 28 11:56 pom.xmldrwxr-xr-x 3 root root 4096 Jul 28 11:56 portal-service # 前端 drwxr-xr-x 4 root root 4096 Jul 28 11:56 product-service # product 商品服务-rw-r--r-- 1 root root 24 Jul 28 11:56 README.mddrwxr-xr-x 4 root root 4096 Jul 28 11:56 stock-service # stock 库存服务 查看 gateway 配置文件123456789101112131415[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev1/gateway-service/src/main/resources[root@k8s-master1 resources]# ls -ltotal 12-rw-r--r-- 1 root root 902 Jul 28 11:56 application-dev.yml-rw-r--r-- 1 root root 912 Jul 28 11:56 application-fat.yml-rw-r--r-- 1 root root 95 Jul 28 11:56 application.yml[root@k8s-master1 resources]# cat application.yml server: port: 9999 # 微服务端口spring: profiles: # 配置文件 active: fat # fat | dev 指定哪个名字使用哪个配置文件 不同环境区分 application: name: @artifactId@ 12345# 不同环境区分 # 微服务都是jar包,如果修改配置文件也不能进去解压修改比较麻烦 # 2种方法1. java -jar -Dspring.profiles.active=dev xxx.jar2. 采用配置中心: apollo,disconf(百度) 为所有微服务的配置做集中管理,运维部署服务端,开发继承agent 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master1 resources]# cat application-fat.yml spring: cloud: gateway: discovery: locator: #开启以服务id去注册中心上获取转发地址 enabled: true ##小写serviceId lower-case-service-id: true # gateway 网关 路由规则 根据请求的url转发 routes: - id: product-service uri: lb://product-service filters: - StripPrefix=1 predicates: - Path=/product/** - id: order-service uri: lb://order-service filters: - StripPrefix=1 predicates: - Path=/order/** - id: stock-service uri: lb://stock-service filters: - StripPrefix=1 predicates: - Path=/stock/**# 指定 eurekaeureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://127.0.0.1:$&#123;server.port&#125;/eureka/ 查看微服务的配置文件1234567891011121314151617181920212223242526272829303132333435363738394041# 查看商品微服务的配置文件[root@k8s-master1 simple-microservice-dev1]# cd product-service/[root@k8s-master1 product-service]# ls -lpom.xml product-service-api # 进一步解耦 实际的apiproduct-service-biz # 公共依赖 打包在这里# 查看biz[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev1/product-service/product-service-biz/src/main/resources# 微服务配置文件[root@k8s-master1 resources]# vim application.yml server: port: 8010spring: profiles: active: fat application: name: product-service# 连接数据库和其他配置[root@k8s-master1 resources]# cat application-fat.yml spring: datasource: url: jdbc:mysql://192.168.31.70:3306/tb_product?characterEncoding=utf-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver# 连接 eurekaeureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://127.0.0.1:$&#123;server.port&#125;/eureka/# 其他微服务基本一致 查看 eureka 配置文件1234567891011121314151617181920212223242526272829303132[root@k8s-master1 simple-microservice-dev1]# cd eureka-service/src/main/resources/[root@k8s-master1 resources]# ls -ltotal 12-rw-r--r-- 1 root root 356 Jul 28 11:56 application-dev.yml-rw-r--r-- 1 root root 337 Jul 28 11:56 application-fat.yml-rw-r--r-- 1 root root 105 Jul 28 11:56 application.yml[root@k8s-master1 resources]# vim application.yml server: port: 8888spring: application: name: legendshop-basic-eureka profiles: active: dev[root@k8s-master1 resources]# cat application-dev.yml eureka: server: renewal-percent-threshold: 0.9 enable-self-preservation: false eviction-interval-timer-in-ms: 40000 instance: hostname: 127.0.0.1 # 重要: 采用 ip 注册 ,默认可能是主机名,但是k8s中pod的主机名是不能通信的，所以需要使用pod的ip注册 prefer-ip-address: true client: register-with-eureka: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ fetch-registry: false 完成代码的编译构建 以上看的都是程序开发的源码，现在查看编译构建状态的代码 dev2 123# Maven项目对象模型(POM)，可以通过一小段描述信息来管理项目的构建，报告和文档的项目管理工具软件# -P 选择要编译的对象mvn clean package -D maven.test.skip=true -P prod dev2中有构建脚本 dockerfile 123456789101112131415161718[root@k8s-master1 gateway-service]# cd /opt/microservic-code/simple-microservice-dev2/gateway-service[root@k8s-master1 gateway-service]# ls -ltotal 12-rw-r--r-- 1 root root 239 Jul 28 11:56 Dockerfile-rw-r--r-- 1 root root 1094 Jul 28 11:56 pom.xmldrwxr-xr-x 3 root root 4096 Jul 28 11:56 src# 一般情况开发写,定义如何构建镜像 # jdk镜像 一般也可以自己自己定制，可以尝试docker仓库中的[root@k8s-master1 gateway-service]# cat Dockerfile FROM java:8-jdk-alpineLABEL maintainer lizhenliang/www.ctnrs.com# 镜像时区 RUN apk add -U tzdata &amp;&amp; \ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/gateway-service.jar ./EXPOSE 9999CMD java -jar /gateway-service.jar 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 单独测试打包 和 整个项目打包[root@k8s-master1 simple-microservice-dev2]# ls -l total 60drwxr-xr-x 4 root root 4096 Jul 28 11:56 basic-commondrwxr-xr-x 2 root root 4096 Jul 28 11:56 dbdrwxr-xr-x 3 root root 4096 Jul 28 11:56 eureka-servicedrwxr-xr-x 3 root root 4096 Jul 28 11:56 gateway-service-rw-r--r-- 1 root root 11357 Jul 28 11:56 LICENSE-rw-r--r-- 1 root root 420 Jul 28 11:56 lombok.configdrwxr-xr-x 4 root root 4096 Jul 28 11:56 order-service-rw-r--r-- 1 root root 5419 Jul 28 11:56 pom.xmldrwxr-xr-x 3 root root 4096 Jul 28 11:56 portal-servicedrwxr-xr-x 4 root root 4096 Jul 28 11:56 product-service-rw-r--r-- 1 root root 24 Jul 28 11:56 README.mddrwxr-xr-x 4 root root 4096 Jul 28 11:56 stock-service# 这个路径下的 pom.xml 完成整个项目打包# 实际的生产环境是 发布哪个微服务 打包哪个# 老师整个没解决好依赖 所有整个打包# 需要jdk+maven环境[root@k8s-master1 simple-microservice-dev2]# yum install java-1.8.0-openjdk maven # mvn构建打包[root@k8s-master1 simple-microservice-dev2]# mvn clean package -D maven.test.skip=true[INFO] simple-microservice ............................... SUCCESS [6.338s][INFO] basic-common ...................................... SUCCESS [0.001s][INFO] basic-common-core ................................. SUCCESS [1:34.738s][INFO] gateway-service ................................... SUCCESS [56.667s][INFO] eureka-service .................................... SUCCESS [3.798s][INFO] product-service ................................... SUCCESS [0.001s][INFO] product-service-api ............................... SUCCESS [0.281s][INFO] stock-service ..................................... SUCCESS [0.001s][INFO] stock-service-api ................................. SUCCESS [0.249s][INFO] product-service-biz ............................... SUCCESS [1.349s][INFO] stock-service-biz ................................. SUCCESS [0.329s][INFO] order-service ..................................... SUCCESS [0.001s][INFO] order-service-api ................................. SUCCESS [0.202s][INFO] order-service-biz ................................. SUCCESS [0.643s][INFO] basic-common-bom .................................. SUCCESS [0.001s][INFO] portal-service .................................... SUCCESS [1.272s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 2:46.295s[INFO] Finished at: Mon Dec 30 15:36:50 CST 2019[INFO] Final Memory: 74M/383M[INFO] ------------------------------------------------------------------------# 项目目录下出现 target 目录 [root@k8s-master1 simple-microservice-dev2]# cd gateway-service/[root@k8s-master1 gateway-service]# lsDockerfile pom.xml src target 构建项目镜像并推送到镜像仓库123456789101112131415161718192021222324252627282930313233343536373839404142# 当前是手动构建 使用脚本循环创建# 每个微服务都由一个docker 镜像# 先准备好 harbor 镜像仓库# 登录镜像仓库[root@k8s-master1 k8s]# docker login 172.17.70.252# 查看脚本[root@k8s-master1 k8s]# vim docker_build.sh #!/bin/bash# 镜像仓库地址docker_registry=172.17.70.252# kubectl create secret docker-registry registry-pull-secret --docker-server=$docker_registry --docker-username=admin --docker-password=lx@68328153 --docker-email=admin@ctnrs.com -n ms# 服务列表service_list="eureka-service gateway-service order-service product-service stock-service portal-service"service_list=$&#123;1:-$&#123;service_list&#125;&#125;work_dir=$(dirname $PWD)current_dir=$PWD# mvn 构建cd $work_dirmvn clean package -Dmaven.test.skip=true# 循环每个服务 进入目录 先查看是否有 biz 有的话进入 # 推送镜像 microservice 镜像仓库for service in $service_list; do cd $work_dir/$service if ls |grep biz &amp;&gt;/dev/null; then cd $&#123;service&#125;-biz fi service=$&#123;service%-*&#125; image_name=$docker_registry/microservice/$&#123;service&#125;:$(date +%F-%H-%M-%S) docker build -t $&#123;image_name&#125; . docker push $&#123;image_name&#125; # sed -i -r "s#(image: )(.*)#\1$image_name#" $&#123;current_dir&#125;/$&#123;service&#125;.yaml # kubectl apply -f $&#123;current_dir&#125;/$&#123;service&#125;.yamldone# 执行 [root@k8s-master1 k8s]# sh docker_build.sh 部署Spring Cloud项目部署环境要求 CoreDNS Ingress Controller MySQL 12345678[root@k8s-master2 tomcat-java-demo]# yum -y install mariadb mariadb-server[root@k8s-master2 tomcat-java-demo]# systemctl start mariadb[root@k8s-master2 tomcat-java-demo]# systemctl enable mariadb[root@k8s-master2 tomcat-java-demo]# mysql_secure_installation# 授权访问用户[root@k8s-master2 tomcat-java-demo]# mysql -uroot -pMariaDB [(none)]&gt; grant all on *.* to 'root'@'%' identified by '123456'; 架构图 eureka 有状态部署 gateway portal 需要对外暴露 微服务通过注册中心，无需service 服务编排 每个微服务都需要yaml部署文件 别忘记更换镜像地址 gateway123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475cd /opt/microservic-code/simple-microservice-dev3/k8s[root@k8s-master1 k8s]# cat gateway.yaml ---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: gateway namespace: ms spec: rules: - host: gateway.ctnrs.com http: paths: - path: / backend: serviceName: gateway servicePort: 9999---apiVersion: v1kind: Servicemetadata: name: gateway namespace: msspec: ports: - port: 9999 name: gateway selector: project: ms app: gateway---apiVersion: apps/v1kind: Deployment metadata: name: gateway namespace: ms spec: replicas: 2 selector: matchLabels: project: ms app: gateway template: metadata: labels: project: ms app: gateway spec: imagePullSecrets: - name: registry-pull-secret containers: - name: gateway image: 172.17.70.252/microservice/gateway:2019-12-30-16-00-51 imagePullPolicy: Always ports: - protocol: TCP containerPort: 9999 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10 portal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@k8s-master1 k8s]# cat portal.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: portal namespace: ms spec: rules: - host: portal.ctnrs.com http: paths: - path: / backend: serviceName: portal servicePort: 8080---apiVersion: v1kind: Servicemetadata: name: portal namespace: msspec: ports: - port: 8080 name: portal selector: project: ms app: portal---apiVersion: apps/v1kind: Deployment metadata: name: portal namespace: ms spec: replicas: 2 selector: matchLabels: project: ms app: portal template: metadata: labels: project: ms app: portal spec: imagePullSecrets: - name: registry-pull-secret containers: - name: portal image: 172.17.70.252/microservice/portal:2019-12-30-16-01-04 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8080 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 60 periodSeconds: 10 eureka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# StatefulSet [root@k8s-master1 k8s]# cat eureka.yaml ---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: eureka namespace: ms spec: rules: - host: eureka.ctnrs.com http: paths: - path: / backend: serviceName: eureka servicePort: 8888---apiVersion: v1kind: Servicemetadata: name: eureka namespace: msspec: clusterIP: None ports: - port: 8888 name: eureka selector: project: ms app: eureka---apiVersion: apps/v1kind: StatefulSetmetadata: name: eureka namespace: ms spec: replicas: 3 selector: matchLabels: project: ms app: eureka serviceName: "eureka" template: metadata: labels: project: ms app: eureka spec: imagePullSecrets: - name: registry-pull-secret containers: - name: eureka image: 172.17.70.252/microservice/eureka:2019-12-30-15-56-33 ports: - protocol: TCP containerPort: 8888 # 变量 MY_POD_NAME 是 内置变量 metadata.name = eureka-0(0-2) 由于是 StatefulSet env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8888 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8888 initialDelaySeconds: 60 periodSeconds: 10 123456789101112# 用在这里 [root@k8s-master1 k8s]# cat ../eureka-service/Dockerfile FROM java:8-jdk-alpineLABEL maintainer lizhenliang/www.ctnrs.comRUN apk add -U tzdata &amp;&amp; \ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY ./target/eureka-service.jar ./EXPOSE 8888CMD java -jar -Deureka.instance.hostname=$&#123;MY_POD_NAME&#125;.eureka.ms /eureka-service.jar# 启动的时候指定了 hostname=eureka-0(0-2)# 实例名 = eureka-0.eureka.ms order1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master1 k8s]# cat order.yaml apiVersion: apps/v1kind: Deployment metadata: name: order namespace: ms spec: replicas: 2 selector: matchLabels: project: ms app: order template: metadata: labels: project: ms app: order spec: imagePullSecrets: - name: registry-pull-secret containers: - name: order image: 172.17.70.252/microservice/order:2019-12-30-16-00-54 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8020 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8020 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8020 initialDelaySeconds: 60 periodSeconds: 10 product1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master1 k8s]# cat product.yaml apiVersion: apps/v1kind: Deployment metadata: name: product namespace: ms spec: replicas: 2 selector: matchLabels: project: ms app: product template: metadata: labels: project: ms app: product spec: imagePullSecrets: - name: registry-pull-secret containers: - name: product image: 172.17.70.252/microservice/product:2019-12-30-16-00-57 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8010 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8010 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8010 initialDelaySeconds: 60 periodSeconds: 10 stock1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master1 k8s]# cat stock.yaml apiVersion: apps/v1kind: Deployment metadata: name: stock namespace: ms spec: replicas: 2 selector: matchLabels: project: ms app: stock template: metadata: labels: project: ms app: stock spec: imagePullSecrets: - name: registry-pull-secret containers: - name: stock image: 172.17.70.252/microservice/stock:2019-12-30-16-01-01 imagePullPolicy: Always ports: - protocol: TCP containerPort: 8030 resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi readinessProbe: tcpSocket: port: 8030 initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: tcpSocket: port: 8030 initialDelaySeconds: 60 periodSeconds: 10 导入数据库文件到MySQL1234567891011121314[root@k8s-master1 db]# scp *.sql root@172.17.70.252:/tmpMariaDB [(none)]&gt; create database tb_order CHARACTER SET utf8mb4;MariaDB [(none)]&gt; create database tb_stock CHARACTER SET utf8mb4;MariaDB [(none)]&gt; create database tb_product CHARACTER SET utf8mb4; MariaDB [(none)]&gt; use tb_order;MariaDB [tb_order]&gt; source /tmp/order.sql;MariaDB [tb_order]&gt; use tb_stock;MariaDB [tb_stock]&gt; source /tmp/stock.sql;MariaDB [tb_stock]&gt; use tb_product;MariaDB [tb_product]&gt; source /tmp/product.sql; 修改配置文件 根据实际配置修改配置文件 eureka的连裤串 mysql连库串 大多在微服务上 如果有修改 需要重新执行脚本打包镜像，还要修改部署的yaml文件，手动很麻烦 eureka的连库串 = StatefulSet 有状态部署 12345678910111213141516171819202122232425262728cd /opt/microservic-code/simple-microservice-dev3/eureka-service/src/main/resources[root@k8s-master1 resources]# cat application.yml server: port: 8888spring: application: name: eureka-server profiles: active: fat[root@k8s-master1 resources]# cat application-fat.yml eureka: server: renewal-percent-threshold: 0.9 enable-self-preservation: false eviction-interval-timer-in-ms: 40000 instance: hostname: 127.0.0.1 prefer-ip-address: false client: register-with-eureka: true serviceUrl: defaultZone: http://eureka-0.eureka.ms:$&#123;server.port&#125;/eureka/,http://eureka-1.eureka.ms:$&#123;server.port&#125;/eureka/,http://eureka-2.eureka.ms:$&#123;server.port&#125;/eureka/ fetch-registry: true# 通过有状态部署 得到的 eureka-0 eureka-1 eureka-2 这个信息dev2还是开发人员的地址# 数据库也要修改 123456789101112131415161718192021222324252627282930313233343536373839404142# gateway[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev3/gateway-service/src/main/resources[root@k8s-master1 resources]# cat application-fat.yml spring: cloud: gateway: discovery: locator: #开启以服务id去注册中心上获取转发地址 enabled: true ##小写serviceId lower-case-service-id: true routes: - id: product-service uri: lb://product-service filters: - StripPrefix=1 predicates: - Path=/product/** - id: order-service uri: lb://order-service filters: - StripPrefix=1 predicates: - Path=/order/** - id: stock-service uri: lb://stock-service filters: - StripPrefix=1 predicates: - Path=/stock/**eureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka 12345678910111213141516171819202122232425262728# portal[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev3/portal-service/src/main/resources[root@k8s-master1 resources]# cat application-fat.yml eureka: instance: prefer-ip-address: true client: service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka register-with-eureka: true fetch-registry: truespring: freemarker: allow-request-override: false allow-session-override: false cache: true charset: UTF-8 check-template-location: true content-type: text/html enabled: true expose-request-attributes: false expose-session-attributes: false expose-spring-macro-helpers: true prefer-file-system-access: true suffix: .ftl template-loader-path: classpath:/templates/ 12345678910111213141516171819# order[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev3/order-service/order-service-biz/src/main/resources[root@k8s-master1 resources]# vim application-fat.yml spring: datasource: url: jdbc:mysql://172.17.70.252:3306/tb_order?characterEncoding=utf-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Drivereureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka 1234567891011121314151617181920# product[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev3/product-service/product-service-biz/src/main/resources[root@k8s-master1 resources]# vim application-fat.yml spring: datasource: url: jdbc:mysql://172.17.70.252:3306/tb_product?characterEncoding=utf-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Drivereureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka 123456789101112131415161718192021# stock[root@k8s-master1 resources]# cd /opt/microservic-code/simple-microservice-dev3/stock-service/stock-service-biz/src/main/resources[root@k8s-master1 resources]# vim application-fat.yml spring: datasource: url: jdbc:mysql://172.17.70.252:3306/tb_stock?characterEncoding=utf-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Drivereureka: instance: prefer-ip-address: true client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka-0.eureka.ms:8888/eureka,http://eureka-1.eureka.ms:8888/eureka,http://eureka-2.eureka.ms:8888/eureka 123# 1. 有修改执行 重新生成镜像[root@k8s-master1 k8s]# sh docker_build.sh # 2. 需要对应所有yaml修改镜像名称 部署资源创建 命名空间1234567891011[root@k8s-master1 k8s]# kubectl create ns msnamespace/ms created[root@k8s-master1 k8s]# kubectl get nsNAME STATUS AGEdefault Active 13dingress-nginx Active 13dkube-node-lease Active 13dkube-public Active 13dkube-system Active 13dkubernetes-dashboard Active 13dms Active 4s 创建镜像仓库资源凭证123456[root@k8s-master1 k8s]# kubectl create secret docker-registry registry-pull-secret --docker-server=172.17.70.252 --docker-username=admin --docker-password=lx@68328153 --docker-email=admin@ctnrs.com -n ms[root@k8s-master1 k8s]# kubectl get secret -n msNAME TYPE DATA AGEdefault-token-jt5r5 kubernetes.io/service-account-token 3 77sregistry-pull-secret kubernetes.io/dockerconfigjson 1 11s 部署 eureka12345678910111213141516171819202122# 有状态应用部署 会一个一个启动 # 健康检查 70秒 [root@k8s-master1 k8s]# kubectl apply -f eureka.yaml [root@k8s-master1 k8s]# kubectl get pods -n msNAME READY STATUS RESTARTS AGEeureka-0 0/1 Running 0 4s[root@k8s-master1 k8s]# kubectl get pods -n ms -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESeureka-0 1/1 Running 0 3m41s 10.244.0.79 k8s-master1 &lt;none&gt; &lt;none&gt;eureka-1 1/1 Running 0 2m35s 10.244.1.88 k8s-node1 &lt;none&gt; &lt;none&gt;eureka-2 1/1 Running 0 82s 10.244.2.123 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 k8s]# kubectl exec -it eureka-0 sh -n ms/ # nslookup eurekanslookup: can't resolve '(null)': Name does not resolveName: eurekaAddress 1: 10.244.0.79 eureka-0.eureka.ms.svc.cluster.localAddress 2: 10.244.1.88 eureka-1.eureka.ms.svc.cluster.localAddress 3: 10.244.2.123 eureka-2.eureka.ms.svc.cluster.local 12345678910111213141516171819# 访问管理页面[root@k8s-master1 k8s]# kubectl get pods,svc,ingress -n ms -o wideNAME HOSTS ADDRESS PORTS AGEingress.extensions/eureka eureka.ctnrs.com 80 13m# 三个node节点都可以访问 [root@k8s-master1 k8s]# kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGEnginx-ingress-controller-cc7p6 1/1 Running 5 6d6hnginx-ingress-controller-gjrb8 1/1 Running 5 6d6hnginx-ingress-controller-mwqcv 1/1 Running 5 6d6h# 绑定hosts 访问 管理页面60.205.217.112 eureka.ctnrs.com# 刷新会换成另外一个节点地址 ingress -&gt; pod# unavailable-replicas 有显示 说明集群节点有问题 # 可用的节点 是其他两个节点 部署 gateway12# 资源不太够 我就部署一个副本[root@k8s-master1 k8s]# kubectl apply -f gateway.yaml 部署 portal1[root@k8s-master1 k8s]# kubectl apply -f portal.yaml 部署所有微服务12345# 如果测试的资源不足 可以用1个副本[root@k8s-master1 k8s]# kubectl apply -f stock.yaml [root@k8s-master1 k8s]# kubectl apply -f product.yaml [root@k8s-master1 k8s]# kubectl apply -f order.yaml 部署的服务 都被注册中心发现 测试访问 portal12# 加入hosts 47.240.14.16 eureka.ctnrs.com portal.ctnrs.com gateway.ctnrs.com 程序调用gateway域名的位置123# 都是前端程序调用# 现有节点可以随意扩容[root@k8s-master1 simple-microservice-dev3]# grep gateway. -r portal-service/src/main/resources/ 在K8S中部署Eureka集群 相互注册 提供健康检查，podIP检查，如果不正常就不会将客户端的请求发送给它 剔除一个eureka，集群也可正常工作 1234# 如果直接对接k8s# service -- eureka# ingress -- gateway# configmap -- 配置中心 生产环境踩坑经验分享限制了容器资源，还经常被杀死12345678910111213env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.nameresources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi 123456781. k8s部署java应用 ，docker不能自动发现java设置的内存，jvm限制无用2. java内存不够会继续申请，k8s的limits 如果超出限制会直接kill 变成OOM状态3. 所以这个限制必须让java设置，到达内存最大不去申请4. jdk1.8之前需要 5. 在Dockerfile 启动命令里 加上限制内存参数，对内存回收利用 6. CMD java -jar $JAVA_OPTS /gateway-service.jar7. jdk1.9以上会自动识别。1.8用的比较多 要手动指定 滚动更新之健康检查重要性 误判健康检查的失败，需要根据应用启动时间判断 java启动时间较慢 最好设置一分钟或以上 readinessProbe 就绪检查 它检查的是Ready 1/1 准备就绪才会分配流量,否则不会被访问到。微服务不受service控制，所以加不加无所谓 livenessProbe 存活检查 判断pod中的应用是否启动成功，比如判断服务端口，如果不成功会根据拉起策略重建，直到启动成功为止 具体要看java服务的启动时间来设置 periodSeconds 周期,每间隔10秒来判断一次,可以设置较长 12345678910readinessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10livenessProbe: tcpSocket: port: 9999 initialDelaySeconds: 60 periodSeconds: 10 滚动更新之流量丢失 运行的服务pod 已经被注册到 注册中心里了 如果滚动更新 已分配的流量怎么办 只能为每个pod 增加退出时间，让pod等待退出时间 再去销毁 滚动更新是突然kill 没有更新提示,kill之后根据退出时间等待销毁 优雅退出 1https://mp.weixin.qq.com/s?__biz=MzAwNTM5Njk3Mw==&amp;mid=2247487699&amp;idx=1&amp;sn=7bf9c62b02e017de5666ab4ade9ef3d1&amp;chksm=9b1c1051ac6b99476bf665bf1e10ac3aca9b0b995c2da944909621660547f2dd382845302aba&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1577617482898&amp;sharer_shareid=e3aed5c2fdec06d648a99b41c36fe174#rd 配置中心 和 注册中心 和 数据库1231. 配置中心 apollo 安装服务端，配置文件在里面配置好2. 开发人员 将客户端植入到程序里 实时连接，如果有新的事件触发会更新3. 配置中心 是有状态的 保存配置文件 121. eureka 可以部署在pod中，也可以部署在pod之外,只要网络互通即可2. 网关使用 spring-cloud-zuul 1231. 还是建议微服务中的数据存储使用原先的数据库存储，而不是使用有状态部署2. 使用k8s部署存储会有运维成本，之前怎么维护就怎么维护3. k8s适用于 无状态应用，有状态的应用不经常变动,放在k8s中意义不大,测试环境倒是可以考虑快速启动 增加pod流程 新增的pod会先加入 eureka里 如果有客户端去请求 eureka 查询这个pod所在的后面业务功能IP，eureka健康检查通过新的pod的后就可以响应回去 gateway就作为请求的客户端，根据url查询对应的pod,eureka查询出对应的ip根据负载均衡策略给gateway,gateway再去响应。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12 k8s 日志收集]]></title>
    <url>%2F2019%2F12%2F24%2Fk8s-12%2F</url>
    <content type="text"><![CDATA[收集哪些日志 k8s 系统的组件日志 k8s pod中应用程序日志 主流的日志方案 容器中的日志怎么收集 简易安装 ELK 环境jdk123456789101112131415# [root@k8s-node3 yum.repos.d]# yum -y remove java-1.8.0-openjdk[root@k8s-node3 ~]# tar -zxvf jdk-11.0.5_linux-x64_bin.tar.gz -C /usr/local/[root@k8s-node3 ~]# vim /etc/profileexport JAVA_HOME=/usr/local/jdk-11.0.5export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH [root@k8s-node3 ~]# source /etc/profile[root@k8s-node3 ~]# java -versionjava version "11.0.5" 2019-10-15 LTSJava(TM) SE Runtime Environment 18.9 (build 11.0.5+10-LTS)Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.5+10-LTS, mixed mode) yum 安装 elk 6.812345678910111213# JKD 11[root@k8s-node3 ~]# tar -zxvf jdk-11.0.5_linux-x64_bin.tar.gz -C /usr/local/[root@k8s-node3 ~]# vim /etc/yum.repos.d/elastic.repo [logstash-6.x]name=Elastic repository for 6.x packagesbaseurl=https://artifacts.elastic.co/packages/6.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md 12# 安装 elk [root@k8s-node3 ~]# yum install logstash elasticsearch kibana 配置启动 kibana12345# 修改配置文件[root@k8s-node3 ~]# grep ^'[a-Z]' /etc/kibana/kibana.yml server.port: 5601 # 端口server.host: "0.0.0.0" # 外网访问elasticsearch.hosts: ["http://localhost:9200"] # elasticsearch 地址 当前是本地部署 12# 启动 kibana[root@k8s-node3 ~]# systemctl start kibana 配置启动 elasticsearch12# 本地部署 保持默认 也没有优化[root@k8s-node3 ~]# vim /etc/elasticsearch/elasticsearch.yml 1234# 启动[root@k8s-node3 ~]# ln -s /usr/local/jdk-11.0.5/bin/java /usr/bin/java[root@k8s-node3 ~]# systemctl start elasticsearch 123456789# 查看服务状态[root@k8s-node3 ~]# systemctl status elasticsearch[root@k8s-node3 ~]# systemctl status kibana[root@k8s-node3 ~]# ps -ef|grep java[root@k8s-node3 ~]# ps -ef|grep kibana# 访问 kibana 页面http://123.56.14.192:5601/ 1# 点击监控 elasticsearch 配置 logstash12345678910111213141516171819202122232425262728# 本地启动logstash 监听 filebeat 输出到es和控制台[root@k8s-node3 conf.d]# cd /etc/logstash/conf.d/[root@k8s-node3 conf.d]# vim logstash-to-es.confinput &#123; # 输入插件 指定输入源 beats beats&#123; port =&gt; 5044 &#125;&#125;filter &#123; # 过滤匹配插件&#125;output &#123; # 输出插件 elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] # 索引 按天存储 index =&gt; "k8s-log-%&#123;+YYYY-MM-dd&#125;" &#125; # 输出到控制台 调试 stdout &#123; codec =&gt; rubydebug &#125;&#125;# 启动[root@k8s-node3 conf.d]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash-to-es.conf 配置 filebeat12# 镜像地址: https://www.docker.elastic.co/## configmap 管理 filebeat 配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@k8s-master1 ELK-Logs]# cat k8s-logs.yaml apiVersion: v1kind: ConfigMapmetadata: name: k8s-logs-filebeat-config namespace: kube-system data: filebeat.yml: |- filebeat.prospectors: - type: log # 采集目录 paths: - /messages fields: app: k8s type: module fields_under_root: true output.logstash: # lostash 地址 hosts: ['172.17.70.252:5044']---apiVersion: apps/v1kind: DaemonSet metadata: name: k8s-logs namespace: kube-systemspec: selector: matchLabels: project: k8s app: filebeat template: metadata: labels: project: k8s app: filebeat spec: containers: - name: filebeat image: docker.elastic.co/beats/filebeat:6.8.6 args: [ "-c", "/etc/filebeat.yml", "-e", ] resources: requests: cpu: 100m memory: 100Mi limits: cpu: 500m memory: 500Mi securityContext: runAsUser: 0 volumeMounts: - name: filebeat-config # 挂载配置文件 会写到 /etc/filebeat.yml filebeat 启动会使用到这个文件 mountPath: /etc/filebeat.yml subPath: filebeat.yml # 挂载 日志 文件 - name: k8s-logs mountPath: /messages volumes: - name: k8s-logs hostPath: # 挂载宿主机文件 /var/log/messages 到 /messages 文件 path: /var/log/messages type: File - name: filebeat-config configMap: name: k8s-logs-filebeat-config 123456789101112131415161718192021# 每个node 都会创建 daemonset 去采集自己下面的k8s组件日志# 镜像导出/导入[root@k8s-node2 ~]# docker save -o filebeat-6.8.6.tar docker.elastic.co/beats/filebeat:6.8.6 [root@k8s-node2 ~]# scp filebeat-6.8.6.tar root@172.17.70.253:/root[root@k8s-node1 ~]# docker load --input filebeat-6.8.6.tar [root@k8s-master1 ELK-Logs]# kubectl apply -f k8s-logs.yaml configmap/k8s-logs-filebeat-config createddaemonset.apps/k8s-logs created[root@k8s-master1 ELK-Logs]# kubectl get pods -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-6d8cfdd59d-xwv6v 1/1 Running 16 9d 10.244.2.105 k8s-node2 &lt;none&gt; &lt;none&gt;k8s-logs-8h7fh 1/1 Running 0 28s 10.244.0.70 k8s-master1 &lt;none&gt; &lt;none&gt;k8s-logs-cpnmn 1/1 Running 0 28s 10.244.2.108 k8s-node2 &lt;none&gt; &lt;none&gt;k8s-logs-l8fph 1/1 Running 0 28s 10.244.1.79 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-gb6n7 1/1 Running 13 9d 172.17.70.251 k8s-master1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-j59qp 1/1 Running 11 9d 172.17.70.253 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-w6s9d 1/1 Running 11 9d 172.17.70.254 k8s-node2 &lt;none&gt; &lt;none&gt;metrics-server-7dbbcf4c7-gxb9m 1/1 Running 16 9d 10.244.2.102 k8s-node2 &lt;none&gt; &lt;none&gt; 1234567891011121314151617181920# 查看pod上的日志[root@k8s-master1 ELK-Logs]# kubectl exec -it k8s-logs-8h7fh bash -n kube-system[root@k8s-logs-8h7fh filebeat]# ls -lh /messages -rw------- 1 root root 72M Dec 27 03:58 /messages# 配置文件[root@k8s-logs-8h7fh filebeat]# vi /etc/filebeat.yml filebeat.prospectors: - type: log paths: - /messages fields: app: k8s type: module fields_under_root: trueoutput.logstash: hosts: ['172.17.70.252:5044'] 收集 Nginx 日志php demo 部署123456[root@k8s-master1 php-demo]# kubectl apply -f namespace.yaml kubectl create secret docker-registry registry-pull-secret --docker-username=admin --docker-password=lx@68328153 --docker-email=253911339@qq.com --docker-server=172.17.70.252 -n test[root@k8s-master1 php-demo]# kubectl apply -f mysql.yaml [root@k8s-master1 php-demo]# kubectl apply -f deployment.yaml [root@k8s-master1 php-demo]# kubectl apply -f service.yaml [root@k8s-master1 php-demo]# kubectl apply -f ingress.yaml 改造 php deployment1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# pod里增加一个 filebeat 容器 用来收集日志 [root@k8s-master1 ELK-Logs]# cat nginx-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: php-demo namespace: testspec: replicas: 3 selector: matchLabels: project: www app: php-demo template: metadata: labels: project: www app: php-demo spec: imagePullSecrets: - name: registry-pull-secret containers: - name: nginx image: 172.17.70.252/project/php-demo:1.0 imagePullPolicy: Always ports: - containerPort: 80 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi livenessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 6 timeoutSeconds: 20 volumeMounts: - name: nginx-logs mountPath: /usr/local/nginx/logs - name: filebeat image: docker.elastic.co/beats/filebeat:6.8.6 # filebeat.yml 通过 configMap 保存 args: [ "-c", "/etc/filebeat.yml", "-e", ] resources: # 限制 limits: memory: 500Mi # 需求 requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 volumeMounts: - name: filebeat-config mountPath: /etc/filebeat.yml subPath: filebeat.yml - name: nginx-logs mountPath: /usr/local/nginx/logs volumes: # 两个容器共享目录 - name: nginx-logs emptyDir: &#123;&#125; # 配置文件 - name: filebeat-config configMap: name: filebeat-nginx-config 12345678910111213141516171819202122232425262728293031# 配置文件[root@k8s-master1 ELK-Logs]# cat filebeat-nginx-configmap.yaml apiVersion: v1kind: ConfigMapmetadata: name: filebeat-nginx-config namespace: test data: filebeat.yml: |- filebeat.prospectors: - type: log paths: - /usr/local/nginx/logs/access.log # tags: ["access"] fields: app: www type: nginx-access fields_under_root: true - type: log paths: - /usr/local/nginx/logs/error.log # tags: ["error"] fields: app: www type: nginx-error fields_under_root: true output.logstash: hosts: ['172.17.70.252:5044'] 启动 configmap12[root@k8s-master1 ELK-Logs]# kubectl apply -f filebeat-nginx-configmap.yaml configmap/filebeat-nginx-config created 滚动更新 deployment1234567# 变成了 两个容器 [root@k8s-master1 ELK-Logs]# kubectl get pods -n testNAME READY STATUS RESTARTS AGEdb-0 1/1 Running 0 25mphp-demo-69dc8b596c-8kvw8 2/2 Running 0 31sphp-demo-69dc8b596c-jznj4 2/2 Running 0 29sphp-demo-69dc8b596c-jzxm6 2/2 Running 0 27s 进入filebeat容器查看12345678910111213141516171819202122# 多个容器用 -n 指定[root@k8s-master1 ELK-Logs]# kubectl exec -it php-demo-69dc8b596c-8kvw8 bash -c filebeat -n test[root@php-demo-69dc8b596c-8kvw8 filebeat]# # 可以看到日志的挂载目录[root@php-demo-69dc8b596c-8kvw8 filebeat]# cd /usr/local/nginx/logs/[root@php-demo-69dc8b596c-8kvw8 logs]# lsaccess.log error.log# 健康检查 日志 deoloy中配置了 index.html 的健康检查[root@php-demo-69dc8b596c-8kvw8 logs]# tail /usr/local/nginx/logs/access.log 10.244.0.1 - - [28/Dec/2019:17:49:06 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:49:16 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:49:26 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:49:36 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:49:46 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:49:56 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:50:06 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:50:16 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:50:26 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16"10.244.0.1 - - [28/Dec/2019:17:50:36 +0800] "GET /index.html HTTP/1.1" 200 52 "-" "kube-probe/1.16" 修改 filebeat 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 之前的配置文件中 无论收集什么日志 索引都在 k8s-log-日期# 新增索引# 不同的日志存储到 不同的索引数据中 方便检索和管理[root@k8s-node3 conf.d]# vim logstash-to-es.confinput &#123; # 输入插件 指定输入源 beats beats&#123; port =&gt; 5044 &#125;&#125;filter &#123; # 过滤匹配插件&#125;output &#123; if [app] == "www" &#123; if [type] == "nginx-access" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "nginx-access-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; else if [type] == "nginx-error" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "nginx-error-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; else if [type] == "tomcat-catalina" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "tomcat-catalina-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; &#125; else if [app] == "k8s" &#123; if [type] == "module" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "k8s-log-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; &#125; stdout &#123; codec=&gt; rubydebug &#125;&#125;[root@k8s-node3 conf.d]# vim logstash-to-es.confinput &#123; # 输入插件 指定输入源 beats beats&#123; port =&gt; 5044 &#125;&#125;filter &#123; # 过滤匹配插件&#125;output &#123; if [app] == "www" &#123; if [type] == "nginx-access" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "nginx-access-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; else if [type] == "nginx-error" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "nginx-error-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; else if [type] == "tomcat-catalina" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "tomcat-catalina-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; &#125; else if [app] == "k8s" &#123; if [type] == "module" &#123; elasticsearch &#123; hosts =&gt; ["http://127.0.0.1:9200"] index =&gt; "k8s-log-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; &#125; stdout &#123; codec=&gt; rubydebug &#125;&#125; 1[root@k8s-node3 conf.d]# /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/logstash-to-es.conf 1# filebeat 命令行也应该正常输出 kibana 配置1# 查看 索引管理 1# 配置索引 1# 刷新页面 查看数据 收集 Tomcat 日志启动 tomcat 项目12345# 数据存储 mysql 使用同一个[root@k8s-master1 java-demo]# kubectl apply -f deployment.yaml [root@k8s-master1 java-demo]# kubectl apply -f service.yaml [root@k8s-master1 java-demo]# kubectl apply -f ingress.yaml 更新 deployment1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[root@k8s-master1 ELK-Logs]# cat tomcat-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-java-demo namespace: testspec: replicas: 3 selector: matchLabels: project: www app: java-demo template: metadata: labels: project: www app: java-demo spec: imagePullSecrets: - name: registry-pull-secret containers: - name: tomcat image: 172.17.70.252/project/java-demo:1.0.1 imagePullPolicy: Always ports: - containerPort: 8080 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 1Gi limits: cpu: 1 memory: 2Gi livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 60 timeoutSeconds: 20 readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 60 timeoutSeconds: 20 volumeMounts: - name: tomcat-logs mountPath: /usr/local/tomcat/logs - name: filebeat image: docker.elastic.co/beats/filebeat:6.8.6 args: [ "-c", "/etc/filebeat.yml", "-e", ] resources: limits: memory: 500Mi requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 volumeMounts: - name: filebeat-config mountPath: /etc/filebeat.yml subPath: filebeat.yml - name: tomcat-logs mountPath: /usr/local/tomcat/logs volumes: - name: tomcat-logs emptyDir: &#123;&#125; - name: filebeat-config configMap: name: filebeat-config filebeat 配置文件12345678910111213141516171819202122232425[root@k8s-master1 ELK-Logs]# cat filebeat-tomcat-configmap.yaml apiVersion: v1kind: ConfigMapmetadata: name: filebeat-config namespace: test data: filebeat.yml: |- filebeat.prospectors: - type: log paths: - /usr/local/tomcat/logs/catalina.* # tags: ["tomcat"] fields: app: www type: tomcat-catalina fields_under_root: true # 多层匹配 catalina 日志 异常日志有很多条 左中括号 到 下一个左中括号中的 为一条日志 multiline: pattern: '^\[' negate: true match: after output.logstash: hosts: ['172.17.70.252:5044'] 更新部署12345678910111213[root@k8s-master1 ELK-Logs]# kubectl apply -f filebeat-tomcat-configmap.yaml configmap/filebeat-config created[root@k8s-master1 ELK-Logs]# kubectl apply -f tomcat-deployment.yaml deployment.apps/tomcat-java-demo configured# 注意查看资源 不够就先把里面的nginx 关闭[root@k8s-master1 ELK-Logs]# kubectl get pods -n testNAME READY STATUS RESTARTS AGEdb-0 1/1 Running 0 130mtomcat-java-demo-6454cf7d7c-pnfj4 2/2 Running 0 2mtomcat-java-demo-6454cf7d7c-sbpj5 2/2 Running 0 2mtomcat-java-demo-6454cf7d7c-szbhh 2/2 Running 0 2m 建立索引 查看日志121. 部署jar包 原理一样 部署filebeat 2. 查日志在kibana中搜索]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11 k8s 项目迁移 简单版]]></title>
    <url>%2F2019%2F12%2F24%2Fk8s-11%2F</url>
    <content type="text"><![CDATA[部署前期 准备工作 与 注意事项 准备基础镜像 并 推送到 镜像仓库 部署php/java项目 准备工作 部署的项目情况 1、业务架构及服务 (dubbo、spring cloud) 2、第三方服务 mysql,redis,zookeeper,eruka,mq 3、服务之间怎么通信? 4、资源消耗：硬件，带宽 部署项目时,用到的k8s资源 1、namespace 不同的项目隔离 , 或者隔离不同的环境(test,prod,dev) 2、无状态应用部署 (deployment) 3、有状态应用部署 (statefulset,pv,pvc) 4、发布项目暴露外部访问(service，ingress) 5、信息存储 secret，configmap 准备基础镜像 123[root@k8s-node2 nfs]# docker search lizhenlianglizhenliang/nginx-phplizhenliang/tomcat 编排部署 1、编写yaml文件 工作流程 1、kubectl -&gt; yaml -&gt; 镜像仓库拉取镜像 -&gt; service(集群内部)，ingresss -&gt; 暴露给外部访问 部署 Java 应用制作镜像 如果有cicd环境 自动完成 (代码拉取-&gt;代码编译构建-&gt;镜像打包-&gt;推送到镜像仓库) 没有 就手动构建 镜像为交付物 准备 pv 和 pvc安装 nfs 服务 选定 172.17.70.254 作为 nfs 服务端 12# 安装 nfs 所有node都需要安装 [root@k8s-node2 nfs]# yum install nfs-utils -y 1234567# 配置共享目录[root@k8s-node2 ifs]# mkdir -p /ifs/kubernetes/[root@k8s-node2 nfs]# vim /etc/exports/ifs/kubernetes *(rw,no_root_squash)# 启动服务[root@k8s-node2 ifs]# systemctl start nfs 1234567891011121314151617181920212223242526# 如过遇到nfs无法启动的检查步骤[root@k8s-node2 ifs]# find /etc/ -name '*rpcbind.socket*'/etc/systemd/system/sockets.target.wants/rpcbind.socket[root@k8s-node2 ifs]# vim /etc/systemd/system/sockets.target.wants/rpcbind.socket[Unit]Description=RPCbind Server Activation Socket[Socket]ListenStream=/var/run/rpcbind.sock# RPC netconfig can't handle ipv6/ipv4 dual socketsBindIPv6Only=ipv6-only#ListenStream=0.0.0.0:111ListenDatagram=0.0.0.0:111#ListenStream=[::]:111 # 如果监听了ipv6地址，将这一行注释即可ListenDatagram=[::]:111[Install]WantedBy=sockets.target# 重载一下 再启动[root@k8s-node2 ifs]# systemctl daemon-reload[root@k8s-node2 ifs]# systemctl restart rpcbind.socket[root@k8s-node2 ifs]# systemctl start nfs 基于nfs 配置 PV 动态供给12345678# 上传nfs-client 修改IP地址为nfs服务端地址# 由于k8s不支持 nfs的StorageClass 所有这些文件要从官网下载# https://github.com/kubernetes-incubator/external-storage[root@k8s-master1 nfs-client]# ls -ltotal 12-rw-r--r-- 1 root root 225 Nov 30 20:50 class.yaml # StorageClass 动态pv创建-rw-r--r-- 1 root root 993 Dec 13 09:25 deployment.yaml # 创建 nfs-client pod-rw-r--r-- 1 root root 1526 Nov 30 20:50 rbac.yaml # nfs pod 访问 apiserver 123456# 创建[root@k8s-master1 nfs-client]# kubectl apply -f .[root@k8s-master1 nfs-client]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-5dd6f66f47-99w7k 1/1 Running 0 20s 123456789[root@k8s-master1 nfs-client]# cat class.yaml apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: # 指定 StorageClass 创建 pv 的名称,创建项目的时候用到 name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'parameters: archiveOnDelete: "true" 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@k8s-master1 nfs-client]# cat deployment.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: apps/v1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs # nfs地址 - name: NFS_SERVER value: 172.17.70.254 # 共享目录 - name: NFS_PATH value: /ifs/kubernetes volumes: - name: nfs-client-root nfs: server: 172.17.70.254 path: /ifs/kubernetes 部署 harbor仓库1234567# 该主机也需要二进制安装docker [root@k8s-master2 src]# scp root@172.17.70.254:/opt/src/docker-18.09.6.tgz .[root@k8s-master2 src]# scp root@172.17.70.254:/usr/lib/systemd/system/docker.service .[root@k8s-master2 src]# scp root@172.17.70.254:/etc/docker/daemon.json .[root@k8s-master2 src]# tar -zxvf docker-18.09.6.tgz [root@k8s-master2 src]# mv docker/* /usr/bin/[root@k8s-master2 src]# mv docker.service /usr/lib/systemd/system 12345678[root@k8s-master2 src]# mkdir -p /etc/docker[root@k8s-master2 src]# mv daemon.json /etc/docker/[root@k8s-master2 src]# cat /etc/docker/daemon.json &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], # 镜像加速 "insecure-registries": ["172.17.70.252"] # 可信任IP 回头换成harbor地址&#125; 123[root@k8s-node1 src]# systemctl start docker[root@k8s-node1 src]# systemctl enable docker[root@k8s-node1 src]# docker info 1234# 上传文件[root@k8s-master2 src]# ls -l-rw-r--r-- 1 root root 17237024 Nov 13 14:33 docker-compose-Linux-x86_64-rw-r--r-- 1 root root 580462944 Nov 13 14:34 harbor-offline-installer-v1.8.4.tgz 12345# 部署 compose[root@k8s-master2 src]# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose[root@k8s-master2 src]# chmod +x /usr/local/bin/docker-compose [root@k8s-master2 src]# docker-compose -versiondocker-compose version 1.25.0dev, build bc57a1bd 1234567891011121314151617# 部署 harbor[root@k8s-master2 src]# tar -xf harbor-offline-installer-v1.8.4.tgz -C /opt/[root@k8s-master2 src]# cd /opt/harbor/# 修改主机名和管理员密码、数据库密码hostname: 172.17.70.252 # httpharbor_admin_password: 123456 # 访问密码database: password: 123456# 准备[root@k8s-master2 src]# ./prepare# 安装[root@k8s-master2 src]# ./install.sh# web访问http://123.56.14.192# 列出docker-compose ps 免https使用12345678910111213141516[root@Docker harbor]# vim /etc/docker/daemon.json # 写入进项仓库 IP+port&#123; "registry-mirrors": ["http://f1361db2.m.daocloud.io"], "insecure-registries": ["172.17.70.252"]&#125;# 重启docker systemctl daemon-reloadsystemctl restart docker.service[root@Docker nginx]# docker infoInsecure Registries: 172.17.70.252 127.0.0.0/8 登录1234# 两个node节点都先登录下 好下载基础镜像[root@k8s-node1 cfg]# docker login 172.17.70.252Username: adminPassword: 下载 pod基础镜像 并上传到私有仓库1234[root@k8s-master2 opt]# docker pull lizhenliang/pause-amd64:3.0 [root@k8s-master2 opt]# docker tag lizhenliang/pause-amd64:3.0 172.17.70.252/base/pause-amd64:3.0[root@k8s-master2 opt]# docker login 172.17.70.252[root@k8s-master2 opt]# docker push 172.17.70.252/base/pause-amd64:3.0 构建基础镜像 nginx php tomcat 构建项目镜像安装Git1[root@k8s-node3 opt]# yum install git 下载项目123456789101112[root@k8s-node3 opt]# cd /opt/[root@k8s-node3 opt]# git clone https://github.com/lizhenliang/tomcat-java-demo.git[root@k8s-node3 opt]# ls -ltotal 32drwxr-xr-x 2 root root 4096 Nov 20 16:33 db # 数据库sql文件-rw-r--r-- 1 root root 148 Nov 20 16:33 Dockerfile # 构建镜像-rw-r--r-- 1 root root 11357 Nov 20 16:33 LICENSE-rw-r--r-- 1 root root 1930 Nov 20 16:33 pom.xml # maven构建环境-rw-r--r-- 1 root root 270 Nov 20 16:33 README.mddrwxr-xr-x 3 root root 4096 Nov 20 16:33 src # 源码目录 编译源码123456789101112131415161718192021# 安装 jdk 和 maven 环境[root@k8s-node3 tomcat-java-demo]# yum search openjdk[root@k8s-node3 tomcat-java-demo]# yum install java-1.8.0-openjdk maven# mvn 编译[root@k8s-node3 tomcat-java-demo]# mvn clean package -Dmaven.test.skip=true...[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.882s[INFO] Finished at: Wed Nov 20 17:08:13 CST 2019[INFO] Final Memory: 22M/95M[INFO] ------------------------------------------------------------------------# 查看生成的war包[root@k8s-node3 tomcat-java-demo]# lsdb Dockerfile LICENSE pom.xml README.md src target[root@k8s-node3 tomcat-java-demo]# cd target/[root@k8s-master2 target]# ls -l *.war-rw-r--r-- 1 root root 18265402 Nov 20 17:08 ly-simple-tomcat-0.0.1-SNAPSHOT.war 构建项目镜像1234567# 引用了老师的tomcat镜像运行环境[root@k8s-master2 tomcat-java-demo]# vim Dockerfile FROM lizhenliang/tomcatLABEL maintainer www.ctnrs.comRUN rm -rf /usr/local/tomcat/webapps/*ADD target/*.war /usr/local/tomcat/webapps/ROOT.war 123456789# 构建镜像[root@k8s-master2 tomcat-java-demo]# docker build -t 172.17.70.252/project/java-demo .[root@k8s-master2 tomcat-java-demo]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE172.17.70.252/project/java-demo latest f656fe851ad7 24 seconds ago 406M# 推送到镜像仓库[root@k8s-master2 tomcat-java-demo]# docker login 172.17.70.252[root@k8s-master2 tomcat-java-demo]# docker push 172.17.70.252/project/java-demo 无状态部署 tomcat tomcat deployment service ingress namespace # 逻辑项目隔离 创建 命名空间123456[root@k8s-master1 java-demo]# vim namespace.yaml apiVersion: v1kind: Namespacemetadata: name: test 123456789101112[root@k8s-master1 java-demo]# kubectl apply -f namespace.yaml namespace/test created[root@k8s-master1 java-demo]# kubectl get nsNAME STATUS AGEdefault Active 8dingress-nginx Active 8dkube-node-lease Active 8dkube-public Active 8dkube-system Active 8dkubernetes-dashboard Active 8dtest Active 19s 部署 deployment12345678910# registry-pull-secret 创建仓库凭证[root@k8s-master1 java-demo]# kubectl create secret docker-registry registry-pull-secret --docker-username=admin --docker-password=lx@68328153 --docker-email=253911339@qq.com --docker-server=172.17.70.252 -n test secret/registry-pull-secret created [root@k8s-master1 java-demo]# kubectl get secret -n testNAME TYPE DATA AGEdefault-token-mnnbn kubernetes.io/service-account-token 3 14mregistry-pull-secret kubernetes.io/dockerconfigjson 1 12s 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@k8s-master1 templates]# vim /opt/java-demo/deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-java-demo namespace: testspec: replicas: 3 selector: matchLabels: project: www app: java-demo template: metadata: labels: # 项目标签 project: www app: java-demo spec: # 镜像仓库凭据 imagePullSecrets: - name: registry-pull-secret containers: - name: tomcat image: 172.17.70.252/project/java-demo:latest # Always 每次创建 Pod 都会重新拉取一次镜像 imagePullPolicy: Always ports: - containerPort: 8080 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 1Gi limits: cpu: 1 memory: 2Gi # 健康检查 livenessProbe: httpGet: path: / port: 8080 # 容器启动60秒后 开始健康检查 initialDelaySeconds: 60 timeoutSeconds: 20 # 健康检查,检查失败，k8s会把Pod从service endpoints中剔除 readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 60 timeoutSeconds: 20 123456# 创建启动[root@k8s-master1 java-demo]# kubectl get pods -n testNAME READY STATUS RESTARTS AGEtomcat-java-demo-d5d9c46cb-7zw4h 0/1 Running 0 7stomcat-java-demo-d5d9c46cb-lltt5 0/1 Running 0 7stomcat-java-demo-d5d9c46cb-qbc9g 0/1 Running 0 7s 项目发布 service12345678910111213141516[root@k8s-master1 java-demo]# vim service.yaml apiVersion: v1kind: Servicemetadata: name: tomcat-java-demo namespace: testspec: selector: project: www app: java-demo ports: - name: web port: 80 targetPort: 8080 type: NodePort 1234567891011121314151617# 创建并测试[root@k8s-master1 java-demo]# kubectl apply -f service.yaml service/tomcat-java-demo created[root@k8s-master1 java-demo]# kubectl get svc,ep -n testNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/tomcat-java-demo NodePort 10.0.0.72 &lt;none&gt; 80:31193/TCP 10sNAME ENDPOINTS AGEendpoints/tomcat-java-demo 10.244.0.62:8080,10.244.1.72:8080,10.244.2.95:8080 10s[root@k8s-master1 java-demo]# curl -I 10.0.0.72HTTP/1.1 200 Content-Type: text/html;charset=utf-8Content-Language: en-USContent-Length: 2538Date: Wed, 25 Dec 2019 07:58:37 GMT 暴露项目 Ingress1234567891011121314151617[root@k8s-master1 java-demo]# vim ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: tomcat-java-demo namespace: testspec: rules: - host: java.ctnrs.com http: paths: - path: / backend: # 对应 servicename serviceName: tomcat-java-demo servicePort: 80 123456[root@k8s-master1 java-demo]# kubectl apply -f ingress.yaml ingress.extensions/tomcat-java-demo created[root@k8s-master1 java-demo]# kubectl get ing -n testNAME HOSTS ADDRESS PORTS AGEtomcat-java-demo java.ctnrs.com 80 103s 测试访问12345[root@k8s-master1 java-demo]# cat /etc/hosts39.106.100.108 java.ctnrs.com39.106.168.181 java.ctnrs.com60.205.217.112 java.ctnrs.com 有状态部署 mysql mysql statefulset Headless Service (dns绑定) pv,pvc (stotageclass pv自动供给) 123# Headless Service常规 Service：一组POD的访问策略，提供负载均衡服务发现Headless Service: 不需要Cluster-IP，他会直接绑定到PODIP 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master1 java-demo]# vim mysql.yaml apiVersion: v1kind: Servicemetadata: name: mysql namespace: testspec: ports: - port: 3306 name: mysql clusterIP: None selector: app: mysql-public---apiVersion: apps/v1kind: StatefulSetmetadata: name: db namespace: testspec: serviceName: "mysql" selector: matchLabels: app: mysql-public template: metadata: labels: app: mysql-public spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: "123456" - name: MYSQL_DATABASE value: test ports: - containerPort: 3306 volumeMounts: - mountPath: "/var/lib/mysql" name: mysql-data volumeClaimTemplates: - metadata: name: mysql-data spec: accessModes: ["ReadWriteMany"] # 动态pv name storageClassName: "managed-nfs-storage" resources: requests: storage: 2Gi 1234567891011121314151617# 创建并测试[root@k8s-master1 java-demo]# kubectl apply -f mysql.yaml [root@k8s-master1 java-demo]# kubectl get pods,svc -n testNAME READY STATUS RESTARTS AGEpod/db-0 1/1 Running 0 31spod/tomcat-java-demo-d5d9c46cb-7zw4h 1/1 Running 0 45mpod/tomcat-java-demo-d5d9c46cb-lltt5 1/1 Running 0 45mpod/tomcat-java-demo-d5d9c46cb-qbc9g 1/1 Running 0 45mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/mysql ClusterIP None &lt;none&gt; 3306/TCP 73sservice/tomcat-java-demo NodePort 10.0.0.72 &lt;none&gt; 80:31193/TCP 38m[root@k8s-master1 java-demo]# kubectl describe pod db-0 -n test[root@k8s-master1 java-demo]# kubectl get pv,pvc -n test 123456789101112131415161718192021222324252627# 登录测试[root@k8s-master1 java-demo]# kubectl exec -it db-0 bash -n testroot@db-0:/# mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.28 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.00 sec) 导入数据123456789101112131415161718192021222324252627282930# copy文件[root@k8s-node3 db]# scp tables_ly_tomcat.sql root@172.17.70.251:/opt/java-demo# 上传到容器[root@k8s-master1 java-demo]# kubectl cp tables_ly_tomcat.sql db-0:/tmp -n test[root@k8s-master1 java-demo]# kubectl exec -it db-0 bash -n testroot@db-0:/# mysql -uroot -pmysql&gt; source /tmp/tables_ly_tomcat.sql;# 查看mysql&gt; use test;Database changedmysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| user |+----------------+1 row in set (0.01 sec)mysql&gt; desc user;+-------+--------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(100) | NO | | NULL | || age | int(3) | NO | | NULL | || sex | char(1) | YES | | NULL | |+-------+--------------+------+-----+---------+----------------+4 rows in set (0.00 sec) 修改代码 数据库连接地址123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 修改源码 然后重新打包镜像# 密码不要忘记修改 因为是连库串# nslookup mysql.test# ping db-0.mysql.test# db-0.service名字.命名空间# db-0.mysql.test# 通过这个地址 连接mysqlpod的固定地址 这样即使pod销毁了地址不变[root@k8s-node3 tomcat-java-demo]# vim src/main/resources/application.yml ... datasource: url: jdbc:mysql://db-0.mysql.test:3306/test?characterEncoding=utf-8 username: root password: 123456...# 重新打包镜像部署 版本号变更[root@k8s-node3 tomcat-java-demo]# mvn clean package -Dmaven.test.skip=true[root@k8s-node3 tomcat-java-demo]# docker build -t 172.17.70.252/project/java-demo:1.0.1 .[root@k8s-node3 tomcat-java-demo]# docker push 172.17.70.252/project/java-demo:1.0.1 # 重新部署 deployment # 滚动更新时间跟 健康检查有关[root@k8s-master1 java-demo]# vim deployment.yaml ... containers: - name: tomcat image: 172.17.70.252/project/java-demo:1.0.1...[root@k8s-master1 java-demo]# kubectl delete -f deployment.yaml [root@k8s-master1 java-demo]# kubectl apply -f deployment.yaml [root@k8s-master1 java-demo]# kubectl get pods -n testNAME READY STATUS RESTARTS AGEdb-0 1/1 Running 0 38mtomcat-java-demo-54d46868df-7vjs8 0/1 Running 0 5stomcat-java-demo-54d46868df-bhcsp 0/1 Running 0 5stomcat-java-demo-54d46868df-mfjlc 0/1 Running 0 5s# 检查连库文件是否变更[root@k8s-master1 java-demo]# kubectl exec -it tomcat-java-demo-54d46868df-7vjs8 bash -n test[root@tomcat-java-demo-54d46868df-7vjs8 tomcat]# vi webapps/ROOT/WEB-INF/classes/application.yml # 页面添加后 查看数据库数据root@db-0:/# mysql -uroot -p --default-character-set=utf8Enter password: mysql&gt; use test;mysql&gt; select * from user;+----+-----------+-----+------+| id | name | age | sex |+----+-----------+-----+------+| 1 | 哈哈哈 | 18 | F |+----+-----------+-----+------+1 row in set (0.00 sec) 部署 PHP 项目 项目源码地址 1https://github.com/lizhenliang/php-demo 拉取项目源码123# 拉取项目源码[root@k8s-node3 harbor]# cd /opt/[root@k8s-node3 opt]# git clone https://github.com/lizhenliang/php-demo.git 数据库配置1234# 共用之前java创建的mysql数据库# 地址: db-0.mysql.test# username: root# password: 123456 12345678910111213[root@k8s-node3 php-demo]# vim /opt/php-demo/wp-config.php /** WordPress数据库的名称 */define('DB_NAME', 'test');/** MySQL数据库用户名 */define('DB_USER', 'root');/** MySQL数据库密码 */define('DB_PASSWORD', '123456');/** MySQL主机 */define('DB_HOST', 'db-0.mysql.test'); 创建镜像123# 创建镜像[root@k8s-node3 php-demo]# cd /opt/php-demo/[root@k8s-node3 php-demo]# docker build -t 172.17.70.252/project/php-demo:1.0 . 12# 推送镜像到镜像仓库[root@k8s-node3 php-demo]# docker push 172.17.70.252/project/php-demo:1.0 创建命名空间123456789101112131415161718[root@k8s-master1 php-demo]# vim namespace.yaml apiVersion: v1kind: Namespacemetadata: name: test[root@k8s-master1 php-demo]# kubectl apply -f namespace.yaml namespace/test created[root@k8s-master1 php-demo]# kubectl get nsNAME STATUS AGEdefault Active 9dingress-nginx Active 9dkube-node-lease Active 9dkube-public Active 9dkube-system Active 9dkubernetes-dashboard Active 9dtest Active 3s 创建仓库凭证1234567[root@k8s-master1 php-demo]# kubectl create secret docker-registry registry-pull-secret --docker-username=admin --docker-password=lx@68328153 --docker-email=253911339@qq.com --docker-server=172.17.70.252 -n testsecret/registry-pull-secret created[root@k8s-master1 php-demo]# kubectl get secret -n testNAME TYPE DATA AGEdefault-token-nh896 kubernetes.io/service-account-token 3 4m15sregistry-pull-secret kubernetes.io/dockerconfigjson 1 6s 创建 deployment123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master1 php-demo]# vim deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: php-demo namespace: testspec: replicas: 3 selector: matchLabels: project: www app: php-demo template: metadata: labels: project: www app: php-demo spec: imagePullSecrets: - name: registry-pull-secret containers: - name: nginx image: 172.17.70.252/project/php-demo:1.0 imagePullPolicy: Always ports: - containerPort: 80 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi livenessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 6 timeoutSeconds: 20 readinessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 6 timeoutSeconds: 20 12345[root@k8s-master1 php-demo]# kubectl get pods -n testNAME READY STATUS RESTARTS AGEphp-demo-7d685c9cb6-7qhgq 1/1 Running 0 41sphp-demo-7d685c9cb6-h4b6b 1/1 Running 0 41sphp-demo-7d685c9cb6-kdwr5 1/1 Running 0 41s 创建 service12345678910111213141516[root@k8s-master1 php-demo]# vim service.yaml apiVersion: v1kind: Servicemetadata: name: php-demo namespace: testspec: selector: project: www app: php-demo ports: - name: web port: 80 targetPort: 80 type: NodePort 123456789[root@k8s-master1 php-demo]# kubectl apply -f service.yaml service/php-demo created[root@k8s-master1 php-demo]# kubectl get svc,ep -n testNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/php-demo NodePort 10.0.0.39 &lt;none&gt; 80:31677/TCP 14sNAME ENDPOINTS AGEendpoints/php-demo 10.244.0.67:80,10.244.1.75:80,10.244.2.101:80 14s 1234567891011121314151617# 创建 ingress[root@k8s-master1 php-demo]# vim ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: name: php-demo namespace: testspec: rules: - host: php.ctnrs.com http: paths: - path: / backend: serviceName: php-demo servicePort: 80 12[root@k8s-master1 php-demo]# kubectl apply -f ingress.yaml ingress.extensions/php-demo created 创建 mysql 有状态应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master1 php-demo]# vim mysql.yaml apiVersion: v1kind: Servicemetadata: name: mysql namespace: testspec: ports: - port: 3306 name: mysql clusterIP: None selector: app: mysql-public---apiVersion: apps/v1kind: StatefulSetmetadata: name: db namespace: testspec: serviceName: "mysql" selector: matchLabels: app: mysql-public template: metadata: labels: app: mysql-public spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: "123456" - name: MYSQL_DATABASE value: test ports: - containerPort: 3306 volumeMounts: - mountPath: "/var/lib/mysql" name: mysql-data volumeClaimTemplates: - metadata: name: mysql-data spec: accessModes: ["ReadWriteMany"] # 动态pv name storageClassName: "managed-nfs-storage" resources: requests: storage: 2Gi 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master1 php-demo]# kubectl apply -f mysql.yamlservice/mysql createdstatefulset.apps/db created[root@k8s-master1 php-demo]# kubectl get pods,svc -n testNAME READY STATUS RESTARTS AGEpod/db-0 1/1 Running 0 5spod/php-demo-7d685c9cb6-7qhgq 1/1 Running 0 10mpod/php-demo-7d685c9cb6-h4b6b 1/1 Running 0 10mpod/php-demo-7d685c9cb6-kdwr5 1/1 Running 0 10mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/mysql ClusterIP None &lt;none&gt; 3306/TCP 5sservice/php-demo NodePort 10.0.0.39 &lt;none&gt; 80:31677/TCP 7m53s# 由于我之前测试删除了存储，所有这是个新的数据库# 登录测试[root@k8s-master1 java-demo]# kubectl exec -it db-0 bash -n testroot@db-0:/# mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.28 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.00 sec) 测试域名访问1234567891011121314151617181920# 阿里云还没有屏蔽签测试域名# 屏蔽了 测试 nodeIp+端口39.106.100.108 php.ctnrs.com 39.106.168.181 php.ctnrs.com 60.205.217.112 php.ctnrs.com [root@k8s-master1 php-demo]# kubectl get ing -n testNAME HOSTS ADDRESS PORTS AGEphp-demo php.ctnrs.com 80 10m[root@k8s-master1 php-demo]# kubectl get svc -n testNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmysql ClusterIP None &lt;none&gt; 3306/TCP 6m1sphp-demo NodePort 10.0.0.39 &lt;none&gt; 80:31677/TCP 13m39.106.100.108:31677# 安装 会初始化数据到 mysql中 # 都是项目打包制作镜像时 填写好的配置]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10 k8s 容器持久化存储]]></title>
    <url>%2F2019%2F12%2F09%2Fk8s-10%2F</url>
    <content type="text"><![CDATA[Volume &amp; PersistentVolume12官方文档https://kubernetes.io/docs/concepts/storage/volumes/ Kubernetes中的Volume提供了在容器中挂载外部存储的能力 Pod需要设置卷来源（spec.volume）和挂载点（spec.containers.volumeMounts）两个信息后才可以使用相应的Volume 本地数据卷 hostPath emptyDir Volume 概念 容器中的文件在磁盘上是临时存放的，这给容器中运行的特殊应用程序带来一些问题。 首先，当容器崩溃时，kubelet 将重新启动容器，容器中的文件将会丢失——因为容器会以干净的状态重建。 其次，当在一个 Pod 中同时运行多个容器时，常常需要在这些容器之间共享文件。 Kubernetes 抽象出 Volume 对象来解决这两个问题。 Volume 支持的类型 本地 emptyDir hostPath 网络 自建存储 ceph emptyDir 创建一个空卷，挂载到Pod中的容器。 Pod删除该卷也会被删除,随着pod的生命周期 而存在 应用场景：Pod中容器之间数据共享,一个pod中有多个容器,他们之间完成数据共享,不使用数据卷容器之间的文件系统是隔离的，只能看到自己的 使用数据卷让容器之间某个目录达到共享 12345678910111213141516171819202122232425[root@k8s-master demo2]# vim emptydir.yaml apiVersion: v1kind: Podmetadata: name: my-podspec: containers: - name: write image: centos:7 command: ["bash","-c","for i in &#123;1..100&#125;;do echo $i &gt;&gt; /data/hello;sleep 1;done"] volumeMounts: - name: data mountPath: /data - name: read image: centos:7 command: ["bash","-c","tail -f /data/hello"] volumeMounts: - name: data mountPath: /data volumes: - name: data emptyDir: &#123;&#125; 123456789101112[root@k8s-master demo2]# kubectl apply -f emptydir.yaml pod/my-pod created[root@k8s-master demo2]# kubectl get podNAME READY STATUS RESTARTS AGEmy-pod 2/2 Running 0 4s# 两个容器 # write负责写# read读取到了数据卷中的数据# -f 实时输出 [root@k8s-master demo2]# kubectl logs my-pod -c read -f hostPath 挂载 Node 文件系统上文件或者目录到Pod中的容器。 应用场景：Pod中容器需要访问宿主机文件 hostPath 有点像 Bind Mounts emptyDir 有点像 Volume 12345678910111213141516171819202122[root@k8s-master demo2]# vim hostPath.yamlapiVersion: v1kind: Podmetadata: name: my-pod2spec: containers: - name: busybox image: busybox args: - /bin/sh - -c - sleep 36000 volumeMounts: - name: data mountPath: /data volumes: - name: data hostPath: path: /tmp type: Directory 12[root@k8s-master demo2]# kubectl apply -f hostPath.yaml pod/my-pod2 created 12[root@k8s-master demo2]# kubectl get pod -o widemy-pod2 1/1 Running 10.244.0.33 k8s-node1 123456789101112[root@k8s-master demo2]# kubectl exec -it my-pod2 sh/ # cd /data//data # lsAegis-&lt;Guid(5A2C30A2-A87D-490A-9281-6765EDAD7CBA)&gt; systemd-private-4528dd471fc14018952a13c04541edd8-chronyd.service-GCklT8[root@k8s-node1 tmp]# touch pod2/data # ls -ltotal 4srwxr-xr-x 1 root root 0 Nov 9 05:47 Aegis-&lt;Guid(5A2C30A2-A87D-490A-9281-6765EDAD7CBA)&gt;-rw-r--r-- 1 root root 0 Nov 9 07:39 pod2drwx------ 3 root root 4096 Nov 9 05:47 systemd-private-4528dd471fc14018952a13c04541edd8-chronyd.service-GCklT8 NFS（网络存储） 本地数据卷 只能绑定指定的node上,如果node出现问题,node上的pod会被放到其他node上,数据就无从获取 挂载网络卷 就算拉一起一个新的也能访问到 安装nfs 123# 选择master2 作为服务端[root@k8s-master2 ~]# yum install -y nfs-utils# 客户端也就是node上也要安装 配置服务端的访问路径 启动服务端守护进程 1234567[root@k8s-master2 ~]# vim /etc/exports/data/nfs *(rw,no_root_squash)[root@k8s-master2 ~]# systemctl start nfs[root@k8s-master2 ~]# ps -ef|grep nfs# 得有这个目录才能挂载[root@k8s-master2 ~]# mkdir -p /data/nfs 配置启动 123456789101112131415161718192021222324252627282930# k8s帮我们mount 在配置文件里指定，只要安装nfs客户端即可[root@k8s-master demo2]# vim nfs.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-nfs-deploymentspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16 volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html ports: - containerPort: 80 volumes: - name: wwwroot nfs: server: 172.17.70.245 path: /data/nfs 12[root@k8s-master demo2]# kubectl apply -f nfs.yaml deployment.apps/nginx-nfs-deployment created 12345[root@k8s-master demo2]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-nfs-deployment-5b8f7c4d57-9tbgq 1/1 Running 0 3snginx-nfs-deployment-5b8f7c4d57-mgvzh 1/1 Running 0 3snginx-nfs-deployment-5b8f7c4d57-wqwtg 1/1 Running 0 3s 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master demo2]# kubectl exec -it nginx-nfs-deployment-5b8f7c4d57-9tbgq bashroot@nginx-nfs-deployment-5b8f7c4d57-9tbgq:/# cd /usr/share/nginx/html/# 去nfs数据目录服务端加点东西再查看[root@k8s-master2 nfs]# echo "hello nginx" &gt;&gt; index.html# pod中的容器有了同样的文件 挂载成功root@nginx-nfs-deployment-5b8f7c4d57-9tbgq:/usr/share/nginx/html# lsindex.html# 为这组pod创建svc[root@k8s-master1 demo]# vim nfs-svc.yaml apiVersion: v1kind: Servicemetadata: labels: app: nfs-nginx name: nfs-nginxspec: type: NodePort ports: - port: 80 protocol: TCP targetPort: 80 nodePort: 31000 selector: app: nginx[root@k8s-master1 demo]# kubectl apply -f nfs-svc.yaml service/nfs-nginx created[root@k8s-master demo2]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.17.70.246:6443 2d7hmy-service 10.244.0.35:80,10.244.1.35:80,10.244.1.36:80 24h# web访问http://123.56.14.192:31000/# 如果pod被销毁 但是数据不会丢 1234567891011121314151617# 销毁[root@k8s-master demo2]# kubectl delete -f nfs.yaml deployment.apps "nginx-nfs-deployment" deleted# 启动[root@k8s-master demo2]# kubectl apply -f nfs.yaml deployment.apps/nginx-nfs-deployment created[root@k8s-master demo2]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-nfs-deployment-5b8f7c4d57-2pg97 1/1 Running 0 28snginx-nfs-deployment-5b8f7c4d57-84c6d 1/1 Running 0 28snginx-nfs-deployment-5b8f7c4d57-9rh6k 1/1 Running 0 28s[root@k8s-master demo2]# kubectl exec -it nginx-nfs-deployment-5b8f7c4d57-9rh6k bashroot@nginx-nfs-deployment-5b8f7c4d57-9rh6k:/# cd /usr/share/nginx/html/ root@nginx-nfs-deployment-5b8f7c4d57-9rh6k:/usr/share/nginx/html# lsindex.html PersistentVolume 持久存储数据卷 PersistenVolume（PV）：对存储资源创建和使用的抽象，使得存储作为集群中的资源管理。(专业的存储人员来做) 静态 手动创建资源 动态 自动创建PV PersistentVolumeClaim（PVC）：让用户不需要关心具体的Volume实现细节,只关心用多大的容量。 作用: 将存储资源作为集群的一部分来管理,开发者不用关系如何创造出存储,也不必担心暴露存储的位置。 Pod使用持久卷在任何地点都能访问,即使POD销毁再拉起也能使用。 PV是提供存储容量的,PVC是消费存储的。PV与PVC的关系是绑定,绑定后其他人就无法使用了。 pv 静态供给 容器中指定 pvc 创建pvc需求模板 创建pv 创建 pv pv可以是存储人员定义,他们会创建很多pv等待pvc来挂载 123456789101112131415161718192021222324[root@k8s-master1 demo]# cat pv-nfs.yaml apiVersion: v1kind: PersistentVolumemetadata: name: my-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteMany nfs: path: /data/nfs server: 172.31.228.53[root@k8s-master1 demo]# vim pv-nfs.yaml[root@k8s-master1 demo]# kubectl apply -f pv-nfs.yaml persistentvolume/my-pv created[root@k8s-master1 demo]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmy-pv 5Gi RWX Retain Available 4s# Available 可用状态 创建 deployment 指定pvc123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master1 demo]# vim nfs-pvc.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-nfs-deploymentspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16 volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html ports: - containerPort: 80 volumes: - name: wwwroot persistentVolumeClaim: # pv 名称 claimName: my-pvc---apiVersion: v1kind: PersistentVolumeClaimmetadata: # 匹配 claimName name: my-pvcspec: # 访问模式 RMX 可以同时在多个节点上挂载并被不同的Pod使用 accessModes: - ReadWriteMany resources: requests: # 请求存储大小 5G storage: 5Gi 123456789101112131415161718[root@k8s-master1 demo]# kubectl apply -f nfs-pvc.yaml [root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-nfs-deployment-7ccdb4f76f-7m6cp 1/1 Running 0 12snginx-nfs-deployment-7ccdb4f76f-fmmqt 1/1 Running 0 12snginx-nfs-deployment-7ccdb4f76f-x2fbj 1/1 Running 0 12s[root@k8s-master1 demo]# kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/my-pv 5Gi RWX Retain Bound default/my-pvc 2m55sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/my-pvc Bound my-pv 5Gi RWX 39s# my-pv 状态 Bound # my-pvc 状态 Bound VOLUME = my-pv # pv 与 pvc 绑定成功 1234[root@k8s-master1 demo]# kubectl exec -it nginx-nfs-deployment-7ccdb4f76f-7m6cp bashroot@nginx-nfs-deployment-7ccdb4f76f-7m6cp:/# cd /usr/share/nginx/html/root@nginx-nfs-deployment-7ccdb4f76f-7m6cp:/usr/share/nginx/html# lsindex.html 删除 pvc 和 pv 默认情况下 删除的pvc，之前与他绑定的pv也不能够再使用了，需要手动清理pv,但是数据还在 12345678910[root@k8s-master1 demo]# kubectl delete -f nfs-pvc.yaml deployment.apps "nginx-nfs-deployment" deletedpersistentvolumeclaim "my-pvc" deleted[root@k8s-master1 demo]# kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/my-pv 5Gi RWX Retain Released default/my-pvc 9m7s# 手动删除pv[root@k8s-master1 demo]# kubectl delete -f pv-nfs.yaml 123456789101112131415161718192021222324252627# 重新部署应用 , 查看数据[root@k8s-master1 demo]# kubectl apply -f pv-nfs.yaml [root@k8s-master1 demo]# kubectl apply -f nfs-pvc.yaml [root@k8s-master1 demo]# kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/my-pv 5Gi RWX Retain Bound default/my-pvc 11sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/my-pvc Bound my-pv 5Gi RWX 3s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-nfs-deployment-7ccdb4f76f-9p94w 1/1 Running 0 49snginx-nfs-deployment-7ccdb4f76f-jtfsm 1/1 Running 0 49snginx-nfs-deployment-7ccdb4f76f-rllm5 1/1 Running 0 49s[root@k8s-master1 demo]# kubectl exec -it nginx-nfs-deployment-7ccdb4f76f-rllm5 bashroot@nginx-nfs-deployment-7ccdb4f76f-rllm5:/# cd /usr/share/nginx/html/root@nginx-nfs-deployment-7ccdb4f76f-rllm5:/usr/share/nginx/html# cat index.html 123root@nginx-nfs-deployment-7ccdb4f76f-rllm5:/usr/share/nginx/html# echo 666 &gt; index.html [root@k8s-node2 wwwroot]# cd /data/nfs/wwwroot/[root@k8s-node2 wwwroot]# cat index.html 666 总结 PV 静态供给 持久卷静态供给,pod需要申请pvc,可以在deployment同一个yaml中定义 提供数据卷定义,创建 pv pvc会根据绑定关系，尤其是存储的容量和访问模式去匹配pvc 这种情况下创建pv需要手动创建，如何可以自动部署绑定 pv 动态供给 主要是针对容量问题，手动划分非常麻烦，如果pvc的容量匹配不上pv就无法绑定 k8s的动态供给就是可以动态划分容量 Dynamic Provisioning机制工作的核心在于StorageClass的API对象。 StorageClass声明存储插件，用于自动创建PV。 k8s 支持持久卷的存储插件1https://kubernetes.io/docs/concepts/storage/persistent-volumes/ StorageClass StorageClass 是能够自动操作后端存储，并且自动创建pv 。 StorageClass 声明使用哪种存储插件，它来对接存储。 Kubernetes支持动态供给的存储插件： 1https://kubernetes.io/docs/concepts/storage/storage-classes/ 123456789101112# 上传 storage-class[root@k8s-master1 storage-class]# pwd/opt/storage-class[root@k8s-master1 storage-class]# ls -ltotal 20-rw-r--r-- 1 root root 886 Dec 31 16:45 deployment-nfs.yaml-rw-r--r-- 1 root root 842 Dec 31 16:45 nginx-demo.yaml-rw-r--r-- 1 root root 703 Dec 31 16:45 pvc.yaml-rw-r--r-- 1 root root 949 Dec 31 16:45 rbac.yaml-rw-r--r-- 1 root root 120 Dec 31 16:45 storageclass-nfs.yaml StorageClass 定义12345678[root@k8s-master1 storage-class]# cat storageclass-nfs.yaml apiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata: # StorageClass 名字 name: managed-nfs-storage # 提供者标识 外部插件provisioner: fuseim.pri/ifs 提供者 自动创建pv1234567891011121314151617181920212223242526272829303132333435363738394041# 该服务帮我们自动创建pv[root@k8s-master1 storage-class]# vim deployment-nfs.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: # 授权 serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: lizhenliang/nfs-client-provisioner:v2.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs # 定义nfs地址 - name: NFS_SERVER value: 172.31.228.53 - name: NFS_PATH value: /data/nfs volumes: - name: nfs-client-root nfs: server: 172.31.228.53 path: /data/nfs 123参考地址:https://github.com/kubernetes-incubator/external-storagehttps://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client/deploy 授权 动态创建pv插件需要连接apiserver ，所以需要授权 12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-master1 storage-class]# cat rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["list", "watch", "create", "update", "patch"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io 创建 pv 动态供给123456789101112[root@k8s-master1 storage-class]# kubectl apply -f storageclass-nfs.yaml[root@k8s-master1 storage-class]# kubectl apply -f rbac.yaml [root@k8s-master1 storage-class]# kubectl apply -f deployment-nfs.yaml [root@k8s-master1 storage-class]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 9m27s# 查看 自动创建pv的pod 当申请资源的时候这个pod会自动去创建[root@k8s-master1 storage-class]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-57998f486c-8nqsp 1/1 Running 0 86s 自动供给流程 kubectl 部署有状态应用，唯一的网路身份标识符，主机名=dns名称和持久存储 mysql主从的数据不一样的，所以需要存储保持不同的数据 Statefulset 也会去维护存储，网络身份和存储都标识 0 1 2 部署一个应用存储部分会去请求 -&gt; storageclass -&gt; nfs-client-provisioner 这个pod -&gt; 请求nfs创建pv 应用里面指定好 使用哪个 storageclass 就行 pv 动态供给应用案例 Statefulset + MySQL1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master1 storage-class]# cat mysql-demo.yamlapiVersion: v1kind: Servicemetadata: name: mysqlspec: ports: - port: 3306 name: mysql clusterIP: None selector: app: mysql-public---apiVersion: apps/v1kind: StatefulSetmetadata: name: dbspec: serviceName: "mysql" selector: matchLabels: app: mysql-public replicas: 3 template: metadata: labels: app: mysql-public spec: containers: - name: mysql image: mysql:5.7 env: - name: MYSQL_ROOT_PASSWORD value: "123456" - name: MYSQL_DATABASE value: test ports: - containerPort: 3306 volumeMounts: - mountPath: "/var/lib/mysql" name: mysql-data volumeClaimTemplates: - metadata: name: mysql-data spec: accessModes: ["ReadWriteMany"] # 动态pv name storageClassName: "managed-nfs-storage" resources: requests: storage: 2Gi 123456789101112131415161718192021222324[root@k8s-master1 storage-class]# kubectl get pod,pv,pvcNAME READY STATUS RESTARTS AGEpod/db-0 1/1 Running 0 32mpod/db-1 1/1 Running 0 105spod/db-2 1/1 Running 0 28spod/nfs-client-provisioner-57998f486c-8nqsp 1/1 Running 0 91mNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX Delete Bound default/mysql-data-db-0 managed-nfs-storage 32mpersistentvolume/default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX Delete Bound default/mysql-data-db-1 managed-nfs-storage 105spersistentvolume/default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX Delete Bound default/mysql-data-db-2 managed-nfs-storage 28sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql-data-db-0 Bound default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX managed-nfs-storage 32mpersistentvolumeclaim/mysql-data-db-1 Bound default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX managed-nfs-storage 105spersistentvolumeclaim/mysql-data-db-2 Bound default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX managed-nfs-storage 29s[root@k8s-node2 nfs]# ls -ltotal 16drwxrwxrwx 6 polkitd root 4096 Dec 31 18:07 default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845drwxrwxrwx 6 polkitd root 4096 Dec 31 18:37 default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07drwxrwxrwx 6 polkitd root 4096 Dec 31 18:37 default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089edrwxr-xr-x 2 root root 4096 Dec 31 16:04 wwwroot 创建mysql pod 去测试连接数据库12345678910111213141516171819# mysql 通过dns访问# dns=pod名称.service名称.命名空间[root@k8s-master1 storage-class]# kubectl run -it --image mysql:5.7 mysql-client --restart=Never --rm /bin/bashroot@mysql-client:/# mysql -h db-0.mysql.default -uroot -pmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.00 sec)mysql&gt; create database leo; 测试删除一个pod 看能否自动挂载存储12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master1 storage-class]# kubectl delete pod db-0 # 删除后自动拉起了pod[root@k8s-master1 storage-class]# kubectl get pod,pv,pvcNAME READY STATUS RESTARTS AGEpod/db-0 1/1 Running 0 5spod/db-1 1/1 Running 0 12mpod/db-2 1/1 Running 0 11mpod/nfs-client-provisioner-57998f486c-8nqsp 1/1 Running 0 101mNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX Delete Bound default/mysql-data-db-0 managed-nfs-storage 42mpersistentvolume/default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX Delete Bound default/mysql-data-db-1 managed-nfs-storage 12mpersistentvolume/default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX Delete Bound default/mysql-data-db-2 managed-nfs-storage 11mNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql-data-db-0 Bound default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX managed-nfs-storage 42mpersistentvolumeclaim/mysql-data-db-1 Bound default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX managed-nfs-storage 12mpersistentvolumeclaim/mysql-data-db-2 Bound default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX managed-nfs-storage 11m# 查看数据是否还在[root@k8s-master1 storage-class]# kubectl run -it --image mysql:5.7 mysql-client --restart=Never --rm /bin/bashroot@mysql-client:/# mysql -h db-0.mysql.default -uroot -p# 之前创建的库还存在mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || leo || mysql || performance_schema || sys || test |+--------------------+6 rows in set (0.01 sec) 删除 pv12345678910111213141516171819202122232425262728293031321. 先删除 与其关联的 Pod 及 PVC[root@k8s-master1 storage-class]# kubectl delete -f .2. 如果还存在pv和pvc[root@k8s-master1 storage-class]# kubectl get pod,pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX Delete Terminating default/mysql-data-db-0 managed-nfs-storage 50mpersistentvolume/default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX Delete Bound default/mysql-data-db-1 managed-nfs-storage 20mpersistentvolume/default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX Delete Bound default/mysql-data-db-2 managed-nfs-storage 19mNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql-data-db-0 Bound default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845 2Gi RWX managed-nfs-storage 50mpersistentvolumeclaim/mysql-data-db-1 Bound default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07 2Gi RWX managed-nfs-storage 20mpersistentvolumeclaim/mysql-data-db-2 Bound default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e 2Gi RWX managed-nfs-storage 19m# 手动删除[root@k8s-master1 storage-class]# kubectl delete persistentvolumeclaim/mysql-data-db-0 [root@k8s-master1 storage-class]# kubectl delete persistentvolumeclaim/mysql-data-db-1[root@k8s-master1 storage-class]# kubectl delete persistentvolumeclaim/mysql-data-db-2[root@k8s-master1 storage-class]# kubectl delete persistentvolume/default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845[root@k8s-master1 storage-class]# kubectl delete persistentvolume/default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07[root@k8s-master1 storage-class]# kubectl delete persistentvolume/default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089e# 数据还在存储里[root@k8s-node2 nfs]# ls -ldrwxrwxrwx 7 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845drwxrwxrwx 6 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07drwxrwxrwx 6 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089edrwxr-xr-x 2 root root 4096 Dec 31 16:04 wwwroot 重新建立pv,pvc和 mysql有状态部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 如果删除了 pv和pvc 再重新创建pod 是会重新分配存储 [root@k8s-master1 storage-class]# kubectl apply -f storageclass-nfs.yaml[root@k8s-master1 storage-class]# kubectl apply -f rbac.yaml [root@k8s-master1 storage-class]# kubectl apply -f deployment-nfs.yaml [root@k8s-master1 storage-class]# kubectl apply -f mysql-demo.yaml[root@k8s-master1 storage-class]# kubectl get pods,pvc,pvNAME READY STATUS RESTARTS AGEpod/db-0 1/1 Running 0 2m35spod/db-1 1/1 Running 0 103spod/db-2 1/1 Running 0 101spod/nfs-client-provisioner-57998f486c-m5x25 1/1 Running 0 116sNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql-data-db-0 Bound default-mysql-data-db-0-pvc-55657d89-f615-420c-a481-a1d4422c86c5 2Gi RWX managed-nfs-storage 4m7spersistentvolumeclaim/mysql-data-db-1 Bound default-mysql-data-db-1-pvc-e94b491f-380a-48a2-b152-a7c76af2c603 2Gi RWX managed-nfs-storage 103spersistentvolumeclaim/mysql-data-db-2 Bound default-mysql-data-db-2-pvc-a8c0a2d8-af2d-422f-910b-01f2f2a11563 2Gi RWX managed-nfs-storage 101sNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/default-mysql-data-db-0-pvc-55657d89-f615-420c-a481-a1d4422c86c5 2Gi RWX Delete Bound default/mysql-data-db-0 managed-nfs-storage 105spersistentvolume/default-mysql-data-db-1-pvc-e94b491f-380a-48a2-b152-a7c76af2c603 2Gi RWX Delete Bound default/mysql-data-db-1 managed-nfs-storage 103spersistentvolume/default-mysql-data-db-2-pvc-a8c0a2d8-af2d-422f-910b-01f2f2a11563 2Gi RWX Delete Bound default/mysql-data-db-2 managed-nfs-storage 100s# 新建了存储目录 [root@k8s-node2 nfs]# ls -ltotal 28drwxrwxrwx 6 polkitd root 4096 Dec 31 19:02 default-mysql-data-db-0-pvc-55657d89-f615-420c-a481-a1d4422c86c5drwxrwxrwx 7 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-0-pvc-ebc9a67d-1558-4cf6-b35d-004ae6393845drwxrwxrwx 6 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-1-pvc-da2b75e9-7d93-40c6-b47e-9b3df2ebab07drwxrwxrwx 5 polkitd root 4096 Dec 31 19:02 default-mysql-data-db-1-pvc-e94b491f-380a-48a2-b152-a7c76af2c603drwxrwxrwx 6 polkitd root 4096 Dec 31 18:50 default-mysql-data-db-2-pvc-298c380a-8d61-4372-9503-8d886723089edrwxrwxrwx 6 polkitd root 4096 Dec 31 19:02 default-mysql-data-db-2-pvc-a8c0a2d8-af2d-422f-910b-01f2f2a11563[root@k8s-master1 storage-class]# kubectl run -it --image mysql:5.7 mysql-client --restart=Never --rm /bin/bashroot@mysql-client:/# mysql -h db-0.mysql.default -uroot -pmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.00 sec)]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09 k8s Ingress 与外界连通]]></title>
    <url>%2F2019%2F12%2F09%2Fk8s-09%2F</url>
    <content type="text"><![CDATA[Ingress Controller 部署 Ingress 全局负载均衡，7层负载均衡(域名url), nodeport只支持4层(端口) 用户 -&gt; Ingress Controller(Node) -&gt; Pod 他会使用宿主机网络的 80和443端口,要确保node这俩个端口不要被占用 1231. 通过service相关联2. 通过Ingress Controller实现Pod的负载均衡3. 支持TCP/UDP 4层和HTTP 7层 部署12部署文档：https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md 123注意事项：1. 镜像地址修改成国内的：lizhenliang/nginx-ingress-controller:0.20.0 2. 使用宿主机网络：hostNetwork: true 12kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml# 可以先下载到本地 1[root@k8s-master demo2]# kubectl create -f mandatory.yaml [root@k8s-master1 ~]# kubectl get pods -n ingress-nginxNAME READY STATUS RESTARTS AGEnginx-ingress-controller-8nbng 1/1 Running 12 6d8hnginx-ingress-controller-dksf2 1/1 Running 10 6d8hnginx-ingress-controller-rp5t2 1/1 Running 0 2m32s 12[root@k8s-node1 ~]# netstat -antp|grep 80[root@k8s-node1 ~]# netstat -antp|grep 443 Ingress 配置123456789101112131415161718192021[root@k8s-master1 demo]# vim app01-ingress.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: app01-ingress # 注解 annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: # 域名 - host: foo.bar.com http: paths: - path: / backend: # 转发到哪个 service serviceName: app01 # 对应 clusterip 端口 servicePort: 80 1234567891011121314[root@k8s-master demo2]# kubectl apply -f ingress01.yaml [root@k8s-master1 demo]# kubectl get pods,svc,ingNAME READY STATUS RESTARTS AGEpod/app01-64d7b49995-4fdn5 1/1 Running 0 5m17spod/app01-64d7b49995-6rnsp 1/1 Running 0 5m35spod/app01-64d7b49995-d4qsx 1/1 Running 0 5m17sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/app01 NodePort 10.0.0.225 &lt;none&gt; 80:31000/TCP 4m56sservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 6d8hNAME HOSTS ADDRESS PORTS AGEingress.extensions/app01-ingress foo.bar.com 80 56s 12345678910111213141516171819# 测试增加域名解析# 不添加hosts的话 需要将域名接下到 部署控制器的 node节点上# 实际工作环境 就是将 域名 解析到 NodeIP上[root@k8s-master1 demo]# cat /etc/hosts39.106.100.108 foo.bar.com 39.106.168.181 foo.bar.com 60.205.217.112 foo.bar.com[root@k8s-master1 demo]# curl -I foo.bar.comHTTP/1.1 200 OKServer: nginx/1.15.5Date: Mon, 23 Dec 2019 09:28:42 GMTContent-Type: text/htmlContent-Length: 612Connection: keep-aliveVary: Accept-EncodingLast-Modified: Tue, 13 Aug 2019 10:05:00 GMTETag: "5d528b4c-264"Accept-Ranges: bytes 深入的去看看 ingress-nginx12345678910111213141516[root@k8s-master1 demo]# kubectl get pods -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-8nbng 1/1 Running 12 6d8h 172.17.70.254 k8s-node2 &lt;none&gt; &lt;none&gt;nginx-ingress-controller-dksf2 1/1 Running 10 6d8h 172.17.70.251 k8s-master1 &lt;none&gt; &lt;none&gt;nginx-ingress-controller-rp5t2 1/1 Running 0 28m 172.17.70.253 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl exec -it nginx-ingress-controller-8nbng bash -n ingress-nginxwww-data@k8s-node2:/etc/nginx$ # 运行着 nginx www-data@k8s-node2:/etc/nginx$ ps -ef|grep nginxwww-data@k8s-node2:/etc/nginx$ nginx -vnginx version: nginx/1.15.5# ginx-ingress-controller 进程 实时监听apiserver里所有svc，如果有更新就刷新 nginx里面的负载 Ingress HTTPS 域名 -&gt; service 端口 -&gt; pod https 需要定义 tls数字证书 https 需要启动 crt和key 权威结构购买数字域名证书 自签证书 不受浏览器信任 证书保存到 secret里面 然后保存到 secretName下 自签证书 并 引用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 1. 生成证书[root@k8s-master1 cert]# vim certs.sh # 生成 CA 证书cat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "expiry": "87600h", "usages": [ "signing", "key encipherment", "server auth", "client auth" ] &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "Beijing", "ST": "Beijing" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca -# 域名证书 cat &gt; sslexample.foo.com-csr.json &lt;&lt;EOF&#123; "CN": "sslexample.foo.com", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing" &#125; ]&#125;EOF# 生成 sslexample.foo.com 域名证书 crt和key cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes sslexample.foo.com-csr.json | cfssljson -bare sslexample.foo.com[root@k8s-master1 cert]# sh certs.sh [root@k8s-master1 cert]# ls sslexample.foo.com*.pemsslexample.foo.com-key.pem sslexample.foo.com.pem 123456789# 保存到 secret [root@k8s-master1 cert]# kubectl create secret tls sslexample-foo-com --cert=sslexample.foo.com.pem --key=sslexample.foo.com-key.pemsecret/sslexample-foo-com created[root@k8s-master1 cert]# kubectl get secretNAME TYPE DATA AGEdefault-token-l2x75 kubernetes.io/service-account-token 3 7d2hnfs-client-provisioner-token-75bdm kubernetes.io/service-account-token 3 6d2hsslexample-foo-com kubernetes.io/tls 2 16s 1234567891011121314151617181920212223242526# 创建 ingress[root@k8s-master1 demo]# vim app01-ingress-https.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: tls-example-ingressspec: tls: - hosts: # 域名 - sslexample.foo.com # secret 名称 保存的证书配置 secretName: sslexample-foo-com rules: # 域名 - host: sslexample.foo.com http: paths: - path: / backend: # service name serviceName: app01 # service port servicePort: 80 12345678910# 启动 ingress-https[root@k8s-master1 demo]# kubectl delete ing app01-ingressingress.extensions "app01-ingress" deleted[root@k8s-master1 demo]# kubectl apply -f app01-ingress-https.yaml ingress.networking.k8s.io/tls-example-ingress created[root@k8s-master1 demo]# kubectl get ingNAME HOSTS ADDRESS PORTS AGEtls-example-ingress sslexample.foo.com 80, 443 4s 12345678910# 测试[root@k8s-master1 demo]# cat /etc/hosts39.106.100.108 foo.bar.com sslexample.foo.com39.106.168.181 foo.bar.com sslexample.foo.com60.205.217.112 foo.bar.com sslexample.foo.com# 更换个页面测试查看 [root@k8s-master1 demo]# kubectl exec -it app01-64d7b49995-d4qsx bashroot@app01-64d7b49995-d4qsx:/# echo 789 &gt; /usr/share/nginx/html/index.html 使用默认的 Ingress 证书1234567891011121314151617181920212223242526272829# k8s 会默认自签证书 # 我们就不用自签证书 创建secert [root@k8s-master1 demo]# cp app01-ingress-https.yaml app01-ingress-default-https.yaml [root@k8s-master1 demo]# vim app01-ingress-default-https.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: tls-example-ingressspec: tls: - hosts: # 域名 - sslexample.foo.com # secret 没有这个secret secretName: sslexample-foo-com-100 rules: # 域名 - host: sslexample.foo.com http: paths: - path: / backend: # service name serviceName: app01 # service port servicePort: 80 12345678910111213141516171819202122# 启动[root@k8s-master1 demo]# kubectl delete -f app01-ingress-https.yaml ingress.networking.k8s.io "tls-example-ingress" deleted[root@k8s-master1 demo]# kubectl apply -f app01-ingress-default-https.yaml ingress.networking.k8s.io/tls-example-ingress created# 并没有创建新的 secret [root@k8s-master1 demo]# kubectl get secretNAME TYPE DATA AGEdefault-token-l2x75 kubernetes.io/service-account-token 3 7d2hnfs-client-provisioner-token-75bdm kubernetes.io/service-account-token 3 6d3hsslexample-foo-com kubernetes.io/tls 2 20m[root@k8s-master1 demo]# kubectl get ingNAME HOSTS ADDRESS PORTS AGEtls-example-ingress mysslexample.foo.com 80, 443 34s[root@k8s-master1 demo]# cat /etc/hosts39.106.100.108 foo.bar.com mysslexample.foo.com39.106.168.181 foo.bar.com mysslexample.foo.com60.205.217.112 foo.bar.com mysslexample.foo.com 1# 测试 k8s自己颁发的证书 总结 Ingress 支持 四层、七层负载均衡转发 支持自定义service访问策略 只支持基于域名的网站访问 支持TLS 用户 -&gt; 域名 -&gt; 负载均衡 -&gt; 固定NODE的Ingress Controller(node) -&gt; Pod\ 部署多个 nginx-ingress-controller DaemonSet 方式部署到 多个指定标签的node节点上]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08 Kubernetes 控制器（管理Pod）]]></title>
    <url>%2F2019%2F12%2F09%2Fk8s-08%2F</url>
    <content type="text"><![CDATA[控制器控制器的分类 Deployment StatefulSet DaemonSet Job CronJob 11. 控制器也称为 工作负载,它的作用是管理POD Pod 与 controllers的关系 controllers：在集群上管理和运行容器的对象 通过label-selector相关联 Pod通过控制器实现应用的运维，如伸缩，升级等 无状态 与 有状态 服务区别 无状态: deployment 认为所有的POD都是一样的 不用考虑顺序的要求 也不用考虑在哪个Node运行 随意扩容/缩容 有状态: statefulset 数据不完全一致 节点之间存在关系 这种实力之间不对等的关系，以及依靠外部存储的应用。成为有状态应用 部署有状态应用，解决POD独立生命周期，保持Pod启动顺序和唯一性 稳定，唯一的网络标识符,持久存储 有序，优雅的部署和扩展、删除和终止 有序，滚动更新 应用场景：数据库 官方说明 https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ 部署有状态应用Headless Service 常规 Service：一组POD的访问策略，提供负载均衡服务发现 Headless Service: 不需要Cluster-IP，他会直接绑定到PODIP 常规 Service 查看1234567891011121314151617181920[root@k8s-master1 demo]# kubectl get pods,svc,epNAME READY STATUS RESTARTS AGEpod/db2-mysql-76495946b5-kv76b 1/1 Running 2 73mpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 2 77mpod/web-866f97c649-jx9s6 1/1 Running 0 69spod/web-866f97c649-lcdn2 1/1 Running 0 69spod/web-866f97c649-qzwmt 1/1 Running 0 69sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/db2-mysql ClusterIP 10.0.0.159 &lt;none&gt; 3306/TCP 73mservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 24hservice/web NodePort 10.0.0.95 &lt;none&gt; 80:32417/TCP 69sNAME ENDPOINTS AGEendpoints/db2-mysql 10.244.0.23:3306 73mendpoints/kubernetes 172.17.70.251:6443 24hendpoints/web 10.244.0.24:80,10.244.1.17:80,10.244.2.40:80 69s[root@k8s-master1 demo]# curl -I 10.0.0.95 Headless Service12345678910111213141516171819202122232425[root@k8s-master1 demo]# vim headless.yamlapiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None # 设置为None selector: app: nginx[root@k8s-master1 demo]# kubectl apply -f headless.yaml service/nginx created[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx ClusterIP None &lt;none&gt; 80/TCP 4s # 没有内部IP web NodePort 10.0.0.95 &lt;none&gt; 80:32417/TCP 5m34s# 没有IP 使用DNS保证网络唯一标识符 coredns 部署 Coredns 并测试12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-master1 demo]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-xwv6v 1/1 Running 3 125m# 临时任务 不重启 执行后销毁[root@k8s-master1 demo]# vim busybox.yaml apiVersion: v1kind: Podmetadata: name: dns-testspec: containers: - name: busybox image: busybox:1.28.4 args: - /bin/sh - -c - sleep 36000 restartPolicy: Never[root@k8s-master1 demo]# kubectl apply -f busybox.yaml pod/dns-test created# 进入测试 [root@k8s-master1 demo]# kubectl exec -it dns-test sh/ # nslookup kubernetesServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local/ # nslookup webServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: webAddress 1: 10.0.0.95 web.default.svc.cluster.local 创建 StatefulSet1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master1 demo]# kubectl delete -f .[root@k8s-master1 demo]# vim sts.yamlapiVersion: v1kind: Servicemetadata: name: nginx # 与 StatefulSet 的 serviceName 一致 labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-statefulset namespace: defaultspec: serviceName: nginx # 与Headless Service name 一致 replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 12345678910111213[root@k8s-master1 demo]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/db2-mysql-76495946b5-kv76b 1/1 Running 2 117mpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 2 121mpod/nginx-statefulset-0 1/1 Running 0 31spod/nginx-statefulset-1 1/1 Running 0 26spod/nginx-statefulset-2 1/1 Running 0 23sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/db2-mysql ClusterIP 10.0.0.159 &lt;none&gt; 3306/TCP 117mservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 25hservice/metrics-app ClusterIP 10.0.0.24 &lt;none&gt; 80/TCP 24hservice/nginx ClusterIP None &lt;none&gt; 80/TCP 31s 唯一的网络标识1234# 固定的标号 从0开始 删除的时候也是根据这个标号nginx-statefulset-0nginx-statefulset-1nginx-statefulset-2 12345678910111213[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEdb2-mysql-76495946b5-kv76b 1/1 Running 2 125mnfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 2 129mnginx-statefulset-0 1/1 Running 0 7m53snginx-statefulset-1 1/1 Running 0 7m48snginx-statefulset-2 1/1 Running 0 7m45s[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdb2-mysql ClusterIP 10.0.0.159 &lt;none&gt; 3306/TCP 124mkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 25hnginx ClusterIP None &lt;none&gt; 80/TCP 7m31s 123456789101112131415161718192021# 即通过dns来找到POD# 即使POD的IP变更 nginx-statefulset-0 依然可以被svc的nginx 找到 [root@k8s-master1 demo]# kubectl apply -f busybox.yaml [root@k8s-master1 demo]# kubectl exec -it dns-test sh/ # nslookup nginxServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: nginxAddress 1: 10.244.0.25 nginx-statefulset-1.nginx.default.svc.cluster.localAddress 2: 10.244.1.20 nginx-statefulset-0.nginx.default.svc.cluster.localAddress 3: 10.244.2.41 nginx-statefulset-2.nginx.default.svc.cluster.local# pod名称+svc名称/ # nslookup nginx-statefulset-0.nginxServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: nginx-statefulset-0.nginxAddress 1: 10.244.1.20 nginx-statefulset-0.nginx.default.svc.cluster.local StatefulSet 总结 StatefulSet 与 Deployment区别: 有身份的 身份三要素: 域名 主机名 存储(PVC) 12345statefulset 的 POD 名字 == 主机名ClusterIP A记录: &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local ClusterIP = Node A记录格式: &lt;StatefulsetName-index&gt;.&lt;namespace-name&gt;.&lt;service-name&gt;.svc.cluster.localnginx-statefulset-0.nginx.default.svc.cluster.local这个就是 statefulset 的 POD 的固定的访问地址 ，通过域名访问到后面的 POD ，通过DNS维持身份 DaemonSet（部署守护进程） DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时，也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 使用 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个节点上运行 glusterd、ceph。 在每个节点上运行日志收集 daemon，例如fluentd、logstash。 在每个节点上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。 123451. 在每一个Node上运行一个Pod2. 新加入的Node也同样会自动运行一个Pod3. 应用场景：Agent4. 官方案例:https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/ 1234567891011121314151617181920212223242526272829303132[root@k8s-master1 demo]# vim ds.yaml apiVersion: apps/v1kind: DaemonSetmetadata: labels: app: filebeat name: ds-testspec: selector: matchLabels: app: filebeat template: metadata: labels: app: filebeat spec: containers: - image: nginx:1.16 name: logs ports: - containerPort: 80 volumeMounts: - name: varlog mountPath: /tmp/log volumes: - name: varlog hostPath: path: /var/log[root@k8s-master1 demo]# kubectl apply -f ds.yaml daemonset.apps/ds-test created 12345678910111213[root@k8s-master1 demo]# kubectl get pods -o wide# 每个node节点都会启动 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESds-test-8vxs4 1/1 Running 0 60s 10.244.2.42 k8s-node2 &lt;none&gt; &lt;none&gt;ds-test-9vvcm 1/1 Running 0 60s 10.244.0.26 k8s-master1 &lt;none&gt; &lt;none&gt;ds-test-x7tsb 1/1 Running 0 60s 10.244.1.22 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl exec -it ds-test-8vxs4 bashroot@ds-test-8vxs4:/# cd /tmp/log/# 测试就是把目录挂载上来 后期做node日志收集root@ds-test-8vxs4:/tmp/log# lsanaconda boot.log btmp cloud-init.log containers dmesg ecs_network_optimization.log grubby_prune_debug lastlog messages pods sa spooler tuned yum.logaudit boot.log-20191218 chrony cloudinit-deploy.log cron dmesg.old grubby journal maillog ntpstats rhsm secure tallylog wtmp 1234567891011121314151617181920212223242526272829303132# 添加新的node 也同样跑起来[root@k8s-master1 ansible-k8s-deploy]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master1 Ready &lt;none&gt; 26h v1.16.0k8s-node1 Ready &lt;none&gt; 26h v1.16.0k8s-node2 Ready &lt;none&gt; 26h v1.16.0k8s-node3 Ready &lt;none&gt; 33s v1.16.0[root@k8s-master1 ansible-k8s-deploy]# kubectl get podsNAME READY STATUS RESTARTS AGEdb2-mysql-76495946b5-kv76b 1/1 Running 2 174mdns-test 1/1 Running 0 47mds-test-8vxs4 1/1 Running 0 10mds-test-9vvcm 1/1 Running 0 10mds-test-hqtsm 1/1 Running 0 34sds-test-x7tsb 1/1 Running 0 10mnfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 2 178mnginx-statefulset-0 1/1 Running 0 56mnginx-statefulset-1 1/1 Running 0 56mnginx-statefulset-2 1/1 Running 0 56m[root@k8s-master1 ansible-k8s-deploy]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdb2-mysql-76495946b5-kv76b 1/1 Running 2 174m 10.244.0.23 k8s-master1 &lt;none&gt; &lt;none&gt;dns-test 1/1 Running 0 47m 10.244.1.21 k8s-node1 &lt;none&gt; &lt;none&gt;ds-test-8vxs4 1/1 Running 0 10m 10.244.2.42 k8s-node2 &lt;none&gt; &lt;none&gt;ds-test-9vvcm 1/1 Running 0 10m 10.244.0.26 k8s-master1 &lt;none&gt; &lt;none&gt;ds-test-hqtsm 1/1 Running 0 39s 10.244.3.2 k8s-node3 &lt;none&gt; &lt;none&gt;ds-test-x7tsb 1/1 Running 0 10m 10.244.1.22 k8s-node1 &lt;none&gt; &lt;none&gt;nginx-statefulset-0 1/1 Running 0 57m 10.244.1.20 k8s-node1 &lt;none&gt; &lt;none&gt;nginx-statefulset-1 1/1 Running 0 56m 10.244.0.25 k8s-master1 &lt;none&gt; &lt;none&gt;nginx-statefulset-2 1/1 Running 0 56m 10.244.2.41 k8s-node2 &lt;none&gt; &lt;none&gt; Job与CronJob（离线业务） Job 分为 普通任务(Job) 和 定时任务 (CronJob) 一次性执行 应用场景: 离线数据处理，视频解码等业务 Job12345678910111213141516171819202122# 计算任务[root@k8s-master1 ansible-k8s-deploy]# vim job.yamlapiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never backoffLimit: 4[root@k8s-master1 ansible-k8s-deploy]# kubectl apply -f job.yaml job.batch/pi created[root@k8s-master1 ansible-k8s-deploy]# kubectl describe jobs/pi 12345678910111213141516171819202122spec.backoffLimit用于设置Job的容错次数，默认值为6。当Job运行的Pod失败次数到达.spec.backoffLimit次时，Job Controller不再新建Pod，直接停止运行这个Job，将其运行结果标记为Failure。另外，Pod运行失败后再次运行的时间间隔呈递增状态，例如10s，20s，40s。。。[root@k8s-master1 ansible-k8s-deploy]# kubectl get pods# Completed 完成状态NAME READY STATUS RESTARTS AGEpi-2tsx7 0/1 Completed 0 2m29s # 查看 logs 计算结果输出到控制台[root@k8s-master1 demo]# kubectl logs pi-qdbnp# job执行完成后 并不会删除[root@k8s-master1 demo]# kubectl get jobNAME COMPLETIONS DURATION AGEpi 1/1 33s 45s# 删除[root@k8s-master1 demo]# kubectl delete job pijob.batch "pi" deleted# job适合临时任务跑完退出 CronJob 定时任务，像Linux的Crontab一样 应用场景: 通知,备份 crontab的格式如下：分 时 日 月 周 要运行的命令: 第1列分钟0～59 第2列小时0～23） 第3列日1～31 第4列月1～12 第5列星期0～7（0和7表示星期天） 第6列要运行的命令 1234567891011121314151617181920212223[root@k8s-master1 demo]# vim cronjob.yaml apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: "*/1 * * * *" # 用来指定任务运行的周期 jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure[root@k8s-master1 demo]# kubectl apply -f cronjob.yaml cronjob.batch/hello created 1234567891011121314151617[root@k8s-master1 demo]# kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello */1 * * * * False 1 11s 46s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEhello-1576641600-q78xr 0/1 Completed 0 38s[root@k8s-master1 demo]# kubectl logs hello-1576641600-q78xr Wed Dec 18 04:00:05 UTC 2019Hello from the Kubernetes cluster# 到达定时后 会再次执行并产生pod记录[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEhello-1576641600-q78xr 0/1 Completed 0 96shello-1576641660-htkkg 0/1 Completed 0 35s 1234# 删除任务[root@k8s-master1 demo]# kubectl get jobs[root@k8s-master1 demo]# kubectl delete cronjob hellocronjob.batch "hello" deleted 总结 控制器实现K8S编排能力 Deployment: 无状态部署 Statefulset: 有状态部署 访问方式:DNS记录 etcd1，etcd2… 还有 存储 DaemonSet: 守护进程部署 Job &amp; CronJob： 批处理 和 定时任务]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06 K8S 集群网络]]></title>
    <url>%2F2019%2F12%2F06%2Fk8s-base08%2F</url>
    <content type="text"><![CDATA[12345678910111213# 查看cni网桥的接入[root@k8s-master1 opt]# yum install bridge-utils -y[root@k8s-node2 ~]# brctl show cni0bridge name bridge id STP enabled interfacescni0 8000.b6051fb07b10 no veth1938bf5f veth70d9e02c vethcc33255a vethce666639# docker 网桥[root@k8s-node2 ~]# brctl show docker0bridge name bridge id STP enabled interfacesdocker0 8000.0242c8d4d7fe no 1# 网络插件 就是解决跨主机node上的pod通信 123456# kubelet 创建 容器kubelet -&gt; dockerapi (dockershim) # pod的形成 kubelet -&gt; dockerapi -&gt; 容器1kubelet -&gt; 二进制文件 -&gt; 网络配置 -&gt; 容器1 1234# cni 网络规范:# 接入 第三方网络插件# kubelet 去调用网络插件 为容器 创建 网络# 实现解耦,可以接入任何网络组建 交换和路由123# 两个问题: 1. 一个局域网内,主机A 和 主机B 之间通信,数据包传输流程2. 不在一个局域网之内,主机A 和 主机B 通信流程 交换技术 路由器：网络出口 核心层：主要完成数据高效转发、链路备份等 汇聚层：网络策略、安全、工作站交换机的接入、VLAN之间通信等功能 接入层：工作站的接入 交换机工作在OSI参考模型的第二层，即数据链路层。 交换机拥有一条高带宽的背部总线交换矩阵，在同一时间可进行多个端口对之间的数据传输。 交换技术分为2层和3层： 2层：主要用于小型局域网，仅支持在数据链路层转发数据，对工作站接入。 3层：三层交换技术诞生，最初是为了解决广播域的问题，多年发展，三层交换机书已经成为构建中大型网络的主要力量。 广播域 交换机在转发数据时会先进行广播，这个广播可以发送的区域就是一个广播域。 交换机之间对广播帧是透明的，所以交换机之间组成的网络是一个广播域。 路由器的一个接口下的网络是一个广播域，所以路由器可以隔离广播域。 ARP（地址解析协议，在IPV6中用NDP替代） 发送这个广播帧是由ARP协议实现，ARP是通过IP地址获取物理地址的一个TCP/IP协议。 三层交换机 前面讲的二层交换机只工作在数据链路层，路由器则工作在网络层。 而功能强大的三层交换机可同时工作在数据链路层和网络层，并根据 MAC地址或IP地址转发数据包。 VLAN（Virtual Local Area Network）：虚拟局域网 VLAN是一种将局域网设备从逻辑上划分成一个个网段。 一个VLAN就是一个广播域，VLAN之间的通信是通过第3层的路由器来完成的。 VLAN应用非常广泛，基本上大部分网络项目都会划分vlan。 VLAN的主要好处： 分割广播域，减少广播风暴影响范围。 提高网络安全性，根据不同的部门、用途、应用划分不同网段 重点1: 在一个2层交换机下的两台服务器的通信流程 12345主机A(10)主机B(20)网段:192.168.31.0/24 源IP 和 目的IP 都是一个子网四元组: 源IP 源MAC 目的IP 目的MAC ? 在本机查找ARP缓存表,是否存在要发送的数据的目的地之 -&gt; 主机B的MAC地址 如果没有,本机会发送ARP广播包,达到2层交换机，询问20(主机B)的MAC地址是多少 交换机也会查本地ARP缓存表 如果有 就直接响应主机A,主机A得到后重新封装数据包中的目的MAC 如果没有,会发送除主机A之外的所有主机，每个主机都判断目的IP是不是自己,主机B发现是自己将mac地址响应回给交换机,交换机返回给主机A,并保存到自己的缓存表,不同的则将包丢弃 主机A获取到主机B的MAC地址,将目的MAC封装在包内发送给交换机,交换机再转发给主机B。 ARP缓存表记录着经过二层传输的源IP和目的IP的MAC地址,以便下次传输能直接使用。 主机A发送的广播包 是否会被所有的交换机收到？ 交换机A 和 交换机B 并不是直连的 无法收到 交换机A 可以通过 3层交换机 转发给 交换机B ,如果没有3层交换机,上面是路由器就无法收到,路由器隔离广播域。 3层交换机 既可以处理2层数据包也可以处理3层数据包 重点2: 不在一个局域网之内,主机A 和 主机B 通信流程1231. 主机A 和 主机B 不在一个Vlan 2. Vlan1 192.168.31.0/243. Vlan2 192.168.32.0/24 目的地址与本机不在同一个子网，会走默认网关 1234[root@k8s-master1 ~]# ip routedefault via 172.31.239.253 dev eth0 # 默认网关10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink ... 网关会查询路由表 路由器的路由表得知接口是哪个,到达哪个交换机，最后到服务器上。此时交换机会记录mac地址 目的地址 192.168.32.0 网关 接口 B Kubernetes 网络模型 Kubernetes 要求所有的网络插件实现必须满足如下要求： 一个Pod一个IP 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射 Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个。 Kubernetes 网络组件之 Flannel Flannel是CoreOS维护的一个网络组件，Flannel为每个Pod提供全局唯一的IP，Flannel使用ETCD来存储Pod子网与Node IP之间的关系。 flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。 123451. k8s中所有的pod的IP地址都必须是唯一的，如果不唯一,数据包就不知道发送给谁，因为是全联通。2. 如何保证POD的IP唯一,就是再每个node上都分配一个 子网 ,每个node都由单独的子网(不同网段的Vlan)3. Flannel 会预先设计一个大的子网，再从大子网将node分配成小子网, 这些信息都会被存储在 etcd中4. 每个子网与node绑定都有关系记录，用于二层数据包传输5. flanneld守护进程在每台主机上运行，维护路由规则和etcd中的路由信息 Flannel 部署1https://github.com/coreos/flannel 123456789# 每个node节点都以 DaemonSet 形式部署 ,保证每个node节点上运行守护进程# flanneld 守护进程 负责本地路由表设定和维护etcd中的数据,比如将本地分配的子网上报给etcd[root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-k5wl9 1/1 Running 16 9dkube-flannel-ds-amd64-2k5kz 1/1 Running 9 9dkube-flannel-ds-amd64-gvs6b 1/1 Running 16 9dkube-flannel-ds-amd64-hwglz 1/1 Running 16 9d 123456789101112131415161718# 查看 yaml 文件# 1. 使用 ConfigMap 存储了 flannel的配置文件 -&gt; 子网的配置文件# 2. 重要的两个配置:# 预先规划好大子网,然后填写进去,大子网一定不能与物理网络冲突# 还需要对应好 工作模式,VXLAN 是默认的[root@k8s-master1 prometheus-k8s]# vim /tmp/k8s/kube-flannel.yaml # 大子网配置 和 工作模式(封装数据包的方式)... net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;... 12345678910111213141516171819202122232425# 1. 部署后,配置信息会在每个node上的 [root@k8s-master1 cni]# cd /etc/cni/net.d/[root@k8s-master1 net.d]# ls10-flannel.conflist[root@k8s-master1 net.d]# cat 10-flannel.conflist &#123; "cniVersion": "0.2.0", "name": "cbr0", "plugins": [ &#123; "type": "flannel", "delegate": &#123; "hairpinMode": true, "isDefaultGateway": true &#125; &#125;, &#123; "type": "portmap", "capabilities": &#123; "portMappings": true &#125; &#125; ]&#125; 1234561. flannel 的网络配置在[root@k8s-master1 net.d]# cat /var/run/flannel/subnet.envFLANNEL_NETWORK=10.244.0.0/16 # 大子网 FLANNEL_SUBNET=10.244.2.1/24 # 被分配的小子网 可被分配255个小子网FLANNEL_MTU=1450FLANNEL_IPMASQ=true 1233. 二进制文件,kubelet会调用这个二进制文件 为创建的每个pod 配置网络信息，从小子网网段里分配IP地址[root@k8s-master1 net.d]# ls -l /opt/cni/bin/firewall -rwxr-xr-x 1 root root 5968249 Aug 15 18:05 /opt/cni/bin/firewall Flannel 工作模式及原理Flannel支持多种数据转发方式 UDP：最早支持的一种方式，由于性能最差，目前已经弃用。 VXLAN：Overlay Network方案 ? ，源数据包封装在另一种网络包里面进行路由转发和通信 - 隧道方案 Host-GW：Flannel 通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机都有整个容器网络的路由数据了。 - 路由方案 大多数网络插件都具有这两种方案,比如 Calico VXLAN 二进制部署 支持 cni 12345678910[root@k8s-master1 net.d]# cat /opt/kubernetes/cfg/kube-controller-manager.conf# 允许node自动分配网络--allocate-node-cidrs=true \# 指定 pod 网络网段 需要与 Flannel 的网段对应上 kube-flannel.yaml 中的 Network--cluster-cidr=10.244.0.0/16 \# kubelet 配置[root@k8s-master1 net.d]# cat /opt/kubernetes/cfg/kubelet.conf --network-plugin=cni \ 为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。 这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。解决跨网段的数据通信。 下图flannel.1的设备就是VXLAN所需的VTEP设备。示意图如下： 1234# 查看 每个node上flannel分配的子网[root@k8s-node1 ~]# ifconfig # flannel.1 虚拟网卡# 当前这个node上创建的pod 都是从这个子网网段拿到IP，所以每个POD的IP地址不唯一 VXLAN 数据包的传输流程1.容器路由： 容器根据路由表从eth0发出,通过 veth 到达cni0也就到达了node服务器 1234root@my-nginx-67f56d94f7-lwl9c:/# ip routedefault via 10.244.2.1 dev eth0 # 默认路由10.244.0.0/16 via 10.244.2.1 dev eth0 10.244.2.0/24 dev eth0 proto kernel scope link src 10.244.2.61 veth pair(对)1234567891011121314151617181920212223242526272829303132331. veth 设备对 网线的一头在容器,一头在node上[root@k8s-master1 ~]# vim /opt/demo/my-nginx.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: my-nginx name: my-nginxspec: replicas: 3 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - image: nginx:1.7.9 name: nginx[root@k8s-master1 demo]# kubectl apply -f my-nginx.yaml [root@k8s-master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-67f56d94f7-975h7 1/1 Running 0 8m49s 10.244.0.67 k8s-node2 &lt;none&gt; &lt;none&gt;my-nginx-67f56d94f7-lngj2 1/1 Running 0 8m49s 10.244.1.55 k8s-node1 &lt;none&gt; &lt;none&gt;my-nginx-67f56d94f7-lwl9c 1/1 Running 0 8m49s 10.244.2.61 k8s-master1 &lt;none&gt; &lt;none&gt;# 进入master1里面的一个容器 [root@k8s-master1 demo]# kubectl exec -it my-nginx-67f56d94f7-lwl9c bash 12345678910111213141516171819# 创建容器时, node上 会被创建一个新的网络接口 # veth6cdc3884 被挂到了 cni0 上，veth6cdc3884 就是新创建容器的虚拟网卡。# cni0 就像是一个二层交换机,所有的pod的网络都会到达这里# cni0 就是 flannel帮助创建的 拥有独立的IP地址和子网# 如果当前node上还有其他的pod 他们都通过这个 cni0 就可以通信,因为cni0就好比一个二层交换机# 同节点pod通信 走 cni0 # 不同节点 需要走 宿主机路由表 走默认网关: ip route default via 172.31.239.253 dev eth0 # 安装网桥工具 yum install bridge-utils[root@k8s-master1 ~]# brctl show# [root@k8s-master1 ~]# brctl show cni0bridge name bridge id STP enabled interfacescni0 8000.1e9a263ac57d no veth6cdc3884 # 好比交换机的一个端口 vethe28b9770docker0 8000.02427b6ab66d no 123456789101112131415161718192021# 进入容器,里面的 网卡 eth0@if10 和 node上的 veth6cdc3884 就是 一对 veth pair# veth pair 是一种成对出现的特殊网络设备，可以把它们想象成由一根虚拟网线连接起来的一对网卡，# 网卡的一头（eth0@if10）在容器中，另一头（veth6cdc3884）挂在网桥 cni0 上，# 其效果就是将 eth0@if10 也挂在了 cni0 上。# eth0@if10 也已经配置cni0提供的网段 node上的cni07: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000 link/ether 1e:9a:26:3a:c5:7d brd ff:ff:ff:ff:ff:ff inet 10.244.2.1/24 brd 10.244.2.255 scope global cni0# 容器的网络被cni0分配,所以容器的默认路由就是 cni03: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP link/ether 86:8b:cb:72:68:d2 brd ff:ff:ff:ff:ff:ff inet 10.244.2.61/24 brd 10.244.2.255 scope global eth0 valid_lft forever preferred_lft foreverroot@my-nginx-67f56d94f7-lwl9c:/# ip routedefault via 10.244.2.1 dev eth0 # 默认路由10.244.0.0/16 via 10.244.2.1 dev eth0 10.244.2.0/24 dev eth0 proto kernel scope link src 10.244.2.61 12345678910111213# 不同节点 需要走 宿主机路由表 走默认网关[root@k8s-master1 ~]# ip routedefault via 172.31.239.253 dev eth0 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink # flannel 生成 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink # flannel 生成10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 # flannel 生成169.254.0.0/16 dev eth0 scope link metric 1002 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 # docker0 flannel并没有走 172.31.224.0/20 dev eth0 proto kernel scope link src 172.31.228.50 # 默认自带# 现在数据已经到了宿主机上,数据包里面的数据 源IP 10.244.1.10 目的IP 10.244.2.10 2.主机路由： 数据包进入到宿主机虚拟网卡cni0，根据路由表转发到flannel.1虚拟网卡，也就是，来到了隧道的入口。 12345678910111213141516171. 宿主机发现目的IP 10.244.2.10 本机是无法处理，需要走默认网关2. 他会查看哪个是匹配 10.244.2.10 我这里和图上的实例有区别,因为本机node是10.244.2.0/24 实际上会有匹配的default via 172.31.239.253 dev eth0 10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink # flannel 生成 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink # flannel 生成 10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 # flannel 生成# 目的 10.244.1.0/24 下一跳 10.244.1.0 发给 flannel.1 onlink6: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default link/ether e6:1e:c6:83:6d:8c brd ff:ff:ff:ff:ff:ff inet 10.244.2.0/32 scope global flannel.1# 1. 数据到达 node后 根据路由表 转给了 flannel.1 # 2. flannel.1 交给 VXLAN ，VXLAN 来进行封包 3. VXLAN封装： 而这些VTEP设备（二层）之间组成二层网络必须要知道目的MAC地址。这个MAC地址从哪获取到呢？其实在flanneld进程启动后，就会自动添加其他节点ARP记录，可以通过ip命令查看，如下所示： 123[root@k8s-master1 ~]# ip neigh show dev flannel.110.244.0.0 lladdr fa:8c:e4:15:e4:20 PERMANENT10.244.1.0 lladdr 36:9d:f8:71:19:e0 PERMANENT 12345678# 数据包解析:源IP： 10.244.1.10源MAC： 自己目的IP: 10.244.2.10目的MAC : flannel.1 提供，flannel每个节点都存储下一跳网关的MAC地址 ,ip neigh show dev flannel.1 可以看到# 对于宿主机来说这个帧没有实际意义,宿主机发不出去 # 需要二次封包 4. 二次封包： 知道了目的MAC地址，封装二层数据帧（容器源IP和目的IP）后，对于宿主机网络来说这个帧并没有什么实际意义。接下来，Linux内核还要把这个数据帧进一步封装成为宿主机网络的一个普通数据帧，好让它载着内部数据帧，通过宿主机的eth0网卡进行传输。 5. 封装到UDP包发出去: 现在能直接发UDP包嘛？到目前为止，我们只知道另一端的flannel.1设备的MAC地址，却不知道对应的宿主机地址是什么。 flanneld进程也维护着一个叫做FDB的转发数据库，可以通过bridge fdb命令查看： 123[root@k8s-master1 ~]# bridge fdb show dev flannel.136:9d:f8:71:19:e0 dst 172.31.228.52 self permanentfa:8c:e4:15:e4:20 dst 172.31.228.53 self permanent 12345678910111213141516171819# 根据图中 udp数据包解析 将原来的包上面再加一次层 :# 目的就是让这个包直接传输到目的node主机上# 里面一层 容器到容器数据包源IP： 10.244.1.10源MAC： 自己目的IP: 10.244.2.10目的MAC : flannel.1 提供 : ip neigh show dev flannel.1 # 外面一层 两个宿主机源IP： 192.168.31.62源MAC： 自己目的IP: 192.168.31.63目的MAC : flannel.1 提供 : bridge fdb show dev flannel.1 对应目的ip的mac地址[root@k8s-master1 ~]# bridge fdb show dev flannel.136:9d:f8:71:19:e0 dst 172.31.228.52 self permanentfa:8c:e4:15:e4:20 dst 172.31.228.53 self permanent 可以看到，上面用的对方flannel.1的MAC地址对应宿主机IP，也就是UDP要发往的目的地。使用这个目的IP进行封装。 6. 数据包到达目的宿主机 Node1的eth0网卡发出去，发现是VXLAN数据包，把它交给flannel.1设备。flannel.1设备则会进一步拆包，取出原始二层数据帧包，发送ARP请求，经由cni0网桥转发给container。 数据包头部里面会有 VXLAN 标记，加了编号VNI，flannel.1的1就是编号，交给flannel.1去处理 拿到源IP和目的IP，去判断,一看是cni0网桥的交给cni0 cni0进行广播 拿着ip进行广播, 最后转发到容器里 最复杂的地方在于flannel.1的vtep 对数据的封包与解封包 flannel.1 这种封包 会导致效率下降 ，VXLAN这种形式就是Overlay Network方案,源数据包封装到另一层网络里,叠加网络 VXLAN 只是实现封包解封包的技术 VXLAN 在大型网络架构中,效率并不高 Host-GW host-gw模式相比vxlan简单了许多，直接添加路由，将目的主机当做网关，直接路由原始封包。 没有 flannel.1 阶段了 host-gw模式 就像把每一个node主机当做网关 设置flannel使用host-gw模式 12345678[root@k8s-master1 ~]# vim /tmp/k8s/kube-flannel.yaml net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "host-gw" &#125; &#125; 12345678910111213# 更新 flannel 部署# 更新网络相关操作,切换模式,一定要再停机维护的时候进行,因为网络会受到修改,以免出现网络冲突# 肯定要再模拟测试环境完成后 再上线更改[root@k8s-master1 ~]# kubectl delete -f /tmp/k8s/kube-flannel.yaml [root@k8s-master1 ~]# kubectl apply -f /tmp/k8s/kube-flannel.yaml [root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-k5wl9 1/1 Running 17 10dkube-flannel-ds-amd64-5gfps 1/1 Running 0 2m5skube-flannel-ds-amd64-6hpzc 1/1 Running 0 2m5skube-flannel-ds-amd64-ht9ll 1/1 Running 0 2m5s 123456789当你设置flannel使用host-gw模式,flanneld会在宿主机上创建节点的路由表：[root@k8s-master1 ~]# ip routedefault via 172.31.239.253 dev eth0 10.244.0.0/24 via 172.31.228.53 dev eth0 10.244.1.0/24 via 172.31.228.52 dev eth0 10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 169.254.0.0/16 dev eth0 scope link metric 1002 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 172.31.224.0/20 dev eth0 proto kernel scope link src 172.31.228.50 12345678910111213141516171. 目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；2. 并且，它下一跳地址是 172.31.228.52（即：via172.31.228.52）。3. 一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。4. 不会再使用 flannel.1 这个设备,cni0网桥的数据包会走路由表，因为发送的目的IP地址并非同一网段,需要走路由表5. 宿主机重新封包 目的地址就是 路由表中对应的地址 比如 172.31.228.52 （测试里面的 192.168.31.63 ）6. 他看到 172.31.228.52 和自己node服务器所在同一个网段,也就是二层传输,需要获取目的的mac地址7. 如果不知道 172.31.228.52 的mac地址，就会走arp广播 也就是一个局域网之内的主机A和主机B通信# 根据二层数据转发的包源IP： 10.244.1.10源MAC： 自己目的IP: 10.244.1.20目的MAC : ARP广播包响应回来的8. 而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.20，即 container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 container-2 当中。# 这里写的乱的原因是 图和实际的数据不一致,不过大概的路线没错,即走本地路由到达对应node，二层路由# 限制就是 每个node节点的 二层能通 (在同一个网段)# 性能更好,没有封包解封包 小总结 如果追求性能 并且 node的二层网络能通 可以使用 Host-GW 工作模式 如果node不能通过二层通信，需要路由转发(可能不同Vlan)，使用 VXLAN 用户访问 - svc(nodeport) -&gt; iptables/ipvs -&gt; node1\node2\node3 ,如果访问的pod正好是本机的node那么直接走本地的路由访问,如果到了其他node,那么就是跨主机网络通信,需要走路由表,如果工作模式是vxlan,那么要走flannel.1进行封包解封包流程重点是看哪种工作流程和路由规则,一定要搞清楚里面的概念很多,加油吧 Host-GW 二层包转发 阿里云主机的 host-gw方案不通 阿里云不通,但是本地虚拟机是通的,测试环境迁移到虚拟机 Calico BGP 也没通，测试下虚拟机 通不通 Kubernetes 网络方案之 Calico Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。 Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。 此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。 访问控制列表(ACL)是一种基于包过滤的访问控制技术，它可以根据设定的条件对接口上的数据包进行过滤，允许其通过或丢弃。 BGP 概述 实际上，Calico项目提供的网络解决方案，与Flannel的host-gw模式几乎一样。 也就是说，Calico也是基于路由表实现容器数据包转发，但不同于Flannel使用flanneld进程来维护路由信息的做法，而Calico项目使用BGP协议来自动维护整个集群的路由信息。 GP英文全称是Border Gateway Protocol，即边界网关协议，它是一种自治系统间的动态路由发现协议，与其他 BGP 系统交换网络可达信息。 1234567891011# 维护路由信息Calico BGP协议 Flannel Host-GW工作方式基本一致,BGP完成数据交换,大型网络架构的动态协议,技术高大上,性能牛逼Flannel flanneld进程 Flannel 自己做数据交换# BGP 协议 在集群规模达到一定量时 性能更棒BGP 机房多线(双线) BGP 协议 涉及到 静态路由和动态路由，路由选择路径转发,根据路由表,理由表又分为静态和动态静态路由表 由人工添加维护动态路由表 相互感知当网络架构中的路由很多时,甚至跨公司跨机房,需要手动配置？人工两很大,到达一定的规模 就会使用动态路由协议 BGP就是其中一个目的: 动态感知整个网络路由拓扑 路由之间相互学习 为了能让你更清楚理解BGP，举个例子： 在这个图中，有两个自治系统（autonomous system，简称为AS）：AS 1 和 AS 2。 123456789101112131415# 什么是自治系统1. 在互联网中，一个自治系统(AS)是一个有权自主地决定在本系统中应采用何种路由协议的小型单位。2. 这个网络单位可以是一个简单的网络也可以是一个由一个或多个普通的网络管理员来控制的网络群体，3. 它是一个单独的可管理的网络单元（例如一所大学，一个企业或者一个公司个体）。4. 一个自治系统有时也被称为是一个路由选择域（routing domain）。5. 一个自治系统将会分配一个全局的唯一的16位号码，有时我们把这个号码叫做自治系统号（ASN）。简单来说:就是两个不依赖其他公司的网络系统,独立运行管理6. 在正常情况下，自治系统之间不会有任何来往。7. 如果两个自治系统里的主机，要通过 IP 地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。8. BGP协议就是让他们互联的一种方式。BGP的作用: 让两个独立的网络互通,只要两个路由相通,BGP来负责动态添加路由,就可以实现，动态学习相互的路由表信息。 减少人工添加路由 Calico BGP实现 在了解了 BGP 之后，Calico 项目的架构就非常容易理解了，Calico主要由三个部分组成： 1234- Felix：以DaemonSet方式部署，运行在每一个Node节点上，主要负责维护宿主机上路由规则以及ACL规则。- BGP Client（BIRD）：主要负责把 Felix 写入 Kernel 的路由信息分发到集群 Calico 网络。- Etcd：分布式键值存储，保存Calico的策略和网络配置状态。- calicoctl：允许您从简单的命令行界面实现高级策略和网络。 Calico 部署1. 删除Flannel 123456789101112131415# 切换网络别忘记停机维护 夜深人静# 正式环境先做好配置再删除[root@k8s-master1 ~]# kubectl delete -f /tmp/k8s/kube-flannel.yaml # 下载部署文件[root@k8s-master1 k8s]# cd /tmp/k8s/[root@k8s-master1 k8s]# curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico.yaml# 下载完后还需要修改里面配置项：1. 配置连接etcd地址，如果使用https，还需要配置证书。（ConfigMap，Secret）2. 根据实际网络规划修改Pod CIDR（CALICO_IPV4POOL_CIDR）3. 选择工作模式（CALICO_IPV4POOL_IPIP），支持BGP，IPIP[root@k8s-master1 k8s]# vim calico.yaml # 在k8s中 etcd一定要独立部署 12345678# 1 添加证书# 证书位置 [root@k8s-master1 ~]# cd /opt/etcd/ssl/# 放在secret 需要base64编码[root@k8s-master1 ssl]# cat /opt/etcd/ssl/ca.pem |base64 -w 0[root@k8s-master1 ssl]# cat /opt/etcd/ssl/server-key.pem |base64 -w 0[root@k8s-master1 ssl]# cat /opt/etcd/ssl/server.pem |base64 -w 0# 复制粘贴到 calico.yaml 中 1234# 2 指定读取位置 etcd_ca: "/calico-secrets/etcd-ca" etcd_cert: "/calico-secrets/etcd-cert" etcd_key: "/calico-secrets/etcd-key" 123# 3. 指定etcd连接字符串[root@k8s-master1 ssl]# cat /opt/kubernetes/cfg/kube-apiserver.conf https://172.31.228.50:2379,https://172.31.228.52:2379,https://172.31.228.53:2379 1234567# 4. 修改 Pod CIDR # 要与 [root@k8s-master1 ssl]# cat /opt/kubernetes/cfg/kube-controller-manager.conf 中的 --cluster-cidr=10.244.0.0/16 \ 一致[root@k8s-master1 k8s]# vim calico.yaml /CALICO_IPV4POOL_CIDR - name: CALICO_IPV4POOL_CIDR value: "10.244.0.0/16" 1234567891011# 5. 修改工作模式，也有两种 IPIP 和 BGP(应用最多)# 不修改的话 默认是 IPIP 注释掉就是BGP # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: "Always" # 修改成 Never 或者 off 关闭 默认就走BGP # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: "Never" 1234567891011121314151617# 6. 清理网络 每台node都要做# 修改后保存,再删除 flannel网络 # 还需要删除 flannel的虚拟网卡配置 以免冲突 [root@k8s-master1 ssl]# ip link delete cni0[root@k8s-master1 ssl]# ip link delete flannel.1# 把之前 host-gw 路由也删除 保持纯净路由表[root@k8s-master1 ssl]# ip routedefault via 172.31.239.253 dev eth0 10.244.0.0/24 via 172.31.228.53 dev eth0 10.244.1.0/24 via 172.31.228.52 dev eth0 169.254.0.0/16 dev eth0 scope link metric 1002 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 172.31.224.0/20 dev eth0 proto kernel scope link src 172.31.228.50 [root@k8s-master1 ssl]# ip route delete 10.244.0.0/24 via 172.31.228.53 dev eth0 [root@k8s-master1 ssl]# ip route delete 10.244.1.0/24 via 172.31.228.52 dev eth0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 7. 部署 calico[root@k8s-master1 k8s]# kubectl apply -f /tmp/k8s/calico.yaml [root@k8s-master1 k8s]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-5cc7b68d7c-qszdt 1/1 Running 0 6m16scalico-node-pbs44 1/1 Running 0 6m16scalico-node-qdbz4 1/1 Running 0 6m16scalico-node-tk4lg 1/1 Running 0 6m16scoredns-6d8cfdd59d-k5wl9 1/1 Running 23 10dcalico-node 在每台node上都部署,包含了BGP Client（BIRD）和 Felixcalico-kube-controllers 从etcd获取网络规则和策略# 原先运行的pod 如果要是要 calico 网络 都需要重建，切换网络的影响还是挺大的# 我之前的 jenkins和nfs pvc 都需要重建了[root@k8s-master1 demo]# kubectl delete -f my-nginx.yaml [root@k8s-master1 demo]# kubectl apply -f my-nginx.yaml # 重建pod后相当于让网络做了路由学习 查看路由# 阿里云主机不通 我改为 本地虚拟机 多创建几个pod[root@k8s-master1 demo]# kubectl get pods --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdefault my-nginx-77fdbc8556-g9hqp 1/1 Running 0 12m 10.244.36.65 k8s-node1 &lt;none&gt; &lt;none&gt;default my-nginx-77fdbc8556-m7mp7 1/1 Running 0 12m 10.244.159.128 k8s-master1 &lt;none&gt; &lt;none&gt;default my-nginx-77fdbc8556-pdn9z 1/1 Running 0 12m 10.244.169.128 k8s-node2 &lt;none&gt; &lt;none&gt;default my-nginx2-8f86ff956-2tksg 1/1 Running 0 11m 10.244.169.129 k8s-node2 &lt;none&gt; &lt;none&gt;default my-nginx2-8f86ff956-4krd6 1/1 Running 0 11m 10.244.159.129 k8s-master1 &lt;none&gt; &lt;none&gt;default my-nginx2-8f86ff956-b2jld 1/1 Running 0 11m 10.244.36.66 k8s-node1 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-2lmzq 1/1 Running 0 87m 192.168.0.101 k8s-master1 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-mnf44 1/1 Running 0 87m 192.168.0.103 k8s-node2 &lt;none&gt; &lt;none&gt;ingress-nginx nginx-ingress-controller-v22ld 1/1 Running 0 87m 192.168.0.102 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system calico-kube-controllers-5cc7b68d7c-xb5rf 1/1 Running 0 35m 192.168.0.102 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system calico-node-hwt8c 1/1 Running 0 35m 192.168.0.103 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system calico-node-v7c4g 1/1 Running 0 35m 192.168.0.101 k8s-master1 &lt;none&gt; &lt;none&gt;kube-system calico-node-wfp2f 1/1 Running 0 35m 192.168.0.102 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system coredns-6d8cfdd59d-6wwkz 1/1 Running 0 13m 10.244.36.64 k8s-node1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-566cddb686-57z4p 0/1 CrashLoopBackOff 11 87m 10.244.0.2 k8s-node2 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-c4bc5bd44-ntrlj 0/1 CrashLoopBackOff 10 87m 10.244.1.2 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# ping 10.244.36.65[root@k8s-master1 demo]# ping 10.244.169.129[root@k8s-master1 demo]# ip route10.244.36.64/26 via 192.168.0.102 dev eth0 proto bird 10.244.159.128 dev cali196b9d660e1 scope link blackhole 10.244.159.128/26 proto bird 10.244.159.129 dev cali642ae61a996 scope link 10.244.169.128/26 via 192.168.0.103 dev eth0 proto bird Calico 管理工具12345678910111213141516171819202122232425262728293031323334353637383940下载工具：https://github.com/projectcalico/calicoctl/releases[root@k8s-master1 opt]# ls -l calicoctl -rw-r--r-- 1 root root 37090848 Jan 10 17:45 calicoctl[root@k8s-master1 opt]# chmod +x calicoctl [root@k8s-master1 opt]# mv calicoctl /usr/bin/# 查看BGP节点的建立状态 ipv4 看到其他节点[root@k8s-master1 k8s]# /usr/bin/calicoctl node statusCalico process is running.IPv4 BGP status+---------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+---------------+-------------------+-------+----------+-------------+| 192.168.0.102 | node-to-node mesh | up | 02:10:36 | Established || 192.168.0.103 | node-to-node mesh | up | 02:13:43 | Established |+---------------+-------------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.# calicoctl 也有很多其他操作,更多的是向etcd里操作 需要配置文件 上面这个结果是tcp连接做的并没有读取etcd[root@k8s-master1 opt]# netstat -anlp|grep bird # 如果有其他操作比如修改工作模式,需要做配置文件[root@k8s-master1 opt]# mkdir /etc/calico[root@k8s-master1 k8s]# vim /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: datastoreType: "etcdv3" etcdEndpoints: "https://192.168.0.101:2379,https://192.168.0.102:2379,https://192.168.0.103:2379" etcdKeyFile: "/opt/etcd/ssl/server-key.pem" etcdCertFile: "/opt/etcd/ssl/server.pem" etcdCACertFile: "/opt/etcd/ssl/ca.pem 1234567891011# 向etcd读取数据[root@k8s-master1 opt]# calicoctl get nodesNAME k8s-master1 k8s-node1 k8s-node2 # 查看 IPAM的IP地址池：[root@k8s-master1 opt]# calicoctl get ippoolNAME CIDR SELECTOR default-ipv4-ippool 10.244.0.0/16 all() Calico BGP 原理剖析 Pod 1 访问 Pod 2大致流程如下： 123456789101. 数据包从容器1出 到达Veth Pair另一端（宿主机上，以cali前缀开头）；2. 宿主机根据路由规则，将数据包转发给下一跳（网关）；3. 到达Node2，根据路由规则将数据包转发给cali设备，从而到达容器2。[root@k8s-master1 jenkins]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-77fdbc8556-nzhhs 1/1 Running 0 48m 10.244.159.128 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-q759m 1/1 Running 0 48m 10.244.169.128 k8s-node2 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-s7r75 1/1 Running 0 48m 10.24.436.64 k8s-node1 &lt;none&gt; &lt;none&gt;nfs-client-provisioner-769f87c8f6-hw9m7 1/1 Running 0 45m 10.244.36.65 k8s-node1 &lt;none&gt; &lt;none&gt; 123451. 创建pod kubelet 调用 calico 二进制程序创建网络，配置IP地址[root@k8s-node1 tmp]# ls -l /opt/cni/bin/calico-ipam 和 calico # yaml文件中镜像准备# 子网配置信息[root@k8s-node1 tmp]# cat /etc/cni/net.d/10-calico.conflist 1234567891011121314151617181920212223242526272829303132333435363738391. 到达cali之后就是到达了宿主机node服务器身上2. 查看路由表[root@k8s-node1 net.d]# ip routedefault via 172.31.239.253 dev eth0 10.244.36.64 dev cali358923ad8fb scope link blackhole 10.244.36.64/26 proto bird 10.244.36.65 dev calia8a14e44e0f scope link 10.244.159.128/26 via 172.31.228.50 dev eth0 proto bird 10.244.169.128/26 via 172.31.228.53 dev eth0 proto bird ...3. 比如node1数据通往node2 10.24.436.64 -&gt; 10.244.169.128 # 此时数据包里面:源IP： 10.24.436.64目的IP： 10.244.169.128 路由规则走: 10.244.169.128/26 via 172.31.228.53 dev eth0 proto bird # 走eth0口 下一条 172.31.228.53(node2的eth0地址)4. 数据包到达node2后 # 首先看veth对 # 查看node2的路由规则 [root@k8s-node2 kubernetes]# ip routedefault via 172.31.239.253 dev eth0 10.244.36.64/26 via 172.31.228.52 dev eth0 proto bird 10.244.159.128/26 via 172.31.228.50 dev eth0 proto bird 10.244.169.128 dev cali3480203cfd9 scope link # 这一条blackhole 10.244.169.128/26 proto bird ... # 数据目的地址是 10.244.169.128的 都会到达 cali3480203cfd9 也就是那块虚拟网卡cali# cali3480203cfd9 又是容器的 veth 在宿主机上的一端 然后就发往容器了 5. Calico 没有网桥 没有cni0 数据怎么到达node上呢？因为 cali3480203cfd9 cali这个虚拟网卡里面有个 arp代理机制容器1 -&gt; cali -&gt; node1 -&gt; 路由表 -&gt; node2 -&gt; cali -&gt; 容器2 很像host-gw 2层网络互通# 如果2层IP不能互通,就需要使用 IPIP模式 也就是node1 与 node2 无法直接互通的情况 其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。 这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。 不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。 这些节点，我们称为 BGP Peer。 Route Reflector 模式（RR）12参考：https://docs.projectcalico.org/master/networking/bgp 12345678[root@k8s-master1 ~]# calicoctl node status+---------------+-------------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+---------------+-------------------+-------+----------+-------------+| 192.168.0.102 | node-to-node mesh | up | 02:10:36 | Established || 192.168.0.103 | node-to-node mesh | up | 02:13:43 | Established |+---------------+-------------------+-------+----------+-------------+ Node-to-Node 在100台以下 Calico 维护的网络在默认是（Node-to-Node Mesh）全互联模式，Calico集群中的节点之间都会相互建立连接，用于路由交换。 但是随着集群规模的扩大，mesh模式将形成一个巨大服务网格，连接数成倍增加。 12345# 默认连接b1 -&gt; b2 b3b2 -&gt; b1 b3b3 -&gt; b1 b2 ... 增加一个加点 每个几点都要n+1 上百个建立连接就大了 这时就需要使用 Route Reflector（路由器反射）模式解决这个问题。 确定一个或多个Calico节点充当路由反射器，让其他节点从这个RR节点获取路由信息。 1234# 路由反射 至少2-3个 有备用b1 -&gt; 路由反射节点1 b2 -&gt; 路由反射节点2b3 -&gt; 路由反射节点1 1. 关闭 node-to-node BGP网格 123456789101112131415161718192021222324252627282930[root@k8s-master1 demo]# vim bgp.yaml apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: false asNumber: 63400# 执行这部时，整个网络会断开 请在停机维护时操作[root@k8s-master1 demo]# calicoctl apply -f bgp.yaml Successfully applied 1 'BGPConfiguration' resource(s)[root@k8s-master1 demo]# calicoctl get bgpconfigNAME LOGSEVERITY MESHENABLED ASNUMBER default Info false 63400 [root@k8s-master1 demo]# ping 10.244.169.132PING 10.244.169.132 (10.244.169.132) 56(84) bytes of data.# 网络已经不通ASN号可以通过获取 # calicoctl get nodes --output=wide[root@k8s-master1 demo]# calicoctl get nodes --output=wideNAME ASN IPV4 IPV6 k8s-master1 (63400) 192.168.0.101/24 k8s-node1 (63400) 192.168.0.102/24 k8s-node2 (63400) 192.168.0.103/24 2. 配置指定节点充当路由反射器 为方便让BGP Peer轻松选择节点，通过标签选择器匹配。 给路由器反射器节点打标签: 12345678910111213kubectl label node my-node route-reflector=true[root@k8s-master1 demo]# kubectl label node k8s-node2 route-reflector=true node/k8s-node2 labeled[root@k8s-master1 demo]# calicoctl node statusCalico process is running.IPv4 BGP statusNo IPv4 peers found.IPv6 BGP statusNo IPv6 peers found. 12345678910111213# 然后配置路由器反射器节点 routeReflectorClusterID：[root@k8s-master1 demo]# calicoctl get node k8s-node2 -o yaml &gt; node.yaml[root@k8s-master1 demo]# vim node.yaml ...spec: bgp: ipv4Address: 192.168.0.103/24 routeReflectorClusterID: 244.0.0.1 # 集群ID...[root@k8s-master1 demo]# calicoctl apply -f node.yaml Successfully applied 1 'Node' resource(s) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 将非路由反射器节点 连接路由反射器 获取交换信息[root@k8s-master1 demo]# vim bgp2.yaml apiVersion: projectcalico.org/v3kind: BGPPeermetadata: name: peer-with-route-reflectorsspec: nodeSelector: all() # 所有的node 都去找 route-reflector == 'true' 这个标签 peerSelector: route-reflector == 'true'[root@k8s-master1 demo]# calicoctl apply -f bgp2.yaml Successfully applied 1 'BGPPeer' resource(s)[root@k8s-master1 demo]# calicoctl get bgppeerNAME PEERIP NODE ASN peer-with-route-reflectors all() 0 # 查看节点的连接状态 只显示路由反射器的节点[root@k8s-master1 demo]# calicoctl node statusCalico process is running.IPv4 BGP status+---------------+---------------+-------+----------+-------------+| PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO |+---------------+---------------+-------+----------+-------------+| 192.168.0.103 | node specific | up | 02:56:50 | Established |+---------------+---------------+-------+----------+-------------+[root@k8s-master1 demo]# ip routedefault via 192.168.0.2 dev eth0 proto static metric 100 10.244.36.64/26 via 192.168.0.102 dev eth0 proto bird blackhole 10.244.159.128/26 proto bird 10.244.159.130 dev cali1f778af678e scope link 10.244.169.128/26 via 192.168.0.103 dev eth0 proto bird [root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-77fdbc8556-6qfxs 1/1 Running 0 28s 10.244.169.130 k8s-node2 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-wc4mr 1/1 Running 0 28s 10.244.159.130 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-z9cpp 1/1 Running 0 28s 10.244.36.67 k8s-node1 &lt;none&gt; &lt;none&gt;# 网络可以通[root@k8s-master1 demo]# ping 10.244.169.130[root@k8s-master1 demo]# ping 10.244.36.67PING 10.244.36.67 (10.244.36.67) 56(84) bytes of data.64 bytes from 10.244.36.67: icmp_seq=1 ttl=63 time=1.01 ms 123456789101112# 使用路由反射器解决当bgp越来越大时的路由消耗# RP模式可以设置多个节点,再其他的node上打个标签 就会自动加入进来# 一般两个节点以上kubectl label node k8s-node1 route-reflector=true calicoctl get node k8s-node2 -o yaml &gt; node.yaml增加:routeReflectorClusterID: 244.0.0.1 # 集群ID# 不允许跨VLAN 2层传输 源地址和目的地址都是容器 # 增加路由器 每个集群pod的网段和节点下一跳 才可以跨vlan 需要验证# 得把容器的网段写成静态路由# IPIP 支持跨子网 IPIP 模式 在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。 而这个限制对于 Calico 来说，也同样存在。 修改为IPIP模式： 123456789101112131415[root@k8s-master1 demo]# calicoctl get ipPool -o yaml &gt; ipip.yaml[root@k8s-master1 demo]# vim ipip.yaml ... cidr: 10.244.0.0/16 ipipMode: Always...# 也可以在原来的yaml里面修改 再重新创建[root@k8s-master1 demo]# calicoctl apply -f ipip.yamlSuccessfully applied 1 'IPPool' resource(s)[root@k8s-master1 demo]# calicoctl get ippool -o wideNAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTOR default-ipv4-ippool 10.244.0.0/16 true Always Never false all() 123456789101112# ip route 变化 tunl0 隧道网卡 有自己的IP 10.244.159.131[root@k8s-master1 demo]# ip route10.244.36.64/26 via 192.168.0.102 dev tunl0 proto bird onlink blackhole 10.244.159.128/26 proto bird 10.244.159.130 dev cali1f778af678e scope link 10.244.169.128/26 via 192.168.0.103 dev tunl0 proto bird onlink # bgp支持2层网络,如果要实现3层,那么需要路由器# 原先的数据包中 源IP是容器IP 目的IP也是容器IP 但是路由器里没有相应规则,所以数据不可达# 要想实现数据可达,需要一层路由器,并完成路由表信息# IPIP 模式 可以对数据包进行隧道 与 VXLAN类似# IPIP 也将原始数据包 进行封装和解封装 IPIP示意图： Pod 1 访问 Pod 2大致流程如下： 123451. 数据包从容器1出,到达Veth Pair另一端（宿主机上，以cali前缀开头）；2. 进入IP隧道设备（tunl0），由Linux内核IPIP驱动封装在宿主机网络的IP包中（新的IP包目的地之是原IP包的下一跳地址，即192.168.31.63），这样，就成了Node1 到Node2的数据包；3. 数据包经过路由器三层转发到Node2；4. Node2收到数据包后，网络协议栈会使用IPIP驱动进行解包，从中拿到原始IP包；5. 然后根据路由规则，根据路由规则将数据包转发给cali设备，从而到达容器2。 123456789[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-77fdbc8556-gfqd5 1/1 Running 0 12m 10.244.36.72 k8s-node1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-sm5ln 1/1 Running 0 12m 10.244.159.135 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-tbm4n 1/1 Running 0 12m 10.244.169.132 k8s-node2 &lt;none&gt; &lt;none&gt;# 10.244.36.72 -&gt; 10.244.169.132 流程:# 10.244.36.72 -&gt; pod calif -&gt; 10.244.169.132/26 via 192.168.0.103 dev tunl0 proto bird onlink# -&gt; 10.244.169.132 dev cali8567c6b7944 scope link -&gt; pod calif -&gt; 10.244.169.132 12345678910# 里面一层 容器到容器数据包 原始IP包源IP： 10.244.1.10目的IP: 10.244.2.10# 外面一层 两个宿主机 TCP源IP： 192.168.31.62目的IP: 192.168.32.63# IPIP与 VXLAN一样 工作在三层# bgp 与 host-gw 工作在二层 不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。 所以建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。 CNI 网络方案优缺点及最终选择123456先考虑几个问题： - 需要细粒度网络访问控制？ Calico 支持 acl , Flannel 不支持 acl- 追求网络性能？ bgp 或者 host-gw- 服务器之间是否可以跑BGP协议？ 公有云不支持bgp协议- 集群规模多大？ 简单的 Flannel - 是否有维护能力？ Calico 路由多 一旦出现问题 查看起来非常难 办公网络与K8S网络如何互通 测试环境 k8s 集群 开发人员 办公网络 微服务通过 podip 调用 办公网络 如何 访问 podip 打通网络 网络策略为什么需要网络隔离？ CNI插件插件解决了不同Node节点Pod互通问题，从而形成一个扁平化网络，默认情况下，Kubernetes 网络允许所有 Pod 到 Pod 的流量 在一些场景中，我们不希望Pod之间默认相互访问，例如： 应用程序间的访问控制。例如微服务A允许访问微服务B，微服务C不能访问微服务A 开发环境命名空间不能访问测试环境命名空间Pod 当Pod暴露到外部时，需要做Pod白名单 多租户网络环境隔离 所以，我们需要使用network policy对Pod网络进行隔离。支持对Pod级别和Namespace级别网络访问控制。 acl Pod网络入口方向隔离 基于Pod级网络隔离：只允许特定对象访问Pod（使用标签定义），允许白名单上的IP地址或者IP段访问Pod 基于Namespace级网络隔离：多个命名空间，A和B命名空间Pod完全隔离。 acl Pod网络出口方向隔离 拒绝某个Namespace上所有Pod访问外部 基于目的IP的网络隔离：只允许Pod访问白名单上的IP地址或者IP段 基于目标端口的网络隔离：只允许Pod访问白名单上的端口 1234561. 对 default 空间下 带标签 role: db 的pod 做网络策略2. 入口策略: 这个网段 cidr: 172.17.0.0/16 除了 172.17.1.0/24 以外的ip 都可以访问3. myproject 这个命名空间 可以访问 4. 标签有 role: frontend 的pod 可以访问5. 可以访问的port是 6379端口 6. 出口策略: 可以访问 10.0.0.0/24 这个网段的 5978端口 12345配置解析：- podSelector：用于选择策略应用到的Pod组。- policyTypes：其可以包括任一Ingress，Egress或两者。该policyTypes字段指示给定的策略用于Pod的入站流量、还是出站流量，或者两者都应用。如果未指定任何值，则默认值为Ingress，如果网络策略有出口规则，则设置egress。- Ingress：from是可以访问的白名单，可以来自于IP段、命名空间、Pod标签等，ports是可以访问的端口。- Egress：这个Pod组可以访问外部的IP段和端口。 入站、出站网络流量访问控制案例Pod访问限制 准备测试环境，1组web pod，2个client pod 1234567891011121314[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-77fdbc8556-gfqd5 1/1 Running 0 53m 10.244.36.72 k8s-node1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-sm5ln 1/1 Running 0 53m 10.244.159.135 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-tbm4n 1/1 Running 0 53m 10.244.169.132 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSmy-nginx-77fdbc8556-gfqd5 1/1 Running 0 54m app=my-nginx,pod-template-hash=77fdbc8556my-nginx-77fdbc8556-sm5ln 1/1 Running 0 54m app=my-nginx,pod-template-hash=77fdbc8556my-nginx-77fdbc8556-tbm4n 1/1 Running 0 54m app=my-nginx,pod-template-hash=77fdbc85561. 限制当前命名空间下 app=my-nginx pod 做网络隔离 2. 只允许default命名空间携带run=client1标签的Pod访问 80端口 123456789101112131415161718192021222324[root@k8s-master1 demo]# vim ns.yaml apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: app: my-nginx policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: project: default - podSelector: matchLabels: run: client1 ports: - protocol: TCP port: 80 12345隔离策略配置：1. Pod对象: default命名空间携带app: my-nginx标签的Pod2. 允许访问端口： 803. 允许访问对象： default命名空间携带run=client1标签的Pod4. 拒绝访问对象： 除允许访问对象外的所有对象 12345678910111213141516171819202122232425262728293031323334353637# 先在不做策略的时候 测试访问 run client1 和 run client2 [root@k8s-master1 demo]# kubectl run client1 --generator=run-pod/v1 --image=busybox --command -- sleep 36000[root@k8s-master1 demo]# kubectl run client2 --generator=run-pod/v1 --image=busybox --command -- sleep 36000[root@k8s-master1 demo]# kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSclient1 1/1 Running 0 3m20s run=client1client2 1/1 Running 0 4s run=client2my-nginx-77fdbc8556-gfqd5 1/1 Running 0 64m app=my-nginx,pod-template-hash=77fdbc8556my-nginx-77fdbc8556-sm5ln 1/1 Running 0 64m app=my-nginx,pod-template-hash=77fdbc8556my-nginx-77fdbc8556-tbm4n 1/1 Running 0 64m app=my-nginx,pod-template-hash=77fdbc8556[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESclient1 1/1 Running 0 73s 10.244.169.133 k8s-node2 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-gfqd5 1/1 Running 0 61m 10.244.36.72 k8s-node1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-sm5ln 1/1 Running 0 61m 10.244.159.135 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-tbm4n 1/1 Running 0 61m 10.244.169.132 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl exec -it client1 sh# 可以ping通pod / # ping 10.244.36.72PING 10.244.36.72 (10.244.36.72): 56 data bytes64 bytes from 10.244.36.72: seq=0 ttl=62 time=33.420 ms# 可以访问80 下载页面/ # wget 10.244.36.72Connecting to 10.244.36.72 (10.244.36.72:80)saving to 'index.html'index.html 100% |********************************************************************************************************************************************************************************************| 612 0:00:00 ETA'index.html' saved[root@k8s-master1 demo]# kubectl exec -it client2 sh/ # wget 10.244.36.72saving to 'index.html'index.html 100% |********************************************************************************************************************************************************************************************| 612 0:00:00 ETA'index.html' saved 12345# 增加规则 # Calico 支持 acl , Flannel 不支持 acl , Flannel 没法做 NetworkPolicy[root@k8s-master1 demo]# kubectl apply -f ns.yaml networkpolicy.networking.k8s.io/test-network-policy created# 规则是 只有携带 run=client1 标签的pod 可以访问 1234567891011121314151617# 测试[root@k8s-master1 demo]# kubectl exec -it client2 sh/ # rm -rf index.html / # wget 10.244.36.72Connecting to 10.244.36.72 (10.244.36.72:80) # 不通，无法访问[root@k8s-master1 demo]# kubectl exec -it client1 sh/ # rm -rf index.html / # wget 10.244.36.72Connecting to 10.244.36.72 (10.244.36.72:80)saving to 'index.html'index.html 100% |********************************************************************************************************************************************************************************************| 612 0:00:00 ETA'index.html' saved# client1 可以下载访问# client2 无法下载访问# 实际环境 多加上几组标签,精确粒度限制 命名空间隔离 两个开发团队,都有自己的命名空间,他们之间的pod都不可以相互访问 1234[root@k8s-master1 demo]# kubectl run client3 --generator=run-pod/v1 --image=busybox -n kube-system --command -- sleep 36000# 取消刚才的限制 [root@k8s-master1 demo]# kubectl delete -f ns.yaml networkpolicy.networking.k8s.io "test-network-policy" deleted 12345678910111213141516171819202122[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESclient1 1/1 Running 0 27m 10.244.169.133 k8s-node2 &lt;none&gt; &lt;none&gt;client2 1/1 Running 0 24m 10.244.169.134 k8s-node2 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-gfqd5 1/1 Running 0 88m 10.244.36.72 k8s-node1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-sm5ln 1/1 Running 0 88m 10.244.159.135 k8s-master1 &lt;none&gt; &lt;none&gt;my-nginx-77fdbc8556-tbm4n 1/1 Running 0 88m 10.244.169.132 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl get pods -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScalico-kube-controllers-5cc7b68d7c-xb5rf 1/1 Running 1 7h15m 192.168.0.102 k8s-node1 &lt;none&gt; &lt;none&gt;calico-node-hwt8c 1/1 Running 1 7h15m 192.168.0.103 k8s-node2 &lt;none&gt; &lt;none&gt;calico-node-v7c4g 1/1 Running 1 7h15m 192.168.0.101 k8s-master1 &lt;none&gt; &lt;none&gt;calico-node-wfp2f 1/1 Running 1 7h15m 192.168.0.102 k8s-node1 &lt;none&gt; &lt;none&gt;client3 1/1 Running 0 8m18s 10.244.159.136 k8s-master1 &lt;none&gt; &lt;none&gt;coredns-6d8cfdd59d-6wwkz 1/1 Running 1 6h54m 10.244.36.69 k8s-node1 &lt;none&gt; &lt;none&gt;# 两个命名空间的pod 可以互通[root@k8s-master1 demo]# kubectl exec -it client3 sh -n kube-system/ # ping 10.244.36.72PING 10.244.36.72 (10.244.36.72): 56 data bytes64 bytes from 10.244.36.72: seq=0 ttl=62 time=0.630 ms 12345678910111213141516171819202122# 需求：1. default命名空间下所有pod可以互相访问，2. 其他命名空间不能访问default命名空间Pod。[root@k8s-master1 demo]# vim ns2.yaml apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: deny-from-other-namespaces namespace: defaultspec: podSelector: &#123;&#125; policyTypes: - Ingress ingress: - from: - podSelector: &#123;&#125; # podSelector: &#123;&#125;： default命名空间下所有Pod# from.podSelector: &#123;&#125; : 如果未配置具体的规则，默认不允许 123456789101112131415161718192021# 执行测试[root@k8s-master1 demo]# kubectl apply -f ns2.yaml networkpolicy.networking.k8s.io/deny-from-other-namespaces created# kube-system下的client3无法ping通 default下的pod[root@k8s-master1 demo]# kubectl exec -it client3 sh -n kube-system/ # ping 10.244.36.72PING 10.244.36.72 (10.244.36.72): 56 data bytes# default下的pod 可以正常互通[root@k8s-master1 demo]# kubectl exec -it client2 sh / # ping 10.244.36.72PING 10.244.36.72 (10.244.36.72): 56 data bytes64 bytes from 10.244.36.72: seq=0 ttl=62 time=13.809 ms# 也可以ping通 kube-system 下的/ # ping 10.244.159.136PING 10.244.159.136 (10.244.159.136): 56 data bytes64 bytes from 10.244.159.136: seq=0 ttl=62 time=0.538 ms# 测试dns解析 需要部署 busybox镜像 指定版本 1.28.4 Flannel 实现网络隔离1234567891011121314151. 需要 canal 插件支持2. 阿里云可以试试 vpc 能不能跑 Calico bgp3. 虚拟机可以跑 Calico 机房: 1. Calico bgp 打通2. 路由反射器3. 对接每个节点的网关路由器4. lvs 对 pod 做负载均衡 支持:Calico BGP 1000个节点以内Flannel 100个节点以内 大公司有资源小公司还是 一个业务一个圈子好一些 12345678910111213141516171. 阿里云主机 Flannel: vxlan 互通 host-gw 不通 Calico: bgp 不通 Route Reflector 模式（RR）不通 ipip 未测试 2. 本地虚拟机 Flannel: vxlan 互通 host-gw 互通 Calico: bgp 互通 Route Reflector 模式（RR）互通 ipip: 互通]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05 Helm 应用包管理器]]></title>
    <url>%2F2019%2F12%2F06%2Fk8s-base07%2F</url>
    <content type="text"><![CDATA[为什么需要 Helm？ K8S上的应用对象，都是由特定的资源描述组成，包括deployment、service等。 都保存各自文件中或者集中写到一个配置文件。然后kubectl apply –f 部署。 如果应用只由一个或几个这样的服务组成，上面部署方式足够了。 而对于一个复杂的应用，会有很多类似上面的资源描述文件，例如微服务架构应用，组成应用的服务可能多达十个，几十个。 如果有更新或回滚应用的需求，可能要修改和维护所涉及的大量资源文件，而这种组织和管理应用的方式就显得力不从心了。 且由于缺少对发布过的应用版本管理和控制，使Kubernetes上的应用维护和更新等面临诸多的挑战，主要面临以下问题： 如何将这些服务作为一个整体管理 这些资源文件如何高效复用 不支持应用级别的版本管理 Helm 介绍 Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。 Helm有3个重要概念: helm：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。 Chart：应用描述，一系列用于描述 k8s 资源相关文件的集合。 Release：基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在k8s中创建出真实运行的资源对象。 Helm v3 变化 2019年11月13日， Helm团队发布 Helm v3的第一个稳定版本。 该版本主要变化如下： 架构变化 最明显的变化是 Tiller的删除 Release名称可以在不同命名空间重用 支持将 Chart 推送至 Docker 镜像仓库中 使用JSONSchema验证chart values 其他 123456789101112131. 为了更好地协调其他包管理者的措辞 `Helm CLI `个别更名helm delete 更名为 helm uninstallhelm inspect 更名为 helm showhelm fetch 更名为 helm pull但以上旧的命令当前仍能使用。2. 移除了用于本地临时搭建 `Chart Repository `的 `helm serve` 命令。3. 自动创建名称空间在不存在的命名空间中创建发行版时，Helm 2创建了命名空间。Helm 3遵循其他Kubernetes对象的行为，如果命名空间不存在则返回错误。4. 不再需要`requirements.yaml`, 依赖关系是直接在`chart.yaml`中定义。 Helm 客户端部署 Helm 客户端 Helm客户端下载地址：https://github.com/helm/helm/releases 解压移动到/usr/bin/目录即可。 123wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ Helm 常用命令 命令 描述 create 创建一个chart并指定名字 dependency 管理chart依赖 get 下载一个release。可用子命令：all、hooks、manifest、notes、values history 获取release历史 install 安装一个chart list 列出release package 将chart目录打包到chart存档文件中 pull 从远程仓库中下载chart并解压到本地 # helm pull stable/mysql –untar repo 添加，列出，移除，更新和索引chart仓库。可用子命令：add、index、list、remove、update rollback 从之前版本回滚 search 根据关键字搜索chart。可用子命令：hub、repo show 查看chart详细信息。可用子命令：all、chart、readme、values status 显示已命名版本的状态 template 本地呈现模板 uninstall 卸载一个release upgrade 更新一个release version 查看helm客户端版本 配置国内 Chart 仓库 微软仓库（http://mirror.azure.cn/kubernetes/charts/） 这个仓库强烈推荐，基本上官网有的chart这里都有。 阿里云仓库（https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ） 官方仓库（https://hub.kubeapps.com/charts/incubator） 官方chart仓库，国内有点不好使。 123helm repo add stable http://mirror.azure.cn/kubernetes/chartshelm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo update 1234567891011121314151617[root@k8s-master1 ~]# helm repo add stable http://mirror.azure.cn/kubernetes/charts"stable" has been added to your repositories[root@k8s-master1 ~]# helm repo listNAME URL stable http://mirror.azure.cn/kubernetes/charts[root@k8s-master1 ~]# helm search repo mysqlNAME CHART VERSION APP VERSION DESCRIPTION stable/mysql 1.6.2 5.7.28 Fast, reliable, scalable, and easy to use open-...stable/mysqldump 2.6.0 2.4.1 A Helm chart to help backup MySQL databases usi...stable/prometheus-mysql-exporter 0.5.2 v0.11.0 A Helm chart for prometheus mysql exporter with...stable/percona 1.2.0 5.7.17 free, fully compatible, enhanced, open source d...stable/percona-xtradb-cluster 1.0.3 5.7.19 free, fully compatible, enhanced, open source d...stable/phpmyadmin 4.2.4 4.9.2 phpMyAdmin is an mysql administration frontend stable/gcloud-sqlproxy 0.6.1 1.11 DEPRECATED Google Cloud SQL Proxy stable/mariadb 7.3.1 10.3.21 Fast, reliable, scalable, and easy to use open-... 123# 查看仓库中所有chart[root@k8s-master1 ~]# helm search repo stable[root@k8s-master1 ~]# helm search repo stable |grep swift 添加多个仓库1234567891011121314151617181920212223[root@k8s-master1 ~]# helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts "aliyun" has been added to your repositories[root@k8s-master1 ~]# helm repo listNAME URL stable http://mirror.azure.cn/kubernetes/charts aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts[root@k8s-master1 ~]# helm search repo mysqlNAME CHART VERSION APP VERSION DESCRIPTION aliyun/mysql 0.3.5 Fast, reliable, scalable, and easy to use open-...stable/mysql 1.6.2 5.7.28 Fast, reliable, scalable, and easy to use open-...stable/mysqldump 2.6.0 2.4.1 A Helm chart to help backup MySQL databases usi...stable/prometheus-mysql-exporter 0.5.2 v0.11.0 A Helm chart for prometheus mysql exporter with...aliyun/percona 0.3.0 free, fully compatible, enhanced, open source d...aliyun/percona-xtradb-cluster 0.0.2 5.7.19 free, fully compatible, enhanced, open source d...stable/percona 1.2.0 5.7.17 free, fully compatible, enhanced, open source d...stable/percona-xtradb-cluster 1.0.3 5.7.19 free, fully compatible, enhanced, open source d...stable/phpmyadmin 4.2.4 4.9.2 phpMyAdmin is an mysql administration frontend aliyun/gcloud-sqlproxy 0.2.3 Google Cloud SQL Proxy aliyun/mariadb 2.1.6 10.1.31 Fast, reliable, scalable, and easy to use open-...stable/gcloud-sqlproxy 0.6.1 1.11 DEPRECATED Google Cloud SQL Proxy stable/mariadb 7.3.1 10.3.21 Fast, reliable, scalable, and easy to use open-... 删除存储库1[root@k8s-master1 ~]# helm repo remove aliyun Helm 基本使用 主要介绍三个命令: chart install chart update chart rollback 使用 chart 部署一个应用123# 查找chart [root@k8s-master1 ~]# helm search repo[root@k8s-master1 ~]# helm search repo mysql 1234# 查看chart信息[root@k8s-master1 ~]# helm show chart stable/mysql# values 相当于 模板的变量 yaml文件中的动态字段需要动态传值[root@k8s-master1 ~]# helm show values stable/mysql 123456# 安装 db1 是 Release 的名称# 部署后会弹出 使用信息[root@k8s-master1 ~]# helm install db1 stable/mysql# 获取密码[root@k8s-master1 ~]# kubectl get secret --namespace default db1-mysql -o jsonpath="&#123;.data.mysql-root-password&#125;" | base64 --decode; echo5R4VprCnNT 1234# 查看部署状态[root@k8s-master1 ~]# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONdb1 default 1 2019-12-17 11:23:30.533875697 +0800 CST deployed mysql-1.6.2 5.7.28 查看状态12# 查看发布状态[root@k8s-master1 ~]# helm status db1 12345678910# 查看pod状态[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEdb1-mysql-868b97747b-tnpwk 0/1 Pending 0 9m59s # 等待metrics-app-7674cfb699-nzmdz 1/1 Running 0 132mmetrics-app-7674cfb699-thjzk 1/1 Running 0 132mmetrics-app-7674cfb699-xvfpx 1/1 Running 0 132mnfs-client-provisioner-5dd6f66f47-w5t6w 1/1 Running 0 137mweb-6f4b67f8cc-j2ccg 1/1 Running 0 148mweb-6f4b67f8cc-mljnd 1/1 Running 0 148m 123456789# 查看事件[root@k8s-master1 ~]# kubectl describe pod db1-mysql-868b97747b-tnpwk# 没有 pvcEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler pod has unbound immediate PersistentVolumeClaims (repeated 2 times) Warning FailedScheduling &lt;unknown&gt; default-scheduler pod has unbound immediate PersistentVolumeClaims (repeated 2 times) 123# 绑定pvc的方法:1. 静态2. 动态 1234# 查看pvc[root@k8s-master1 ~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdb1-mysql Pending 12m 123456789101112131415161718192021# 创建一个pvc匹配到 db1-mysql [root@k8s-master1 ~]# kubectl describe pvc db1-mysqlName: db1-mysqlNamespace: defaultStorageClass: Status: PendingVolume: Labels: app=db1-mysql chart=mysql-1.6.2 heritage=Helm release=db1Annotations: &lt;none&gt;Finalizers: [kubernetes.io/pvc-protection]Capacity: Access Modes: VolumeMode: FilesystemMounted By: db1-mysql-868b97747b-tnpwkEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal FailedBinding 3m57s (x42 over 14m) persistentvolume-controller no persistent volumes available for this claim and no storage class is set 1234567891011121314151617181920212223242526[root@k8s-master1 ~]# kubectl get pvc db1-mysql -o yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: creationTimestamp: "2019-12-17T03:23:30Z" finalizers: - kubernetes.io/pvc-protection labels: app: db1-mysql chart: mysql-1.6.2 heritage: Helm release: db1 name: db1-mysql namespace: default resourceVersion: "18423" selfLink: /api/v1/namespaces/default/persistentvolumeclaims/db1-mysql uid: 8fc33553-3154-448b-92a5-cff2a7c3b757spec: accessModes: - ReadWriteOnce # 所有节点可以读写 resources: requests: storage: 8Gi # 8G存储 volumeMode: Filesystemstatus: phase: Pending 创建 pv123456789101112131415161718192021222324# 模板https://kubernetes.io/docs/concepts/storage/persistent-volumes/# Persistent Volumes# Each PV contains a spec and status, which is the specification and status of the volume.apiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 12# nfs主机上创建目录[root@k8s-node2 ~]# mkdir /ifs/kubernetes/db 123456789101112131415# 创建 pv[root@k8s-master1 pv]# vim pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 8Gi accessModes: - ReadWriteOnce nfs: path: /ifs/kubernetes/db server: 172.17.70.254 1234567891011121314[root@k8s-master1 pv]# kubectl apply -f pv.yaml persistentvolume/pv0003 created[root@k8s-master1 pv]# kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/pv0003 8Gi RWO Retain Bound default/db1-mysql 70spersistentvolume/pvc-56346fbf-b298-4573-a0dd-1429325dcb71 16Gi RWO Delete Bound kube-system/prometheus-data-prometheus-0 managed-nfs-storage 151mNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/db1-mysql Bound pv0003 8Gi RWO 25m[root@k8s-master1 pv]# kubectl get podsNAME READY STATUS RESTARTS AGEdb1-mysql-868b97747b-tnpwk 1/1 Running 0 26m 登录测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 由于网络原因 就不使用官网容器测试了 直接登录测试# 先拿到密码[root@k8s-master1 pv]# helm list[root@k8s-master1 pv]# helm status db1[root@k8s-master1 pv]# kubectl get secret --namespace default db1-mysql -o jsonpath="&#123;.data.mysql-root-password&#125;" | base64 --decode; echo5R4VprCnNT# 登录 mysql[root@k8s-master1 pv]# kubectl get pods[root@k8s-master1 pv]# kubectl exec -it db1-mysql-868b97747b-tnpwk bashroot@db1-mysql-868b97747b-tnpwk:/# mysql -uroot -p5R4VprCnNTmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 58Server version: 5.7.28 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; create database test;Query OK, 1 row affected (0.01 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.01 sec)# 看下变量 发现有的是空的root@db1-mysql-868b97747b-tnpwk:/# echo $MYSQL_ROOT_PASSWORD5R4VprCnNTroot@db1-mysql-868b97747b-tnpwk:/# echo $MYSQL_PORT root@db1-mysql-868b97747b-tnpwk:/# echo $MYSQL_HOST 使用自己的 nfs pv自动供给 修改chart的部署选项 上面部署的mysql一开始并没有成功，这是因为并不是所有的chart都能按照默认配置运行成功，可能会需要一些环境依赖，例如PV。 所以我们需要自定义chart配置选项，安装过程中有两种方法可以传递配置数据： –values（或-f）：指定带有覆盖的YAML文件。这可以多次指定，最右边的文件优先 –set：在命令行上指定替代。如果两者都用，–set优先级高 values 使用 先将修改的变量写到一个文件中 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 pv]# helm show values stable/mysql &gt; values.yaml # 后面看默认的位置# 只保留要配置的地方 比如pvc# 增加一些配置 如 创建用户 数据库 等# 查看下自动供给[root@k8s-master1 ~]# kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 169m# 修改配置文件[root@k8s-master1 pv]# vim values.yaml ## Specify password for root user## Default: random 10 character stringmysqlRootPassword: testing## Create a database usermysqlUser: k8s## Default: random 10 character stringmysqlPassword: k8s123## Create a databasemysqlDatabase: k8s## Persist data to a persistent volumepersistence: enabled: true ## database data Persistent Volume Storage Class ## If defined, storageClassName: &lt;storageClass&gt; ## If set to "-", storageClassName: "", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS &amp; OpenStack) ## storageClass: "managed-nfs-storage" accessMode: ReadWriteOnce size: 8Gi 12345678910111213141516171819202122232425262728293031323334353637383940414243# 以上将创建具有名称的默认MySQL用户k8s，并授予此用户访问新创建的k8s数据库的权限，但将接受该图表的所有其余默认值。# 指定配置文件部署[root@k8s-master1 pv]# helm install db2 -f values.yaml stable/mysql[root@k8s-master1 pv]# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONdb1 default 1 2019-12-17 11:23:30.533875697 +0800 CST deployed mysql-1.6.2 5.7.28 db2 default 1 2019-12-17 12:08:36.436489783 +0800 CST deployed mysql-1.6.2 5.7.28 # 直接可以运行[root@k8s-master1 pv]# kubectl get podsNAME READY STATUS RESTARTS AGEdb1-mysql-868b97747b-tnpwk 1/1 Running 0 45mdb2-mysql-76495946b5-7x9jw 1/1 Running 0 44s# 进入测试root@db2-mysql-76495946b5-7x9jw:/# mysql -uroot -ptestingmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 19Server version: 5.7.28 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || k8s || mysql || performance_schema || sys |+--------------------+5 rows in set (0.05 sec)root@db2-mysql-76495946b5-7x9jw:/# mysql -uk8s -pk8s123 1234# 小总结: 如果helm使用官方创建，有一些依赖需要提前准备,比如pv1. 引用values.yaml2. --set set 使用12345678910111213[root@k8s-master1 pv]# helm install db3 --set persistence.storageClass="managed-nfs-storage" stable/mysql[root@k8s-master1 pv]# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONdb1 default 1 2019-12-17 11:23:30.533875697 +0800 CST deployed mysql-1.6.2 5.7.28 db2 default 1 2019-12-17 12:08:36.436489783 +0800 CST deployed mysql-1.6.2 5.7.28 db3 default 1 2019-12-17 12:13:53.856941497 +0800 CST deployed mysql-1.6.2 5.7.28 [root@k8s-master1 pv]# kubectl get podsNAME READY STATUS RESTARTS AGEdb1-mysql-868b97747b-tnpwk 1/1 Running 0 51mdb2-mysql-76495946b5-7x9jw 1/1 Running 0 6m1sdb3-mysql-59585b7656-vwfvw 1/1 Running 0 40s 12# set传入需要遵循语法 values 结构化数据# values yaml与set使用： 拉取整个chart包123456789101112131415161718192021222324252627# --untar 拉取后直接解压[root@k8s-master1 pv]# helm pull stable/mysql --untar [root@k8s-master1 pv]# lsmysql[root@k8s-master1 pv]# cd mysql/[root@k8s-master1 mysql]# ls -ltotal 40-rw-r--r-- 1 root root 502 Dec 17 12:17 Chart.yaml -rw-r--r-- 1 root root 22284 Dec 17 12:17 README.mddrwxr-xr-x 3 root root 4096 Dec 17 12:17 templates-rw-r--r-- 1 root root 5646 Dec 17 12:17 values.yaml # 覆盖的就是这个配置文件 # 部署mysql的yaml文件目录[root@k8s-master1 mysql]# ls -l templates/total 52-rw-r--r-- 1 root root 292 Dec 17 12:17 configurationFiles-configmap.yaml-rw-r--r-- 1 root root 8610 Dec 17 12:17 deployment.yaml-rw-r--r-- 1 root root 1290 Dec 17 12:17 _helpers.tpl-rw-r--r-- 1 root root 295 Dec 17 12:17 initializationFiles-configmap.yaml-rw-r--r-- 1 root root 1797 Dec 17 12:17 NOTES.txt-rw-r--r-- 1 root root 868 Dec 17 12:17 pvc.yaml-rw-r--r-- 1 root root 1475 Dec 17 12:17 secrets.yaml-rw-r--r-- 1 root root 328 Dec 17 12:17 serviceaccount.yaml-rw-r--r-- 1 root root 800 Dec 17 12:17 servicemonitor.yaml-rw-r--r-- 1 root root 1104 Dec 17 12:17 svc.yamldrwxr-xr-x 2 root root 4096 Dec 17 12:17 tests helm install 命令可以从多个来源安装 chart存储库 本地chart存档（helm install foo-0.1.1.tgz） chart目录（helm install path/to/foo） 完整的URL（helm install https://example.com/charts/foo-1.2.3.tgz） 构建一个 Helm Chart自动生成目录123456789[root@k8s-master1 ~]# helm create mychartCreating mychart[root@k8s-master1 ~]# cd mychart/[root@k8s-master1 mychart]# ls -ldrwxr-xr-x 2 root root 4096 Dec 17 16:54 charts-rw-r--r-- 1 root root 905 Dec 17 16:54 Chart.yamldrwxr-xr-x 3 root root 4096 Dec 17 16:54 templates-rw-r--r-- 1 root root 1490 Dec 17 16:54 values.yaml 12345678910111213141516171819202122232425# 启动这个默认自动创建的 mychart 会发现是一个 nginx服务[root@k8s-master1 ~]# helm install test mychart/[root@k8s-master1 ~]# kubectl get pods | grep testtest-mychart-b5cd6d7c8-5mz8b 1/1 Running 0 42s[root@k8s-master1 ~]# kubectl get pods -o wide | grep testtest-mychart-b5cd6d7c8-5mz8b 1/1 Running 0 59s 10.244.2.9 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# curl -I 10.244.2.9HTTP/1.1 200 OKServer: nginx/1.16.0Date: Tue, 17 Dec 2019 08:58:21 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 23 Apr 2019 10:18:21 GMTConnection: keep-aliveETag: "5cbee66d-264"Accept-Ranges: bytes[root@k8s-master1 ~]# helm listNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONdb1 default 1 2019-12-17 11:23:30.533875697 +0800 CST deployed mysql-1.6.2 5.7.28 db2 default 1 2019-12-17 12:08:36.436489783 +0800 CST deployed mysql-1.6.2 5.7.28 db3 default 1 2019-12-17 12:13:53.856941497 +0800 CST deployed mysql-1.6.2 5.7.28 test default 1 2019-12-17 16:57:09.278738153 +0800 CST deployed mychart-0.1.0 1.16.0 123456789101112131415# 文件内容: [root@k8s-master1 ~]# tree /root/mychart//root/mychart/├── charts # 目录里存放这个chart依赖的所有子chart。├── Chart.yaml # 用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。├── templates # 目录里面存放所有yaml模板文件。│ ├── deployment.yaml│ ├── _helpers.tpl # 放置模板助手的地方，可以在整个 chart 中重复使用│ ├── ingress.yaml│ ├── NOTES.txt # 用于介绍Chart帮助信息，helm install 部署后展示给用户。例如：如何使用这个 Chart、列出缺省的设置等。│ ├── serviceaccount.yaml│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml # 动态变量 用于存储 templates 目录中模板文件中用到变量的值。 简单制作一个 chart创建1234567# 将文件和目录清空[root@k8s-master1 mychart]# tree.├── charts├── Chart.yaml├── templates└── values.yaml 123456789101112131415161718192021222324# 创建nginx的 deployment[root@k8s-master1 templates]# kubectl create deployment mychart --image=nginx:1.16 -o yaml --dry-run &gt; deployment.yaml# 删除不要的配置[root@k8s-master1 templates]# vim deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: mychart name: mychartspec: replicas: 1 selector: matchLabels: app: mychart template: metadata: labels: app: mychart spec: containers: - image: nginx:1.16 name: nginx 模板 Helm最核心的就是模板，即模板化的K8S manifests文件。 它本质上就是一个Go的template模板。Helm在Go template模板的基础上，还会增加很多东西。 如一些自定义的元数据信息、扩展的库以及一些类似于编程形式的工作流，例如条件语句、管道等等。这些东西都会使得我们的模板变得更加丰富。 12345678910111213141516171819202122# 修改成 动态模板# 有了模板，我们怎么把我们的配置融入进去呢？用的就是这个values文件。这两部分内容其实就是chart的核心功能。[root@k8s-master1 templates]# vim deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; .Values.name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicas &#125;&#125; selector: matchLabels: app: mychart template: metadata: labels: app: mychart spec: containers: - image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag &#125;&#125; name: nginx 1234567# 加入变量 引入名字需要一致[root@k8s-master1 templates]# vim ../values.yaml name: helloreplicas: 3image: nginximageTag: 1.15 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 安装[root@k8s-master1 ~]# helm install hello mychart/NAME: helloLAST DEPLOYED: Tue Dec 17 17:56:12 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None# 卸载[root@k8s-master1 ~]# helm uninstall hellorelease "hello" uninstalled# 查看[root@k8s-master1 ~]# kubectl get pods -o wide| grep hellohello-d8ccdfdd8-cw7qh 1/1 Running 0 38s 10.244.2.10 k8s-node2 &lt;none&gt; &lt;none&gt;hello-d8ccdfdd8-mqh76 1/1 Running 0 38s 10.244.1.9 k8s-node1 &lt;none&gt; &lt;none&gt;hello-d8ccdfdd8-skdjw 1/1 Running 0 38s 10.244.0.11 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 ~]# helm list|grep hellohello default 1 2019-12-17 17:56:12.316160884 +0800 CST deployed mychart-0.1.0 1.16.0 # 查看渲染后的部署文件# 我们希望能在一个地方统一定义这些会经常变换的字段，这就需要用到Chart的模板了。[root@k8s-master1 ~]# helm get manifest hello---# Source: mychart/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: hellospec: replicas: 3 selector: matchLabels: app: mychart template: metadata: labels: app: mychart spec: containers: - image: nginx:1.15 name: nginx 升级 发布新版本的chart时，或者当您要更改发布的配置时，可以使用该helm upgrade 命令。 123456789101112131415161718192021222324# 修改values.yaml[root@k8s-master1 mychart]# vim values.yaml name: helloreplicas: 3image: nginximageTag: 1.16# 升级[root@k8s-master1 mychart]# helm upgrade hello /root/mychart/Release "hello" has been upgraded. Happy Helming!NAME: helloLAST DEPLOYED: Tue Dec 17 18:04:30 2019NAMESPACE: defaultSTATUS: deployedREVISION: 2TEST SUITE: None[root@k8s-master1 mychart]# kubectl get pods -o wide| grep hellohello-5b97fb4c85-hssrt 1/1 Running 0 7s 10.244.0.12 k8s-master1 &lt;none&gt; &lt;none&gt;hello-5b97fb4c85-wlwnj 1/1 Running 0 3s 10.244.1.10 k8s-node1 &lt;none&gt; &lt;none&gt;hello-5b97fb4c85-x2zp7 1/1 Running 0 4s 10.244.2.11 k8s-node2 &lt;none&gt; &lt;none&gt;hello-d8ccdfdd8-mqh76 0/1 Terminating 0 8m25s 10.244.1.9 k8s-node1 &lt;none&gt; &lt;none&gt;hello-d8ccdfdd8-skdjw 1/1 Terminating 0 8m25s 10.244.0.11 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 mychart]# curl -I 10.244.0.12 回滚 如果在发布后没有达到预期的效果，则可以使用helm rollback回滚到之前的版本。 12# 回滚到上一个版本[root@k8s-master1 mychart]# helm rollback hello 12345678# 回滚到指定版本[root@k8s-master1 mychart]# helm history helloREVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Dec 17 17:56:12 2019 superseded mychart-0.1.0 1.16.0 Install complete2 Tue Dec 17 18:04:30 2019 superseded mychart-0.1.0 1.16.0 Upgrade complete3 Tue Dec 17 18:07:56 2019 deployed mychart-0.1.0 1.16.0 Rollback to 1 [root@k8s-master1 mychart]# helm rollback hello 2 卸载1234# 卸载[root@k8s-master1 ~]# helm list[root@k8s-master1 ~]# helm uninstall hellorelease "hello" uninstalled 打包123# 可以打包推送的charts仓库共享别人使用。[root@k8s-master1 ~]# helm package mychartSuccessfully packaged chart and saved it to: /root/mychart-0.1.0.tgz 深入学习 HelmChart 模板 Helm最核心的就是模板，即模板化的K8S manifests文件。 它本质上就是一个Go的template模板。Helm在Go template模板的基础上，还会增加很多东西。 如一些自定义的元数据信息、扩展的库以及一些类似于编程形式的工作流，例如条件语句、管道等等。这些东西都会使得我们的模板变得更加丰富。 123[root@k8s-master1 ~]# helm create app01Creating app01[root@k8s-master1 ~]# cd app01/ 1234# 清除默认生成的模板文件[root@k8s-master1 templates]# rm -rf *# 保留变量 12345# 创建deployment 和 service[root@k8s-master1 templates]# kubectl create deployment web --image=nginx:1.16 --dry-run -o yaml &gt; deployment.yaml[root@k8s-master1 templates]# kubectl apply -f deployment.yaml [root@k8s-master1 templates]# kubectl expose deployment web --port=80 --target-port=80 --dry-run -o yaml &gt; service.yaml[root@k8s-master1 templates]# kubectl delete deploy web 12345678910111213141516# 直接部署[root@k8s-master1 templates]# kubectl apply -f .deployment.apps/web createdservice/web created[root@k8s-master1 templates]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/db2-mysql-76495946b5-kv76b 1/1 Running 2 7h27mpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 2 7h31mpod/web-866f97c649-vrmrm 1/1 Running 0 5sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/db2-mysql ClusterIP 10.0.0.159 &lt;none&gt; 3306/TCP 7h27mservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 30hservice/metrics-app ClusterIP 10.0.0.24 &lt;none&gt; 80/TCP 30hservice/web ClusterIP 10.0.0.29 &lt;none&gt; 80/TCP 5s 1234567891011121314151617181920212223242526272829# helm 部署[root@k8s-master1 templates]# helm install app01 /root/app01/NAME: app01LAST DEPLOYED: Thu Dec 19 08:00:41 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 templates]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 3 23hpod/web-866f97c649-b76cr 1/1 Running 0 5sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 47hservice/metrics-app ClusterIP 10.0.0.24 &lt;none&gt; 80/TCP 46hservice/web ClusterIP 10.0.0.206 &lt;none&gt; 80/TCP 5s[root@k8s-master1 templates]# curl -I 10.0.0.206HTTP/1.1 200 OKServer: nginx/1.16.1Date: Thu, 19 Dec 2019 00:00:54 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 13 Aug 2019 10:05:00 GMTConnection: keep-aliveETag: "5d528b4c-264"Accept-Ranges: bytes 动态使用模板使用内置对象 Release.Name release 名称 Release.Name release 名字 Release.Namespace release 命名空间 Release.Service release 服务的名称 Release.Revision release 修订版本号，从1开始累加 1# 部署另外的应用 修改标签选择器和镜像名称 1234567891011121314151617181920212223# 通过模板渲染[root@k8s-master1 templates]# vim deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; .Chart.name &#125;&#125; # Chart.yaml定义变量 app: &#123;&#123; .Release.Name &#125;&#125; # 内置变量 name: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicas &#125;&#125; # Values.yaml 定义变量 副本数 selector: matchLabels: app: &#123;&#123; .Values.label &#125;&#125; # Pod 标签 template: metadata: labels: app: &#123;&#123; .Values.label &#125;&#125; # Pod 标签 spec: containers: - image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag &#125;&#125; # 镜像 name: &#123;&#123; .Release.Name &#125;&#125; 1234567891011121314151617[root@k8s-master1 templates]# vim service.yaml apiVersion: v1kind: Servicemetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: # 匹配POD标签 app: &#123;&#123; .Values.label &#125;&#125; 12345678# 修改 values.yaml 引用变量的值[root@k8s-master1 app01]# &gt;values.yaml [root@k8s-master1 app01]# vim values.yaml replicas: 3image: nginximageTag: 1.17label: app01 调试验证 Helm也提供了--dry-run --debug调试参数，帮助你验证模板正确性。 在执行helm install时候带上这两个参数就可以把对应的values值和渲染的资源清单打印出来，而不会真正的去部署一个release。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master1 app01]# helm install web --dry-run /root/app01/NAME: webLAST DEPLOYED: Thu Dec 19 08:44:16 2019NAMESPACE: defaultSTATUS: pending-installREVISION: 1TEST SUITE: NoneHOOKS:MANIFEST:---# Source: app01/templates/service.yamlapiVersion: v1kind: Servicemetadata: labels: chart: app01 app: web name: webspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: # 匹配POD标签 app: app01---# Source: app01/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: chart: app01 app: web name: webspec: replicas: 3 selector: matchLabels: app: app01 template: metadata: labels: app: app01 spec: containers: - image: nginx:1.17 name: web 12345678910111213141516171819202122232425262728293031323334# 执行验证[root@k8s-master1 app01]# helm install web /root/app01/NAME: webLAST DEPLOYED: Thu Dec 19 08:45:22 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 app01]# kubectl get pods,svc,epNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 3 24hpod/web-79dd649678-2q7s9 1/1 Running 0 5spod/web-79dd649678-l95t2 1/1 Running 0 5spod/web-79dd649678-v9tjv 1/1 Running 0 5sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 47hservice/web ClusterIP 10.0.0.98 &lt;none&gt; 80/TCP 5sNAME ENDPOINTS AGEendpoints/kubernetes 172.17.70.251:6443 47hendpoints/web 10.244.0.43:80,10.244.1.42:80,10.244.2.52:80 5s[root@k8s-master1 app01]# curl -I 10.0.0.98HTTP/1.1 200 OKServer: nginx/1.17.6Date: Thu, 19 Dec 2019 00:47:09 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 19 Nov 2019 12:50:08 GMTConnection: keep-aliveETag: "5dd3e500-264"Accept-Ranges: bytes 12345678910111213141516171819202122232425262728293031323334353637383940414243# 升级更新[root@k8s-master1 app01]# vim values.yaml replicas: 3image: nginximageTag: 1.16label: app01[root@k8s-master1 app01]# helm upgrade web /root/app01/Release "web" has been upgraded. Happy Helming!NAME: webLAST DEPLOYED: Thu Dec 19 08:48:16 2019NAMESPACE: defaultSTATUS: deployedREVISION: 2TEST SUITE: None[root@k8s-master1 app01]# kubectl get pod,svc,epNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 3 24hpod/web-559fb69f57-4gsqj 1/1 Running 0 12spod/web-559fb69f57-9hsx2 1/1 Running 0 14spod/web-559fb69f57-ghvls 1/1 Running 0 11spod/web-79dd649678-l95t2 0/1 Terminating 0 3m7sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 47hservice/metrics-app ClusterIP 10.0.0.24 &lt;none&gt; 80/TCP 47hservice/web ClusterIP 10.0.0.98 &lt;none&gt; 80/TCP 3m7sNAME ENDPOINTS AGEendpoints/kubernetes 172.17.70.251:6443 47hendpoints/web 10.244.0.44:80,10.244.1.43:80,10.244.2.53:80 3m7s # 一组新的pod[root@k8s-master1 app01]# curl -I 10.0.0.98HTTP/1.1 200 OKServer: nginx/1.16.1 # 镜像版本变化Date: Thu, 19 Dec 2019 00:48:38 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 13 Aug 2019 10:05:00 GMTConnection: keep-aliveETag: "5d528b4c-264"Accept-Ranges: bytes 使用通用模板 创建新的POD123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master1 app01]# vim values.yaml replicas: 1image: lizhenliang/java-demoimageTag: latestlabel: java-demo[root@k8s-master1 app01]# helm install java-demo /root/app01/NAME: java-demoLAST DEPLOYED: Thu Dec 19 08:57:34 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 app01]# helm install java-demo /root/app01/NAME: java-demoLAST DEPLOYED: Thu Dec 19 08:57:34 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 app01]# kubectl get pods,svc,epNAME READY STATUS RESTARTS AGEpod/java-demo-89485b486-gfnzd 0/1 ContainerCreating 0 12spod/nfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 3 24hpod/web-559fb69f57-4gsqj 1/1 Running 0 9m28spod/web-559fb69f57-9hsx2 1/1 Running 0 9m30spod/web-559fb69f57-ghvls 1/1 Running 0 9m27sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/java-demo ClusterIP 10.0.0.118 &lt;none&gt; 80/TCP 12sservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 47hservice/metrics-app ClusterIP 10.0.0.24 &lt;none&gt; 80/TCP 47hservice/web ClusterIP 10.0.0.98 &lt;none&gt; 80/TCP 12mNAME ENDPOINTS AGEendpoints/kubernetes 172.17.70.251:6443 47hendpoints/web 10.244.0.44:80,10.244.1.43:80,10.244.2.53:80 12m Values Values对象是为Chart模板提供值，这个对象的值有4个来源： chart 包中的 values.yaml 文件 父 chart 包的 values.yaml 文件 通过 helm install 或者 helm upgrade 的 -f或者 --values参数传入的自定义的 yaml 文件 通过 --set 参数传入的值 chart 的 values.yaml 提供的值可以被用户提供的 values 文件覆盖，而该文件同样可以被 --set提供的参数所覆盖。 12# 通过set传值创建[root@k8s-master1 app01]# helm install web3 --set replicas=1 /root/app01/ 管道与函数12345678910# quote 函数增加双引号chart: &#123;&#123; quote .Chart.Name &#125;&#125; # 将后面的值作为参数传递给quote函数。[root@k8s-master1 templates]# helm install web05 --dry-run /root/app01/...metadata: labels: chart: "app01"... 1234567891011121314151617181920# default 默认值# default函数，该函数允许在模板中指定默认值，以防止该值被忽略掉。# 例如忘记定义，执行helm install 会因为缺少字段无法创建资源，这时就可以定义一个默认值。# 如果values.yaml里面test存在,则会使用模板文件中的值[root@k8s-master1 templates]# vim deployment.yaml ... app: &#123;&#123; quote .Values.label &#125;&#125; test: &#123;&#123; default "hello" .Values.test &#125;&#125;...[root@k8s-master1 templates]# helm install web05 --dry-run /root/app01/...spec: replicas: 1 selector: matchLabels: app: "java-demo" test: hello... 其他函数： 123缩进：&#123;&#123; .Values.resources | indent 12 &#125;&#125;大写：&#123;&#123; upper .Values.resources &#125;&#125;首字母大写：&#123;&#123; title .Values.resources &#125;&#125; 流程控制Helm模板语言提供以下流程控制语句： 1234# 满足更复杂的数据逻辑处理 if/else 条件块with 指定范围range 循环块 if … else12345678910111213141516171819202122232425262728[root@k8s-master1 templates]# vim deployment.yaml # 如果 test = “k8s” 那么 devops的值就是123# 判断会留下空行,需要去除spec: replicas: &#123;&#123; .Values.replicas &#125;&#125; selector: matchLabels: app: &#123;&#123; quote .Values.label &#125;&#125; test: &#123;&#123; default "hello" .Values.test &#125;&#125; &#123;&#123; if eq .Values.test "k8s"&#125;&#125; devops: 123 &#123;&#123; else &#125;&#125; devops: 456 &#123;&#123; end &#125;&#125;[root@k8s-master1 templates]# helm install web05 --dry-run /root/app01/...spec: replicas: 1 selector: matchLabels: app: "java-demo" test: k8s devops: 123... 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 去除空行 加上 - [root@k8s-master1 templates]# vim deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; quote .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicas &#125;&#125; selector: matchLabels: app: &#123;&#123; quote .Values.label &#125;&#125; test: &#123;&#123; default "hello" .Values.test &#125;&#125; &#123;&#123;- if eq .Values.test "k8s"&#125;&#125; devops: 123 &#123;&#123;- else &#125;&#125; devops: 456 &#123;&#123;- end &#125;&#125; template: metadata: labels: app: &#123;&#123; .Values.label &#125;&#125; spec: containers: - image: &#123;&#123; .Values.image &#125;&#125;:&#123;&#123; .Values.imageTag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;[root@k8s-master1 templates]# helm install web05 --dry-run /root/app01/...spec: replicas: 1 selector: matchLabels: app: "java-demo" test: k8s devops: 123 template: metadata: labels: app: java-demo spec: containers: - image: lizhenliang/java-demo:latest name: web05... 修改回实例变量12345# 使用默认模板[root@k8s-master1 app01]# cp values.yaml values.yaml_bak [root@k8s-master1 ~]# helm create /root/app02[root@k8s-master1 app01]# cp /root/app02/values.yaml .cp: overwrite ‘./values.yaml’? y 123456789[root@k8s-master1 app01]# vim values.yamlreplicaCount: 1image: repository: nginx tag: 1.16 pullPolicy: IfNotPresent ... 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 修改deployment和service文件的变量引用[root@k8s-master1 app01]# vim templates/deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; selector: matchLabels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; template: metadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; spec: containers: - image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;[root@k8s-master1 app01]# vim templates/service.yaml apiVersion: v1kind: Servicemetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: # 匹配POD标签 app: &#123;&#123; .Release.Name &#125;&#125; chart: &#123;&#123; .Chart.Name &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 测试 [root@k8s-master1 app01]# helm install web05 --dry-run /root/app01/NAME: web05LAST DEPLOYED: Thu Dec 19 10:44:59 2019NAMESPACE: defaultSTATUS: pending-installREVISION: 1TEST SUITE: NoneHOOKS:MANIFEST:---# Source: app01/templates/service.yamlapiVersion: v1kind: Servicemetadata: labels: chart: app01 app: web05 name: web05spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: # 匹配POD标签 app: web05 chart: app01---# Source: app01/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: chart: app01 app: web05 name: web05spec: replicas: 1 selector: matchLabels: chart: app01 app: web05 template: metadata: labels: chart: app01 app: web05 spec: containers: - image: nginx:1.16 name: web05 资源限制判断12345678# 修改 values 增加资源限制resources: limits: cpu: 100m memory: 128Mi # requests: # cpu: 100m # memory: 128Mi 1234567891011121314151617181920212223242526272829303132333435# 修改 deployment # 加上条件判断,如果values没有资源的值 就使用&#123;&#125;空 如果有就使用 [root@k8s-master1 app01]# vim templates/deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; selector: matchLabels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; template: metadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; spec: containers: - image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125; &#123;&#123;- if .Values.resources &#125;&#125; resources: limits: cpu: &#123;&#123; .Values.resources.limits.cpu &#125;&#125; memory: &#123;&#123; .Values.resources.limits.memory &#125;&#125; &#123;&#123;- else &#125;&#125; resources: &#123;&#125; &#123;&#123;- end &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master1 app01]# helm install web05 --dry-run /root/app01/NAME: web05LAST DEPLOYED: Thu Dec 19 11:07:36 2019NAMESPACE: defaultSTATUS: pending-installREVISION: 1TEST SUITE: NoneHOOKS:MANIFEST:---# Source: app01/templates/service.yamlapiVersion: v1kind: Servicemetadata: labels: chart: app01 app: web05 name: web05spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: # 匹配POD标签 app: web05 chart: app01---# Source: app01/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: chart: app01 app: web05 name: web05spec: replicas: 1 selector: matchLabels: chart: app01 app: web05 template: metadata: labels: chart: app01 app: web05 spec: containers: - image: nginx:1.16 name: web05 resources: limits: cpu: 100m memory: 128Mi 12345678910[root@k8s-master1 app01]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb05-6bcbfb5fdc-7fwn2 1/1 Running 0 81s 10.244.0.45 k8s-master1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 app01]# kubectl describe node k8s-master1... Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default web05-6bcbfb5fdc-7fwn2 100m (5%) 100m (5%) 128Mi (7%) 128Mi (7%) 42s... 设置开关123456789# 设置开关 判断 enable: false | trueresources: enable: false limits: cpu: 100m memory: 128Mi # requests: # cpu: 100m # memory: 128Mi 1234567891011121314[root@k8s-master1 app01]# vim templates/deployment.yaml spec: containers: - image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125; &#123;&#123;- if .Values.resources.enable &#125;&#125; resources: limits: cpu: &#123;&#123; .Values.resources.limits.cpu &#125;&#125; memory: &#123;&#123; .Values.resources.limits.memory &#125;&#125; &#123;&#123;- else &#125;&#125; resources: &#123;&#125; &#123;&#123;- end &#125;&#125; ingress 开关1234567891011121314151617181920212223242526272829303132333435363738# 如果Values存在ingress 就进行资源配置 否则不进行[root@k8s-master1 templates]# vim ingress.yaml&#123;&#123;- if .Values.ingress.enabled &#125;&#125;# 如果Values存在 ingress 就要配置 资源文件apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: rules: - http: paths: - path: /testpath backend: serviceName: test servicePort: 80&#123;&#123; end &#125;&#125;[root@k8s-master1 app01]# vim values.yaml...ingress: enabled: false annotations: &#123;&#125; # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: "true" hosts: - host: chart-example.local paths: [] tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local... 123# 启用 如果是false则不会创建ingress: enabled: true 12345678910111213141516[root@k8s-master1 app01]# helm install web /root/app01/NAME: webLAST DEPLOYED: Thu Dec 19 11:39:30 2019NAMESPACE: defaultSTATUS: deployedREVISION: 1TEST SUITE: None[root@k8s-master1 app01]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-5dd6f66f47-9gb4k 1/1 Running 3 27hweb-944f58c9c-n2nw9 1/1 Running 0 14s[root@k8s-master1 app01]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEtest-ingress * 80 43s with 控制变量作用域123456[root@k8s-master1 app01]# vim values.yamlnodeSelector: # node 标签 team: a gpu: ok 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 正常写法[root@k8s-master1 ~]# vim /root/app01/templates/deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; selector: matchLabels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; template: metadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; spec: containers: &#123;&#123; if .Values.nodeSelector &#125;&#125; nodeSelector: team: &#123;&#123; .Values.nodeSelector.team &#125;&#125; gpu: &#123;&#123; .Values.nodeSelector.gpu &#125;&#125; &#123;&#123; end &#125;&#125; - image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125; &#123;&#123;- if .Values.resources.enable &#125;&#125; resources: limits: cpu: &#123;&#123; .Values.resources.limits.cpu &#125;&#125; memory: &#123;&#123; .Values.resources.limits.memory &#125;&#125; &#123;&#123;- else &#125;&#125; resources: &#123;&#125; &#123;&#123;- end &#125;&#125;[root@k8s-master1 app01]# helm install web05 --dry-run /root/app01/...# Source: app01/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: chart: app01 app: web05 name: web05spec: replicas: 1 selector: matchLabels: chart: app01 app: web05 template: metadata: labels: chart: app01 app: web05 spec: nodeSelector: team: a gpu: true containers: - image: nginx:1.16 name: web05 resources: limits: cpu: 100m memory: 128Mi... 123456789101112131415# 使用 with # 值会被取出 spec: &#123;&#123;- with .Values.nodeSelector &#125;&#125; nodeSelector: team: &#123;&#123; .team &#125;&#125; gpu: &#123;&#123; .gpu &#125;&#125; &#123;&#123;- end &#125;&#125;... spec: nodeSelector: team: a gpu: ok... 123456789# with + toYaml + 缩进# 先看有多少个 空格 # | nindent 8 缩进8个空格并且换行 不加n不换行 spec: &#123;&#123;- with .Values.nodeSelector &#125;&#125; nodeSelector: &#123;&#123;- toYaml . | nindent 8 &#125;&#125; &#123;&#123;- end &#125;&#125; range 循环 在 Helm 模板语言中，使用 range关键字来进行循环操作。 循环内部我们使用的是一个 .，这是因为当前的作用域就在当前循环内，这个 .引用的当前读取的元素。 1234test: - 1 - 2 - 3 12345678910# Source: app01/templates/configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: web05data: test: | 1 2 3 变量 在with中使用内置变量 直接使用$引用 1234567# 在with里面引用内置变量会报错 找不到 spec: &#123;&#123;- with .Values.nodeSelector &#125;&#125; nodeSelector: app: &#123;&#123; .Release.Name &#125;&#125; &#123;&#123;- toYaml . | nindent 8 &#125;&#125; &#123;&#123;- end &#125;&#125; 1234567891011121314# 使用$引用内置变量 spec: &#123;&#123;- with .Values.nodeSelector &#125;&#125; nodeSelector: app: &#123;&#123; $.Release.Name &#125;&#125; &#123;&#123;- toYaml . | nindent 8 &#125;&#125; &#123;&#123;- end &#125;&#125;... nodeSelector: app: web05 gpu: ok team: a... 12345678# 在with上面定义变量 spec: &#123;&#123;- $releaseName := .Release.Name -&#125;&#125; &#123;&#123;- with .Values.nodeSelector &#125;&#125; nodeSelector: app: &#123;&#123; $releaseName &#125;&#125; &#123;&#123;- toYaml . | nindent 8 &#125;&#125; &#123;&#123;- end &#125;&#125; 12345678# range 使用变量 # 正常使用 containers: - image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125; name: &#123;&#123; .Release.Name &#125;&#125; env: - name: TEST value: 123 12345678910111213141516# 引用变量 错误引用 &#123;&#123;- range .Values.env &#125;&#125; env: - name: &#123;&#123; . &#125;&#125; value: &#123;&#123; . &#125;&#125; &#123;&#123;- end &#125;&#125;... env: - name: -Xmx1g value: -Xmx1g env: - name: JAVA value: JAVA... 1234567891011121314# 正确引用 env: &#123;&#123;- range $key,$val := .Values.env &#125;&#125; - name: &#123;&#123; $key &#125;&#125; value: &#123;&#123; $val &#125;&#125; &#123;&#123;- end &#125;&#125;... env: - name: JAVA_OPTS value: -Xmx1g - name: NAME value: JAVA... 命名模板 命名模板：使用define定义，template引入，在templates目录中默认下划线_开头的文件为公共模板(_helpers.tpl) 重复使用的代码块 放到命名模板 123456# 定义[root@k8s-master1 templates]# vim _helpers.tpl&#123;&#123;- define "name" -&#125;&#125;&#123;&#123;- .Chart.Name -&#125;&#125;-&#123;&#123; .Release.Name &#125;&#125;&#123;&#123;- end -&#125;&#125; 12345678# 引用 apiVersion: apps/v1kind: Deploymentmetadata: labels: chart: &#123;&#123; .Chart.Name &#125;&#125; app: &#123;&#123; .Release.Name &#125;&#125; name: &#123;&#123; template "name" . &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637# include 支持函数处理 # 模板里面定投写 在引用的时候缩进空格[root@k8s-master1 templates]# vim _helpers.tpl &#123;&#123;- define "name" -&#125;&#125;&#123;&#123;- .Chart.Name -&#125;&#125;-&#123;&#123; .Release.Name &#125;&#125;&#123;&#123;- end -&#125;&#125;&#123;&#123;- define "labels" -&#125;&#125;app: &#123;&#123; template "name" . &#125;&#125;chart: "&#123;&#123; .Chart.Name &#125;&#125;-&#123;&#123; .Chart.Version &#125;&#125;"release: "&#123;&#123; .Release.Name &#125;&#125;"&#123;&#123;- end -&#125;&#125;# 引入apiVersion: apps/v1kind: Deploymentmetadata: labels: &#123;&#123;- include "labels" . | nindent 4 &#125;&#125; name: &#123;&#123; template "name" . &#125;&#125;...# 结果...# Source: app01/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: app: app01-web05 chart: "app01-0.1.0" release: "web05" name: app01-web05... 开发自己的 Chart Java应用实例 先创建模板 1helm create demo 修改Chart.yaml，Values.yaml，添加常用的变量 在templates目录下创建部署镜像所需要的yaml文件，并变量引用yaml里经常变动的字段 创建模板目录12345678910[root@k8s-master1 opt]# cd /opt/[root@k8s-master1 opt]# helm create demoCreating demo[root@k8s-master1 demo]# cd /opt/demo/[root@k8s-master1 demo]# mkdir bak[root@k8s-master1 demo]# cp Chart.yaml values.yaml bak/# 清理文件注释[root@k8s-master1 demo]# egrep -v '#|^$' /opt/demo/bak/Chart.yaml &gt; Chart.yaml 修改 Chart.yaml12345678910[root@k8s-master1 demo]# vim Chart.yaml apiVersion: v2name: demodescription: My java demotype: application# chart 版本version: 0.1.0# app 版本appVersion: 1.16.0 修改 Values.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243# 只保留用到的变量 [root@k8s-master1 demo]# vim values.yaml # Default values for demo.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image: repository: nginx pullPolicy: IfNotPresentimagePullSecrets: []service: type: ClusterIP port: 80ingress: enabled: false annotations: &#123;&#125; # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: "true" hosts: - host: chart-example.local paths: [] tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.localresources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128MinodeSelector: &#123;&#125;tolerations: [] 准备应用 yaml 文件1234[root@k8s-master1 templates]# rm -rf tests/[root@k8s-master1 templates]# rm -rf serviceaccount.yaml [root@k8s-master1 templates]# lsdeployment.yaml _helpers.tpl ingress.yaml NOTES.txt service.yaml 123456789# 三个公共模板# _helpers.tpl 公共模板 service deploy ingress 都会用到 name: &#123;&#123; include "demo.fullname" . &#125;&#125;# deploy 标签&#123;&#123;- include "demo.labels" . | nindent 4 &#125;&#125;# 标签选择器 service.selector 一致&#123;&#123;- include "demo.selectorLabels" . | nindent 8 &#125;&#125; 123456# 打包[root@k8s-master1 opt]# helm package demo/# 再启动一个[root@k8s-master1 opt]# cp demo/values.yaml ./[root@k8s-master1 opt]# helm install web2 -f values.yaml demo-0.1.0.tgz 使用 Harbor 作为 Chart 仓库]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 k8s 弹性伸缩]]></title>
    <url>%2F2019%2F12%2F06%2Fk8s-base06%2F</url>
    <content type="text"><![CDATA[传统弹性伸缩的困境123451. 简单理解,资源不够了加机器,资源多了减机器2. 从传统意义上，弹性伸缩主要解决的问题是 容量规划与实际负载的矛盾。3. 资源池能否快速扩大,快速回收4. 快速的流量突发难以提前预知5. 已知的可以提前准备,如遇到活动业务 从传统意义上，弹性伸缩:主要解决的问题是 容量规划 与 实际负载 的矛盾。 蓝色水位线表示集群资源容量随着负载的增加不断扩容，红色曲线表示集群资源实际负载变化。 弹性伸缩就是要解决当实际负载增大，而集群资源容量没来得及反应的问题。 Kubernetes 中弹性伸缩存在的问题 常规的做法是给集群资源预留保障集群可用，通常20%左右。 这种方式看似没什么问题，但放到Kubernetes中，就会发现如下2个问题。 机器规格不统一,造成机器利用率百分比碎片化 在一个Kubernetes集群中，通常不只包含一种规格的机器， 假设集群中存在4C8G与16C32G两种规格的机器，对于10%的资源预留，这两种规格代表的意义是完全不同的。 特别是在缩容的场景下，为了保证缩容后集群稳定性，我们一般会一个节点一个节点从集群中摘除， 那么如何判断节点是否可以摘除其利用率百分比就是重要的指标。 此时如果大规格机器有较低的利用率被判断缩容，那么很有可能会造成节点缩容后，容器重新调度后的争抢。 如果优先缩容小规格机器，则可能造成缩容后资源的大量冗余。 机器利用率不能单纯依靠宿主机计算 在大部分生产环境中，资源利用率都不会保持一个高的水位，但从调度来讲，调度应该保持一个比较高的水位，这样才能保障集群稳定性，又不过多浪费资源。 弹性伸缩概念的延伸 不是所有的业务都存在峰值流量，越来越细分的业务形态带来更多成本节省和可用性之间的跳转。 不同类型的负载对于弹性伸缩的要求有所不同，在线负载对弹出时间敏感，离线任务对价格敏感，定时任务对调度敏感。 1231. 在线负载型：微服务、网站、API2. 离线任务型：离线计算、机器学习3. 定时任务型：定时批量计算 kubernetes 弹性伸缩布局 在 Kubernetes 的生态中，在多个维度、多个层次提供了不同的组件来满足不同的伸缩场景。 有三种弹性伸缩： 1234567- CA（Cluster Autoscaler）：Node级别自动扩/缩容 适用于,无状态应用 cluster-autoscaler组件 适用于,无状态应用 - HPA（Horizontal Pod Autoscaler）：Pod个数自动扩/缩容- VPA（Vertical Pod Autoscaler）：Pod配置自动扩/缩容，主要是CPU、内存 addon-resizer组件 适用于,有状态应用 如果在云上建议 HPA 结合 cluster-autoscaler 的方式进行集群的弹性伸缩管理。 Node 自动扩容/缩容Cluster AutoScaler 扩容：Cluster AutoScaler 定期检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。 缩容：Cluster AutoScaler 也会定期监测 Node 的资源使用情况，当一个 Node 长时间资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除。此时，原来的 Pod 会自动调度到其他 Node 上面。 12345支持的云提供商：# aliyun: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md# AWS: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md# Azure: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md Ansible 扩容 Node 自动化流程： 1234561. 触发新增Node2. 调用Ansible脚本部署组件3. 检查服务是否可用4. 调用API将新Node加入集群或者启用Node自动加入5. 观察新Node状态6. 完成Node扩容，接收新Pod 扩容1234567# 模拟场景：创建超出当前资源的 pod，使当前k8s环境又需要扩容node的需求# 1. 查看当前各个node节点配置# 服务器配置是 2c2G[root@k8s-master1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0 1234567891011121314151617181920# 2. 创建超出当前资源的 pod [root@k8s-master1 ansible-k8s-deploy]# kubectl run web --image=nginx:1.16 --replicas=4 --requests='cpu=1,memory=256Mi'kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/web created[root@k8s-master1 ansible-k8s-deploy]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-584dbdb99d-2cv2z 0/1 Pending 0 8s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;web-584dbdb99d-crfdz 1/1 Running 0 8s 10.244.1.13 k8s-node1 &lt;none&gt; &lt;none&gt;web-584dbdb99d-hclmf 1/1 Running 0 8s 10.244.0.11 k8s-node2 &lt;none&gt; &lt;none&gt;web-584dbdb99d-mtthk 0/1 Pending 0 8s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;[root@k8s-master1 ansible-k8s-deploy]# kubectl describe pod/web-584dbdb99d-2cv2z# scheduler 调度器提示 2个node 都不满足 cpu不充足Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/2 nodes are available: 2 Insufficient cpu. 1234# 3. 准备 k8s-node3 节点[root@k8s-master1 ansible-k8s-deploy]# cat hosts [newnode]172.17.70.252 node_name=k8s-node3 123# 4. 部署k8s-node3节点[root@k8s-master1 ansible-k8s-deploy]# ansible-playbook -i hosts add-node.yaml -uroot -k[root@k8s-master2 ~]# ps -ef|grep kube 1234567891011121314151617181920212223242526272829303132333435363738# 5. 手动加入集群 # 如果是物理服务器,别忘记时间需要同步 ntpdate time.windows.com [root@k8s-master1 ansible-k8s-deploy]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-y6GEkP8FXoD3VJnj-BkmrKBO6vSPA32Coi3Wf2Vu9og 8m6s kubelet-bootstrap Pending[root@k8s-master1 ansible-k8s-deploy]# kubectl certificate approve node-csr-y6GEkP8FXoD3VJnj-BkmrKBO6vSPA32Coi3Wf2Vu9og[root@k8s-master1 ansible-k8s-deploy]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-y6GEkP8FXoD3VJnj-BkmrKBO6vSPA32Coi3Wf2Vu9og 9m44s kubelet-bootstrap Approved,Issued# daemonset 自动部署了 flannel[root@k8s-master1 ansible-k8s-deploy]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0k8s-node3 Ready &lt;none&gt; 42s v1.16.0[root@k8s-master1 ansible-k8s-deploy]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-584dbdb99d-2cv2z 1/1 Running 0 7m40sweb-584dbdb99d-crfdz 1/1 Running 0 7m40sweb-584dbdb99d-hclmf 1/1 Running 0 7m40sweb-584dbdb99d-mtthk 0/1 Pending 0 7m40s# 资源多起来一台 还是资源不够 所有还需要扩容~ 但是扩容的操作已经成功[root@k8s-master1 ansible-k8s-deploy]# kubectl describe node k8s-node3# 修改配置 再启动[root@k8s-master1 ansible-k8s-deploy]# kubectl delete deployment/web[root@k8s-master1 ansible-k8s-deploy]# kubectl run web --image=nginx:1.16 --replicas=4 --requests='cpu=500m,memory=256Mi'[root@k8s-master1 ansible-k8s-deploy]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-868b6f9d7f-5j4d9 1/1 Running 0 3sweb-868b6f9d7f-98wc4 1/1 Running 0 3sweb-868b6f9d7f-spmsd 1/1 Running 0 3sweb-868b6f9d7f-wlnw7 1/1 Running 0 3s 缩容1234# 考虑的较多# 1. 集群中哪个节点可以删除 资源利用率最低 # 2. 跑的POD 最少 # 3. 人工干预 12345678# 平滑移除了一个 k8s 节点,正确流程如下:# 1. 获取节点列表# 可以下线pod最少的node,这次测试 就下线node3 [root@k8s-master1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0k8s-node3 Ready &lt;none&gt; 8m45s v1.16.0 123456789# 2. 设置不可调度# 防止有新的pod调度过来[root@k8s-master1 ~]# kubectl cordon k8s-node3node/k8s-node3 cordoned[root@k8s-master1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0k8s-node3 Ready,SchedulingDisabled &lt;none&gt; 10m v1.16.0 123456789101112131415# 3. 驱逐这个节点上的已有的pod 到其他节点# 将节点设置为 维护状态并驱逐pod,但是忽略驱逐daemonsets (--ignore-daemonsets)# 在驱逐之前也要考虑计算其他节点是否有足够的资源,可以接纳node3上的pod[root@k8s-master1 ~]# kubectl drain k8s-node3 --ignore-daemonsets[root@k8s-master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-868b6f9d7f-5j4d9 1/1 Running 0 9m33s 10.244.1.14 k8s-node1 &lt;none&gt; &lt;none&gt;web-868b6f9d7f-ctxk5 1/1 Running 0 48s 10.244.1.15 k8s-node1 &lt;none&gt; &lt;none&gt;web-868b6f9d7f-wlnw7 1/1 Running 0 9m33s 10.244.0.12 k8s-node2 &lt;none&gt; &lt;none&gt;web-868b6f9d7f-wngdw 1/1 Running 0 48s 10.244.0.13 k8s-node2 &lt;none&gt; &lt;none&gt;# 驱逐流程:1. 删除下线节点上的pod2. pod 是deployment 维护副本数,是由控制器来维护操作并拉起指定的副本数，该操作会走调度 12345678# 4. 移除节点# 该节点上已经没有任何资源了，可以直接移除节点[root@k8s-master1 ~]# kubectl delete node k8s-node3node "k8s-node3" deleted[root@k8s-master1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0 12345# 总结:1. 如果直接关闭node3服务器,模拟自动出现故障,k8s大约需要5分钟才能调度,而驱逐是即时操作,不如驱逐效率2. 重启node3的kubelet 可以重新发送加入请求,并加入到集群中3. 如何自动通过csr 怎么配置4. 当前是自动化 和 人工干预 对node的扩容和缩容 Pod 自动扩容/缩容（HPA） Horizontal Pod Autoscaler（HPA，Pod水平自动伸缩），根据资源利用率或者自定义指标自动调整 replication controller, deployment 或 replica set， 实现部署的自动扩展和缩减，让部署的规模接近于实际服务的负载。HPA不适于无法缩放的对象，例如DaemonSet。 11. 针对pod的利用率做计算 然后调整副本数量 HPA 基本原理 Kubernetes 中的组件 Metrics Server 持续采集所有 Pod 副本的指标数据。 (heapster 在k8s 1.13 版本后 被弃用了) HPA 控制器通过 Metrics Server 的 API（Heapster 的 API 或聚合 API）获取这些数据，基于用户定义的扩缩容规则进行计算，得到目标 Pod 副本数量。 当目标 Pod 副本数量与当前副本数量不同时，HPA 控制器就向 Pod 的副本控制器（Deployment、RC 或 ReplicaSet）发起 scale 操作，调整 Pod 的副本数量，完成扩缩容操作。 123451. HPA需要创建规则，里面定义好缩容和扩容范围,设置好对象,设置阈值2. HPA会去从Metrics Server中 获取指标,判断是否到达HPA的阈值,如果是,会去执行deployment scale去扩容3. 如果长期在低使用率会帮我们缩容POD副本4. Metrics Server 从cadvisor中获取5. cadvisor 提供哪些指标? CPU、内存 的使用率，cadvisor 采集容器的资源利用率 扩容冷却 和 缩容冷却 在弹性伸缩中，冷却周期是不能逃避的一个话题， 由于评估的度量标准是动态特性，副本的数量可能会不断波动。有时被称为颠簸，所以在每次做出扩容缩容后，冷却时间是多少。 在 HPA 中，默认的扩容冷却周期是 3 分钟，缩容冷却周期是 5 分钟。 可以通过调整 kube-controller-manager 组件启动参数设置冷却时间： 12--horizontal-pod-autoscaler-downscale-delay ： 扩容冷却--horizontal-pod-autoscaler-upscale-delay ： 缩容冷却 12345678910111213141516pod1 pod2 pod3 # cpu使用率 50%HPA # cpu阈值 60% 周期对比pod的cpu使用率 # HPA 指标: 1. 最大创建POD数量 最小POD数量2. 阈值3. 对象 操作哪一组POD# 转化扩容: cpu利用率 70%了 扩容 3-10个副本缩容: 马上利用率下降了 到20%了 由10个 降低到 5个 这个转化时间频率不能太高,如果太高或者利用率间隙触发,业务不稳定，HPA拥有冷却机制1. 第一次扩容之后,第二次扩容需要经过扩容冷却时间, 默认三分钟2. 第一次缩容之后,第二次缩容需要经过缩容冷却时间, 默认五分钟3. 这些都在 kube-controller-manager 组件参数中设置4. 不断的扩容、缩容这种现象，成为颠簸 HPA 的演进历程 目前 HPA 已经支持了 autoscaling/v1、autoscaling/v2beta1 和 autoscaling/v2beta2 三个大版本。 目前大多数人比较熟悉是 autoscaling/v1 ，这个版本只支持 CPU 一个指标的弹性伸缩。 而 autoscaling/v2beta1 增加了支持自定义指标， 如第三方支持QPS autoscaling/v2beta2 又额外增加了外部指标支持。 而产生这些变化不得不提的是Kubernetes社区对监控与监控指标的认识与转变。从早期Heapster到Metrics Server再到将指标边界进行划分，一直在丰富监控生态。 121. autoscaling/v1版本 只暴露了cpu 为什么没有暴露内存 ? 因为内存不是很好的提现弹性伸缩的指标,都是应用在调整2. autoscaling/v2beta2 增加更多的指标 基于 CPU 指标缩放Kubernetes API Aggregation 在 Kubernetes 1.7 版本引入了聚合层，允许第三方应用程序通过将自己注册到kube-apiserver上，仍然通过 API Server 的 HTTP URL 对新的 API 进行访问和操作。 为了实现这个机制，Kubernetes 在 kube-apiserver 服务中引入了一个 API 聚合层（API Aggregation Layer），用于将扩展 API 的访问请求转发到用户服务的功能。 当你访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端 。 通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。 如果你使用kubeadm部署的，默认已开启。 如果你使用二进制方式部署的话，需要在kube-APIServer中添加启动参数，增加以下配置： 123456789101112131415# vi /opt/kubernetes/cfg/kube-apiserver.conf...# 指定ca根证书和客户端证书 访问聚合层的认证--requestheader-client-ca-file=/opt/kubernetes/ssl/ca.pem \--proxy-client-cert-file=/opt/kubernetes/ssl/server.pem \--proxy-client-key-file=/opt/kubernetes/ssl/server-key.pem \# server.pem 证书cn名称 = kubernetes --requestheader-allowed-names=kubernetes \# 请求头信息 --requestheader-extra-headers-prefix=X-Remote-Extra- \--requestheader-group-headers=X-Remote-Group \--requestheader-username-headers=X-Remote-User \# 启用聚合层路由 --enable-aggregator-routing=true \... 在设置完成 重启 kube-apiserver 服务，就启用 API 聚合功能了。 开启 kube-apiserver 聚合层123[root@k8s-master1 ~]# vim /opt/kubernetes/cfg/kube-apiserver.conf [root@k8s-master1 ssl]# systemctl restart kube-apiserver[root@k8s-master1 ssl]# systemctl status kube-apiserver autoscaling/v1（CPU指标实践）部署 Metrics Server Metrics Server 是一个集群范围的资源使用情况的数据聚合器。作为一个应用部署在集群中。 Metrics Server 从每个节点上Kubelet(内置的cadvisor)公开的摘要API收集指标。 Metrics Server 通过Kubernetes聚合器注册在Master APIServer中。 1234567 HPA 获取整体POD的CPU资源利用率 Metrics 汇总 pod1(cadvisor) pod2(cadvisor) pod3(cadvisor) 123456789101112131415161718192021222324252627[root@k8s-master1 ~]# yum install lrzsz unzip[root@k8s-master1 ~]# unzip metrics-server.zip [root@k8s-master1 ~]# cd metrics-server# 修改参数:[root@k8s-master1 metrics-server]# vim metrics-server-deployment.yaml... containers: - name: metrics-server image: lizhenliang/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-insecure-tls # 忽略证书 - --kubelet-preferred-address-types=InternalIP # 找到nodeIP，默认使用注册节点名称,修改成IP...# 部署[root@k8s-master1 metrics-server]# kubectl apply -f . [root@k8s-master1 metrics-server]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-mh5pk 1/1 Running 2 22hkube-flannel-ds-amd64-5bfgl 1/1 Running 2 22hkube-flannel-ds-amd64-pjjgj 1/1 Running 2 22hmetrics-server-7dbbcf4c7-vzdn2 1/1 Running 0 28s# 检查是否正常工作[root@k8s-master1 metrics-server]# kubectl logs metrics-server-7dbbcf4c7-vzdn2 -n kube-system[root@k8s-master1 metrics-server]# kubectl get apiservice 12345678910111213# 获取资源利用率 metrics-server 工作正常 采集到了数据[root@localhost metrics-server]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master1 119m 5% 1075Mi 57% k8s-node1 59m 2% 630Mi 33% k8s-node2 61m 3% 672Mi 35% [root@localhost metrics-server]# kubectl top podsNAME CPU(cores) MEMORY(bytes) web-868b6f9d7f-d4rwc 0m 2Mi web-868b6f9d7f-dj9nb 0m 3Mi web-868b6f9d7f-n6gv8 0m 3Mi web-868b6f9d7f-s6hk4 0m 2Mi 12345678910111213141516171819202122232425# 可通过Metrics API在Kubernetes中获得资源使用率指标，例如容器CPU和内存使用率。这些度量标准既可以由用户直接访问# 例如，通过使用`kubectl top`命令，也可以由集群中的控制器（例如，Horizontal Pod Autoscaler）用于进行决策。# 测试：hpa获取资源利用率是使用api接口 返回json格式[root@localhost metrics-server]# kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes&#123;"kind":"NodeMetricsList","apiVersion":"metrics.k8s.io/v1beta1","metadata":&#123;"selfLink":"/apis/metrics.k8s.io/v1beta1/nodes"&#125;,"items":[&#123;"metadata":&#123;"name":"k8s-master1","selfLink":"/apis/metrics.k8s.io/v1beta1/nodes/k8s-master1","creationTimestamp":"2019-12-11T11:48:23Z"&#125;,"timestamp":"2019-12-11T11:47:31Z","window":"30s","usage":&#123;"cpu":"141907959n","memory":"1121116Ki"&#125;&#125;,&#123;"metadata":&#123;"name":"k8s-node1","selfLink":"/apis/metrics.k8s.io/v1beta1/nodes/k8s-node1","creationTimestamp":"2019-12-11T11:48:23Z"&#125;,"timestamp":"2019-12-11T11:47:40Z","window":"30s","usage":&#123;"cpu":"88731198n","memory":"646736Ki"&#125;&#125;,&#123;"metadata":&#123;"name":"k8s-node2","selfLink":"/apis/metrics.k8s.io/v1beta1/nodes/k8s-node2","creationTimestamp":"2019-12-11T11:48:23Z"&#125;,"timestamp":"2019-12-11T11:47:32Z","window":"30s","usage":&#123;"cpu":"55617927n","memory":"693224Ki"&#125;&#125;]&#125;[root@localhost metrics-server]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master1 94m 4% 1074Mi 57% k8s-node1 89m 4% 634Mi 33% k8s-node2 173m 8% 662Mi 35% 启动三个web副本12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-master1 hpa]# mkdir -p /root/hpa[root@k8s-master1 hpa]# vim app.yaml apiVersion: v1kind: Servicemetadata: name: webspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: run: web---apiVersion: apps/v1kind: Deploymentmetadata: labels: run: web name: webspec: replicas: 3 selector: matchLabels: run: web template: metadata: labels: run: web spec: containers: - image: nginx:1.16 name: web ports: - containerPort: 80 resources: requests: cpu: 100m memory: 100Mi 1234567891011121314151617181920212223[root@k8s-master1 hpa]# kubectl delete deploy web [root@k8s-master1 hpa]# kubectl apply -f app.yaml service/web unchangeddeployment.apps/web created[root@k8s-master1 hpa]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-6f4b67f8cc-kz295 1/1 Running 0 5sweb-6f4b67f8cc-nmfpv 1/1 Running 0 5sweb-6f4b67f8cc-s9mvc 1/1 Running 0 5s# 测试访问 [root@k8s-master1 hpa]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/web-6f4b67f8cc-kz295 1/1 Running 0 3m47spod/web-6f4b67f8cc-nmfpv 1/1 Running 0 3m47spod/web-6f4b67f8cc-s9mvc 1/1 Running 0 3m47sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 39mservice/web ClusterIP 10.0.0.129 &lt;none&gt; 80/TCP 10m[root@k8s-master1 hpa]# curl 10.0.0.129 创建 HPA 策略12345678910111213141516171819202122# 自动创建[root@k8s-master1 hpa]# kubectl autoscale --help[root@k8s-master1 hpa]# kubectl autoscale deployment web --min=2 --max=10 -o yaml --dry-run &gt; hpa-v1.yaml# 修改后[root@k8s-master1 hpa]# vim hpa-v1.yaml apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: webspec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web targetCPUUtilizationPercentage: 60# scaleTargetRef：表示当前要伸缩对象是谁 # targetCPUUtilizationPercentage：当整体的资源利用率超过60%的时候，会进行扩容。低于60%且在2个副本上会被缩容 12345678# 创建并查看 需要等待一小会 hpa会去调用api汇总web的使用率[root@k8s-master1 hpa]# kubectl apply -f hpa-v1.yaml horizontalpodautoscaler.autoscaling/web created# 现在web中的nginx 还没有什么使用率[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 0%/60% 2 10 3 17s 1234567# 长时间没有利用率 帮我们缩容了[root@k8s-master1 hpa]# kubectl describe hpa webEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 9m19s horizontal-pod-autoscaler New size: 2; reason: All metrics below target 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 开启压测# 安装ab工具# ab -n 100000 -c 100 http://10.0.0.129/status.php# -n 请求数# -c 并发数[root@k8s-master1 hpa]# yum install httpd-tools[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 0%/60% 2 10 2 18m# 没有坐limit限制下 进行ab测试[root@k8s-master1 hpa]# ab -n 1000000 -c 100 http://10.0.0.129/index.html[root@k8s-master1 ~]# kubectl top podsNAME CPU(cores) MEMORY(bytes) web-6f4b67f8cc-nmfpv 701m 3Mi web-6f4b67f8cc-s9mvc 662m 3Mi # 负载上来了 并且副本个数动态增加了 实验成功后 可以关闭压测[root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 681%/60% 2 10 8 23m[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-6f4b67f8cc-9h2c6 1/1 Running 0 39sweb-6f4b67f8cc-czpp4 1/1 Running 0 39sweb-6f4b67f8cc-frd8k 1/1 Running 0 55sweb-6f4b67f8cc-ls6z9 1/1 Running 0 24sweb-6f4b67f8cc-nfnhs 1/1 Running 0 39sweb-6f4b67f8cc-nmfpv 1/1 Running 0 48mweb-6f4b67f8cc-s9mvc 1/1 Running 0 48mweb-6f4b67f8cc-sxtbq 1/1 Running 0 24sweb-6f4b67f8cc-tkmzr 1/1 Running 0 39sweb-6f4b67f8cc-vnbcj 1/1 Running 0 54s# 等待几分钟 负载下来后 查看缩容[root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEweb Deployment/web 0%/60% 2 10 10 23m[root@k8s-master1 ~]# kubectl describe hpa webEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 18m horizontal-pod-autoscaler New size: 2; reason: All metrics below target Normal SuccessfulRescale 102s horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 86s horizontal-pod-autoscaler New size: 8; reason: cpu resource utilization (percentage of request) above target Normal SuccessfulRescale 71s horizontal-pod-autoscaler New size: 10; reason: cpu resource utilization (percentage of request) above target 工作流程：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor) autoscaling/v2beta2（多指标） 为满足更多的需求，HPA 还有 autoscaling/v2beta1 和 autoscaling/v2beta2 两个版本。 这两个版本的区别是 autoscaling/v1beta1支持了 Resource Metrics（CPU）和 Custom Metrics（应用程序指标）， 而在 autoscaling/v2beta2的版本中额外增加了 External Metrics 的支持。 1# v2版本支持 自定义指标 ,可以对接第三方监控的指标 1234567891011121314151617181920212223242526# 自动生成[root@k8s-master1 hpa]# kubectl get hpa.v2beta2.autoscaling -o yaml &gt; hpa-v2.yaml[root@k8s-master1 hpa]# vim hpa-v2.yaml apiVersion: v1items:apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: web namespace: defaultspec: maxReplicas: 10 minReplicas: 2 metrics: - resource: name: cpu target: averageUtilization: 60 type: Utilization type: Resource scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web 123456789metrics中的type字段有四种类型的值：Object、Pods、Resource、External。 - Resource：指的是当前伸缩对象下的pod的cpu和memory指标，只支持Utilization和AverageValue类型的目标值。- Object：指的是指定k8s内部对象的指标，数据需要第三方adapter提供，只支持Value和AverageValue类型的目标值。- Pods：指的是伸缩对象Pods的指标，数据需要第三方的adapter提供，只允许AverageValue类型的目标值。- External：指的是k8s外部的指标，数据同样需要第三方的adapter提供，只支持Value和AverageValue类型的目标值。 工作流程：hpa -&gt; apiserver -&gt; kube aggregation -&gt; metrics-server -&gt; kubelet(cadvisor)工作流程：hpa -&gt; apiserver -&gt; kube aggregation -&gt; prometheus-adapter -&gt; prometheus -&gt; pods 1prometheus-adapter 完成注册 和 数据格式的转换 基于 Prometheus 自定义指标缩放 资源指标只包含CPU、内存，一般来说也够了。但如果想根据自定义指标:如请求qps/5xx错误数来实现HPA，就需要使用自定义指标了， 目前比较成熟的实现是 Prometheus Custom Metrics。 自定义指标由Prometheus来提供，再利用k8s-prometheus-adpater聚合到apiserver，实现和核心指标（metric-server)同样的效果。 Prometheus 介绍 Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。 为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目。 Prometheus 特点： 多维数据模型：由度量名称和键值对标识的时间序列数据 PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询 不依赖分布式存储，单个服务器节点可直接工作 基于HTTP的pull方式采集时间序列数据 推送时间序列数据通过PushGateway组件支持 通过服务发现或静态配置发现目标 多种图形模式及仪表盘支持（grafana） Prometheus Server：收集指标和存储时间序列数据，并提供查询接口 ClientLibrary：客户端库 Push Gateway：短期存储指标数据。主要用于临时性的任务 Exporters：采集已有的第三方服务监控指标并暴露metrics Alertmanager：告警 Web UI：简单的Web控制台 Prometheus 部署1# 只需要部署服务端 部署nfs自动供给 12345678910111213141516171819202122232425262728293031323334353637# 安装nfs# 选择 master2 作为服务端# 客户端也所有 node上也要安装[root@k8s-master2 ~]# yum install -y nfs-utils[root@k8s-node2 ~]# mkdir -p /ifs/kubernetes[root@k8s-node2 ~]# cat /etc/exports/ifs/kubernetes *(rw,no_root_squash)[root@k8s-node2 ~]# systemctl start nfs[root@k8s-node2 ~]# ps -ef|grep nfs# 上传nfs-client 修改IP地址为nfs服务端地址[root@k8s-master1 nfs-client]# ls -ltotal 12-rw-r--r-- 1 root root 225 Nov 30 20:50 class.yaml-rw-r--r-- 1 root root 993 Dec 13 09:25 deployment.yaml-rw-r--r-- 1 root root 1526 Nov 30 20:50 rbac.yaml[root@k8s-master1 nfs-client]# cat deployment.yaml - name: NFS_SERVER value: 172.17.70.254 - name: NFS_PATH value: /ifs/kubernetes volumes: - name: nfs-client-root nfs: server: 172.17.70.254 path: /ifs/kubernetes# 创建[root@k8s-master1 nfs-client]# kubectl apply -f .[root@k8s-master1 nfs-client]# kubectl get podsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-5dd6f66f47-27rcz 1/1 Running 0 5m50sweb-6f4b67f8cc-nmfpv 1/1 Running 1 17hweb-6f4b67f8cc-s9mvc 1/1 Running 1 17h 安装 Prometheus 123456789101112131415161718192021222324[root@k8s-master1 prometheus]# kubectl apply -f prometheus-rbac.yaml # 授权[root@k8s-master1 prometheus]# kubectl apply -f prometheus-configmap.yaml # 配置文件[root@k8s-master1 prometheus]# kubectl apply -f prometheus-rules.yaml # 告警文件[root@k8s-master1 prometheus]# kubectl apply -f prometheus-service.yaml # 告警规则[root@k8s-master1 prometheus]# kubectl apply -f prometheus-statefulset.yaml # 服务[root@k8s-master1 prometheus]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-h59sl 1/1 Running 1 18hkube-flannel-ds-amd64-9s7s2 1/1 Running 1 18hkube-flannel-ds-amd64-j77m8 1/1 Running 1 18hkube-flannel-ds-amd64-pqw7p 1/1 Running 2 18hmetrics-server-7dbbcf4c7-29ms2 1/1 Running 1 18hprometheus-0 2/2 Running 0 58s[root@k8s-master1 prometheus]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.0.0.2 &lt;none&gt; 53/UDP,53/TCP 18hmetrics-server ClusterIP 10.0.0.52 &lt;none&gt; 443/TCP 18hprometheus NodePort 10.0.0.237 &lt;none&gt; 9090:30090/TCP 4m5s# 访问web 没有部署node 并不影响,只要能采集到pod就行http://39.106.100.108:30090/graphhttp://39.106.100.108:30090/targets 1# 自定义指标逻辑很负载 做的不是很多 部署一个监控应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@k8s-master1 hpa]# vim metrics-app.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: app: metrics-app name: metrics-appspec: replicas: 3 selector: matchLabels: app: metrics-app template: metadata: labels: app: metrics-app annotations: prometheus.io/scrape: "true" prometheus.io/port: "80" prometheus.io/path: "/metrics" spec: containers: - image: lizhenliang/metrics-app name: metrics-app ports: - name: web containerPort: 80 resources: requests: cpu: 200m memory: 256Mi readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 periodSeconds: 5---apiVersion: v1kind: Servicemetadata: name: metrics-app labels: app: metrics-appspec: ports: - name: web port: 80 targetPort: 80 selector: app: metrics-app 12345678910[root@k8s-master1 hpa]# kubectl apply -f metrics-app.yaml [root@k8s-master1 hpa]# kubectl get podsNAME READY STATUS RESTARTS AGEmetrics-app-7674cfb699-67252 1/1 Running 0 15smetrics-app-7674cfb699-crq64 1/1 Running 0 15smetrics-app-7674cfb699-m7ksd 1/1 Running 0 15snfs-client-provisioner-5dd6f66f47-27rcz 1/1 Running 0 45mweb-6f4b67f8cc-nmfpv 1/1 Running 1 18hweb-6f4b67f8cc-s9mvc 1/1 Running 1 18h 查看监控指标12345678910[root@k8s-master1 hpa]# curl 10.0.0.36Hello! My name is metrics-app-7674cfb699-crq64. The last 10 seconds, the average QPS has been 0.5. Total requests served: 24[root@k8s-master1 hpa]# curl 10.0.0.36/metrics# HELP http_requests_total The amount of requests in total# TYPE http_requests_total counterhttp_requests_total 53 # 当前pod总共请求了多少次 # HELP http_requests_per_second The amount of requests per second the latest ten seconds# TYPE http_requests_per_second gaugehttp_requests_per_second 0.5 # 吞吐量 10秒之内 0.5个http请求 qps是每秒，吞吐率是一定时间范围之内 查看采集数据123456789# 1. 暴露指标# 2. 需要告诉监控采集哪些指标 annotations: prometheus.io/scrape: "true" # prometheus可以采集我 prometheus.io/port: "80" # 采集我的端口 prometheus.io/path: "/metrics" # 连接路径 # 3. prometheus 服务发现 部署 Custom Metrics Adapter1234567# 先准备下helm环境：wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gztar zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/helm repo add stable http://mirror.azure.cn/kubernetes/chartshelm repo updatehelm repo list 12345678910111213141516171819[root@k8s-master1 ~]# helm repo listNAME URL stable http://mirror.azure.cn/kubernetes/charts[root@k8s-master1 ~]# helm install prometheus-adapter stable/prometheus-adapter --namespace kube-system --set prometheus.url=http://prometheus.kube-system,prometheus.port=9090[root@k8s-master1 ~]# helm ls -n kube-systemNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONprometheus-adapter kube-system 1 2019-12-13 10:35:45.714034137 +0800 CST deployed prometheus-adapter-1.4.0 v0.5.0 [root@k8s-master1 ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-h59sl 1/1 Running 1 19hkube-flannel-ds-amd64-9s7s2 1/1 Running 1 19hkube-flannel-ds-amd64-j77m8 1/1 Running 1 19hkube-flannel-ds-amd64-pqw7p 1/1 Running 2 19hmetrics-server-7dbbcf4c7-29ms2 1/1 Running 1 18hprometheus-0 2/2 Running 0 57mprometheus-adapter-77b7b4dd8b-bwjrs 1/1 Running 0 57s 1234567# 确保适配器注册到APIServer[root@k8s-master1 ~]# kubectl logs prometheus-adapter-77b7b4dd8b-bwjrs -n kube-system[root@k8s-master1 ~]# kubectl get apiservice# 测试数据[root@k8s-master1 ~]# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1"[root@k8s-master1 ~]# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq 创建 HPA 策略123456789101112131415161718192021222324252627[root@k8s-master1 hpa]# vim hpa-v2.yamlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: metrics-app-hpa namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: metrics-app minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: http_requests_per_second target: type: AverageValue averageValue: 800m # 800m 即0.8个/秒[root@k8s-master1 hpa]# kubectl apply -f hpa-v2.yamlhorizontalpodautoscaler.autoscaling/metrics-app-hpa created 1234[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/web &lt;unknown&gt;/800m 2 10 2 28sweb Deployment/web 0%/60% 2 10 2 18h 配置适配器收集特定的指标 用来数据转换 12345678910111213# 该规则将http_requests在2分钟的间隔内收集该服务的所有Pod的平均速率。[root@k8s-master1 hpa]# kubectl edit cm prometheus-adapter -n kube-system rules: # 复制下面的规则 - seriesQuery: 'http_requests_total&#123;kubernetes_namespace!="",kubernetes_pod_name!=""&#125;' resources: overrides: kubernetes_namespace: &#123;resource: "namespace"&#125; kubernetes_pod_name: &#123;resource: "pod"&#125; name: matches: "^(.*)_total" as: "$&#123;1&#125;_per_second" metricsQuery: 'sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)' 123456789101112131415# 重启pod加载生效[root@k8s-master1 hpa]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-h59sl 1/1 Running 1 19hkube-flannel-ds-amd64-9s7s2 1/1 Running 1 19hkube-flannel-ds-amd64-j77m8 1/1 Running 1 19hkube-flannel-ds-amd64-pqw7p 1/1 Running 2 19hmetrics-server-7dbbcf4c7-29ms2 1/1 Running 1 19hprometheus-0 2/2 Running 0 86mprometheus-adapter-77b7b4dd8b-bwjrs 1/1 Running 0 29m[root@k8s-master1 hpa]# kubectl delete pod prometheus-adapter-77b7b4dd8b-bwjrs -n kube-system# 等待重建[root@k8s-master1 hpa]# kubectl get pods -n kube-system 123456# 再测试数据[root@k8s-master1 hpa]# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second" | jq[root@k8s-master1 hpa]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 416m/800m 1 10 2 29sweb Deployment/web 0%/60% 2 10 2 18h 12345678910111213[root@k8s-master1 hpa]# ab -n 100000 -c 100 http://10.0.0.136/metrics[root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 100091m/800m 1 10 3 3m32sweb Deployment/web 0%/60% 2 10 2 28m[root@k8s-master1 ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEmetrics-app-hpa Deployment/metrics-app 123285m/800m 1 10 6 3m46sweb Deployment/web 0%/60% 2 10 2 28m# 扩容成功 对接一个应用需要做哪些事123456# 总结1. 应用程序需要暴露/metrics监控指标，并且是 promettheus 数据格式2. promettheus 采集入库3. 针对数据添加adapter规则(promsql)4. adapter 注册 apiservice5. 编写HPA规则 （对应指标名称）]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 Ansible-Roles 快速入门]]></title>
    <url>%2F2019%2F11%2F30%2Fansible-base03%2F</url>
    <content type="text"><![CDATA[Roles 快速入门Roles 简介 Roles是基于已知文件结构自动加载某些变量文件，任务和处理程序的方法。 按角色对内容进行分组，适合构建复杂的部署环境。 Roles 目录结构1234567891011121314151617# 根据自己需要定义目录 site.yml # 入口文件webservers.ymlfooservers.ymlroles/ # 目录 common/ # 应用结果 tasks/ # task任务 handlers/ # 触发任务 files/ # 文件目录 templates/ # jinjia渲染文件目录 vars/ # 变量 defaults/ # 默认目录 meta/ # 元数据 webservers/ tasks/ defaults/ meta/ 简单示例1234567- `tasks` - 包含角色要执行的任务的主要列表。- `handlers` - 包含处理程序，此角色甚至在此角色之外的任何地方都可以使用这些处理程序。- `defaults` - 角色的默认变量- `vars` - 角色的其他变量- `files` - 包含可以通过此角色部署的文件。- `templates` - 包含可以通过此角色部署的模板。- `meta` - 为此角色定义一些元数据。请参阅下面的更多细节。 123456789101112[root@master playbook]# tree /etc/ansible/playbook//etc/ansible/playbook/├── roles│ ├── mysql│ ├── nginx│ │ ├── files│ │ ├── tasks│ │ │ └── main.yaml│ │ └── templates│ │ └── nginx.conf│ └── php└── site.yaml 12345678# 入口文件[root@master playbook]# vim /etc/ansible/playbook/site.yaml - hosts: web gather_facts: false # roles名字 roles: - nginx 1234567891011121314151617# 工作流程[root@master playbook]# vim /etc/ansible/playbook/roles/nginx/tasks/main.yaml # 默认读取该yaml文件,只写tasks任务#- name: 安装 nginx# yum:# name=http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm# state=latest#- name: 启动 Nginx# systemd:# name=nginx# state=started# enabled=yes- name: 生成虚拟主机 默认找templates目录 template: src=nginx.conf dest=/tmp 123456789101112131415# 模板文件[root@master playbook]# vim /etc/ansible/playbook/roles/nginx/templates/nginx.conf upstream web&#123; # 拿到web组,循环得到web组里每行的数据 &#123;% for host in groups['web'] %&#125; # 取得每行配置的 hostname,如果是IP最好,如果是组几名需要取IP server &#123;&#123; hostvars[host].inventory_hostname &#125;&#125;; &#123;% endfor %&#125;&#125;server &#123; listen: &#123;&#123; http_port &#125;&#125;; server_name: &#123;&#123; domain_name &#125;&#125;; location / &#123; root /usr/share/html; 1234567# 变量定义[root@master playbook]# vim /etc/ansible/group_vars/web.yaml work: Kuberneteshttp_port: 19090domain_name: www.teamshub100.comdomain: www.teamshub200.com 12# 执行[root@master playbook]# ansible-playbook site.yaml 使用角色12345678910111213141516171819# 定义多个:[root@master playbook]# vim site.yaml - name: web 安装 hosts: web gather_facts: false # roles名字 roles: - nginx - tomcat tags: web- name: mysql 安装 hosts: db gather_facts: false roles: - mysql tags: db 123456789101112131415[root@master playbook]# tree ├── roles│ ├── mysql│ │ └── tasks│ │ └── main.yaml│ ├── nginx│ │ ├── files│ │ ├── tasks│ │ │ └── main.yaml│ │ └── templates│ │ └── nginx.conf│ └── tomcat│ └── tasks│ └── main.yaml└── site.yaml 123[root@master playbook]# ansible-playbook site.yaml # 指定部署[root@master playbook]# ansible-playbook site.yaml --tags=db 角色控制123456- name: 0.系统初始化 gather_facts: false hosts: all roles: - common tags: common 生成 roles 目录1234567891011121314151617181920[root@master roles]# ansible-galaxy init test[root@master roles]# tree test/test/├── defaults│ └── main.yml├── files├── handlers│ └── main.yml├── meta│ └── main.yml├── README.md├── tasks│ └── main.yml├── templates├── tests│ ├── inventory│ └── test.yml└── vars └── main.yml]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 Ansible-Playbook 快速入门]]></title>
    <url>%2F2019%2F11%2F30%2Fansible-base02%2F</url>
    <content type="text"><![CDATA[Playbook 快速入门Playbook 简介 Playbooks是Ansible的配置，部署和编排语言。 他们可以描述您希望在远程机器做哪些事或者描述IT流程中一系列步骤。 使用易读的YAML格式组织Playbook文件。 如果Ansible模块是您工作中的工具，那么Playbook就是您的使用说明书，而您的主机资产文件就是您的原材料。 与adhoc任务执行模式相比，Playbooks使用ansible是一种完全不同的方式，并且功能特别强大。 12# 官方文档：https://docs.ansible.com/ansible/latest/user_guide/playbooks.html 简单示例12# 卸载掉之前安装的nginx# [root@master ansible]# ansible web -m yum -a "name=nginx state=absent" 123# 创建 playbook 目录[root@master ansible]# mkdir -p /etc/ansible/playbook[root@master ansible]# cd /etc/ansible/playbook 123456789101112131415161718[root@master playbook]# vim install_nginx.yaml- hosts: web remote_user: root # 不动态获取客户端属性信息 优化执行速度 gather_facts: false # tasks定义任务 tasks: - name: YUM安装Nginx最新版本 yum: name=http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm state=latest - name: 启动Nginx systemd: name=nginx state=started enabled=yes 123# 测试执行[root@master playbook]# ansible-playbook install_nginx.yaml -C[root@master playbook]# ansible-playbook install_nginx.yaml 1234567# 客户端验证[root@localhost ~]# netstat -tnlp | grep 80tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1819/nginx: master [root@localhost ~]# rpm -qa|grep nginxnginx-1.16.1-1.el7.ngx.x86_64[root@localhost ~]# nginx -vnginx version: nginx/1.16.1 常用定义主机和主机组1234567891011121314# work 普通用户# become 提权# become_user 提权到root用户- hosts: web remote_user: work become: yes become_user: root # 命令行使用# -k 输入用户密码# -b 提权# -k 输入提权密码ansible-playbook nginx.yaml -u work -k -b -K 定义变量 变量是应用于多个主机的便捷方式 实际在主机执行之前，变量会对每个主机添加，然后在执行中引用。 命令行传递123456789# -e VAR=VALUE 传递参数[root@master playbook]# ansible web -m debug -a "var=user_name" -e user_name=leo10.0.0.202 | SUCCESS =&gt; &#123; "user_name": "leo"&#125;10.0.0.203 | SUCCESS =&gt; &#123; "user_name": "leo"&#125; 主机变量与组变量1234567891011121314151617181920212223242526272829303132333435363738# 在Inventory中定义变量[root@master playbook]# vim /etc/ansible/hosts [web]10.0.0.202 role=node110.0.0.203 role=node2[web:vars] # web主机组变量work=k8s[all:vars] # 所有主机组变量user=harrisli[root@master playbook]# ansible web -m debug -a "var=role" 10.0.0.202 | SUCCESS =&gt; &#123; "role": "node1"&#125;10.0.0.203 | SUCCESS =&gt; &#123; "role": "node2"&#125;[root@master playbook]# ansible web -m debug -a "var=work" 10.0.0.202 | SUCCESS =&gt; &#123; "work": "k8s"&#125;10.0.0.203 | SUCCESS =&gt; &#123; "work": "k8s"&#125;[root@master playbook]# ansible all -m debug -a "var=user" 10.0.0.201 | SUCCESS =&gt; &#123; "user": "harrisli"&#125;10.0.0.202 | SUCCESS =&gt; &#123; "user": "harrisli"&#125;10.0.0.203 | SUCCESS =&gt; &#123; "user": "harrisli"&#125; 单文件存储变量1234561. Ansible中的首选做法是不将变量存储在Inventory中。2. 除了将变量直接存储在Inventory文件之外，主机和组变量还可以存储在相对于Inventory文件的单个文件中。3. 组变量：group_vars 存放的是组变量group_vars/all.yml 表示所有主机有效，等同于[all:vars]grous_vars/etcd.yml 表示etcd组主机有效，等同于[etcd:vars] 123456789101112131415# 所有主机有效[root@master ansible]# mkdir -p /etc/ansible/group_vars[root@master group_vars]# vim /etc/ansible/group_vars/all.yamluser: Harrisli[root@master group_vars]# ansible all -m debug -a "var=user"10.0.0.203 | SUCCESS =&gt; &#123; "user": "Harrisli"&#125;10.0.0.201 | SUCCESS =&gt; &#123; "user": "Harrisli"&#125;10.0.0.202 | SUCCESS =&gt; &#123; "user": "Harrisli"&#125; 1234567891011# 主机组有效，文件名要与主机组一致[root@master group_vars]# vim /etc/ansible/group_vars/web.yamlwork: Kubernetes[root@master group_vars]# ansible web -m debug -a "var=work"10.0.0.202 | SUCCESS =&gt; &#123; "work": "Kubernetes"&#125;10.0.0.203 | SUCCESS =&gt; &#123; "work": "Kubernetes"&#125; 在Playbook中定义1234- hosts: web vars: http_port: 80 server_name: www.ctnrs.com 123456789101112131415161718192021222324252627[root@master playbook]# vim install_nginx.yaml - hosts: web remote_user: root # 不动态获取客户端属性信息 优化执行速度 gather_facts: false # 变量引用 vars: http_port: 80 # tasks定义任务 tasks: # - name: YUM安装Nginx最新版本 # yum: # name=http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm # state=latest # # - name: 启动Nginx # systemd: # name=nginx # state=started # enabled=yes - debug: var=http_port[root@master playbook]# ansible-playbook install_nginx.yaml Register变量123456# 将命令的结果保存成一个变量- shell: /usr/bin/uptime register: result- debug: var: result 1234567891011121314151617181920212223242526272829303132[root@master playbook]# vim install_nginx.yaml - hosts: web remote_user: root # 不动态获取客户端属性信息 优化执行速度 gather_facts: false # 变量引用 vars: http_port: 80 # tasks定义任务 tasks: # - name: YUM安装Nginx最新版本 # yum: # name=http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm # state=latest # # - name: 启动Nginx # systemd: # name=nginx # state=started # enabled=yes - name: 测试playbook变量 debug: var=http_port - name: 测试register shell: netstat -tnlp|grep 80 register: res - debug: var=res # 返回一个字典 - debug: var=["stdout"] # 取出字典中的某个key的值 12# 相当于将执行结果显示[root@master playbook]# ansible-playbook install_nginx.yaml 任务列表 每个playbook包含一系列任务。这些任务按照顺序执行。 在playbook中，所有主机都会执行相同的任务指令。playbook目的是将选择的主机映射到任务。 123456789[root@master playbook]# vim show.yaml- hosts: web gather_facts: false tasks: - name: 安装Tomcat debug: msg: "install tomcat" 语法检查与调试123# 测试运行，不实际操作：# debug模块在执行期间打印语句，对于调试变量或表达式，而不必停止play。# 与'when：'指令一起调试更佳。 123456789101112[root@master playbook]# vim show.yaml- hosts: web gather_facts: false tasks: - name: 打印组名 debug: msg: "&#123;&#123;group_names&#125;&#125;" - name: 打印主机名 debug: msg: "&#123;&#123;inventory_hostname&#125;&#125;" 1[root@master playbook]# ansible-playbook -C show.yaml 任务控制 如果你有一个大的剧本，那么能够在不运行整个剧本的情况下运行特定部分。 使用tags标记 让程序按照阶段部署 12345678910111213141516171819202122[root@master playbook]# vim show.yaml - hosts: web gather_facts: false tasks: - name: 安装 Nginx debug: msg: "install Nginx start..." tags: install_nginx - name: 安装 Tomcat debug: msg: "install Tomcat start..." tags: install_tomcat# 单独执行 install_tomcat[root@master playbook]# ansible-playbook -C show.yaml --tags install_tomcat# 跳过 --skip-tags[root@master playbook]# ansible-playbook -C show.yaml --skip-tags install_nginx 流程控制条件 when1234567891011[root@master playbook]# vim when.yaml - hosts: web gather_facts: false tasks: - name: 流程控制 条件 shell: echo 123 register: res - debug: msg=&#123;&#123;res&#125;&#125; when: res.stdout == '123' 循环 常用循环语句 12345678910111213[root@master playbook]# vim create_file.yaml - hosts: web gather_facts: false tasks: - name: 批量创建文件 file: path=/tmp/&#123;&#123;item&#125;&#125; state=touch with_items: - "1.txt" - "2.txt" - "3.txt" tags: create 12345678910111213[root@master tmp]# touch &#123;a,b,c&#125;.html[root@master playbook]# vim copy_file.yaml - hosts: web gather_facts: false tasks: - name: 批量复制文件 copy: src=&#123;&#123;item&#125;&#125; dest=/tmp with_fileglob: - "/tmp/*.html" tags: copy 模板 template123456789101112131415161718192021222324252627282930# template 调用jinjia模板 并copy文件[root@master playbook]# mkdir config[root@master config]# vim nginx.conf # 也可有直接使用domain &#123;% set domain_name = domain %&#125;server &#123; listen: &#123;&#123; http_port &#125;&#125;; server_name: &#123;&#123; domain_name &#125;&#125;; location / &#123; root /usr/share/html; &#125;&#125;[root@master playbook]# vim config_nginx.yaml - hosts: web remote_user: root # 不动态获取客户端属性信息 优化执行速度 gather_facts: false # 变量引用 vars: http_port: 8080 domain: "www.teamshub.com" tasks: - name: 生成虚拟主机 template: src=config/nginx.conf dest=/tmp tags: config_nginx 条件 和 循环12345678910111213141516171819[root@master config]# vim nginx.conf &#123;% set domain_name = domain %&#125;upstream web&#123; # 拿到web组,循环得到web里所有的主机 &#123;% for host in groups['web'] %&#125; # 取得每行配置的 hostname,如果是IP最好,如果是主机名需要取IP server &#123;&#123; hostvars[host].inventory_hostname &#125;&#125;; &#123;% endfor %&#125;&#125;server &#123; listen: &#123;&#123; http_port &#125;&#125;; server_name: &#123;&#123; domain_name &#125;&#125;; location / &#123; root /usr/share/html; &#125;&#125;]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 Ansible 快速入门]]></title>
    <url>%2F2019%2F11%2F30%2Fansible-base01%2F</url>
    <content type="text"><![CDATA[Ansible 架构 Inventory：Ansible管理的主机信息，包括IP地址、SSH端口、账号、密码等 Modules： 任务均有模块完成，也可以自定义模块，例如经常用的脚本。 Plugins： 使用插件增加Ansible核心功能，自身提供了很多插件，也可以自定义插件。例如connection插件，用于连接目标主机。 Playbooks：“剧本”，模块化定义一系列任务，供外部统一调用。Ansible核心功能。 Ansible 安装与配置Ansible 使用要求1234567服务端要求• Python2.6/2.7/3.x• RedHat，Debian，CentOS，OS X等。不支持Windows被管理端要求• OpenSSH• Python2.6/2.7/3.x 安装 Ansible1234# 推荐 yum 安装[root@localhost ~]# yum install ansible -y[root@localhost ~]# ansible --version[root@localhost ~]# ansible --help 配置文件1234567891011[root@master ~]# vim /etc/ansible/ansible.cfg [defaults] # inventory = /etc/ansible/hosts # 被管理的主机资源清单# forks = 5 # 执行任务并发数,越大服务端消耗# become = root # 提权# remote_port = 22 # ssh远程端口host_key_checking = False # 主机key认证检查,建议关闭,减少交互# timeout = 10 # 连接超时时间log_path = /var/log/ansible.log # 日志路径private_key_file = /root/.ssh/id_rsa # 秘钥认证的私钥路径,基于密钥对 主机资源清单123456789101112131415161718192021# 示例1：未分组的主机green.example.comblue.example.com192.168.100.1192.168.100.10# 示例2：属于webservers组主机集合[webservers]alpha.example.orgbeta.example.org192.168.1.100192.168.1.110www[001:006].example.com 示例3：属于dbservers组主机集合[dbservers]db01.intranet.mydomain.netdb02.intranet.mydomain.net10.25.1.5610.25.1.57db-[99:101]-node.example.com 1234567891011# 加入被管理服务器资源1. 未分组的主机统一的组名叫做 all 2. 可以是IP也可以是主机名,主机名需要配置host3. 写入了ansible登陆的用户和密码[root@master ~]# vim /etc/ansible/hosts [web]10.0.0.202 ansible_ssh_user=root ansible_ssh_pass=22222210.0.0.203 ansible_ssh_user=root ansible_ssh_pass=222222 指定主机组执行命令123456[root@master ~]# ansible --help# 默认模块# -m MODULE_NAME, --module-name MODULE_NAME# module name to execute (default=command)[root@master ~]# ansible web -m command -a "df -h" all 所有主机执行12345678910# 1. 未分组的主机统一的组名叫做 all [root@master ~]# vim /etc/ansible/hosts ## db-[99:101]-node.example.com10.0.0.201 ansible_ssh_user=root ansible_ssh_pass=222222[web]10.0.0.202 ansible_ssh_user=root ansible_ssh_pass=22222210.0.0.203 ansible_ssh_user=root ansible_ssh_pass=222222 通过IP或者主机名 指定执行1[root@master ~]# ansible 10.0.0.202 -a "df -h" 主机和主机组变量1官方手册 https://docs.ansible.com/ 12345678# 在资源清单里设置 主机定义[root@master ~]# vim /etc/ansible/hosts [web]10.0.0.202 ansible_ssh_user=root ansible_ssh_pass=222222 http_port=8010.0.0.203 ansible_ssh_user=root ansible_ssh_pass=222222 http_port=80[root@master ~]# ansible web -a "echo &#123;&#123; http_port &#125;&#125;" 1234567891011121314# 主机组定义# 主机定义 优先级大于 主机组,主机组相当于一个默认定义[root@master ~]# vim /etc/ansible/hosts [web]10.0.0.202 ansible_ssh_user=root ansible_ssh_pass=222222 http_port=8010.0.0.203 ansible_ssh_user=root ansible_ssh_pass=222222 [web:vars]http_port=8080server_name=www.ctnrs.com[root@master ~]# ansible web -a "echo &#123;&#123; http_port &#125;&#125;"[root@master ~]# ansible web -a "echo &#123;&#123; server_name &#125;&#125;" 123456# 组变量分解到单个文件[root@master ~]# mkdir -p /etc/ansible/group_vars/[root@master ~]# vim /etc/ansible/group_vars/web.yamlhttp_port: 9090server_name: www.teamshub.com 1234# 优先级总结:1. 主机定义2. 文件定义3. 主机组定义 Ansible ad-hoc 命令121. ad-hoc 简单的批量管理2. ploybook 任务编排 命令行工具常用选项123456789101112131415161718192021222324251. 格式：ansible &lt;host-pattern&gt; [ options ]2. 选项：-a MODULE_ARGS, --args=MODULE_ARGS 模块参数-C, --check 运行检查，不执行任何操作-e EXTRA_VARS, --extra-vars=EXTRA_VARS 设置附加变量 key=value-f FORKS, --forks=FORKS 指定并行进程数量，默认5 -i INVENTORY, --inventory=INVENTORY 指定主机清单文件路径--list-hosts 输出匹配的主机列表，不执行任何操作-m MODULE_NAME, --module-name=MODULE_NAME 执行的模块名，默认command--syntax-check 语法检查playbook文件，不执行任何操作-t TREE, --tree=TREE 将日志输出到此目录-v, --verbose 详细信息，-vvv更多, -vvvv debug--version 查看程序版本3. 连接选项：控制谁连接主机和如何连接-k, --ask-pass 请求连接密码--private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE 私钥文件-u REMOTE_USER, --user=REMOTE_USER 连接用户，默认None-T TIMEOUT, --timeout=TIMEOUT 覆盖连接超时时间，默认10秒4. 提权选项：控制在目标主机以什么用户身份运行-b, --become 以另一个用户身份操作--become-method=BECOME_METHOD 提权方法，默认sudo--become-user=BECOME_USER 提权后的用户身份，默认root-K, --ask-become-pass 提权密码 shell 模块12# 默认command模块不支持 | 管道符 、&amp;&amp; 和 &gt; 重定向# 所以使用shell模块 12345678910[root@master ~]# ansible web -m shell -a "echo 123 &gt; /tmp/a.log"10.0.0.202 | CHANGED | rc=0 &gt;&gt;10.0.0.203 | CHANGED | rc=0 &gt;&gt;[root@master ~]# ansible web -m shell -a "cat /tmp/a.log"10.0.0.203 | CHANGED | rc=0 &gt;&gt;12310.0.0.202 | CHANGED | rc=0 &gt;&gt;123 SSH 密码认证123456[root@master ~]# vim /etc/ansible/hosts # 别忘记关闭 指纹认证 host_key_checking = False[web]10.0.0.202 ansible_ssh_user=root ansible_ssh_pass=22222210.0.0.203 ansible_ssh_user=root ansible_ssh_pass=222222 SSH 密钥对认证123456789101112131415161718# 推荐使用免交互方式# 生成 ssh 秘钥对[root@master ~]# ssh-keygen [root@master ~]# ls -l .ssh/total 12-rw-------. 1 root root 1679 Nov 30 18:01 id_rsa # 私钥 登录认证-rw-r--r--. 1 root root 393 Nov 30 18:01 id_rsa.pub # 公钥 放在目标主机上-rw-r--r--. 1 root root 516 Nov 30 17:01 known_hosts# 将公钥拷贝到目标主机[root@master ~]# ssh-copy-id root@10.0.0.202[root@master ~]# ssh-copy-id root@10.0.0.203# 输入秘钥后 免认证登录[root@master ~]# ssh 10.0.0.202[root@node2 ~]# ls -l .ssh/authorized_keys 123456789# 无需在配置用户密码[root@master ansible]# vim /etc/ansible/hosts [web]10.0.0.20210.0.0.203[root@master ansible]# ansible web -a "df -h" Ansible 常用模块12# 官方文档https://docs.ansible.com/ansible/latest/user_guide/intro_adhoc.html#intro-adhoc 执行shell命令（command和shell）1234567# -u 指定用户操作# -k 指定用户密码[root@node1 ~]# useradd work[root@node1 ~]# passwd work[root@master ansible]# ansible web -a "pwd" -u work -k 12# 执行多条命令[root@master ansible]# ansible web -m shell -a "ls /opt;touch /opt/ansible.log ls /opt" sudo 的使用1234567891011[work@node2 ~]$ sudo ls -l /rootWe trust you have received the usual lecture from the local SystemAdministrator. It usually boils down to these three things: #1) Respect the privacy of others. #2) Think before you type. #3) With great power comes great responsibility.[sudo] password for work: work is not in the sudoers file. This incident will be reported. 123456789101112131415# 配置 sudo 两种模式 # 1. 提权的时候无需输入密码# 2. 需要输入密码# 3. 只配置node1 查看区别[root@node2 ~]# visudo ## Allow root to run any commands anywhereroot ALL=(ALL) ALLwork ALL=(ALL) ALL[root@master ansible]# ansible web -m shell -a "ls -l /root" -u work -k --become --become-user root --ask-become-pass# 输入了两次密码,第一次是因为执行用户是work,work并没有免交互,第二次是sudo密码# node2 并没有sudo权限# --ask-become-pass == -K 文件管理（copy和file）123# copy 将本地文件copy到目标主机[root@master opt]# ansible web -m copy -a "src=/opt/nginx-1.16.1.tar.gz dest=/tmp" [root@master opt]# ansible web -m shell -a "ls -l /tmp" 12# copy 目录[root@master ansible]# ansible web -m copy -a "src=/etc/ansible dest=/tmp" 12345678910111213141516# file 创建文件或目录# 通过状态区分[root@master opt]# ansible web -m file -a "dest=/tmp/src mode=600 owner=work state=directory" # 查看所有状态[root@master opt]# ansible web -m file -a "dest=/tmp/src mode=600 owner=work state=" ..."msg": "value of state must be one of: absent, directory, file, hard, link, touch, got: "...# 创建目录# 删除目录# touch 创建空文件[root@master ansible]# ansible web -m file -a "path=/tmp/src state=directory"[root@master opt]# ansible web -m file -a "dest=/tmp/src state=absent" [root@master opt]# ansible web -m shell -a "ls -l /tmp" 管理软件包（yum）1234567# present 安装软件[root@master opt]# ansible web -m yum -a "name=memcached state=present" [root@node1 ~]# rpm -qa | grep memcached# absent 卸载[root@master opt]# ansible web -m yum -a "name=memcached state=absent"[root@node1 ~]# rpm -qa | grep memcached 123456# 使用远程rpm安装present，latest：表示安装absent：表示卸载[root@master ansible]# ansible web -m yum -a "name=http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm state=present"[root@localhost opt]# rpm -qa | grep nginxnginx-1.16.1-1.el7.ngx.x86_64 用户和组（user）12345678910# 创建用户[root@master opt]# ansible web -m user -a "name=foo password=foo123"[root@node1 ~]# id foouid=1001(foo) gid=1001(foo) groups=1001(foo)# 删除用户[root@master opt]# ansible web -m user -a "name=foo state=absent"# nologin 用户[root@master opt]# ansible web -m user -a "name=foo password=foo123 shell=/sbin/nologin " 从源代码管理系统部署（git）1234# 要想使用git模块,目标主机必须有git命令# 可以用于部署[root@master opt]# ansible web -m yum -a "name=git state=present"[root@master opt]# ansible web -m git -a "repo=https://github.com/ansible/ansible.git dest=/opt" -u root 管理服务（service|systemctl）12345678910111213141516# 安装[root@master opt]# ansible web -m yum -a "name=memcached state=present"# 查看服务[root@node1 ansible]# systemctl status memcached# 启动[root@master opt]# ansible web -m service -a "name=memcached state=started" -u root[root@node1 ansible]# systemctl status memcached[root@node1 ansible]# ps -ef|grep memcached# 停止服务[root@master opt]# ansible web -m service -a "name=memcached state=stopped" -u root# 开机启动[root@master opt]# ansible web -m service -a "name=memcached enabled=true" -u root 123456789# centos7 使用 systemd模块[root@master ansible]# ansible web -m systemd -a "name=nginx state=started enabled=yes"[root@localhost opt]# ps -ef|grep nginxroot 5178 1 0 09:11 ? 00:00:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 5179 5178 0 09:11 ? 00:00:00 nginx: worker processroot 5216 1955 0 09:12 pts/0 00:00:00 grep --color=auto nginx[root@localhost opt]# netstat -tnlp|grep 80tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 5178/nginx: master unarchive 解压1234- name: 解压 unarchive: src=test.tar.gz dest=/tmp debug 调试123456789# 执行过程中打印语句。- debug: msg: System &#123;&#123; inventory_hostname &#125;&#125; has uuid &#123;&#123; ansible_product_uuid &#125;&#125;- name: 显示主机已知的所有变量 debug: var: hostvars[inventory_hostname] verbosity: 4 1234# 打印主机组变量# ansible变量级别: 主机 主机组 # hostvars 打印每个主机默认的变量,主机名,IP...用于playbook[root@master ansible]# ansible web -m debug -a "var=hostvars" 收集目标主机信息（setup）12345678# 常用信息# ploybook会使用到ansible_nodenameansible_os_familyansible_pkg_mgransible_processoransible_processor_cores 12345678910111213141516171819# 过滤[root@master opt]# ansible web -m setup -a "filter=ansible_nodename"10.0.0.202 | SUCCESS =&gt; &#123; "ansible_facts": &#123; "ansible_nodename": "node1", "discovered_interpreter_python": "/usr/bin/python" &#125;, "changed": false&#125;10.0.0.203 | SUCCESS =&gt; &#123; "ansible_facts": &#123; "ansible_nodename": "node2", "discovered_interpreter_python": "/usr/bin/python" &#125;, "changed": false&#125;[root@master opt]# ansible web -m setup -a "filter=ansible_*_mb"]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 自动化部署 K8S（离线版）]]></title>
    <url>%2F2019%2F11%2F10%2Fk8s-base05%2F</url>
    <content type="text"><![CDATA[服务器规划 角色 IP 组件 k8s-master1 192.168.0.101 kube-apiserver kube-controller-manager kube-scheduler etcd k8s-master2 192.168.0.104 kube-apiserver kube-controller-manager kube-scheduler k8s-node1 192.168.0.102 kubelet kube-proxy docker etcd k8s-node2 192.168.0.103 kubelet kube-proxy docker etcd Load Balancer（Master） 192.168.0.201 192.168.0.200 (VIP) nginx keepalived Load Balancer（Backup） 192.168.0.202 nginx keepalived 部署架构图 Roles组织 K8S各组件部署解析 梳理流程和Roles结构 如果配置文件有不固定内容，使用jinja渲染 人工干预改动的内容应统一写到一个文件中 部署说明系统初始化 关闭 selinux 关闭 firewalld 关闭 swap 时钟同步 hosts 文件 常用基础命令 创建roles初始化目录1234# 创建roles初始化目录[root@localhost ~]# mkdir -p ansible-k8s-deploy # 项目目录[root@localhost ~]# mkdir -p ansible-k8s-deploy/roles # roles目录[root@localhost ~]# mkdir -p ansible-k8s-deploy/group_vars # 变量目录 系统初始化 执行模块123# 创建任务和模板目录[root@localhost ~]# mkdir -p ansible-k8s-deploy/roles/common/tasks # tasks任务目录[root@localhost ~]# mkdir -p ansible-k8s-deploy/roles/common/templates # 模板目录 1234567891011121314151617181920212223242526272829303132333435[root@localhost ~]# cd ansible-k8s-deploy/roles/common/tasks/[root@k8s-master1 tasks]# vim main.yaml---# 系统初始化 所有节点上执行- name: 关闭 selinux # lineinfile 正则匹配，更改某个关键参数值 lineinfile: dest: /etc/selinux/config regexp: '^SELINUX=' line: 'SELINUX=disabled'- name: 关闭 firewalld systemd: name: firewalld state: stopped enabled: no- name: 关闭 swap lineinfile: dest: /etc/fstab regexp: "UUID.*swap" line: ""- name: 关闭 swap 和 selinux 即时生效 shell: setenforce 0 ; swapoff -a- name: 设置 hosts template: src=hosts.j2 dest=/etc/hosts- name: 设置 主机名 shell: hostnamectl set-hostname &#123;&#123;node_name|quote&#125;&#125;# 欠缺 时间同步服务器# https://www.cnblogs.com/bowen-li/p/s155201.html 12345678# 模板[root@k8s-master1 common]# vim templates/hosts.j2 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6&#123;% for host in groups['k8s'] %&#125;&#123;&#123; hostvars[host].inventory_hostname &#125;&#125; &#123;&#123; hostvars[host].node_name &#125;&#125;&#123;% endfor %&#125; 1234567891011121314[root@k8s-master1 ansible-k8s-deploy]# tree ../../├── anaconda-ks.cfg└── ansible-k8s-deploy ├── ansible.cfg # 配置文件 ├── group_vars # 变量目录 ├── hosts # 配置资源清单 ├── roles │ └── common │ ├── tasks │ │ └── main.yaml │ └── templates │ └── hosts.j2 └── single-master-deploy.yaml 1234567891011121314# hosts文件[root@localhost ansible-k8s-deploy]# cat hosts [master]# 如果部署单Master，只保留一个Master节点192.168.0.101 node_name=k8s-master1# 192.168.0.104 node_name=k8s-master2[node]192.168.0.102 node_name=k8s-node1192.168.0.103 node_name=k8s-node2[k8s:children]masternode 12# 执行[root@localhost ansible-k8s-deploy]# ansible-playbook -i hosts single-master-deploy.yaml -uroot -k Etcd集群部署 生成etcd证书 部署三个etcd集群 查看集群状态 部署Maste 生成apiserver证书 部署apiserver、controller-manager和scheduler组件 启动TLS Bootstrapping 部署Node 安装Docker 部署kubelet和kube-proxy 在Master上允许为新Node颁发证书 授权apiserver访问kubelet 12345678[root@k8s-node1 k8s]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEkubernetesui/dashboard v2.0.0-beta4 6802d83967b9 3 months ago 84MBkubernetesui/metrics-scraper v1.0.1 709901356c11 4 months ago 40.1MBlizhenliang/flannel v0.11.0-amd64 ff281650a721 10 months ago 52.6MBlizhenliang/nginx-ingress-controller 0.20.0 a3f21ec4bd11 14 months ago 513MBlizhenliang/coredns 1.2.2 367cdc8433a4 15 months ago 39.2MBlizhenliang/pause-amd64 3.0 99e59f495ffa 3 years ago 747kB 部署插件（准备好镜像) Flannel Web UI CoreDNS Ingress Controller Master高可 增加Master节点（与Master1一致） 部署Nginx负载均衡器 Nginx+Keepalived高可用 修改Node连接VIP]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 二进制搭建 Kubernetes集群]]></title>
    <url>%2F2019%2F11%2F10%2Fk8s-base04%2F</url>
    <content type="text"><![CDATA[单master集群搭建系统初始化123456789101112131415161718192021222324关闭防火墙：systemctl stop firewalldsystemctl disable firewalld关闭selinux：setenforce 0 # 临时sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久关闭swap：swapoff -a # 临时vim /etc/fstab # 永久 注释掉swap那项，阿里云没有，只有ext4同步系统时间：# ntpdate time.windows.com添加hosts：# vim /etc/hosts172.17.70.251 k8s-master1172.17.70.252 k8s-master2172.17.70.253 k8s-node1172.17.70.254 k8s-node2修改主机名：hostnamectl set-hostname k8s-master 1234567# 快速添加主机名 cat &gt;&gt; /etc/hosts &lt;&lt; EOF172.17.70.251 k8s-master1172.17.70.252 k8s-master2172.17.70.253 k8s-node1172.17.70.254 k8s-node2EOF etcd 集群理解 ssl 证书123456789集群中的通信都是基于https进行交互当前架构中,etcd之间需要https通信,k8s内部master(apiserver) 也需要通信, 需要两条证书证书分为: 自签证书 和 权威机构颁发的证书(赛门铁克) 3000元 www.ctnrs.com 泛域名*.ctnrs.com 比较贵根证书自签证书 不受信任 内部服务之间 具备一定加密 内部之间程序调用不对外，可以使用证书包括:crt(数字证书) key(私钥) 配置到web服务器权威机构颁发的证书 https是绿色的安全的，自签是！感叹号不可信任权威证书如果使用到k8s里，其实也不知道是否可行，因为证书里涉及到可信任的IP列表，可能有影响https -&gt; CA -&gt; crt|key 生成 etcd 证书 cfssl 与 openssl一样都可以生成证书 cfssl 使用json文件内容生成，比较直观 1234567891011121314# 任意一台机器 上传压缩包[root@k8s-master1 opt]# ls -l-rw-r--r-- 1 root root 5851667 Nov 12 16:49 TLS.tar.gz# 解压[root@k8s-master1 opt]# tar -zxvf TLS.tar.gz [root@k8s-master1 TLS]# ls -ltotal 18820-rwxr-xr-x 1 root root 10376657 Oct 2 08:37 cfssl-rwxr-xr-x 1 root root 6595195 Oct 2 08:37 cfssl-certinfo-rwxr-xr-x 1 root root 2277873 Oct 2 08:37 cfssljson -rwxr-xr-x 1 root root 344 Oct 3 08:26 cfssl.sh # 安装脚本 上面三个是已下载的二进制文件drwxr-xr-x 2 root root 4096 Oct 3 10:46 etcd # etcd证书目录drwxr-xr-x 2 root root 4096 Oct 3 10:46 k8s # k8s证书目录 cfssl 下载与安装1234567# 脚本中 含有下载地址 和 安装 [root@k8s-master1 TLS]# cat cfssl.sh #curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl#curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson#curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfocp -rf cfssl cfssl-certinfo cfssljson /usr/local/binchmod +x /usr/local/bin/cfssl* 1234[root@k8s-master1 TLS]# cp -rf cfssl cfssl-certinfo cfssljson /usr/local/bin[root@k8s-master1 TLS]# chmod +x /usr/local/bin/cfssl*[root@k8s-master1 TLS]# ls /usr/local/bin/cfssl cfssl-certinfo cfssljson 创建 CA证书和私钥1234567[root@k8s-master1 TLS]# cd /opt/TLS/etcd/[root@k8s-master1 etcd]# ls -l-rw-r--r-- 1 root root 287 Oct 3 13:12 ca-config.json # 相当于CA机构-rw-r--r-- 1 root root 209 Oct 3 13:12 ca-csr.json # CA的请求文件-rwxr-xr-x 1 root root 178 Oct 3 13:58 generate_etcd_cert.sh # 执行脚本-rw-r--r-- 1 root root 306 Oct 3 08:26 server-csr.json # 配置文件 123456# 创建 CA证书和私钥 用来颁发证书[root@k8s-master1 etcd]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@k8s-master1 etcd]# ls -l *.pem-rw------- 1 root root 1679 Nov 12 16:58 ca-key.pem # ca私钥-rw-r--r-- 1 root root 1265 Nov 12 16:58 ca.pem # ca证书 自签 etcd 证书1234567891011121314151617181920212223# hosts 可信任IP，里面包含所有etcd节点的IP [root@k8s-master1 etcd]# vim server-csr.json &#123; "CN": "etcd", "hosts": [ "172.17.70.251", "172.17.70.253", "172.17.70.254" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing" &#125; ]&#125; 123# 向ca请求证书# 指定好ca的证书和私钥,配置文件,生成 server开头的证书[root@k8s-master1 etcd]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server 1234567# 生成server开头的 数字证书和私钥[root@k8s-master1 etcd]# ls -l *.pem-rw------- 1 root root 1679 Nov 12 16:58 ca-key.pem-rw-r--r-- 1 root root 1265 Nov 12 16:58 ca.pem-rw------- 1 root root 1679 Nov 12 17:05 server-key.pem # crt-rw-r--r-- 1 root root 1338 Nov 12 17:05 server.pem # key etcd 简介 CoreOS 开源 etcd首先是一个键值存储仓库,用于配置共享和服务发现。 etcd 负责保存 Kubernetes Cluster 的配置信息和各种资源的状态信息。当数据发生变化时，etcd 会快速地通知 Kubernetes 相关组件。 官方推荐奇数节点部署,常见的有3 5 7 分别对应 1 2 3个冗余节点 3台etcd会先选举出1台为主节点,负责写消息,主节点同步给2个从节点。 当主节点挂了,两台从节点会选举出一台新的主节点 介绍文档 1https://blog.csdn.net/bbwangj/article/details/82584988 12345# 优点1. 简单。使用Go语言编写部署简单；使用HTTP作为接口使用简单；2. 使用Raft算法保证强一致性让用户易于理解。3. 数据持久化。etcd默认数据一更新就进行持久化。4. 安全。etcd支持SSL客户端安全认证。 部署 etcd 集群节点1234# 环境172.17.70.251 k8s-master1 etcd172.17.70.253 k8s-node1 etcd172.17.70.254 k8s-node2 etcd 12# 二进制包下载地址https://github.com/etcd-io/etcd/releases 安装 etcd123456[root@k8s-master1 opt]# mkdir -p /opt/src[root@k8s-master1 opt]# mv TLS.tar.gz etcd.tar.gz /opt/src/[root@k8s-master1 src]# ls -ltotal 15628-rw-r--r-- 1 root root 10148977 Nov 12 17:12 etcd.tar.gz-rw-r--r-- 1 root root 5851667 Nov 12 16:49 TLS.tar.gz 12# 解压[root@k8s-master1 src]# tar -zxvf etcd.tar.gz -C /opt/ 123456[root@k8s-master1 opt]# ls -l /opt/total 16drwxr-xr-x 5 root root 4096 Oct 2 22:13 etcd # etcd 工作目录-rw-r--r-- 1 root root 1078 Oct 2 23:10 etcd.service # etcd 启动服务drwxr-xr-x 2 root root 4096 Nov 12 17:12 srcdrwxr-xr-x 4 root root 4096 Oct 3 08:26 TLS 12345678910111213141516171819202122232425262728293031323334353637# 查看启动文件 注意工作目录 # 启动文件里面设置了etct的工作目录 /opt/etcd/bin/etcd 和 配置文件 /opt/etcd/cfg/etcd.conf# 有2种配置证书的选项 但是使用的都是相同的一套证书 # 集群内容使用证书 peer开头3条配置选项# 面向客户端的证书 cert-file、key-file、trusted-ca-file [root@k8s-master1 opt]# vim etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.confExecStart=/opt/etcd/bin/etcd \ --name=$&#123;ETCD_NAME&#125; \ --data-dir=$&#123;ETCD_DATA_DIR&#125; \ --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \ --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster-state=new \ --cert-file=/opt/etcd/ssl/server.pem \ --key-file=/opt/etcd/ssl/server-key.pem \ --peer-cert-file=/opt/etcd/ssl/server.pem \ --peer-key-file=/opt/etcd/ssl/server-key.pem \ --trusted-ca-file=/opt/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 12345678# 查看工作目录# 更换etcd版本:可从etcd的github下载最新版本 替换bin下的二进制文件[root@k8s-master1 opt]# cd /opt/etcd[root@k8s-master1 etcd]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Oct 2 22:13 bin # etcd 可执行文件 etcdctl 客户端 drwxr-xr-x 2 root root 4096 Oct 3 08:32 cfg # 配置文件drwxr-xr-x 2 root root 4096 Oct 3 08:36 ssl # 证书文件 需要将 etcd 证书加入里面 123# 先删除掉之前的证书,一会替换刚才生成的[root@k8s-master1 etcd]# cd ssl[root@k8s-master1 ssl]# rm -rf * 修改 etcd 配置文件12345678910111213141516171819202122232425262728# 节点之间通信 2380,面向客户端 2379 ,需要https通信 拥有证书# 如果要重装etcd 需要清空下面的目录 ETCD_DATA_DIR="/var/lib/etcd/default.etcd" # 不同的etcd节点需要修改 ip [root@k8s-master1 ssl]# cd /opt/etcd/cfg/[root@k8s-master1 cfg]# vim etcd.conf #[Member]# 集群节点唯一名称 不同的etcd节点需要修改 ETCD_NAME="etcd-1"# 数据目录 ETCD_DATA_DIR="/var/lib/etcd/default.etcd"# etcd 集群内部通信使用的urlETCD_LISTEN_PEER_URLS="https://172.17.70.251:2380"# 外部客户端使用的url,外部程序连接ETCD_LISTEN_CLIENT_URLS="https://172.17.70.251:2379"#[Clustering]# 给集群内其他成员访问的url,节点之间通信ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.17.70.251:2380"# 外部客户端使用的url ETCD_ADVERTISE_CLIENT_URLS="https://172.17.70.251:2379"# 初始集群成员列表,集群节点信息 name+IP+端口ETCD_INITIAL_CLUSTER="etcd-1=https://172.17.70.251:2380,etcd-2=https://172.17.70.253:2380,etcd-3=https://172.17.70.254:2380"# 集群的名称认证 判断是否该集群的节点ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"# 初始集群状态，new为新建集群, 已有加入需要将其设置为"exist"。ETCD_INITIAL_CLUSTER_STATE="new" 拷贝证书1234567# 将证书copy 到ssl目录[root@k8s-master1 ssl]# cp /opt/TLS/etcd/&#123;ca,server-key,server&#125;.pem .[root@k8s-master1 ssl]# ls -ltotal 12-rw-r--r-- 1 root root 1265 Nov 12 17:50 ca.pem-rw------- 1 root root 1679 Nov 12 17:50 server-key.pem-rw-r--r-- 1 root root 1338 Nov 12 17:50 server.pem 部署其他 etcd 节点1234# scp分发 再修改配置文件的 ETCD_NAME 和 IP [root@k8s-master1 opt]# scp -r etcd root@172.17.70.253:/opt/[root@k8s-master1 opt]# scp -r etcd root@172.17.70.254:/opt/# 千万别忘记修改配置文件 拷贝启动服务文件1234# 本机的别忘记了 [root@k8s-master1 opt]# scp etcd.service root@172.17.70.251:/usr/lib/systemd/system/[root@k8s-master1 opt]# scp etcd.service root@172.17.70.253:/usr/lib/systemd/system/[root@k8s-master1 opt]# scp etcd.service root@172.17.70.254:/usr/lib/systemd/system/ 启动服务12345# 启动第一个节点会一直处于等待状态,直到有集群节点加入# 全部etcd节点启动[root@k8s-master1 opt]# systemctl daemon-reload[root@k8s-master1 opt]# systemctl start etcd[root@k8s-master1 opt]# systemctl enable etcd 服务检查12# 日志查看[root@k8s-master1 opt]# tailf /var/log/messages 1234567891011121314# etcdctl 相当于客户端访问 # 别的应用程序想使用etcd集群必须要制定这3个证书,指定endpoints3个地址+端口 # k8s-apiserver也要指定etcd三个证书和三个集群节点 # 查看集群状态 [root@k8s-master1 opt]# /opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints="https://172.17.70.251:2379,https://172.17.70.253:2379,https://172.17.70.254:2379" \cluster-healthmember 262549f6716e3f5 is healthy: got healthy result from https://172.17.70.254:2379member 2d2843149c61e4a9 is healthy: got healthy result from https://172.17.70.253:2379member 49b9eb405dd15ee9 is healthy: got healthy result from https://172.17.70.251:2379cluster is healthy 123456789# 查询 member 列表[root@k8s-master1 opt]# /opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints="https://172.17.70.251:2379,https://172.17.70.253:2379,https://172.17.70.254:2379" \member list262549f6716e3f5: name=etcd-3 peerURLs=https://172.17.70.254:2380 clientURLs=https://172.17.70.254:2379 isLeader=false2d2843149c61e4a9: name=etcd-2 peerURLs=https://172.17.70.253:2380 clientURLs=https://172.17.70.253:2379 isLeader=false49b9eb405dd15ee9: name=etcd-1 peerURLs=https://172.17.70.251:2380 clientURLs=https://172.17.70.251:2379 isLeader=true 部署 Master 节点自签 APIServer SSL证书1234567[root@k8s-master1 k8s]# ls -ltotal 20-rw-r--r-- 1 root root 294 Oct 3 13:12 ca-config.json-rw-r--r-- 1 root root 263 Oct 3 13:12 ca-csr.json-rwxr-xr-x 1 root root 321 Oct 3 08:46 generate_k8s_cert.sh-rw-r--r-- 1 root root 230 Oct 3 13:12 kube-proxy-csr.json # 为 node上的kube-proxy 颁发证书-rw-r--r-- 1 root root 718 Oct 3 08:45 server-csr.json # 为 aipserver颁发证书 123456789101112131415161718[root@k8s-master1 k8s]# cat kube-proxy-csr.json &#123; "CN": "system:kube-proxy", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445# apiserver 证书# hosts 里面添加可信任的IP # 应用程序(服务器IP) 访问 https API (自签证书)# 1. 证书添加IP可信任 # 2. 携带CA证书去访问(http转发怎么办)# 有谁会去访问? LB主从,vip,master 写多点没有关系 根据规划 nodeip 其实可以不用写[root@k8s-master1 k8s]# vim server-csr.json &#123; "CN": "kubernetes", "hosts": [ "10.0.0.1", # service 第一个IP地址 "127.0.0.1", # 本地 "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local", "172.17.70.251", "172.17.70.252", "172.17.70.253", "172.17.70.254", "172.17.71.1", "172.17.71.2", "172.17.71.3", "172.17.71.4", "172.17.71.5", "172.17.71.6" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125; 12# 执行脚本 生成证书[root@k8s-master k8s]# ./generate_k8s_cert.sh 1234567[root@k8s-master1 k8s]# ls -l *.pem-rw------- 1 root root 1675 Nov 13 10:22 ca-key.pem-rw-r--r-- 1 root root 1359 Nov 13 10:22 ca.pem-rw------- 1 root root 1679 Nov 13 10:22 kube-proxy-key.pem-rw-r--r-- 1 root root 1403 Nov 13 10:22 kube-proxy.pem-rw------- 1 root root 1679 Nov 13 10:22 server-key.pem-rw-r--r-- 1 root root 1684 Nov 13 10:22 server.pem 部署 apiserver12345678# 部署步骤配置文件 -&gt; systemd管理组件 -&gt; 启动# 二进制包下载地址# 1. 下载Server Binaries node的也在里面# 2. 页面打不开 用迅雷下载# 3. 将需要使用的包 替换 如master节点里面的https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161 上传解压压缩包1234567# k8s-master.tar.gz[root@k8s-master1 src]# ls -ltotal 104276-rw-r--r-- 1 root root 1078 Nov 12 17:20 etcd.service-rw-r--r-- 1 root root 10148977 Nov 12 17:12 etcd.tar.gz-rw-r--r-- 1 root root 90767613 Nov 13 10:30 k8s-master.tar.gz-rw-r--r-- 1 root root 5851667 Nov 12 16:49 TLS.tar.gz 12# 解压 得到三个服务的启动文件 和 kubernetes的工作目录[root@k8s-master1 src]# tar -zxvf k8s-master.tar.gz -C /opt/ 123456[root@k8s-master1 kubernetes]# ls -ltotal 16drwxr-xr-x 2 root root 4096 Oct 3 09:06 bindrwxr-xr-x 2 root root 4096 Oct 3 08:55 cfgdrwxr-xr-x 2 root root 4096 Oct 2 23:13 logsdrwxr-xr-x 2 root root 4096 Oct 3 10:34 ssl 拷贝证书123456789[root@k8s-master1 ssl]# cd /opt/kubernetes/ssl/[root@k8s-master1 ssl]# cp /opt/TLS/k8s/*.pem .[root@k8s-master1 ssl]# rm -rf kube-proxy* # master不需要 kube-proxu证书[root@k8s-master1 ssl]# ls -ltotal 24-rw------- 1 root root 1675 Nov 13 10:34 ca-key.pem-rw-r--r-- 1 root root 1359 Nov 13 10:34 ca.pem-rw------- 1 root root 1679 Nov 13 10:34 server-key.pem # apiserver公钥 master访问-rw-r--r-- 1 root root 1684 Nov 13 10:34 server.pem # apiserver证书 master访问 apiserver 配置文件12# apiserver 配置文件官方文档https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master1 cfg]# vim kube-apiserver.conf KUBE_APISERVER_OPTS="--logtostderr=false \# 输出日志级别,一般设置2,从小到大越来越详细--v=2 \# 日志目录--log-dir=/opt/kubernetes/logs \# etcd 集群节点 需要修改--etcd-servers=https://192.168.31.61:2379,https://192.168.31.62:2379,https://192.168.31.63:2379 \# 内网IP--bind-address=192.168.31.61 \# 端口--secure-port=6443 \# 通告地址 告诉node连接哪里 一般是内网--advertise-address=192.168.31.61 \# 使用超级权限创建容器--allow-privileged=true \# 重要,serviceIP范围 --service-cluster-ip-range=10.0.0.0/24 \# 启用k8s高级插件--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \# 鉴权模式 访问apiserver 通过RBAC访问 基于角色 Noda自动认证--authorization-mode=RBAC,Node \# bootstrap:为每个node办法证书，node需要请求apiserver,node告诉apiserver我要加入集群,请为我颁发证书# 给每个node一个最低权限的用户，然后来请求apiserver--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \# service 端口范围--service-node-port-range=30000-32767 \# apiserver 访问 kubelet 使用的证书# kubelet 只能让 aipserver访问--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \# apiserver https 使用证书--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \# service-account 像是个用户--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \# 连接etcd的证书--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \# 日志审计 保留多少天--audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log" 部署 controller-manager配置文件12345678910111213141516171819202122232425262728[root@k8s-master1 cfg]# vim kube-controller-manager.conf KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \# 日志--v=2 \--log-dir=/opt/kubernetes/logs \# 集群选举 本身高可用启动--leader-elect=true \# 连接 apiserver 8080是apiserver默认监听端口--master=127.0.0.1:8080 \--address=127.0.0.1 \# 允许cni插件 自动分配IP --allocate-node-cidrs=true \# 要与cni插件的 集群网段一致--cluster-cidr=10.244.0.0/16 \# serviceIP 范围 要与 apiserver里面一致--service-cluster-ip-range=10.0.0.0/24 \# 集群签名证书 node 加入集群 会自动办法 kubelet证书 # kubelet证书 由controller-manager 颁发# controller-manager 使用下面的ca 办法证书--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \# 签署 service-account 证书 --root-ca-file=/opt/kubernetes/ssl/ca.pem \# service-account 私钥--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \# kubelet证书的有效期时间--experimental-cluster-signing-duration=87600h0m0s" 部署 schedulerscheduler 配置文件1234567891011[root@k8s-master1 cfg]# vim kube-scheduler.conf KUBE_SCHEDULER_OPTS="--logtostderr=false \# 日志--v=2 \--log-dir=/opt/kubernetes/logs \# 参加选举--leader-elect \# 连接本地的apiserver--master=127.0.0.1:8080 \--address=127.0.0.1" 修改配置文件123456781. 只需要修改 apiserver.conf 里面的etcd集群和本地IP2. 剩下的两个服务都是连接本地[root@k8s-master1 cfg]# vim kube-apiserver.conf --etcd-servers=https://172.17.70.251:2379,https://172.17.70.253:2379,https://172.17.70.254:2379 \--bind-address=172.17.70.251 \--secure-port=6443 \--advertise-address=172.17.70.251 \ 配置服务启动文件1234567[root@k8s-master1 opt]# mv kube*.service /usr/lib/systemd/system/[root@k8s-master1 opt]# ls -l /usr/lib/systemd/system/kube-*-rw-r--r-- 1 root root 286 Oct 2 23:13 /usr/lib/systemd/system/kube-apiserver.service-rw-r--r-- 1 root root 321 Oct 2 23:13 /usr/lib/systemd/system/kube-controller-manager.service-rw-r--r-- 1 root root 285 Oct 2 23:13 /usr/lib/systemd/system/kube-scheduler.service 启动服务123456systemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 查看日志123456# 查看日志[root@k8s-master opt]# cd /opt/kubernetes/logs/[root@k8s-master1 logs]# less /opt/kubernetes/logs/kube-apiserver.INFO[root@k8s-master1 logs]# less /opt/kubernetes/logs/kube-controller-manager.INFO [root@k8s-master1 logs]# less /opt/kubernetes/logs/kube-scheduler.INFO kubectl配置12# 将kubectl 放入/usr/bin 方便使用[root@k8s-master1 kubernetes]# cp /opt/kubernetes/bin/kubectl /usr/local/bin/ 查看集群状态1234567891011[root@k8s-master1 kubernetes]# kubectl get nodeNo resources found in default namespace.# 过会回更新成功,能看到即可,失败会有错误[root@k8s-master1 kubernetes]# kubectl get csNAME AGEcontroller-manager &lt;unknown&gt;scheduler &lt;unknown&gt;etcd-2 &lt;unknown&gt;etcd-1 &lt;unknown&gt;etcd-0 &lt;unknown&gt; 查看进程1[root@k8s-master1 kubernetes]# ps -ef|grep kube 启用 TLS Bootstrapping12345678# 启用了 为 kubelet TLS Bootstrapping 授权# bootstrap: # 1. 为每个node办法证书，node需要请求apiserver,node告诉apiserver我要加入集群,请为我颁发证书# 2. 给每个node一个最低权限的用户，然后来请求apiserver# 机器越来越多手动颁发证书比较麻烦,这个机制可以自动颁发证书[root@k8s-master1 kubernetes]# cat /opt/kubernetes/cfg/kube-apiserver.conf |grep bootstrap--enable-bootstrap-token-auth=true \ 12345678910# 引用的token文件[root@k8s-master1 kubernetes]# cat /opt/kubernetes/cfg/kube-apiserver.conf |grep token--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \# 查看token # 账号的格式: token,用户,uid,用户组 # 只要带着token过来 系统的这个用户就给帮忙办法证书[root@k8s-master1 kubernetes]# cat /opt/kubernetes/cfg/token.csvc47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,"system:node-bootstrapper" 给 kubelet-bootstrap授权1234567891011# 如果不给与权限，token可以用但是用户没有权限 也不会正常办法# 将 kubelet-bootstrap 用户绑定到 "system:node-bootstrapper" 组里 # 这个组是系统自带的 主要用于让node能使用bootstrapper办法证书,权限比较低# 赋予 system:node-bootstrapper 权限 kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap# 提示成功clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created token 自行生成替换123# apiserver配置的token必须要与node节点bootstrap.kubeconfig配置里一致。# 部署node的时候会看到head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 部署 Node 节点12172.17.70.253 k8s-node1172.17.70.254 k8s-node2 Docker 二进制安装12# Docker 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ 上传解压安装包1234[root@k8s-node1 ~]# mkdir -p /opt/src[root@k8s-node1 src]# ls -l-rw-r--r-- 1 root root 128129460 Nov 13 14:23 k8s-node.tar.gz[root@k8s-node1 src]# tar -zxvf k8s-node.tar.gz 二进制安装docker123[root@k8s-node1 src]# tar -zxvf docker-18.09.6.tgz [root@k8s-node1 src]# mv docker/* /usr/bin/[root@k8s-node1 src]# mv docker.service /usr/lib/systemd/system docker配置文件12345678[root@k8s-node1 src]# mkdir -p /etc/docker[root@k8s-node1 src]# mv daemon.json /etc/docker/[root@k8s-node1 src]# cat /etc/docker/daemon.json &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], # 镜像加速 "insecure-registries": ["172.17.70.252"] # 可信任IP 回头换成harbor地址&#125; 启动docker服务123[root@k8s-node1 src]# systemctl start docker[root@k8s-node1 src]# systemctl enable docker[root@k8s-node1 src]# docker info 部署 harbor仓库1234567# 该主机也需要二进制安装docker [root@k8s-master2 src]# scp root@172.17.70.254:/opt/src/docker-18.09.6.tgz .[root@k8s-master2 src]# scp root@172.17.70.254:/usr/lib/systemd/system/docker.service .[root@k8s-master2 src]# scp root@172.17.70.254:/etc/docker/daemon.json .[root@k8s-master2 src]# tar -zxvf docker-18.09.6.tgz [root@k8s-master2 src]# mv docker/* /usr/bin/[root@k8s-master2 src]# mv docker.service /usr/lib/systemd/system 12345678[root@k8s-master2 src]# mkdir -p /etc/docker[root@k8s-master2 src]# mv daemon.json /etc/docker/[root@k8s-master2 src]# cat /etc/docker/daemon.json &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], # 镜像加速 "insecure-registries": ["172.17.70.252"] # 可信任IP 回头换成harbor地址&#125; 123[root@k8s-node1 src]# systemctl start docker[root@k8s-node1 src]# systemctl enable docker[root@k8s-node1 src]# docker info 1234# 上传文件[root@k8s-master2 src]# ls -l-rw-r--r-- 1 root root 17237024 Nov 13 14:33 docker-compose-Linux-x86_64-rw-r--r-- 1 root root 580462944 Nov 13 14:34 harbor-offline-installer-v1.8.4.tgz 12345# 部署 compose[root@k8s-master2 src]# mv docker-compose-Linux-x86_64 /usr/local/bin/docker-compose[root@k8s-master2 src]# chmod +x /usr/local/bin/docker-compose [root@k8s-master2 src]# docker-compose -versiondocker-compose version 1.25.0dev, build bc57a1bd 1234567891011121314151617# 部署 harbor[root@k8s-master2 src]# tar -xf harbor-offline-installer-v1.8.4.tgz -C /opt/[root@k8s-master2 src]# cd /opt/harbor/# 修改主机名和管理员密码、数据库密码hostname: 172.17.70.252 # httpharbor_admin_password: 123456 # 访问密码database: password: 123456# 准备[root@k8s-master2 src]# ./prepare# 安装[root@k8s-master2 src]# ./install.sh# web访问http://123.56.14.192# 列出docker-compose ps 免https使用12345678910111213141516[root@Docker harbor]# vim /etc/docker/daemon.json # 写入进项仓库 IP+port&#123; "registry-mirrors": ["http://f1361db2.m.daocloud.io"], "insecure-registries": ["172.17.70.252"]&#125;# 重启docker systemctl daemon-reloadsystemctl restart docker.service[root@Docker nginx]# docker infoInsecure Registries: 172.17.70.252 127.0.0.0/8 登录1234# 两个node节点都先登录下 好下载基础镜像[root@k8s-node1 cfg]# docker login 172.17.70.252Username: adminPassword: 下载 pod基础镜像 并上传到私有仓库1234[root@k8s-master2 opt]# docker pull lizhenliang/pause-amd64:3.0 [root@k8s-master2 opt]# docker tag lizhenliang/pause-amd64:3.0 172.17.70.252/base/pause-amd64:3.0[root@k8s-master2 opt]# docker login 172.17.70.252[root@k8s-master2 opt]# docker push 172.17.70.252/base/pause-amd64:3.0 部署 kubelet配置文件介绍1234567891011121314[root@k8s-node1 src]# tree kubernetes/kubernetes/├── bin│ ├── kubelet│ └── kube-proxy├── cfg│ ├── bootstrap.kubeconfig│ ├── kubelet.conf│ ├── kubelet-config.yml│ ├── kube-proxy.conf│ ├── kube-proxy-config.yml│ └── kube-proxy.kubeconfig├── logs└── ssl 123456789101112# 1. conf 基本配置文件# 2. kubeconfig 连接apiserver配置文件# 3. yml 主要配置文件 早起版本没有yml配置文件 [root@k8s-node1 src]# tree kubernetes/cfg/kubernetes/cfg/├── bootstrap.kubeconfig├── kubelet.conf├── kubelet-config.yml├── kube-proxy.conf├── kube-proxy-config.yml # 可以动态更新配置文件 beta kube-proxy└── kube-proxy.kubeconfig 配置 kubelet.conf123456789101112131415161718192021222324[root@k8s-node1 src]# mv kubernetes /opt/[root@k8s-node1 cfg]# vim /opt/kubernetes/cfg/kubelet.conf # 日志KUBELET_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \# 主机名 注册到k8s 名称唯一--hostname-override=k8s-node1 \# 启用cni网络插件--network-plugin=cni \# kubelet.kubeconfig 配置文件路径--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \# bootstrap.kubeconfig 配置文件路径--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \# yaml 配置文件路径--config=/opt/kubernetes/cfg/kubelet-config.yml \# 给kubelet颁发的证书路径--cert-dir=/opt/kubernetes/ssl \# 基础镜像 启动每个pod都会启动的镜像 # 管理pod的命名空间 这个后期可以换成私有仓库的# 这个镜像可以先放到公有仓库,以免认证# --pod-infra-container-image=lizhenliang/pause-amd64:3.0" --pod-infra-container-image=172.17.70.252/base/pause-amd64:3.0" 配置 bootstrap.kubeconfig123456789101112131415161718192021222324252627# 用于自动办法kubelet证书# 他会先连接apiserver,master会办法证书到node上# kubectl config 生成 [root@k8s-node1 cfg]# vim bootstrap.kubeconfig apiVersion: v1clusters:- cluster: # 携带 ca 证书 certificate-authority: /opt/kubernetes/ssl/ca.pem # master 地址 server: https://172.17.70.251:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubelet-bootstrap name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kubelet-bootstrap user: # 务必于master上的token.csv里的一致 token: c47ffb939f5ca36231d9e3121a252940 TLS Bootstrapping 机制流程（kubelet）12345671. node节点上的 kubelet 启动2. 根据 bootstrap.kubeconfig 配置文件 去请求apiserver3. apiserver 会效验 token 是否是可用可信任的4. apiserver 会效验 ca 证书5. 都通过之后 给node上的kubelet颁发证书6. 启动成功7. 如果启动失败 那么就是没有认证通过 查看token和使用的证书 配置 kubelet-config.yml1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-node1 cfg]# vim kubelet-config.yml kind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 0.0.0.0# kubelet端口port: 10250readOnlyPort: 10255# 需要与 docker info | grep cgroupfs 一致cgroupDriver: cgroupfs# kubelet 默认配置的内部dns地址 为了设置dns服务clusterDNS:- 10.0.0.2# 集群的域clusterDomain: cluster.local# 不适用swapfailSwapOn: false# kubelet 授权authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /opt/kubernetes/ssl/ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s# 垃圾回收策略evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%# 最大文件打开数maxOpenFiles: 1000000# 最大跑的pod数量 maxPods: 110 部署 kube-proxy配置 kube-proxy.conf1234567[root@k8s-node1 cfg]# vim kube-proxy.conf # 日志KUBE_PROXY_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \# 指定yml文件路径--config=/opt/kubernetes/cfg/kube-proxy-config.yml" 配置 kube-proxy.kubeconfig12345678910111213141516171819202122232425262728# kube-proxy 连接 apiserver的配置文件 # 每个组件都会连接 apiserver # kubelet的需要node认证会 自动生成[root@k8s-node1 cfg]# vim kube-proxy.kubeconfig apiVersion: v1clusters:- cluster: # ca 证书 certificate-authority: /opt/kubernetes/ssl/ca.pem # apiserver 地址 server: https://172.17.70.251:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: # kube-proxy 证书 client-certificate: /opt/kubernetes/ssl/kube-proxy.pem client-key: /opt/kubernetes/ssl/kube-proxy-key.pem 配置 kube-proxy-config.yml1234567891011121314151617181920212223# 动态调整kube-proxy 配置[root@k8s-node1 cfg]# vim kube-proxy-config.yml kind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1# 监听地址address: 0.0.0.0# 监听端口metricsBindAddress: 0.0.0.0:10249clientConnection: # 客户端连接配置文件 kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig# 注册名称hostnameOverride: k8s-node1# 集群 service 网段clusterCIDR: 10.0.0.0/24# 网络模式:ipvs性能较好 也可以选择iptables mode: ipvsipvs: scheduler: "rr"iptables: masqueradeAll: true 拉取证书12[root@k8s-master1 k8s]# cd /opt/TLS/k8s/[root@k8s-master1 k8s]# scp ca.pem kube-proxy*.pem root@172.17.70.253:/opt/kubernetes/ssl/ 配置启动服务12[root@k8s-node1 opt]# cd /opt/src/[root@k8s-node1 src]# mv *.service /usr/lib/systemd/system/ 启动服务123456[root@k8s-node1 opt]# systemctl start kubelet[root@k8s-node1 opt]# systemctl enable kubelet# 认证之后再启动 kube-proxy[root@k8s-node1 opt]# systemctl start kube-proxy[root@k8s-node1 opt]# systemctl enable kube-proxy 1234567# 查看日志[root@k8s-node1 logs]# tailf /opt/kubernetes/logs/kubelet.INFO # 使用 bootstrap kubeconfig 去向apiserver请求颁发证书# 如果这块是拒绝的 那么看看token是否一致,是否授权,ca证书是否一致I1113 16:07:13.213148 1949 bootstrap.go:119] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig fileI1113 16:07:13.214389 1949 bootstrap.go:150] No valid private key and/or certificate found, reusing existing private key or creating a new one 允许给Node颁发证书1234# 在master上查看是否有新的node 请求办法证书[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-TyUXN4xk-rZIW1XjZg3IY1fD_3_pfgE-4-l7BzwWUvI 4m32s kubelet-bootstrap Pending 12345# 允许该node加入集群并颁发证书# kubectl certificate approve "NodeNAME" [root@k8s-master1 k8s]# kubectl certificate approve node-csr-TyUXN4xk-rZIW1XjZg3IY1fD_3_pfgE-4-l7BzwWUvI certificatesigningrequest.certificates.k8s.io/node-csr-TyUXN4xk-rZIW1XjZg3IY1fD_3_pfgE-4-l7BzwWUvI approved 1234# 查看node kubelet日志 # 变成不能更新cni配置 因为我们还没有部署cniE1113 16:15:19.537610 1949 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedW1113 16:15:21.440199 1949 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d 1234# 现在node还是准备状态 需要部署完cni插件 [root@k8s-master1 k8s]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 2m15s v1.16.0 自动为 node 颁发证书1234567891011# 是否为 node 颁发了证书 [root@k8s-node1 ssl]# ls -ltotal 24-rw-r--r-- 1 root root 1359 Nov 13 16:02 ca.pem-rw------- 1 root root 1269 Nov 13 16:14 kubelet-client-2019-11-13-16-14-06.pem # 自动颁发的证书 加入了集群lrwxrwxrwx 1 root root 58 Nov 13 16:14 kubelet-client-current.pem -&gt; /opt/kubernetes/ssl/kubelet-client-2019-11-13-16-14-06.pem-rw-r--r-- 1 root root 2173 Nov 13 16:07 kubelet.crt-rw------- 1 root root 1675 Nov 13 16:07 kubelet.key-rw------- 1 root root 1679 Nov 13 16:02 kube-proxy-key.pem-rw-r--r-- 1 root root 1403 Nov 13 16:02 kube-proxy.pem 自动生成配置文件 kubelet.kubeconfig12345678910111213141516171819202122232425262728# 自动生成 kubelet.kubeconfig 配置文件# kubelet.kubeconfig 负载连接 apiserver # 使用颁发的证书 kubelet-client-current.pem[root@k8s-node1 ssl]# cd /opt/kubernetes/cfg/[root@k8s-node1 cfg]# vim kubelet.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem server: https://172.17.70.251:6443 name: default-clustercontexts:- context: cluster: default-cluster namespace: default user: default-auth name: default-contextcurrent-context: default-contextkind: Configpreferences: &#123;&#125;users:- name: default-auth user: client-certificate: /opt/kubernetes/ssl/kubelet-client-current.pem client-key: /opt/kubernetes/ssl/kubelet-client-current.pem 加入 node2 节点123# 相当于重新部署一套 流程一致# 别忘记安装docker # 别忘记拷贝证书 配置文件修改的地方12345678910111213# IP地址 后期可以改完 vip # bootstrap.kubeconfig kube-proxy.kubeconfig[root@k8s-node2 cfg]# grep 251 *[root@k8s-node2 cfg]# grep 251 *bootstrap.kubeconfig: server: https://172.17.70.251:6443kube-proxy.kubeconfig: server: https://172.17.70.251:6443# 文件中主机名 # kubelet.conf kube-proxy-config.yml[root@k8s-node2 cfg]# grep hostname *[root@k8s-node2 cfg]# grep hostname *kubelet.conf:--hostname-override=k8s-node2 \kube-proxy-config.yml:hostnameOverride: k8s-node2 12345678910111213# 同意node2 加入集群[root@k8s-master1 k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-DHJS3UYUAxg7KPrO22SG_Q_4PIUZzndAt_gSVhiMWKc 46s kubelet-bootstrap Pending # 等待node-csr-TyUXN4xk-rZIW1XjZg3IY1fD_3_pfgE-4-l7BzwWUvI 38m kubelet-bootstrap Approved,Issued # 同意[root@k8s-master1 k8s]# kubectl certificate approve node-csr-DHJS3UYUAxg7KPrO22SG_Q_4PIUZzndAt_gSVhiMWKccertificatesigningrequest.certificates.k8s.io/node-csr-DHJS3UYUAxg7KPrO22SG_Q_4PIUZzndAt_gSVhiMWKc approved[root@k8s-master1 k8s]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 32m v1.16.0k8s-node2 NotReady &lt;none&gt; 8s v1.16.0 1234567# 应该先加入节点 再启动kukube-proxy 如果先启动了 就认证后重启下 kube-proxy 服务[root@k8s-node2 logs]# tailf /opt/kubernetes/logs/kube-proxy.ERROR E1113 16:46:57.637897 1956 node.go:124] Failed to retrieve node info: nodes "k8s-node2" not found[root@k8s-node2 logs]# systemctl stop kube-proxy[root@k8s-node2 logs]# rm -rf kube-proxy.*[root@k8s-node2 logs]# systemctl start kube-proxy k8s的分层结构 部署 CNI 网络 (Flannel)12# 二进制包下载地址：https://github.com/containernetworking/plugins/releases 12345# 当前的错误日志# 网络没有发现 网络信息在 /etc/cni/net.d下[root@k8s-node1 cfg]# tail /opt/kubernetes/logs/kubelet.INFO E1113 16:57:39.952919 1949 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedW1113 16:57:41.503503 1949 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d cni 网络插件接口介绍 cni 网络插件 是k8s的网络接口 用于对接第三网网络插件为pod提供网络 必须符合cni的标准 12# cni只是第三方网络接口 看看这些网络都由哪些https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ cni 网络插件接口部署123456# 在node节点 解压插件[root@k8s-node1 opt]# cd /opt/src/# 创建工作目录[root@k8s-node1 src]# mkdir -p /opt/cni/bin # 二进制文件目录,默认kubelet从这个目录找可执行文件 为pod创建网络[root@k8s-node1 src]# mkdir -p /etc/cni/net.d # 生成配置信息目录 [root@k8s-node1 src]# tar -zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin 123# copy给其他node(node2) 别忘记创建目录[root@k8s-node1 bin]# scp -r /opt/cni root@172.17.70.254:/opt/[root@k8s-node2 logs]# mkdir -p /opt/cni/bin \ 123# 确保启用cni [root@k8s-node1 opt]# cat /opt/kubernetes/cfg/kubelet.conf | grep cni--network-plugin=cni \ 部署 flannel 网络插件1234# 在master上部署# 上传yaml文件[root@k8s-master1 src]# ls -l kube-flannel.yaml -rw-r--r-- 1 root root 5032 Nov 13 17:22 kube-flannel.yaml 1234567891011# 实际上是个pod# flannel 要连接apiserver 需要授权 ServiceAccount# cni配置网络信息 会写到 node的/etc/cni/net.d/目录下 然后让kubelet使用# 10.244.0.0/16 使用的网络# 他与 apiserver 里面配置的一致 # [root@k8s-master1 ~]# cat /opt/kubernetes/cfg/kube-controller-manager.conf | grep cluster-cidr# --cluster-cidr=10.244.0.0/16 \# DaemonSet 方式部署 说明每个node都会部署一个 他再每个node上都要保持路由表的维护# 使用宿主机网络 保证宿主机跨主机通信# 关键点 他需要是从网上找镜像 image: lizhenliang/flannel:v0.11.0-amd64# 我们可以去网上下载好这个镜像 部署到自己仓库中 123456789101112131415161718192021222324252627282930[root@k8s-master1 src]# vim kube-flannel.yaml data: cni-conf.json: | &#123; "cniVersion": "0.2.0", "name": "cbr0", "plugins": [ &#123; "type": "flannel", "delegate": &#123; "hairpinMode": true, "isDefaultGateway": true &#125; &#125;, &#123; "type": "portmap", "capabilities": &#123; "portMappings": true &#125; &#125; ] &#125; net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125; 启动pod1234567[root@k8s-master1 src]# kubectl apply -f kube-flannel.yaml podsecuritypolicy.policy/psp.flannel.unprivileged createdclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.apps/kube-flannel-ds-amd64 created 查看pod是否启动123456# 会从 lizhenliang/flannel:v0.11.0-amd64 下载镜像有点慢# 离线部署 提前下载好 导到每个node上本地要是由就不会拉取了[root@k8s-master1 src]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-b48fs 1/1 Running 0 35skube-flannel-ds-amd64-ztvl4 1/1 Running 0 35s 1234567891011121314# 查看pod事件状态 如果是aliyun主机 别忘记node也要出公网啊[root@k8s-master1 opt]# kubectl describe pod kube-flannel-ds-amd64-b48fs -n kube-systemEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned kube-system/kube-flannel-ds-amd64-b48fs to k8s-node2 Normal Pulling 74s kubelet, k8s-node2 Pulling image "lizhenliang/flannel:v0.11.0-amd64" Normal Pulled 70s kubelet, k8s-node2 Successfully pulled image "lizhenliang/flannel:v0.11.0-amd64" Normal Created 70s kubelet, k8s-node2 Created container install-cni Normal Started 70s kubelet, k8s-node2 Started container install-cni Normal Pulled 70s kubelet, k8s-node2 Container image "lizhenliang/flannel:v0.11.0-amd64" already present on machine Normal Created 70s kubelet, k8s-node2 Created container kube-flannel Normal Started 70s kubelet, k8s-node2 Started container kube-flannel 12# 没起来试试 [root@k8s-node2 bin]# systemctl restart kube-proxy 12345# 查看 node工作状态[root@k8s-master1 src]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 85m v1.16.0k8s-node2 Ready &lt;none&gt; 52m v1.16.0 查看pod日志123[root@k8s-master1 src]# kubectl logs kube-flannel-ds-amd64-nnrkz -n kube-systemError from server (NotFound): pods "kube-flannel-ds-amd64-nnrkz" not found# 没有权限 需要授权 通过logs命令可以查看pod日志 1234567# 上传授权yaml文件 并执行[root@k8s-master1 src]# ls -l /opt/src/apiserver-to-kubelet-rbac.yaml -rw-r--r-- 1 root root 745 Nov 13 17:42 /opt/src/apiserver-to-kubelet-rbac.yaml[root@k8s-master1 src]# kubectl apply -f apiserver-to-kubelet-rbac.yaml clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created 12345678910111213141516171819202122232425262728293031323334353637383940# 集群角色授权# 可以访问pod日志等# 赋予给 name: system:kube-apiserver-to-kubelet[root@k8s-master1 src]# vim /opt/src/apiserver-to-kubelet-rbac.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - "" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics - pods/log verbs: - "*"---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: ""roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes 1[root@k8s-master1 src]# kubectl logs kube-flannel-ds-amd64-b48fs -n kube-system 查看 node上的 pod 和网络123456789# 每个node上都会启动个pod# node上会增加flannel 虚拟网卡# pod 通过flannel 跨主机访问 传输数据[root@k8s-master opt]# kubectl get pods -n kube-system -o wide[root@k8s-master1 src]# kubectl get pods -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-flannel-ds-amd64-b48fs 1/1 Running 0 11m 172.17.70.254 k8s-node2 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-ztvl4 1/1 Running 0 11m 172.17.70.253 k8s-node1 &lt;none&gt; &lt;none&gt; 创建 nginx pod 测试部署环境1234567891011121. 当创建pod的时候 部署到的节点会创建cni网络 2. 相当于一个网桥，以后所有的pod流量都会经过这个网桥 pod都会加入到这里面[root@k8s-master1 src]# kubectl create deployment web --image=nginx:1.16deployment.apps/web created[root@k8s-master1 src]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-866f97c649-tj4mb 0/1 ContainerCreating 0 7s &lt;none&gt; k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master1 src]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-866f97c649-tj4mb 1/1 Running 0 113s 10.244.0.2 k8s-node1 &lt;none&gt; &lt;none&gt; 1234567891011# 暴露pod 到集群外部[root@k8s-master1 src]# kubectl expose deployment web --port=80 --type=NodePort service/web exposed[root@k8s-master1 src]# kubectl get pods,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/web-866f97c649-tj4mb 1/1 Running 0 3m40s 10.244.0.2 k8s-node1 &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 6h32m &lt;none&gt;service/web NodePort 10.0.0.225 &lt;none&gt; 80:30966/TCP 14s app=web 12345678# 测试访问node节点上的pod# 都能访问说明集群网络都正常 单master正常[root@k8s-master1 src]# curl 172.17.70.253:30966[root@k8s-master1 src]# curl 172.17.70.254:30966# web访问http://39.106.168.181:30966/http://39.106.100.108:30966/ 测试再加入一个node节点1231. 按照正常流程部署node节点2. 安装好cni插件后 master会帮忙创建网络 别忘记下载远程的 lizhenliang/flannel:v0.11.0-amd64 镜像 要出外网 或者部署本地仓库 K8S 高可用介绍 1234567LB : Nginx LVS HaProxy4层负载 端口转发 stream TCP 性能更好7层负载 业务转发 HTTP 应用层协议满足更多数据转发高可用软件: keepalived 创建vip 健康检查和故障转移 用户访问vip , node 连接 vip阿里云SLB实现 部署 master212345172.17.71.5 aliyun-slb 内网负载均衡172.17.70.245 k8s-master2172.17.70.246 k8s-master172.17.70.247 k8s-node1172.17.70.248 k8s-node2 123456789# 将kubernetes 传过去[root@k8s-master1 k8s]# scp -r /opt/kubernetes root@172.17.70.252:/opt# 管理服务的service文件[root@k8s-master1 k8s]# scp /usr/lib/systemd/system/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125;.service root@172.17.70.252:/usr/lib/systemd/system# 拷贝etcd证书目录[root@k8s-master2 opt]# mkdir -p /opt/etcd[root@k8s-master1 k8s]# scp -r /opt/etcd/ssl root@172.17.70.252:/opt/etcd 12345# 修改配置文件# 修改apiserver配置文件为本地IP：[root@k8s-master2 opt]# cat /opt/kubernetes/cfg/kube-apiserver.conf | grep 252--bind-address=172.17.70.252 \--advertise-address=172.17.70.252 \ 12# 传递 kubectl[root@k8s-master1 k8s]# scp /usr/local/bin/kubectl root@172.17.70.252:/usr/bin/ 12345678# 启动服务[root@k8s-node1 cfg]# systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 12345678910# 查看资源 有数据即为正常[root@k8s-master2 opt]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 16h v1.16.0[root@k8s-master2 opt]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-866f97c649-tj4mb 1/1 Running 1 15h 配置阿里云内网SLB1231. 创建内网负载均衡 得到vip vip需要再证书里啊2. 创建虚拟机服务器组3. 创建负载策略 TCP端口负载转向6443 123456# 测试vpi 6443端口访问[root@k8s-node2 ~]# telnet 172.17.71.5 6443Trying 172.17.71.5...Connected to 172.17.71.5.Escape character is '^]'.^CConnection closed by foreign host. 修改node节点连接配置12345678910111213# 每个node连接SLB的vip[root@k8s-node1 src]# cd /opt/kubernetes/cfg/[root@k8s-node1 cfg]# grep '6443' *bootstrap.kubeconfig: server: https://172.17.70.251:6443kubelet.kubeconfig: server: https://172.17.70.251:6443kube-proxy.kubeconfig: server: https://172.17.70.251:6443# 把IP地址修改成vip[root@k8s-node1 cfg]# sed -i 's#172.17.70.251:6443#172.17.71.5:6443#g' *[root@k8s-node1 cfg]# grep '6443' *bootstrap.kubeconfig: server: https://172.17.71.5:6443kubelet.kubeconfig: server: https://172.17.71.5:6443kube-proxy.kubeconfig: server: https://172.17.71.5:6443 123# 重启kubelet 和 kube-proxy服务[root@k8s-node1 cfg]# systemctl restart kubelet[root@k8s-node1 cfg]# systemctl restart kube-proxy 验证12345# 还可以访问各节点[root@k8s-master1 k8s]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0 1234567891011# 之前节点都连接的 master1上的api # 关闭掉master1上的 apiserver服务# 看看master2 还能不能得到nodesystemctl stop kube-apiserversystemctl stop kube-controller-managersystemctl stop kube-schedulersystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-scheduler 12345# 可以继续使用 证明vpi正常工作[root@k8s-master2 opt]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 17h v1.16.0k8s-node2 Ready &lt;none&gt; 17h v1.16.0 123456789101112131415161718192021222324252627282930313233343536# 测试VIP是否正常工作# 带着token去访问 vip apicurl -k --header "Authorization: Bearer c47ffb939f5ca36231d9e3121a252940" https://172.17.71.5:6443/version[root@k8s-node2 cfg]# curl -k --header "Authorization: Bearer c47ffb939f5ca36231d9e3121a252940" https://172.17.71.5:6443/version&#123; "major": "1", "minor": "16", "gitVersion": "v1.16.0", "gitCommit": "2bd9643cee5b3b3a5ecbd3af49d09018f0773c77", "gitTreeState": "clean", "buildDate": "2019-09-18T14:27:17Z", "goVersion": "go1.12.9", "compiler": "gc", "platform": "linux/amd64"&#125;# 释放负载后 就无法在创建pod了[root@k8s-master1 k8s]# kubectl create deployment web2 --image=nginx:1.16[root@k8s-master1 k8s]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-866f97c649-tj4mb 1/1 Running 1 16h 10.244.0.3 k8s-node1 &lt;none&gt; &lt;none&gt;web2-79f76c99dc-kvqg5 0/1 Pending 0 111s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/2 nodes are available: 2 node(s) had taints that the pod didn't tolerate.Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/2 nodes are available: 2 node(s) had taints that the pod didn't tolerate.[root@k8s-master1 k8s]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 17h v1.16.0k8s-node2 NotReady &lt;none&gt; 17h v1.16.0 ##]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-base03]]></title>
    <url>%2F2019%2F11%2F10%2Fk8s-base03%2F</url>
    <content type="text"><![CDATA[自签Etcd SSL证书配置主机名12345cat &gt;&gt; /etc/hosts &lt;&lt; EOF172.17.70.251 k8s-master1172.17.70.253 k8s-node1172.17.70.254 k8s-node2EOF 生成ca证书12345[root@k8s-master opt]# tar -zxvf TLS.tar.gz[root@k8s-master opt]# cd TLS[root@k8s-master TLS]# sh cfssl.sh [root@k8s-master TLS]# ls /usr/local/bin/cfssl cfssl-certinfo cfssljson 12345# 自建CA [root@k8s-master etcd]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -[root@k8s-master etcd]# ls -l *.pem-rw------- 1 root root 1675 Nov 6 08:40 ca-key.pem # ca的私钥-rw-r--r-- 1 root root 1265 Nov 6 08:40 ca.pem # ca的数字证书 可以拿着这两个去办法域名证书 生成 etcd 证书123456789101112131415161718192021222324# 指定etcd 可信任的IP地址 也就是当前要部署etcd的所有节点IP# 最关键的是 要包含每个etcd节点的ip[root@k8s-master1 etcd]# vim server-csr.json &#123; "CN": "etcd", "hosts": [ "172.17.70.251", "172.17.70.253", "172.17.70.254" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing" &#125; ]&#125; 1234567891011121314151617181920212223# 请求ca颁发证书[root@k8s-master1 etcd]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server[root@k8s-master1 etcd]# ls -ltotal 40-rw-r--r-- 1 root root 287 Oct 3 13:12 ca-config.json-rw-r--r-- 1 root root 956 Nov 12 11:05 ca.csr-rw-r--r-- 1 root root 209 Oct 3 13:12 ca-csr.json-rw------- 1 root root 1675 Nov 12 11:05 ca-key.pem-rw-r--r-- 1 root root 1265 Nov 12 11:05 ca.pem-rwxr-xr-x 1 root root 178 Oct 3 13:58 generate_etcd_cert.sh-rw-r--r-- 1 root root 1013 Nov 12 11:10 server.csr -rw-r--r-- 1 root root 306 Nov 12 11:10 server-csr.json-rw------- 1 root root 1679 Nov 12 11:10 server-key.pem # etcd 使用-rw-r--r-- 1 root root 1338 Nov 12 11:10 server.pem # etcd 使用``` ## 部署 etcd 节点```html# 环境172.17.70.251 k8s-master etcd172.17.70.253 k8s-node1 etcd172.17.70.254 k8s-node2 etcd 下载解压和配置12345[root@k8s-master1 src]# tar -zxvf etcd-v3.2.28-linux-amd64.tar.gz [root@k8s-master1 src]# chown -R root:root etcd-v3.2.28-linux-amd64[root@k8s-master1 src]# mkdir -p /opt/etcd/&#123;cfg,bin,ssl&#125; -p[root@k8s-master1 src]# cd etcd-v3.2.28-linux-amd64[root@k8s-master1 etcd-v3.2.28-linux-amd64]# mv etcd etcdctl /opt/etcd/bin/ 123456789# 证书拷贝[root@k8s-master1 cfg]# cd /opt/etcd/ssl/[root@k8s-master1 ssl]# cp /opt/TLS/etcd/&#123;ca,server-key,server&#125;.pem .[root@k8s-master1 ssl]# ls -ltotal 12-rw-r--r-- 1 root root 1265 Nov 12 11:43 ca.pem-rw------- 1 root root 1679 Nov 12 11:43 server-key.pem-rw-r--r-- 1 root root 1338 Nov 12 11:43 server.pem 1234567891011121314151617# 配置文件# 其他两个etcd节点 需要修改 ETCD_NAME 和 IP地址[root@k8s-master1 cfg]# vim /opt/etcd/cfg/etcd.conf#[Member]ETCD_NAME="etcd-1"ETCD_DATA_DIR="/var/lib/etcd/default.etcd"ETCD_LISTEN_PEER_URLS="https://172.17.70.251:2380"ETCD_LISTEN_CLIENT_URLS="https://172.17.70.251:2379"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.17.70.251:2380"ETCD_ADVERTISE_CLIENT_URLS="https://172.17.70.251:2379"ETCD_INITIAL_CLUSTER="etcd-1=https://172.17.70.251:2380,etcd-2=https://172.17.70.253:2380,etcd-3=https://172.17.70.254:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"ETCD_INITIAL_CLUSTER_STATE="new" 1234567891011121314151617181920212223242526272829303132333435# 服务启动文件# 一会也传给另外两个etcd [root@k8s-master1 cfg]# vim /usr/lib/systemd/system/etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.confExecStart=/opt/etcd/bin/etcd \ --name=$&#123;ETCD_NAME&#125; \ --data-dir=$&#123;ETCD_DATA_DIR&#125; \ --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \ --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster-state=new \ --cert-file=/opt/etcd/ssl/server.pem \ --key-file=/opt/etcd/ssl/server-key.pem \ --peer-cert-file=/opt/etcd/ssl/server.pem \ --peer-key-file=/opt/etcd/ssl/server-key.pem \ --trusted-ca-file=/opt/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 123# 启动服务 加入开机自启动[root@k8s-master1 cfg]# systemctl start etcd[root@k8s-master1 cfg]# systemctl enable etcd 12# 查看日志[root@k8s-master1 cfg]# tailf /var/log/messages 12345678910# 传送etcd给另外两个节点 etcd和启动服务[root@k8s-master1 ~]# scp /usr/lib/systemd/system/etcd.service root@172.17.70.253:/usr/lib/systemd/system/[root@k8s-master1 ~]# scp /usr/lib/systemd/system/etcd.service root@172.17.70.254:/usr/lib/systemd/system/[root@k8s-master1 ~]# scp -r /opt/etcd/ root@172.17.70.253:/opt/[root@k8s-master1 ~]# scp -r /opt/etcd/ root@172.17.70.254:/opt/# 修改name和ip 启动服务[root@k8s-node1 cfg]# vim etcd.conf [root@k8s-node1 cfg]# systemctl start etcd[root@k8s-node1 cfg]# systemctl enable etcdCreated symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service. 验证etcd集群1234/opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints="https://172.17.70.251:2379,https://172.17.70.253:2379,https://172.17.70.254:2379" \cluster-health 1234/opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints="https://172.17.70.251:2379,https://172.17.70.253:2379,https://172.17.70.254:2379" \member list Node安装 Docker12 K8S网络模型 CNI123456# 容器网络接口# kubernetes网络模型设计基本要求1. 一个POD一个IP2. 每个Pod独立IP，Pod内所有容器共享网络(同一个IP)3. 所有容器可以与其他容器通信 跨主机通信4. 所有节点都可以与所有容器通信]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制部署 K8S Harbor 环境]]></title>
    <url>%2F2019%2F11%2F10%2Fk8s-base02%2F</url>
    <content type="text"><![CDATA[系统初始化创建任务目录12345[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/&#123;roles,group_vars&#125;# 创建common的任务和模板目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/common/tasks # tasks任务目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/common/templates # 模板目录 创建任务1234567891011121314151617181920212223242526272829303132333435[root@k8s-master1 ~]# cd ansible-k8s-deploy/roles/common/tasks/[root@k8s-master1 tasks]# vim main.yaml ---# 系统初始化 所有节点上执行- name: 关闭 selinux lineinfile: dest: /etc/selinux/config regexp: '^SELINUX=' line: 'SELINUX=disabled'- name: 关闭 firewalld systemd: name: firewalld state: stopped enabled: no- name: 关闭 swap lineinfile: dest: /etc/fstab regexp: "UUID.*swap" line: ""- name: 关闭 swap 和 selinux 即时生效 shell: setenforce 0 ; swapoff -a- name: 设置 主机名 # 截取hosts文件中 自己的 node_name shell: hostnamectl set-hostname &#123;&#123;node_name|quote&#125;&#125;- name: 设置 hosts # 将templates下的hosts.j2 分发到各个目标主机的/etc/hosts下 template: src=hosts.j2 dest=/etc/hosts 123456# 模板文件 hosts.j2127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6&#123;% for host in groups['node_list'] %&#125;&#123;&#123; hostvars[host].inventory_hostname &#125;&#125; &#123;&#123; hostvars[host].node_name &#125;&#125;&#123;% endfor %&#125; 12345678910111213141516171819202122232425# host 文件[root@k8s-master1 ansible-k8s-deploy]# vim hosts [master]# 如果部署单Master，只保留一个Master节点172.31.228.67 node_name=k8s-master1[node]172.31.228.69 node_name=k8s-node1172.31.228.70 node_name=k8s-node2[harbor]# 本身是想用他做高可用172.31.228.68 node_name=k8s-master2# k8s组下面包含着master和node组[k8s:children]masternode# 用于all资源变量引用[node_list:children]masternodeharbor 12345678910111213# ansible 配置文件[root@k8s-master1 ansible-k8s-deploy]# vim ansible.cfg [defaults]inventory = /hostsforks = 5become = rootremote_port = 22host_key_checking = Falsetimeout = 10log_path = /var/log/ansible.logprivate_key_file = /root/.ssh/id_rsa 123456789101112# 入口文件[root@k8s-master1 ansible-k8s-deploy]# vim single-master-deploy.yaml---- name: 0.系统初始化 gather_facts: false hosts: - k8s - harbor roles: - common tags: common 12345678# 查看目录[root@k8s-master1 ansible-k8s-deploy]# tree /root/ansible-k8s-deploy/roles/common//root/ansible-k8s-deploy/roles/common/├── tasks│ └── main.yaml└── templates └── hosts.j2 执行任务1[root@k8s-master1 ansible-k8s-deploy]# tree /root/ansible-k8s-deploy/ 123456789101112131415161718[root@k8s-master1 ansible-k8s-deploy]# yum install ansible[root@k8s-master1 ansible-k8s-deploy]# ansible-playbook -i hosts single-master-deploy.yaml -uroot -kSSH password: ...PLAY RECAP *********************************************************************************************************************************************************************************************************************************172.31.228.67 : ok=6 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.31.228.68 : ok=6 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.31.228.69 : ok=6 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.31.228.70 : ok=6 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@k8s-master1 ansible-k8s-deploy]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6172.31.228.67 k8s-master1172.31.228.69 k8s-node1172.31.228.70 k8s-node2172.31.228.68 k8s-master2 二级制部署 Docker12345678910111213# Docker 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/# 查看 Kubernetes 所需的Docker版本https://github.com/kubernetes/kubernetes/releases# 选择好K8S版本 进入 查找 Docker 本次选择1.16.8 版本UnchangedThe list of validated docker versions remains unchanged.The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09. (#72823, #72831)# 本次 Docker 版本选择 18.09.6# k8s 1.17和18 版本需要 docker-19 版本 创建任务目录1234# 创建Docker模块的任务和模板目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/docker/tasks # tasks任务目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/docker/files # 文件目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/docker/templates # 模板目录 创建任务123456789101112131415161718192021222324252627282930313233[root@k8s-master1 tasks]# cd /root/ansible-k8s-deploy/roles/docker/tasks/[root@k8s-master1 tasks]# vim main.yaml ---- name: 创建临时目录 # 每台节点都创建 引用 group_vars/all.yaml 的 tmp_dir: '/tmp/k8s' 目录 file: dest=&#123;&#123; tmp_dir &#125;&#125; state=directory- name: 分发并解压 docker 二进制包 unarchive: src=&#123;&#123; item &#125;&#125; dest=&#123;&#123; tmp_dir &#125;&#125; with_fileglob: - "&#123;&#123; software_dir &#125;&#125;/docker-*.tgz"- name: 移动 docker 二进制文件到/usr/bin shell: cp -rf &#123;&#123; tmp_dir &#125;&#125;/docker/* /usr/bin- name: 配置 service 文件 copy: src=docker.service dest=/usr/lib/systemd/system/- name: 创建 docker 配置目录 file: dest=/etc/docker state=directory- name: 配置 docker template: src=daemon.json.j2 dest=/etc/docker/daemon.json- name: 启动 docker systemd: name=docker state=restarted enabled=yes daemon_reload=yes- name: 查看状态 shell: docker info register: docker- debug: var=docker.stdout_lines 12345678910111213# 创建模板 运用配置文件变量[root@k8s-master1 templates]# vim daemon.json.j2 &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], "insecure-registries": ["&#123;&#123; harbor_ip &#125;&#125;"], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m" &#125;, "storage-driver": "overlay2"&#125; 12345678910111213141516171819202122232425262728# 创建配置文件[root@k8s-master1 files]# vim docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.target[Service]Type=notifyExecStart=/usr/bin/dockerdExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.target 12345678910# 增加配置[root@k8s-master1 ansible-k8s-deploy]# cat group_vars/all.yaml # 安装目录tmp_dir: '/tmp/k8s'# 二进制包存放目录software_dir: '/root/binary_pkg'# harbor 地址harbor_ip: "172.31.228.68" 1234567891011121314151617181920# 添加任务到入口文件 使用node_list让每台节点都安装docker环境[root@k8s-master1 ansible-k8s-deploy]# cat single-master-deploy.yaml ---- name: 0.系统初始化 gather_facts: false hosts: - k8s - harbor roles: - common tags: common- name: 1.部署 Docker gather_facts: false hosts: - node_list roles: - docker tags: docker 12345678910# 查看目录[root@k8s-master1 files]# tree /root/ansible-k8s-deploy/roles/docker//root/ansible-k8s-deploy/roles/docker/├── files│ └── docker.service├── tasks│ └── main.yaml└── templates └── daemon.json.j2 执行任务12345# 按任务条目执行任务 --tags=任务名称[root@k8s-master1 ansible-k8s-deploy]# ansible-playbook -i hosts single-master-deploy.yaml -uroot -k --tags=docker# 检查[root@k8s-master1 ansible-k8s-deploy]# docker info Harbor 镜像仓库创建任务目录1234# 创建Harbor模块的任务和模板目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/harbor/tasks # tasks任务目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/harbor/files # 文件目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/harbor/templates # 模板目录 创建任务12345678910111213141516171819202122232425262728[root@k8s-master1 tasks]# vim main.yaml---- name: 创建工作目录 file: dest=&#123;&#123; harbor_work_dir &#125;&#125; state=directory- name: 创建临时目录 file: dest=&#123;&#123; tmp_dir &#125;&#125; state=directory- name: 分发并解压 harbor 安装包 unarchive: src=&#123;&#123; software_dir &#125;&#125;/harbor-offline-installer-*.tgz dest=/opt/- name: 部署 compose copy: src=&#123;&#123; software_dir &#125;&#125;/docker-compose-Linux-x86_64 dest=/usr/local/bin/docker-compose mode=755- name: 分发 harbor 配置文件 template: src=harbor.yml.j2 dest=&#123;&#123; harbor_work_dir &#125;&#125;/harbor.yml- name: 准备安装 harbor shell: cd &#123;&#123; harbor_work_dir &#125;&#125;/ &amp;&amp; ./prepare &amp;&amp; ./install.sh- name: 列出 harbor 状态 shell: cd &#123;&#123; harbor_work_dir &#125;&#125; &amp;&amp; docker-compose ps register: status- debug: var=status.stdout_lines- name: 登录 harbor shell: sleep 5;docker login -u admin -p lx@68328153 &#123;&#123; harbor_ip &#125;&#125; 123456789101112# 添加 harbor.yml.j2 到模板目录,该文件信息过长，主要是替换用户名密码和访问方式等配置hostname: &#123;&#123; harbor_ip &#125;&#125;...http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80...harbor_admin_password: 密码...database: # The password for the root user of Harbor DB. Change this before any production use. password: 密码 123456789101112# 添加配置[root@k8s-master1 group_vars]# vim all.yaml # 安装目录tmp_dir: '/tmp/k8s'# 二进制包存放目录software_dir: '/root/binary_pkg'# harbor 地址和工作目录harbor_ip: "172.31.228.68"harbor_work_dir: "/opt/harbor" 12345678# 查看目录[root@k8s-master1 ansible-k8s-deploy]# tree /root/ansible-k8s-deploy/roles/harbor//root/ansible-k8s-deploy/roles/harbor/├── files├── tasks│ └── main.yaml└── templates └── harbor.yml.j2 123456789101112131415161718192021222324252627# 添加任务到入口文件 harbor安装在K8S集群之外[root@k8s-master1 ansible-k8s-deploy]# vim single-master-deploy.yaml ---- name: 0.系统初始化 gather_facts: false hosts: - k8s - harbor roles: - common tags: common- name: 1.部署 Docker gather_facts: false hosts: - node_list roles: - docker tags: docker- name: 2.部署 Harbor 仓库 gather_facts: false hosts: harbor roles: - harbor tags: harbor 执行任务1[root@k8s-master1 ansible-k8s-deploy]# ansible-playbook -i hosts single-master-deploy.yaml -uroot -k --tags=harbor 1234567891011121314151617# 查看TASK [harbor : debug] ********************ok: [172.31.228.68] =&gt; &#123; "status.stdout_lines": [ " Name Command State Ports ", "------------------------------------------------------------------------------------------------------", "harbor-core /harbor/start.sh Up (health: starting) ", "harbor-db /entrypoint.sh postgres Up (health: starting) 5432/tcp ", "harbor-jobservice /harbor/start.sh Up ", "harbor-log /bin/sh -c /usr/local/bin/ ... Up (health: starting) 127.0.0.1:1514-&gt;10514/tcp", "harbor-portal nginx -g daemon off; Up (health: starting) 80/tcp ", "nginx nginx -g daemon off; Up (health: starting) 0.0.0.0:80-&gt;80/tcp ", "redis docker-entrypoint.sh redis ... Up 6379/tcp ", "registry /entrypoint.sh /etc/regist ... Up (health: starting) 5000/tcp ", "registryctl /harbor/start.sh Up (health: starting) " ]&#125; 准备证书 cfssl1234561. ETCD 应该部署在K8S集群之外2. ETCD 应该使用SSD硬盘 3. 需要先删除 安装包中的 ssl目录 因为是之前生成过的文件rm -rf /root/ansible-k8s-deploy/roles/masterrm -rf /root/ansible-k8s-deploy/roles/noderm -rf /root/ansible-k8s-deploy/ssl 123456789101112131415161718# 生成的 CA 证书和秘钥文件如下： # etcd 证书 用于访问etcd资源[root@k8s-master1 etcd_cert]# lsca-key.pem ca.pem server-key.pem server.pem# k8s服务证书 和 kubectl 访问证书 [root@k8s-master1 k8s_cert]#admin-key.pem # kubectl 访问证书 admin.pemca-key.pemca.pem # 根证书server-key.pem # kube-apiserver 证书server.pem# kube-proxy 证书[root@k8s-master1 k8s_cert]# lsca.pem kube-proxy-key.pem kube-proxy.pem # 1kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。 创建任务目录1234# 创建tls模块的任务和模板目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/tls/tasks # tasks任务目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/tls/files # 文件目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/tls/templates # 模板目录 创建任务12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 任务流程[root@k8s-master1 tasks]# vim main.yaml ---- name: 获取 Ansible 工作目录 shell: pwd |sed 's#roles/tls##' register: root_dir- debug: var=root_dir["stdout"]- name: 创建 tls 工作目录 file: dest=&#123;&#123; root_dir.stdout &#125;&#125;/ssl/&#123;&#123; item &#125;&#125; state=directory with_items: - etcd - k8s- name: 准备 cfssl 工具,二进制包解压到 /usr/bin 目录 unarchive: src=&#123;&#123; software_dir &#125;&#125;/cfssl.tar.gz dest=/usr/bin/ mode=u+x- name: 准备 etcd 证书请求文件 template: src=etcd/&#123;&#123; item &#125;&#125; dest=&#123;&#123; root_dir.stdout &#125;&#125;/ssl/etcd/&#123;&#123; item.split('.')[:-1]|join('.') &#125;&#125; with_items: - ca-config.json.j2 - ca-csr.json.j2 - server-csr.json.j2- name: 准备生成 etcd 证书脚本 copy: src=generate_etcd_cert.sh dest=&#123;&#123; root_dir.stdout &#125;&#125;/ssl/etcd mode=u+x- name: 生成 etcd 证书 shell: cd &#123;&#123; root_dir.stdout &#125;&#125;/ssl/etcd &amp;&amp; /bin/bash generate_etcd_cert.sh- name: 准备 k8s 证书请求文件 template: src=k8s/&#123;&#123; item &#125;&#125; dest=&#123;&#123; root_dir.stdout &#125;&#125;/ssl/k8s/&#123;&#123; item.split('.')[:-1]|join('.') &#125;&#125; with_items: - ca-config.json.j2 - ca-csr.json.j2 - server-csr.json.j2 - admin-csr.json.j2 - kube-proxy-csr.json.j2- name: 准备生成 k8s 证书脚本 copy: src=generate_k8s_cert.sh dest=&#123;&#123; root_dir.stdout &#125;&#125;/ssl/k8s mode=u+x- name: 生成 k8s 证书 shell: cd &#123;&#123; root_dir.stdout &#125;&#125;/ssl/k8s &amp;&amp; /bin/bash generate_k8s_cert.sh 123456789101112131415161718192021222324252627282930313233[root@k8s-master1 ansible-k8s-deploy]# vim single-master-deploy.yaml ---- name: 0.系统初始化 gather_facts: false hosts: - k8s - harbor roles: - common tags: common- name: 1.部署 Docker gather_facts: false hosts: - node_list roles: - docker tags: docker- name: 2.部署 Harbor 仓库 gather_facts: false hosts: harbor roles: - harbor tags: harbor- name: 3.自签证书 gather_facts: false hosts: localhost roles: - tls tags: tls 12345678910111213141516171819# 目录结构[root@k8s-master1 ansible-k8s-deploy]# tree /root/ansible-k8s-deploy/roles/tls//root/ansible-k8s-deploy/roles/tls/├── files│ ├── generate_etcd_cert.sh│ └── generate_k8s_cert.sh├── tasks│ └── main.yaml└── templates ├── etcd │ ├── ca-config.json.j2 │ ├── ca-csr.json.j2 │ └── server-csr.json.j2 └── k8s ├── admin-csr.json.j2 ├── ca-config.json.j2 ├── ca-csr.json.j2 ├── kube-proxy-csr.json.j2 └── server-csr.json.j2 执行任务123456789101112131415161718192021222324252627282930313233343536[root@k8s-master1 ansible-k8s-deploy]# ansible-playbook -i hosts single-master-deploy.yaml -uroot -k --tags=tls# 查看证书生成的目录文件[root@k8s-master1 ansible-k8s-deploy]# tree /root/ansible-k8s-deploy/ssl//root/ansible-k8s-deploy/ssl/├── etcd│ ├── ca-config.json│ ├── ca.csr│ ├── ca-csr.json│ ├── ca-key.pem│ ├── ca.pem│ ├── generate_etcd_cert.sh│ ├── server.csr│ ├── server-csr.json│ ├── server-key.pem│ └── server.pem└── k8s ├── admin.csr ├── admin-csr.json ├── admin-key.pem ├── admin.pem ├── ca-config.json ├── ca.csr ├── ca-csr.json ├── ca-key.pem ├── ca.pem ├── generate_k8s_cert.sh ├── kube-proxy.csr ├── kube-proxy-csr.json ├── kube-proxy-key.pem ├── kube-proxy.pem ├── server.csr ├── server-csr.json ├── server-key.pem └── server.pem 安装 ETCD 集群创建任务目录123[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/etcd/tasks # tasks任务目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/etcd/files # 文件目录[root@k8s-master1 ~]# mkdir -p ansible-k8s-deploy/roles/etcd/templates # 模板目录 创建任务12# 本次预定安装的k8s版本是 1.16.8# etcd版本使用 3.3.20 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master1 etcd]# vim tasks/main.yaml - name: 创建工作目录 file: dest=&#123;&#123; etcd_work_dir &#125;&#125;/&#123;&#123; item &#125;&#125; state=directory with_items: - bin - cfg - ssl- name: 创建临时目录 file: dest=&#123;&#123; tmp_dir &#125;&#125; state=directory- name: 分发并解压 etcd 二进制包 unarchive: src=&#123;&#123; item &#125;&#125; dest=&#123;&#123; tmp_dir &#125;&#125; with_fileglob: - "&#123;&#123; software_dir &#125;&#125;/etcd-v*.tar.gz"- name: 分发 etcd 二进制文件 shell: cp -rf &#123;&#123; tmp_dir &#125;&#125;/etcd-v*/&#123;etcd,etcdctl&#125; &#123;&#123; etcd_work_dir &#125;&#125;/bin- name: 分发证书 # files/etcd_cert/证书 该目录在之前创建tls已经创建,并已将etcd证书传到该目录下 copy: src=etcd_cert/&#123;&#123; item &#125;&#125; dest=&#123;&#123; etcd_work_dir &#125;&#125;/ssl with_items: - ca.pem - server.pem - server-key.pem- name: 分发 etcd 配置文件 template: src=etcd.conf.j2 dest=&#123;&#123; etcd_work_dir &#125;&#125;/cfg/etcd.conf- name: 分发 etcd service 文件 template: src=etcd.service.j2 dest=/usr/lib/systemd/system/etcd.service- name: 启动 etcd systemd: name=etcd state=restarted enabled=yes daemon_reload=yes- name: 分发 etcd 测试脚本 template: src=&#123;&#123; item &#125;&#125; dest=&#123;&#123; tmp_dir &#125;&#125;/&#123;&#123; item.split('.')[:-1]|join('.') &#125;&#125; mode=u+x with_items: - etcd_member_list.sh.j2 - etcd_status.sh.j2- name: 获取 etcd 集群状态 shell: /bin/bash &#123;&#123; tmp_dir &#125;&#125;/etcd_status.sh register: status- debug: var=status.stdout_lines- name: 获取 etcd 集群成员 shell: /bin/bash &#123;&#123; tmp_dir &#125;&#125;/etcd_member_list.sh register: etcd_member_list_status- debug: var=etcd_member_list_status.stdout_lines 12345678910111213141516# 查看目录[root@k8s-master1 etcd]# tree /root/ansible-k8s-deploy/roles/etcd//root/ansible-k8s-deploy/roles/etcd/├── files│ └── etcd_cert│ ├── ca-key.pem│ ├── ca.pem│ ├── server-key.pem│ └── server.pem├── tasks│ └── main.yaml└── templates ├── etcd.conf.j2 ├── etcd_member_list.sh.j2 ├── etcd.service.j2 └── etcd_status.sh.j2 12345678910111213141516171819202122232425262728293031323334353637383940# 查看入口文件[root@k8s-master1 etcd]# cat /root/ansible-k8s-deploy/single-master-deploy.yaml ---- name: 0.系统初始化 gather_facts: false hosts: - k8s - harbor roles: - common tags: common- name: 1.部署 Docker gather_facts: false hosts: - node_list roles: - docker tags: docker- name: 2.部署 Harbor 仓库 gather_facts: false hosts: harbor roles: - harbor tags: harbor- name: 3.自签证书 gather_facts: false hosts: localhost roles: - tls tags: tls- name: 4.部署 ETCD 集群 gather_facts: false hosts: etcd roles: - etcd tags: etcd Master 节点创建工作目录123mkdir -p ansible-k8s-deploy/roles/node/tasksmkdir -p ansible-k8s-deploy/roles/node/files mkdir -p ansible-k8s-deploy/roles/node/templates]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 容器，到底是怎么一回事儿？]]></title>
    <url>%2F2019%2F11%2F10%2Fk8s-base01%2F</url>
    <content type="text"><![CDATA[容器技术的核心功能 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界” 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。 进程的静态表现和动态表现 对于进程来说，它的静态表现就是程序，平常都安安静静地待在磁盘上； 而一旦运行起来，它就变成了计算机里的数据和状态的总和，这就是它的动态表现。 docker 的边界是如何实现的12345[root@k8s-master1 ~]# docker run -it --rm --name test busybox sh/ # ps PID USER TIME COMMAND 1 root 0:00 sh 6 root 0:00 ps Docker 里最开始执行的 sh 就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。 这就意味着，前面执行的 sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。 原本 每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100 这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。 而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。 这样，他就会错误地以为自己就是公司里的第 1 号员工。 这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。 可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。这种技术，就是 Linux 里面的 Namespace 机制。 Linux namespace Linux namespace 实现了 6 项资源隔离，基本上涵盖了一个小型操作系统的运行要素，包括主机名、用户权限、文件系统、网络、进程号、进程间通信。 Namespace 的使用机制 这 6 项资源隔离分别对应 6 种系统调用，通过传入上表中的参数，调用 clone() 函数来完成。 它其实只是 Linux 创建新进程的一个可选参数 在 Linux 系统中创建线程的系统调用是 clone()，比如： 1int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”， 是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。 Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。 这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。 所以说，容器，其实是一种特殊的进程而已。 12345678910[root@k8s-master1 ~]# ps -ef|grep busyroot 2379 1073 0 09:53 pts/0 00:00:00 docker run -it --rm --name test busybox sh[root@k8s-master1 ~]# ls -l /proc/2379/nstotal 0lrwxrwxrwx 1 root root 0 Nov 10 10:10 ipc -&gt; ipc:[4026531839]lrwxrwxrwx 1 root root 0 Nov 10 10:10 mnt -&gt; mnt:[4026531840]lrwxrwxrwx 1 root root 0 Nov 10 10:10 net -&gt; net:[4026531956]lrwxrwxrwx 1 root root 0 Nov 10 10:10 pid -&gt; pid:[4026531836]lrwxrwxrwx 1 root root 0 Nov 10 10:10 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Nov 10 10:10 uts -&gt; uts:[4026531838] 虚拟机和容器 虚拟机的工作原理 Hypervisor 的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。 然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。 用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。 这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。 容器的工作原理 跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程。 只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。 这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。 隔离与限制 Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来说，这些被“隔离”了的进程跟其他进程并没有太大区别。 不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置，因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责，也不会创建任何实体的“容器”，真正对隔离环境负责的是宿主机操作系统本身 用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数。而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。 使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。 容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。 所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。 基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：隔离得不彻底。 哪些地方没有进行隔离 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。 在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。 容器的限制 虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。 这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。 当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups Linux Cgroups 的全称是 Linux Control Group。 它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。 它的输出结果，是一系列文件系统目录 123456789101112[root@k8s-master1 containers]# mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) 在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类 子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： 123[root@k8s-master1 containers]# ls /sys/fs/cgroup/cpucgroup.clone_children cgroup.procs cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat notify_on_release system.slice user.slicecgroup.event_control cgroup.sane_behavior cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares docker release_agent tasks 限制一个docker容器的资源使用 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。 而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录）， 然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令： 123# docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash[root@k8s-master1 ~]# docker run -it -d --name test --cpus=".5" busybox98dad5852fc5e03f7dcb1241b52e24ade1c425deb68bf12a3687e89ea730515e 1234567891. 它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 50 ms 的 CPU 时间，也就是说这个进程只能使用到 50% 的 CPU 带宽。[root@k8s-master1 ~]# cat /sys/fs/cgroup/cpu/docker/98dad5852fc5e03f7dcb1241b52e24ade1c425deb68bf12a3687e89ea730515e/cpu.cfs_period_us 100000[root@k8s-master1 ~]# cat /sys/fs/cgroup/cpu/docker/98dad5852fc5e03f7dcb1241b52e24ade1c425deb68bf12a3687e89ea730515e/cpu.cfs_quota_us 50000# 这个 Docker 容器，只能使用到 50% 的 CPU 带宽 123# 容器里跑一个死循环 吃死CPU 再去查看使用率[root@k8s-master1 ~]# docker exec -it test sh/ # while : ; do : ; done &amp; 12345# 被限制的进程的 PID 写入 container 组里的 tasks 文件[root@k8s-master1 containers]# cat /sys/fs/cgroup/cpu/docker/98dad5852fc5e03f7dcb1241b52e24ade1c425deb68bf12a3687e89ea730515e/tasks135891365913676 总结 一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。 使用namespace 在docker进程启动的时候做隔离,让容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置 然后使用 cgroup 为每个容器创建一个控制组 在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中 控制他们的资源使用率。 这也是容器技术中一个非常重要的概念，即：容器是一个“单进程”模型。 1231. 由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里 PID=1 的进程，也是其他后续创建的所有进程的父进程。2. 这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程，3. 这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程。 1231. 但是，在后面分享容器设计模式时，我还会推荐其他更好的解决办法。2. 这是因为容器本身的设计，就是希望容器和应用能够同生命周期，这个概念对后续的容器编排非常重要。3. 否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了。 另外，跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。 众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息， 比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。 但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。 造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。 生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的 CPU 核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险。 这也是在企业中，容器化应用碰到的一个常见问题，也是容器相较于虚拟机另一个不尽如人意的地方。 问题 如何修复容器中的 top 指令以及 /proc 文件系统中的信息 深入理解容器镜像 Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 用到了一种叫作联合文件系统（Union File System）的能力。 Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。 centos:7使用的UnionFS 为 overlay2 12[root@k8s-master1 docker]# docker infoStorage Driver: overlay2 Docker 镜像使用的 rootfs，往往由多个“层”组成 通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。 当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。 而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。 Docker 容器的本质Flask小例子 使用 Flask 框架启动了一个 Web 服务器，功能是： 如果当前环境中有“NAME”这个环境变量，就把它打印在“Hello”后，否则就打印“Hello world”，最后再打印出当前环境的 hostname。 12345678910111213141516[root@k8s-master1 runtime]# vim app.pyfrom flask import Flaskimport socketimport osapp = Flask(__name__)@app.route('/')def hello(): html = "&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;" \ "&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;" return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname())if __name__ == "__main__": app.run(host='0.0.0.0', port=80) 123# python依赖[root@k8s-master1 runtime]# vim requirements.txtFlask 应用容器化的第一步，制作容器镜像 制作镜像有两个方法,手动构建 和 Dockerfile 构建 手动构建就是基于base镜像一点一点的制作容器,最后提交为镜像 12345678910# 提交本地镜像# commit -m "centos7-ssh" # 添加注释# mycentos # 容器名# test/mycentos-7-ssh # 仓库名称/镜像:版本 不加版本号就是latest 最后的版本[root@linux-node1 tools]# docker commit -m "centos7-ssh" mycentos test/mycentos-7-ssh[root@linux-node1 tools]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest/mycentos-7-ssh latest db3993e49556 About a minute ago 278MB... 使用Dockfile 制作镜像 12345678910111213141516171819202122[root@k8s-master1 runtime]# vim Dockerfile# 使用官方提供的Python开发镜像作为基础镜像FROM python:2.7-slim# 将工作目录切换为/appWORKDIR /app# 将当前目录下的所有内容复制到/app下ADD . /app# 使用pip命令安装这个应用所需要的依赖RUN pip install --trusted-host pypi.python.org -r requirements.txt# 允许外界访问容器的80端口EXPOSE 80# 设置环境变量ENV NAME World# 设置容器进程为：python app.py，即：这个Python应用的启动命令CMD ["python", "app.py"] Dockerfile 的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的 Docker 镜像。并且这些原语，都是按顺序处理的。 12345678910111213141. 比如 FROM 原语，指定了“python:2.7-slim”这个官方维护的基础镜像，从而免去了安装 Python 等语言环境的操作。否则，这一段我们就得这么写了：FROM ubuntu:latestRUN apt-get update -yRUN apt-get install -y python-pip python-dev build-essential...2. RUN 原语就是在容器里执行 shell 命令的意思。3. WORKDIR，意思是在这一句之后，Dockerfile 后面的操作都以这一句指定的 /app 目录作为当前目录。4. ENTRYPOINT。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。5. CMD 的内容就是 ENTRYPOINT 的参数，Docker 容器的启动进程为 ENTRYPOINT，而不是 CMD。6. 默认情况下,Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c7. 在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”8. Dockerfile 里的原语并不都是指对容器内部的操作。9. 就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。 123456789101112131415161718192021222324252627282930313233[root@k8s-master1 runtime]# ls -ltotal 12-rw-r--r-- 1 root root 352 Nov 10 16:04 app.py-rw-r--r-- 1 root root 506 Nov 10 16:09 Dockerfile-rw-r--r-- 1 root root 6 Nov 10 16:07 requirements.txt# 制作镜像[root@k8s-master1 runtime]# docker build -t helloworld .[root@k8s-master1 runtime]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhelloworld latest 605b63c39db7 About a minute ago 148MB[root@k8s-master1 overlay2]# docker inspect helloworld "RootFS": &#123; "Type": "layers", "Layers": [ "sha256:b67d19e65ef653823ed62a5835399c610a40e8205c16f839c5cc567954fcf594", "sha256:bb8510b5f5989e37e21b983be7a4936f6dab0ae7e0f634e592c6ae74990b0629", "sha256:3e27dc6a2fe0c3aa9db3238038379d895bd990e5fd152f924277378f28a64f2c", "sha256:6fb7ce0ece77af2e66fe961545b743ecf12bfd1df549b1cccf55dfda1f3fc770", "sha256:afd237bb63dc3d749b32fc5913c914255bc87c821db1d7779f1b8917abb0dc16", "sha256:365f769a43650e437a2777204ad5d7459d8bf687acd57b9214fb82dcbeae42b1", "sha256:e79da1f2be17dee958c0b2cb67b5ce2d08dc4856ab2fe8e50fb5f689e4cb1a51" ] &#125;,1. -t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。2. docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。3. 而这个过程，实际上可以等同于 Docker 使用基础镜像启动了一个容器，然后在容器中依次执行 Dockerfile 中的原语。 需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层 即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。 docker run 命令启动容器12345[root@k8s-master1 docker]# docker run -d --name hello -p 4000:80 helloworld在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。否则，我就得把进程的启动命令加在后面：$ docker run -p 4000:80 helloworld python app.py docker exec 是怎么做到进入容器里的呢？ Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。 Docker 容器的进程号（PID） 12[root@k8s-master1 docker]# docker inspect --format '&#123;&#123;.State.Pid &#125;&#125;' hello14883 12345678910111213[root@k8s-master1 docker]# ls -l /proc/14883/nstotal 0lrwxrwxrwx 1 root root 0 Nov 10 16:27 ipc -&gt; ipc:[4026532223]lrwxrwxrwx 1 root root 0 Nov 10 16:27 mnt -&gt; mnt:[4026532221]lrwxrwxrwx 1 root root 0 Nov 10 16:22 net -&gt; net:[4026532226]lrwxrwxrwx 1 root root 0 Nov 10 16:27 pid -&gt; pid:[4026532224]lrwxrwxrwx 1 root root 0 Nov 10 16:27 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Nov 10 16:27 uts -&gt; uts:[4026532222]1. 可以看到，一个进程的每种 Linux Namespace，都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件，2. 并且链接到一个真实的 Namespace 文件上。3. 有了这样一个可以“hold 住”所有 Linux Namespace 的文件，我们就可以对 Namespace 做一些很有意义事情了，4. 比如：加入到一个已经存在的 Namespace 当中。 这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。 Docker 还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的 Network Namespace 里，这个参数就是 -net，比如: docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。 Volume（数据卷） 容器里进程新建的文件，怎么才能让宿主机获取到？ 宿主机上的文件和目录，怎么才能让容器里的进程访问到？ 这正是 Docker Volume 要解决的问题：Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。 在 Docker 项目里，它支持两种 Volume 声明方式，可以把宿主机目录挂载进容器的 /test 目录当中： 12$ docker run -v /test ...$ docker run -v /home:/test ... 而这两种声明方式的本质，实际上是相同的：都是把一个宿主机的目录挂载进了容器的 /test 目录。 只不过，在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。 而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。 总结]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07 Deployment 功能及应用场景]]></title>
    <url>%2F2019%2F11%2F08%2Fk8s-07%2F</url>
    <content type="text"><![CDATA[Deployment 控制器Pod与controllers的关系 controllers: 在集群上管理和运行容器的对象 通过label-selector相关联 Pod通过控制器实现应用的运维,如伸缩,滚动升级等 12341. Pod是k8s当中最小的调度单元,但是一般很少直接使用pod2. 使用控制器去对应用做部署3. 控制器是高级对象，Pod是抽象的，控制器可以完成更高级的应用部署4. controllers另外的学名 workload 工作负载 1234567# 在一个yaml文件中可以分成两类:1. 控制器 - 指定控制器和api - 元数据 - 标明标签，通过标签找到一组Pod 2. Pod模板 - 标明标签，关联控制器 123# 知识点总结:1. 控制器管理Pod,使用控制器去部署应用2. 控制器通过标签关联Pod Deployment 功能与应用场景 部署无状态应用 管理Pod和ReplicaSet 具有上线部署、副本设定、滚动升级、回滚等功能 提供声明式更新，例如值只更新一个新的Image 应用场景: Web服务，微服务 11. ReplicaSet也是控制器,但是他得功能比较单一 Deployment YAML文件字段解析 123在k8s集群中创建pod的两种方式：1. 使用命令创建 用于测试2. 使用yaml文件 便于管理和复用 Deployment 部署无状态应用123[root@k8s-master1 demo]# kubectl create deployment --helpUsage: kubectl create deployment NAME --image=image [--dry-run] [options] 123# 测试并输出到yaml格式文件[root@k8s-master1 demo]# kubectl create deployment web --image=nginx1.16 --dry-run -o yaml[root@k8s-master1 demo]# kubectl create deployment web --image=nginx1.16 --dry-run -o yaml &gt; web.yaml 1234567891011121314151617181920212223242526# 修改为应用属性# 1. 副本个数# 2. 标签名# 3. 容器镜像[root@k8s-master1 demo]# vim web.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: web name: webspec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - image: 172.17.70.252/project/java-demo:latest name: java-demo 1234567891011[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-84c6fbbdcc-6tbkx 1/1 Running 0 28s 10.244.0.45 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-ldv48 1/1 Running 0 28s 10.244.0.46 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-qvl9f 1/1 Running 0 28s 10.244.1.48 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl logs web-84c6fbbdcc-6tbkx [root@k8s-master1 demo]# kubectl get deployNAME READY UP-TO-DATE AVAILABLE AGEweb 3/3 3 3 6m23s 暴露应用123在k8s集群中暴露应用两种方式:1. service2. ingress 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master1 demo]# kubectl expose --helpUsage: kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=number-or-name][--name=name] [--external-ip=external-ip-of-service] [--type=type] [options]# kubectl expose service名称 pod资源 pod名称 service端口 pod端口 service类型[root@k8s-master1 demo]# kubectl expose --name=web deployment web --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml &gt; web-service.yaml[root@k8s-master1 demo]# vim web-service.yaml apiVersion: v1kind: Servicemetadata: labels: app: web name: webspec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: web # 关联pod标签 type: NodePort[root@k8s-master1 demo]# kubectl apply -f web-service.yaml [root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 15dweb NodePort 10.0.0.204 &lt;none&gt; 80:31619/TCP 13s[root@k8s-master1 demo]# kubectl edit service/web[root@k8s-master1 demo]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.17.70.251:6443,172.17.70.252:6443 16dweb 10.244.0.45:8080,10.244.0.46:8080,10.244.1.48:8080 3m50s 1# nodeIP+端口测试访问 升级与回滚升级操作 set 升级更新 12345678[root@k8s-master1 demo]# kubectl set --help# 动态修改镜像# 镜像通过tag版本号进行区分[root@k8s-master1 demo]# kubectl set image --helpUsage: kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N[options]# kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1 模拟程序升级 12345678910111213141516171819202122232425262728293031323334353637# 需要修改模拟程序并打包一个新的镜像,推送到镜像仓库,这里先用nginx替代[root@k8s-master1 demo]# cat web.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: web name: webspec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - image: 172.17.70.252/project/java-demo:latest name: java-demo# deployment name# 容器 name[root@k8s-master1 demo]# kubectl set image deployment web java-demo=nginx:1.16deployment.apps/web image updated# 滚动升级 重新创建容器 失败了就不会删除之前的容器# 先启动一个新镜像的pod 成功后再删除之前打好标记的pod 一个一个平滑升级[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-7bddf7b45f-6ntcs 0/1 ContainerCreating 0 4sweb-84c6fbbdcc-6tbkx 1/1 Running 0 30mweb-84c6fbbdcc-ldv48 1/1 Running 0 30mweb-84c6fbbdcc-qvl9f 1/1 Running 0 30m 12345678910# 错误原因 镜像名字写错了 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/web-7bddf7b45f-6ntcs to k8s-node1 Normal BackOff 23s (x4 over 102s) kubelet, k8s-node1 Back-off pulling image "nginx1.16" Warning Failed 23s (x4 over 102s) kubelet, k8s-node1 Error: ImagePullBackOff Normal Pulling 11s (x4 over 110s) kubelet, k8s-node1 Pulling image "nginx1.16" Warning Failed 6s (x4 over 103s) kubelet, k8s-node1 Failed to pull image "nginx1.16": rpc error: code = Unknown desc = Error response from daemon: pull access denied for nginx1.16, repository does not exist or may require 'docker login' Warning Failed 6s (x4 over 103s) kubelet, k8s-node1 Error: ErrImagePull 1234567891011121314[root@k8s-master1 demo]# kubectl set image deployment web java-demo=nginx:1.16# 还需要修改service里面的端口 8080 -&gt; 80[root@k8s-master1 demo]# kubectl edit service/web...spec: clusterIP: 10.0.0.204 externalTrafficPolicy: Cluster ports: - nodePort: 31619 port: 80 protocol: TCP targetPort: 80 # 修改目标端口... 查看升级状态 12# 如果pod做了健康检查,那么在更新的时候可以查看更新状态[root@k8s-master1 demo]# kubectl rollout status deployment/web kubectl patch 补丁命令 1[root@k8s-master1 demo]# kubectl patch --help 回滚操作 rollout 查看deployment历史记录 123456789[root@k8s-master1 demo]# kubectl rollout --help[root@k8s-master1 demo]# kubectl rollout history deployment/webdeployment.apps/web REVISION CHANGE-CAUSE1 &lt;none&gt; # 第一次部署demo 2 &lt;none&gt;3 &lt;none&gt;4 &lt;none&gt; 回滚到上一个版本 123456789101112131415161718[root@k8s-master1 demo]# kubectl rollout undo deployment/webdeployment.apps/web rolled back# 滚动回滚 部署了新的pod[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-68d86dd855-pgjlq 0/1 Terminating 0 13mweb-68d86dd855-pp8mv 0/1 Terminating 0 13mweb-68d86dd855-pq6w6 0/1 Terminating 0 13mweb-8598778cf-2m7lb 1/1 Running 0 9sweb-8598778cf-fnjtd 1/1 Running 0 8sweb-8598778cf-qjsd6 1/1 Running 0 10s[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-8598778cf-2m7lb 1/1 Running 0 14sweb-8598778cf-fnjtd 1/1 Running 0 13sweb-8598778cf-qjsd6 1/1 Running 0 15s 回滚到指定版本 123456# 我这个nginx 回滚到 java-demo 别忘记修改回端口[root@k8s-master1 demo]# kubectl rollout undo deployment/web --to-revision=1deployment.apps/web rolled back[root@k8s-master1 demo]# kubectl edit svc/webservice/web edited 删除应用12345678910111213141516# 删除pod还会拉起,因为创建的是deployment会保证副本数量[root@k8s-master1 demo]# kubectl delete pod web-84c6fbbdcc-7dh5hpod "web-84c6fbbdcc-7dh5h" deleted[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-84c6fbbdcc-w2v5g 1/1 Running 0 3m33sweb-84c6fbbdcc-w56vs 1/1 Running 0 4sweb-84c6fbbdcc-xzglk 1/1 Running 0 3m30s# 需要删除deployment[root@k8s-master1 demo]# kubectl delete deployment/webdeployment.apps "web" deleted[root@k8s-master1 demo]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-84c6fbbdcc-w2v5g 0/1 Terminating 0 4m5sweb-84c6fbbdcc-w56vs 0/1 Terminating 0 36s 12345678910[root@k8s-master1 demo]# kubectl apply -f web.yaml deployment.apps/web created[root@k8s-master1 demo]# kubectl rollout history deployment/webdeployment.apps/web REVISION CHANGE-CAUSE1 &lt;none&gt;# service 和 pod 是不同的一种组件,删除pod并不会影响已创建的svc,这种解耦太棒了# 删除svc[root@k8s-master1 demo]# kubectl delete svc/web 弹性伸缩 scale121. 根据活动扩容实例2. 传统的方式是虚拟机,准备扩容环境 12345678910111213# kubectl scale 快速扩容 承担并发[root@k8s-master1 demo]# kubectl scale --help[root@k8s-master1 demo]# kubectl scale --replicas=5 deployment/web deployment.apps/web scaled[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-84c6fbbdcc-685nc 1/1 Running 0 5m41s 10.244.0.55 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-88289 1/1 Running 0 30s 10.244.1.57 k8s-node2 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-9mdjv 1/1 Running 0 5m41s 10.244.0.56 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-dvdkn 1/1 Running 0 5m41s 10.244.1.55 k8s-node2 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-fpwqt 1/1 Running 0 30s 10.244.1.56 k8s-node2 &lt;none&gt; &lt;none&gt; 如果集群的资源不够,单只扩容pod是不行的 12341. 增加nodes节点或者增加nodes的资源2. 如果扩容node节点 都是根据业务的量去扩容,并且提前规划3. 现在都是手动扩容,缩容,如何自动弹性实现4. 自动需要借助node的资源指标进行弹性扩容,比如cpu,内存等 手动缩容 1234567891011121314[root@k8s-master1 demo]# kubectl scale --replicas=3 deployment/web deployment.apps/web scaled[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-84c6fbbdcc-685nc 1/1 Running 0 9m36s 10.244.0.55 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-88289 0/1 Terminating 0 4m25s 10.244.1.57 k8s-node2 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-9mdjv 1/1 Running 0 9m36s 10.244.0.56 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-dvdkn 1/1 Running 0 9m36s 10.244.1.55 k8s-node2 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-fpwqt 0/1 Terminating 0 4m25s 10.244.1.56 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master1 demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-84c6fbbdcc-685nc 1/1 Running 0 9m41s 10.244.0.55 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-9mdjv 1/1 Running 0 9m41s 10.244.0.56 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-dvdkn 1/1 Running 0 9m41s 10.244.1.55 k8s-node2 &lt;none&gt; &lt;none&gt; Deployment 和 ReplicaSet123456781. 部署1个Deployment,会自动创建1个ReplicaSet和Pod[root@k8s-master1 demo]# kubectl get deploy,rsNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/web 3/3 3 3 12mNAME DESIRED CURRENT READY AGEreplicaset.apps/web-84c6fbbdcc 3 3 3 12m 123451. 部署Deployment -&gt; 创建RS -&gt; RS管理多个Pod2. RS会判断replicas: 3 副本数 也会获取当前Pod的副本数3. 如果少了一个 RS会帮助我们拉起4. RS 帮我们实现了控制副本个数 和 历史版本记录,rollout回滚找rs回滚5. 滚动更新会创建新的RS 1231. Deployment -&gt; 创建RS -&gt; RS管理多个Pod2. 滚动更新 创建新的RS -&gt; 启动新的Pod 关闭之前rs中的1个,直到全部更新完成3. service 指向 pod不变 1234567891011121314[root@k8s-master1 ~]# kubectl set image deployment/web java-demo=nginx:1.16[root@k8s-master1 ~]# kubectl get rs,deployNAME DESIRED CURRENT READY AGEreplicaset.apps/web-68d86dd855 3 3 3 17sreplicaset.apps/web-7bddf7b45f 0 0 0 2mreplicaset.apps/web-84c6fbbdcc 0 0 0 107m[root@k8s-master1 ~]# kubectl rollout history deployment/webdeployment.apps/web REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;3 &lt;none&gt;]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06 k8s部署java应用 简易版本]]></title>
    <url>%2F2019%2F11%2F08%2Fk8s-06%2F</url>
    <content type="text"><![CDATA[项目迁移至K8S流程 制作镜像 控制器管理POD 暴露应用 对外发布 日志监控 制作镜像获取 javademo1234567891011121314151. 安装Git[root@k8s-master2 opt]# yum install git2. 下载项目[root@k8s-master2 opt]# cd /opt/[root@k8s-master2 opt]# git clone https://github.com/lizhenliang/tomcat-java-demo.git[root@k8s-master2 tomcat-java-demo]# ls -ltotal 32drwxr-xr-x 2 root root 4096 Nov 20 16:33 db # 数据库sql文件-rw-r--r-- 1 root root 148 Nov 20 16:33 Dockerfile # 构建镜像-rw-r--r-- 1 root root 11357 Nov 20 16:33 LICENSE-rw-r--r-- 1 root root 1930 Nov 20 16:33 pom.xml # maven构建环境-rw-r--r-- 1 root root 270 Nov 20 16:33 README.mddrwxr-xr-x 3 root root 4096 Nov 20 16:33 src # 源码目录 安装 mariadb 数据库1234[root@k8s-master2 tomcat-java-demo]# yum -y install mariadb mariadb-server[root@k8s-master2 tomcat-java-demo]# systemctl start mariadb[root@k8s-master2 tomcat-java-demo]# systemctl enable mariadb[root@k8s-master2 tomcat-java-demo]# mysql_secure_installation 导入sql文件1234[root@k8s-master2 tomcat-java-demo]# mysql -uroot -pMariaDB [(none)]&gt; use testDatabase changedMariaDB [test]&gt; source /opt/tomcat-java-demo/db/tables_ly_tomcat.sql 测试访问1234567891011121314151617181920# 授权访问用户MariaDB [test]&gt; grant all on test.* to 'test'@'%' identified by '123456';Query OK, 0 rows affected (0.00 sec)# 在node2上测试[root@k8s-node2 ~]# yum install mysql[root@k8s-node2 ~]# mysql -h172.17.70.252 -utest -pMariaDB [(none)]&gt; use testReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [test]&gt; show tables;+----------------+| Tables_in_test |+----------------+| user |+----------------+1 row in set (0.00 sec) 修改连库配置123456[root@k8s-master2 resources]# vim /opt/tomcat-java-demo/src/main/resources/application.yml datasource: url: jdbc:mysql://172.17.70.252:3306/test?characterEncoding=utf-8 username: test password: 123456 编译源码1234567891011121314151617181920[root@k8s-master2 tomcat-java-demo]# yum search openjdk[root@k8s-master2 tomcat-java-demo]# yum install java-1.8.0-openjdk maven# mvn 编译[root@k8s-master2 tomcat-java-demo]# mvn clean package -Dmaven.test.skip=true...[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.882s[INFO] Finished at: Wed Nov 20 17:08:13 CST 2019[INFO] Final Memory: 22M/95M[INFO] ------------------------------------------------------------------------# 查看生成的war包[root@k8s-master2 tomcat-java-demo]# lsdb Dockerfile LICENSE pom.xml README.md src target[root@k8s-master2 tomcat-java-demo]# cd target/[root@k8s-master2 target]# ls -l *.war-rw-r--r-- 1 root root 18265402 Nov 20 17:08 ly-simple-tomcat-0.0.1-SNAPSHOT.war 构建项目镜像1234567891011121314151617# 引用了老师的tomcat镜像运行环境[root@k8s-master2 tomcat-java-demo]# vim Dockerfile FROM lizhenliang/tomcatLABEL maintainer www.ctnrs.comRUN rm -rf /usr/local/tomcat/webapps/*ADD target/*.war /usr/local/tomcat/webapps/ROOT.war# 构建镜像[root@k8s-master2 tomcat-java-demo]# docker build -t 172.17.70.252/project/java-demo .[root@k8s-master2 tomcat-java-demo]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE172.17.70.252/project/java-demo latest f656fe851ad7 24 seconds ago 406M# 推送到镜像仓库[root@k8s-master2 tomcat-java-demo]# docker login 172.17.70.252[root@k8s-master2 tomcat-java-demo]# docker push 172.17.70.252/project/java-demo 控制器管理 PODdeployment1234567891011121314151617181920212223242526272829# web 无状态应用 deployment[root@k8s-master1 opt]# mkdir -p /opt/java-demo[root@k8s-master1 opt]# cd /opt/java-demo/# 生成yaml文件[root@k8s-master1 java-demo]# kubectl create deployment java-demo --help [root@k8s-master1 java-demo]# kubectl create deployment java-demo --image 172.17.70.252/project/java-demo:latest --dry-run -o yaml &gt; deploy.yaml[root@k8s-master1 java-demo]# vim deploy.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: java-demo name: java-demospec: replicas: 3 selector: matchLabels: app: java-demo template: metadata: labels: app: java-demo spec: containers: - image: 172.17.70.252/project/java-demo:latest name: java-demo 12345678910111213141516171819202122# 创建 pod [root@k8s-master1 java-demo]# kubectl apply -f deploy.yaml deployment.apps/java-demo created[root@k8s-master1 java-demo]# kubectl get podsNAME READY STATUS RESTARTS AGEjava-demo-7746bb968c-4drq9 1/1 Running 0 12sjava-demo-7746bb968c-54gtv 1/1 Running 0 12sjava-demo-7746bb968c-jjtzr 1/1 Running 0 12s# 查看日志[root@k8s-master1 java-demo]# kubectl logs java-demo-7746bb968c-4drq9# 删除掉一个pod 会重新创建[root@k8s-master1 java-demo]# kubectl delete pod java-demo-7746bb968c-jjtzrpod "java-demo-7746bb968c-jjtzr" deleted[root@k8s-master1 java-demo]# kubectl get podsNAME READY STATUS RESTARTS AGEjava-demo-7746bb968c-4drq9 1/1 Running 0 2m23sjava-demo-7746bb968c-54gtv 1/1 Running 0 2m23sjava-demo-7746bb968c-s7824 1/1 Running 0 19s service1234567891011121314151617181920212223242526272829303132# service 暴露应用 [root@k8s-master1 java-demo]# kubectl expose --helpUsage: kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP|SCTP] [--target-port=number-or-name][--name=name] [--external-ip=external-ip-of-service] [--type=type] [options]# --port=80 service端口,集群内部使用# --target-prot=8080 容器服务端口,tomcat是8080 # --type=NodePort 生成随机端口,集群外部访问[root@k8s-master1 java-demo]# kubectl expose deployment java-demo --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml &gt; svc.yaml[root@k8s-master1 java-demo]# vim svc.yaml apiVersion: v1kind: Servicemetadata: creationTimestamp: null labels: app: java-demo name: java-demospec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: java-demo type: NodePortstatus: loadBalancer: &#123;&#125; 1234567891011121314# 创建 service[root@k8s-master1 java-demo]# kubectl apply -f svc.yaml service/java-demo created# 查看 [root@k8s-master1 java-demo]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/java-demo-7746bb968c-4drq9 1/1 Running 0 10mpod/java-demo-7746bb968c-54gtv 1/1 Running 0 10mpod/java-demo-7746bb968c-s7824 1/1 Running 0 8m9sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/java-demo NodePort 10.0.0.72 &lt;none&gt; 80:30999/TCP 15sservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 7d6h 测试访问123# nodeIP + 端口访问http://39.106.168.181:30999/http://39.106.100.108:30999/ 12345678910# 测试添加数据 然后 分别查看MariaDB [test]&gt; select * from user;+----+--------+-----+------+| id | name | age | sex |+----+--------+-----+------+| 1 | 小乔 | 28 | F || 2 | 大乔 | 29 | M |+----+--------+-----+------+2 rows in set (0.00 sec)]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05 Service 与 外界连通]]></title>
    <url>%2F2019%2F11%2F08%2Fk8s-05%2F</url>
    <content type="text"><![CDATA[Pod与Service的联系 防止Pod失联 定义一组Pod的访问策略 支持ClusterIP，NodePort以及LoadBalancer三种类型 Service的底层实现主要有Iptables和IPVS二种网络模式 如何转发流量和负载均衡 问题: 1.多个Pod如何负载均衡 2.容器崩溃重新拉起,IP发生变化,如何感知 3.如何通过域名访问应用 123451. Service 的主要作用: 防止Pod失联 2. Service 感知PodIP: 如果一个Pod里面有创建3个副本,其中一个down了需要拉起来一个新的，新的IP又与之前的不一致3. Service 可以动态感知将这个新的Pod加入进来4. 用户不用去考虑PodIP 访问Service即可5. Service负载均衡到Pod Service 定义123456789101112131415[root@k8s-master1 demo]# vim web-service.yaml apiVersion: v1 # api版本 kind: Service # 资源对象 Servicemetadata: # 元数据 labels: app: web name: web # service namespec: ports: - port: 80 # service端口 protocol: TCP targetPort: 8080 # 指向容器的端口 selector: app: web # 如何知道后端关联的pod 标签选择器 type: NodePort # NodePort：在每个Node上分配一个端口作为外部访问入口 12[root@k8s-master demo2]# kubectl apply -f service1.yaml service/my-service created 123456# 列出当前service# kubernetes 默认service [root@k8s-master demo2]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 31hmy-service ClusterIP 10.0.0.123 &lt;none&gt; 80/TCP 26s 123456789# kubernetes这个默认的service是负载 master apiserver的IP# 查看endpoints[root@k8s-master demo2]# kubectl get endpointsNAME ENDPOINTS AGEkubernetes 172.17.70.245:6443,172.17.70.246:6443 31hmy-service 10.244.1.24:80 2m34s[root@k8s-master demo2]# kubectl get ep 每个service对应1个endpoint控制器 endpoints关联pod 查看pod的标签1234567891011121314151617# 查看具有 app=web 的pod[root@k8s-master1 demo]# kubectl get pod -l app=webNAME READY STATUS RESTARTS AGEweb-84c6fbbdcc-6vsj8 1/1 Running 0 2m23sweb-84c6fbbdcc-krrdr 1/1 Running 0 2m23sweb-84c6fbbdcc-q8x8l 1/1 Running 0 2m23s[root@k8s-master1 demo]# kubectl get pod -l app=web -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-84c6fbbdcc-6vsj8 1/1 Running 0 3m2s 10.244.0.60 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-krrdr 1/1 Running 0 3m2s 10.244.0.59 k8s-node1 &lt;none&gt; &lt;none&gt;web-84c6fbbdcc-q8x8l 1/1 Running 0 3m2s 10.244.1.60 k8s-node2 &lt;none&gt; &lt;none&gt;# 容器IP 10.244.0.60 10.244.0.59 10.244.1.60# endpoint控制器 帮我们感知到新启动的pod,加入进来 1234567891011121314151617# 查看svc详细信息[root@k8s-master1 demo]# kubectl describe svc webName: webNamespace: defaultLabels: app=webAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;"apiVersion":"v1","kind":"Service","metadata":&#123;"annotations":&#123;&#125;,"creationTimestamp":null,"labels":&#123;"app":"web"&#125;,"name":"web","namespace":...Selector: app=webType: NodePortIP: 10.0.0.155Port: &lt;unset&gt; 80/TCPTargetPort: 8080/TCPNodePort: &lt;unset&gt; 30669/TCPEndpoints: 10.244.0.59:8080,10.244.0.60:8080,10.244.1.60:8080Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 启动pod关联service123456789101112131415161718192021222324252627[root@k8s-master demo2]# vim nginx-1108.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx name: nginx namespace: defaultspec: replicas: 3 selector: matchLabels: app: nginx # 与pod的labels 保持一致 template: metadata: labels: # 与service的labels 保持一致 app: nginx spec: containers: - image: nginx:1.16 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP restartPolicy: Always 1234567891011121314151617181920[root@k8s-master demo2]# kubectl apply -f nginx-1108.yaml deployment.apps/nginx created[root@k8s-master demo2]# kubectl get pods,svc -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/nginx-594cc45b78-9p9p6 1/1 Running 0 25s 10.244.1.27 k8s-node2 &lt;none&gt; &lt;none&gt;pod/nginx-594cc45b78-ltql8 1/1 Running 0 25s 10.244.1.26 k8s-node2 &lt;none&gt; &lt;none&gt;pod/nginx-594cc45b78-zhxs6 1/1 Running 0 25s 10.244.0.29 k8s-node1 &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 31h &lt;none&gt;service/my-service ClusterIP 10.0.0.123 &lt;none&gt; 80/TCP 22s app=nginx[root@k8s-master demo2]# kubectl get endpointsNAME ENDPOINTS AGEkubernetes 172.17.70.245:6443,172.17.70.246:6443 31hmy-service 10.244.0.29:80,10.244.1.26:80,10.244.1.27:80 5s# service 提供的是4层的TCP 负载均衡 即端口转发 Service 类型123• ClusterIP：默认，分配一个集群内部可以访问的虚拟IP（VIP） • NodePort：在每个Node上分配一个端口作为外部访问入口• LoadBalancer：工作在特定的Cloud Provider上，例如Google Cloud，AWS，OpenStack ClusterIP 集群内部应用中间之间访问 ClusterIP 用于集群内部负载均衡IP 不对外暴露 有两个应用 A应用多个副本 访问 B应用多个副本 B想访问A应用 不能访问PODIP，需要访问ServiceIP的ClusterIP B应用写 ClusterIP 应用访问 通过 Iptables/LVS(IPVS) 转发 给serviceIP 再去负载到后面的POD NodePort 外部使用 暴露一个访问入口 如何让用户访问到 部署的应用 用户访问 -&gt; 暴露的端口 再转发给 -&gt; service -&gt; pods NodePort 会在每个 node 上暴露一个端口 这个端口作为用户访问的 入口port 12345678910111213141516[root@k8s-master demo2]# vim service1.yaml apiVersion: v1kind: Servicemetadata: name: my-service namespace: defaultspec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx 123456789[root@k8s-master demo2]# kubectl delete -f service1.yaml service "my-service" deleted[root@k8s-master demo2]# kubectl apply -f service1.yaml service/my-service created[root@k8s-master demo2]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 31h &lt;none&gt;my-service NodePort 10.0.0.125 &lt;none&gt; 80:31376/TCP 26s app=nginx NodePort 也分配 CLUSTER-IP = 10.0.0.125 NodePort 在node上分配端口 80:31376/TCP 31376就是对外访问端口 1234567[root@k8s-node2 ~]# netstat -tnlp | grep 31376tcp6 0 0 :::31376 :::* LISTEN 2788/kube-proxy [root@k8s-node1 ~]# netstat -tnlp | grep 31376tcp6 0 0 :::31376 :::* LISTEN 1632/kube-proxy # 访问：nodeIP+端口# http://39.106.100.108:31376/ 查看 ipvsadm 规则 安装 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-node1 ~]# yum install ipvsadm -y[root@k8s-node1 yum.repos.d]# ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.17.70.247:31376 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 TCP 172.18.0.1:31376 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 TCP 10.0.0.1:443 rr -&gt; 172.17.70.245:6443 Masq 1 2 0 -&gt; 172.17.70.246:6443 Masq 1 0 0 TCP 10.0.0.2:53 rr -&gt; 10.244.0.20:53 Masq 1 0 0 TCP 10.0.0.125:80 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 TCP 10.244.0.0:31376 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 TCP 10.244.0.1:31376 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 TCP 127.0.0.1:31376 rr -&gt; 10.244.0.29:80 Masq 1 0 0 -&gt; 10.244.1.26:80 Masq 1 0 0 -&gt; 10.244.1.27:80 Masq 1 0 0 UDP 10.0.0.2:53 rr -&gt; 10.244.0.20:53 Masq 1 0 0 域名应该找哪个node 还得有一层负载均衡 这层负载均衡负责 转发给 node集群+对外端口 域名解析 -&gt; 负载均衡器 -&gt; node集群+对外端口 -&gt; pod集群 LoadBalancer 这就是前面加一个LB LB 转发给 每个nodeIP+port 域名解析到LB地址 自建LB 需要知道 生成的端口是多少 再添加到LB 公有云 LoadBalancer LB 可以自动关联 提供访问 访问流程和NodePort一致 Service类型小结 NodePort 用户 -&gt; 域名 -&gt; 负载均衡(阿里云公网)(后端手动添加) -&gt; NodeIP(内网):Port -&gt; PodIP+Port LoadBalancer 用户 -&gt; 域名 -&gt; 负载均衡(阿里云公网)(自动添加) -&gt; NodeIP(内网):Port -&gt; PodIP+Port LoadBalancer 特定云提供商底层LB接口 例如aws,google,openstack,aliyun不知道 NodeIP:Port NodeIP不固定 Port可以固定 1234567# cluster IP 范围[root@k8s-master demo2]# cat /opt/kubernetes/cfg/kube-apiserver.conf | grep service-cluster-ip-range--service-cluster-ip-range=10.0.0.0/24 \# 创建 NodePort 生成范围[root@k8s-master demo2]# cat /opt/kubernetes/cfg/kube-apiserver.conf | grep service-node-port--service-node-port-range=30000-32767 \ 固定 NodePort 端口 端口要提前规划好 不要冲突 12345678910111213141516171819# 也得注意范围区间# 固定到31000[root@k8s-master demo2]# vim service1.yaml apiVersion: v1kind: Servicemetadata: name: my-service namespace: defaultspec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 80 nodePort: 31000 selector: app: nginx 123456789[root@k8s-master demo2]# kubectl apply -f service1.yaml service/my-service configured[root@k8s-master demo2]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 32hmy-service NodePort 10.0.0.125 &lt;none&gt; 80:31000/TCP 28mhttp://39.106.100.108:31000/ Service 代理模式123底层流量转发与负载均衡实现：• Iptables• IPVS 工作进程 kube-proxy 他完成了 流量转发规则 1.12/13 都是 Iptables 创建pod时 kube-proxy 会在node上生成规则4，如果是iptabes 使用iptables-save | more 查看所有规则 流量转发Dnat 123[root@k8s-node1 yum.repos.d]# ps -ef|grep kube-proxyroot 1632 1 0 07:53 ? 00:00:17 /opt/kubernetes/bin/kube-proxy --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --config=/opt/kubernetes/cfg/kube-proxy-config.ymlroot 4672 4070 0 17:19 pts/1 00:00:00 grep --color=auto kube-proxy 12[root@k8s-node1 yum.repos.d]# cat /opt/kubernetes/cfg/kube-proxy-config.yml | grep modemode: ipvs IPVS 如果是iptabes代理 一个service会创建很多的规则 (更新) pod挂了会刷新规则 iptabes规则都是从上到下逐条匹配(匹配延迟) ipvs:LVS基于IPVS内核调度模块实现的负载均衡,LVS基于四层负载均衡,IPVS解决上面的问题 ipvs 基于内核态规则 性能更好 iptables 用户态 12[root@k8s-node1 yum.repos.d]# ipvsadm -ln[root@k8s-node1 yum.repos.d]# ip addr 123# 当创建 svc 会在node上创建一个 kube-ipvs0 虚拟网卡# 他绑定所有 svc IP 来应答所有数据库交给谁来处理# 这些IP 都会通过 ipvs来刷新规则 Iptables VS IPVS 企业中更多使用ipvs 在配置文件中 设置调度算法 1234Iptables： • 灵活，功能强大（可以在数据包不同阶段对包进行操作）• 规则遍历匹配和更新，呈线性时延IPVS： • 工作在内核态，有更好的性能• 调度算法丰富：rr，wrr，lc，wlc，ip hash... 修改ipvs的调度算法123456789101112131415[root@k8s-node1 yum.repos.d]# vim /opt/kubernetes/cfg/kube-proxy-config.yml kind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1address: 0.0.0.0metricsBindAddress: 0.0.0.0:10249clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfighostnameOverride: k8s-node1clusterCIDR: 10.0.0.0/24mode: ipvsipvs: scheduler: "wrr"iptables: masqueradeAll: true 12[root@k8s-node1 yum.repos.d]# systemctl restart kube-proxy[root@k8s-node1 yum.repos.d]# ipvsadm -ln 部署集群内部DNS服务（CoreDNS） service 内部之间用clusterIP 进行通信 clusterIP 并不是固定的 程序也不该写死 项目迁移写死IP很麻烦 访问地址可以通过dns名称 IP有变化也不会影响 集群内部的dns来为service做名称的访问 1231. DNS服务监视Kubernetes API，为每一个Service创建DNS记录用于域名解析。2. ClusterIP A记录格式：&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local3. 示例：my-svc.my-namespace.svc.cluster.local 默认的是 coredns 早期版本 kube-dns 下载地址: 1https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/coredns 1234567891011spec: selector: k8s-app: kube-dns clusterIP: 10.0.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP 123456789101112131415161718# cluster.local 域data: Corefile: | .:53 &#123; errors health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance &#125; 1234# 镜像地址 containers: - name: coredns image: lizhenliang/coredns:1.2.2 12345[root@k8s-master ~]# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-vfzxs 1/1 Running 3 32hkube-flannel-ds-amd64-4jkk8 1/1 Running 9 32hkube-flannel-ds-amd64-vxfdb 1/1 Running 2 32h 测试解析dns1234567891011121314151617# 部署 busybox镜像 指定版本 1.28.4[root@k8s-master1 demo]# vim busybox.yaml apiVersion: v1kind: Podmetadata: name: dns-testspec: containers: - name: busybox image: busybox:1.28.4 args: - /bin/sh - -c - sleep 36000 restartPolicy: Never 1234567891011121314151617181920[root@k8s-master1 demo]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEapp01 NodePort 10.0.0.198 &lt;none&gt; 80:31000/TCP 93mkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 6d1h[root@k8s-master1 demo]# kubectl exec -it dns-test sh/ # nslookup app01Server: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: app01Address 1: 10.0.0.198 app01.default.svc.cluster.local # &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local/ # ping app01.default.svc.cluster.localPING app01.default.svc.cluster.local (10.0.0.198): 56 data bytes64 bytes from 10.0.0.198: seq=0 ttl=64 time=0.053 ms64 bytes from 10.0.0.198: seq=1 ttl=64 time=0.056 ms64 bytes from 10.0.0.198: seq=2 ttl=64 time=0.055 ms 跨命名空间解析123456# 默认是解析当前pod的命名空间下的service名字nslookup my-service # 跨命名空间需要加上 service.命名空间nslookup my-service.defaultnslookup my-service."namespace"]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 Pod 介绍与容器分类]]></title>
    <url>%2F2019%2F11%2F07%2Fk8s-04%2F</url>
    <content type="text"><![CDATA[Pod 简介1234• 最小部署单元 • 一组容器的集合 紧密的服务• 一个Pod中的容器共享网络命名空间,可以使用127.0.0.1访问服务• Pod是短暂的 Pod容器分类123456• Infrastructure Container：基础容器 • 维护整个Pod网络空间• InitContainers：初始化容器，在部署之前做一些初始化操作 • 先于业务容器开始执行• Containers：业务容器 主要使用这个 • 并行启动 基础容器会启动,他是透明的但是可以在node节点使用docker images查看到 应该把pause-amd64:3.0下载下来保存到私有仓库,将地址替换成私有仓库地址 每次创建一个pod,都会创建一个这个容器,他的作用是将pod里面的所有容器放到一个命名空间 12[root@k8s-node1 cfg]# cat /opt/kubernetes/cfg/kubelet.conf --pod-infra-container-image=lizhenliang/pause-amd64:3.0" Pod 实现机制121. 共享网络2. 共享存储 共享网络12345678910# 1. 创建pod后在创建业务容器之前会先创建基础容器pause-amd64:3.0# 2. 创建的业务容器会加入到 pause-amd64:3.0的 network-namespace里 # 3. pod的IP 绑定的是 pause-amd64:3.0 容器里,让其他的容器使用一个 namespace[root@k8s-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8e6be9e3476c b50b08c36b60 "nginx -g 'daemon of…" 4 seconds ago Up 3 seconds k8s_nginx_nginx-deployment-594cc45b78-dzmcw_default_77001f39-25e7-4e5d-b279-f06aa3c17382_0c4383d31c17a b50b08c36b60 "nginx -g 'daemon of…" 4 seconds ago Up 3 seconds k8s_nginx_nginx-deployment-594cc45b78-pm688_default_f8f84530-abc1-4c8c-83f3-8d8caab08217_0d0eea8a485bf 172.17.70.252/base/pause-amd64:3.0 "/pause" 4 seconds ago Up 3 seconds k8s_POD_nginx-deployment-594cc45b78-pm688_default_f8f84530-abc1-4c8c-83f3-8d8caab08217_0102650cf1d5e 172.17.70.252/base/pause-amd64:3.0 "/pause" 4 seconds ago Up 3 seconds k8s_POD_nginx-deployment-594cc45b78-dzmcw_default_77001f39-25e7-4e5d-b279-f06aa3c17382_0 1234# 1个POD里面有多个容器 他们共享一个网络命名空间POD 容器1 Java容器2 Nginx 12345678910111213141516171819# 导出一个正在运行的Pod修改[root@k8s-master1 java-demo]# kubectl get pod java-demo-7746bb968c-4hj9k -o yaml &gt; pod.yaml# 1个Pod运行多个容器,容器使用-name 分割[root@k8s-master1 java-demo]# vim pod.yaml apiVersion: v1kind: Podmetadata: labels: app: my-pod name: my-pod namespace: defaultspec: containers: - name: my-nginx image: nginx:1.7.9 - name: my-java image: 172.17.70.252/project/java-demo:latest 1234567891011121314151617181920212223242526272829# 创建[root@k8s-master1 java-demo]# kubectl apply -f pod.yaml pod/my-pod created# READY 运行个数为2个[root@k8s-master1 java-demo]# kubectl get pods,deploy -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/my-pod 2/2 Running 0 19s 10.244.1.30 k8s-node2 &lt;none&gt; &lt;none&gt;# 看下事件[root@k8s-master1 java-demo]# kubectl describe pod my-podEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/my-pod to k8s-node2 Normal Pulling 2m21s kubelet, k8s-node2 Pulling image "nginx:1.7.9" Normal Pulled 2m14s kubelet, k8s-node2 Successfully pulled image "nginx:1.7.9" Normal Created 2m14s kubelet, k8s-node2 Created container my-nginx Normal Started 2m14s kubelet, k8s-node2 Started container my-nginx Normal Pulling 2m14s kubelet, k8s-node2 Pulling image "172.17.70.252/project/java-demo:latest" Normal Pulled 2m14s kubelet, k8s-node2 Successfully pulled image "172.17.70.252/project/java-demo:latest" Normal Created 2m14s kubelet, k8s-node2 Created container my-java Normal Started 2m14s kubelet, k8s-node2 Started container my-java# 查看node2容器 只启动使用了一个基础镜像,那么就是说 新建的两个容器会加入这一个pause容器的命名空间[root@k8s-node2 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5701076efaf9 172.17.70.252/project/java-demo "catalina.sh run" 3 minutes ago Up 3 minutes k8s_my-java_my-pod_default_363d3a86-4892-4e5b-bb09-8a421cbb7ad8_0013fd701fbeb nginx "nginx -g 'daemon of…" 3 minutes ago Up 3 minutes k8s_my-nginx_my-pod_default_363d3a86-4892-4e5b-bb09-8a421cbb7ad8_00208eacccdce 172.17.70.252/base/pause-amd64:3.0 "/pause" 3 minutes ago Up 3 minutes k8s_POD_my-pod_default_363d3a86-4892-4e5b-bb09-8a421cbb7ad8_0 123456789# 验证是否同一个网络命名空间# 在一个pod中启动多个容器,那么他们是共享同一网络命名空间,IP、prot、mac[root@k8s-master1 java-demo]# kubectl exec -it my-pod bashDefaulting container name to my-nginx.Use 'kubectl describe pod/my-pod -n default' to see all of the containers in this pod.[root@k8s-master1 java-demo]# kubectl exec -it my-pod -c my-nginx bash[root@k8s-master1 java-demo]# kubectl exec -it my-pod -c my-java bash 共享存储1234561. Pod 为亲密性应用而存在亲密性应用场景: - 两个应用之间发生交互 - 两个应用需要通过127.0.0.1 或者 socket通信 - 两个应用需要发生频繁的调用 - 常用的有 nginx 代理 tomcat应用 127.0.0.1:8080 1231. 共享存储不能使用 同一个存储的命名空间2. 因为镜像中的操作系统不太相同,违背了容器的概念3. 存储使用数据卷,将应用数据持久化 12345678910111213141. POD 持久化数据: - 临时数据 - 日志 - 业务数据 重要的 mysql的/data目录 2. 有状态的需要存储数据的应用程序能够在多个节点之间相互漂移node1 pod1 挂掉了,数据不能存储在本地,需要使用存储数据卷node2 pod2 重新拉起一个pod,使用node1上的pod1同一个数据卷,数据不会丢失,接着用node3 node1 node2 node3 Volumes 共享存储 同一Pod下的容器共享数据123456789101112131415161718192021222324252627282930# 1. emptyDir# 2. 创建一个空卷，挂载到Pod中的容器。# 3. Pod删除该卷也会被删除。$ .4 应用场景：Pod中容器之间数据共享[root@k8s-master demo2]# vim emptydir.yaml apiVersion: v1kind: Podmetadata: name: my-pod2spec: containers: - name: write image: centos:7 command: ["bash","-c","for i in &#123;1..100&#125;;do echo $i &gt;&gt; /data/hello;sleep 1;done"] volumeMounts: - name: data mountPath: /data - name: read image: centos:7 command: ["bash","-c","tail -f /data/hello"] volumeMounts: - name: data mountPath: /data volumes: - name: data emptyDir: &#123;&#125; 123456789101112131415161718192021222324# 创建了两个容器 read 和 write# 一个Pod里的多个容器会被分配到一个Node节点上# 会在当前的节点创建一个空目录 /data,然后两个容器都挂载这个目录 [root@k8s-master1 java-demo]# kubectl apply -f emptydir.yaml pod/my-pod2 created[root@k8s-master1 java-demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-pod 2/2 Running 0 35m 10.244.1.30 k8s-node2 &lt;none&gt; &lt;none&gt;my-pod2 2/2 Running 0 34s 10.244.0.33 k8s-node1 &lt;none&gt; &lt;none&gt;# 进入写容器查看[root@k8s-master1 java-demo]# kubectl exec -it my-pod2 -c write bash[root@my-pod2 /]# cd /data/[root@my-pod2 data]# tailf hello 8788# 进入读容器查看[root@k8s-master1 java-demo]# kubectl exec -it my-pod2 -c read bash[root@my-pod2 data]# tailf hello # 看read的日志 他后台运行的就是tailf [root@k8s-master1 java-demo]# kubectl logs my-pod2 -c read Pod 存在的意义Pod 镜像拉取策略 imagePullPolicy 123• IfNotPresent：默认值，镜像在宿主机上不存在时才拉取• Always：每次创建 Pod 都会重新拉取一次镜像• Never： Pod 永远不会主动拉取这个镜像 12345678# 导出的yaml文件中 默认值就是IfNotPresent containers: - image: nginx:1.16 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP 拉取私有仓库镜像 harbord的认证凭据配置可信任12345678[root@k8s-node1 ~]# cat /etc/docker/daemon.json &#123; "registry-mirrors": ["http://bc437cce.m.daocloud.io"], "insecure-registries": ["172.17.70.245"]&#125; # 添加或者修改 需要重启dockersystemctl restart docker.service 12345678910# 镜像仓库 上传一个镜像[root@k8s-node1 ~]# docker login 172.17.70.245# 上传一个tomcat镜像[root@k8s-node1 ~]# docker pull tomcat# docker tag SOURCE_IMAGE[:TAG] 172.17.70.245/project/IMAGE[:TAG]# docker push 172.17.70.245/project/IMAGE[:TAG][root@k8s-node1 ~]# docker push 172.17.70.245/project/tomcat 配置k8s认证 docker主机虽然login 但是不代表k8s也是通过认证的,她不是登录的状态 因为私有仓库下载需要凭证 12# 参考文档 https://kubernetes.io/zh/docs/concepts/containers/images/ 测试k8s无凭证拉取私有仓库镜像 docker 与 k8s 登录私有仓库的凭证 不是一套 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 创建一个tomcat.yaml# 由于本地有镜像了 所以改下拉取策略 一直会去重新拉取[root@k8s-master demo]# cp mynginx.yaml tomcat-deployment.yaml[root@k8s-master demo]# vim tomcat-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: tomcat name: tomcatspec: replicas: 3 selector: matchLabels: app: tomcat template: metadata: labels: app: tomcat spec: containers: - image: 172.17.70.245/project/tomcat # 私有镜像仓库 imagePullPolicy: Always # 拉取策略 name: tomcat ports: - containerPort: 8080 # 容器端口---apiVersion: v1kind: Servicemetadata: name: tomcat-service labels: app: tomcatspec: type: NodePort # 随机分配 ports: - port: 80 # service负载端口 targetPort: 8080 # 容器端口 selector: app: tomcat 123[root@k8s-master demo]# kubectl apply -f tomcat-deployment.yaml deployment.apps/tomcat createdservice/tomcat-service unchanged 1234567891011121314# 镜像无法拉取[root@k8s-master demo]# kubectl get pods,svc,deployNAME READY STATUS RESTARTS AGEpod/tomcat-d54b746dc-f5pht 0/1 ImagePullBackOff 0 54spod/tomcat-d54b746dc-kz9vj 0/1 ImagePullBackOff 0 54spod/tomcat-d54b746dc-tqstj 0/1 ImagePullBackOff 0 54sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 24hservice/tomcat-service NodePort 10.0.0.126 &lt;none&gt; 80:31882/TCP 2m9sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/tomcat 0/3 3 0 54s 查看pod事件1[root@k8s-master demo]# kubectl describe pod tomcat-d54b746dc-f5pht 配置凭据1234567891011121314151617181920# 获取docker的认证信息# node节点上登录都存在一致的# 保存登录harbor的认证信息[root@k8s-node1 ~]# cat .docker/config.json &#123; "auths": &#123; "172.17.70.245": &#123; "auth": "YWRtaW46bHg2ODMyODE1Mw==" &#125; &#125;, "HttpHeaders": &#123; "User-Agent": "Docker-Client/18.09.6 (linux)" &#125;&#125;# 编码 base64 不换行[root@k8s-node1 ~]# cat .docker/config.json|base64 -w 0ewoJImF1dGhzIjogewoJCSIxNzIuMTcuNzAuMjQ1IjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NmJIZzJPRE15T0RFMU13PT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA5LjYgKGxpbnV4KSIKCX0KfQ== 1234567891011121314# Secret 配置# Pod只能引用和它相同namespace的ImagePullSecrets， 所以需要为每一个namespace做配置# 使用 Secret 来保存认证信息# registry-pull-secret 就是拉取镜像的策略[root@k8s-master ~]# vim registry-pull-secret.yamlapiVersion: v1kind: Secretmetadata: name: registry-pull-secretdata: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSIxNzIuMTcuNzAuMjQ1IjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NmJIZzJPRE15T0RFMU13PT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE4LjA5LjYgKGxpbnV4KSIKCX0KfQ==type: kubernetes.io/dockerconfigjson 123456789[root@k8s-master ~]# kubectl create -f registry-pull-secret.yaml secret/registry-pull-secret created# 如果数据是0 就是没有成功保存进去[root@k8s-master ~]# kubectl get secretNAME TYPE DATA AGEdefault-token-m8m58 kubernetes.io/service-account-token 3 24hregistry-pull-secret kubernetes.io/dockerconfigjson 1 76s 12345678910# 配置凭据 spec: imagePullSecrets: - name: registry-pull-secret containers: - image: 172.17.70.245/project/tomcat imagePullPolicy: Always name: tomcat ports: - containerPort: 8080 1234# 创建[root@k8s-master demo]# kubectl apply -f tomcat-deployment.yaml deployment.apps/tomcat configured # 有修改service/tomcat-service unchanged 1234567891011121314# 可以创建啦[root@k8s-master demo]# kubectl get pods,svc,deployNAME READY STATUS RESTARTS AGEpod/tomcat-67c68f7479-c68fx 1/1 Running 0 107spod/tomcat-67c68f7479-q8jrx 1/1 Running 0 96spod/tomcat-67c68f7479-v27gp 1/1 Running 0 108sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 25hservice/tomcat-service NodePort 10.0.0.126 &lt;none&gt; 80:31882/TCP 39mNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/tomcat 3/3 3 3 38m 12345# 测试访问http://39.106.100.108:31882http://123.56.14.192:31882# 仓库三个pod 都会拉取1次 仓库显示下载次数3次 Pod 资源限制 Pod和Container的资源请求和限制： 不能让容器占用所有的物理资源 12# 官方示例https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ 1234• spec.containers[].resources.limits.cpu• spec.containers[].resources.limits.memory• spec.containers[].resources.requests.cpu• spec.containers[].resources.requests.memory 12limits 资源的总限制requests 创建pod最低 如果node上没有资源，就不会调度到node上，必须满足才能调度上去 123456789# 调用了docker自身限制requests: # 创建容器时要满足memory: "64Mi" # 内存64Mcpu: "250m" # 1核cpu的25%limits: # 最大使用memory: "128Mi" # 最大使用内存 128Mcpu: "500m" # 最大使用cpu 1核心 50% 0.5个cpu 1234567891011121314151617181920212223242526272829[root@k8s-master demo]# vim pod2.yaml apiVersion: v1kind: Podmetadata: name: frontendspec: containers: - name: db image: mysql:5.6 env: - name: MYSQL_ROOT_PASSWORD value: "password" resources: requests: memory: "64Mi" cpu: "250m" limits: memory: "128Mi" cpu: "500m" - name: wp image: wordpress resources: requests: memory: "64Mi" cpu: "250m" limits: memory: "128Mi" cpu: "500m" 12[root@k8s-master demo]# kubectl apply -f pod2.yaml pod/frontend created 12345678910111213141516171819202122232425[root@k8s-master demo]# kubectl get pod# 在一个pod中创建了两个容器 wordpress 和 mysql# 本地没有镜像会去下载镜像 然后再启动容器NAME READY STATUS RESTARTS AGEfrontend 0/2 ContainerCreating 0 28stomcat-67c68f7479-c68fx 1/1 Running 0 56mtomcat-67c68f7479-q8jrx 1/1 Running 0 56mtomcat-67c68f7479-v27gp 1/1 Running 0 56m[root@k8s-master demo]# kubectl describe pod frontendEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/frontend to k8s-node1 Normal Pulling 65s kubelet, k8s-node1 Pulling image "wordpress" Normal Created 50s kubelet, k8s-node1 Created container wp Normal Started 50s kubelet, k8s-node1 Started container wp Normal Pulled 50s kubelet, k8s-node1 Successfully pulled image "wordpress" Normal Pulling 24s (x3 over 2m37s) kubelet, k8s-node1 Pulling image "mysql" Normal Created 23s (x3 over 66s) kubelet, k8s-node1 Created container db Normal Started 23s (x3 over 65s) kubelet, k8s-node1 Started container db Normal Pulled 23s (x3 over 66s) kubelet, k8s-node1 Successfully pulled image "mysql" Warning BackOff 2s (x3 over 38s) kubelet, k8s-node1 Back-off restarting failed container 12345# 查看pod日志 pod中两个容器会有选择[root@k8s-master demo]# kubectl logs frontendError from server (BadRequest): a container name must be specified for pod frontend, choose one of: [db wp][root@k8s-master demo]# kubectl logs frontend db 检查节点容量和分配的数量123456789101112# kubectl describe nodes k8s-node1# 显示的是pod的总和数据[root@k8s-master demo]# kubectl describe nodes k8s-node1# 查看命名空间[root@k8s-master demo]# kubectl get nsNAME STATUS AGEdefault Active 26hkube-node-lease Active 26hkube-public Active 26hkube-system Active 26h Pod 重启策略 再K8S中 POD 是没有重启的概念，每次都是重建 jod 计划任务 都是一次性的 不适合 Always Always 适合 web服务 一直在运行 挂了再拉起一个 12345678910• Always：当容器终止退出后，总是重启容器，默认策略。• OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。• Never:：当容器终止推出，从不重启容器。spec: containers: - name: foo image: janedoe/awesomeapp:v1 restartPolicy: Always 异常退出重启1234567891011121314151617181920[root@k8s-master demo]# kubectl edit deployment tomcatrestartPolicy: Always # 默认策略# 写个例子 # busybox镜像 重启30秒后exit退出[root@k8s-master demo]# vim pod3.yaml apiVersion: v1kind: Podmetadata: name: foospec: containers: - name: busybox image: busybox args: - /bin/sh - -c - sleep 30; exit 3 123456789[root@k8s-master demo]# kubectl apply -f pod3.yaml # RESTARTS 重启的次数 运行了32秒 重启了一次 说明pod是默认Always重启策略[root@k8s-master demo]# kubectl get podsNAME READY STATUS RESTARTS AGEfoo 1/1 Running 1 32stomcat-67c68f7479-c68fx 1/1 Running 0 108mtomcat-67c68f7479-q8jrx 1/1 Running 0 108mtomcat-67c68f7479-v27gp 1/1 Running 0 108m 改成正常退出不重启123456789101112131415[root@k8s-master demo]# vim pod3.yaml apiVersion: v1kind: Podmetadata: name: foospec: containers: - name: busybox image: busybox args: - /bin/sh - -c - sleep 30; exit 0 restartPolicy: Never 123456789101112131415[root@k8s-master demo]# kubectl delete -f pod3.yaml pod "foo" deleted[root@k8s-master demo]# kubectl apply -f pod3.yaml pod/foo created[root@k8s-master demo]# kubectl get podsNAME READY STATUS RESTARTS AGEfoo 0/1 Completed 0 36stomcat-67c68f7479-c68fx 1/1 Running 0 113mtomcat-67c68f7479-q8jrx 1/1 Running 0 113mtomcat-67c68f7479-v27gp 1/1 Running 0 113m# Completed 完成 pod不再会重启，重建# 正常拉起是25秒左右 # 持久运行的应用就是默认值 根据yaml文件删除pod1[root@k8s-master demo]# kubectl delete -f pod3.yaml Pod 健康检查 probes探针1https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ service endpoints 就是service关联的pod的ip地址和端口 健康检查的目的 就是再服务出现问题的情况下 能够重启pod 12345[root@k8s-master ~]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.17.70.245:6443,172.17.70.246:6443 28htomcat-service 10.244.0.24:8080,10.244.0.25:8080,10.244.1.15:8080 3h56m Probe有以下两种类型 12341. livenessProbe如果检查失败，将杀死容器，根据Pod的restartPolicy来操作。2. readinessProbe如果检查失败，Kubernetes会把Pod从service endpoints中剔除。 Probe支持以下三种检查方法 1234561. httpGet发送HTTP请求，返回200-400范围状态码为成功。2. exec执行Shell命令返回状态码是0为成功。3. tcpSocket发起TCP Socket建立成功。 健康检查示例1234567891011121314151617181920212223242526[root@k8s-master demo]# vim pod4.yaml # 文件不存在就是非0 # initialDelaySeconds: 5 # 容器启动5秒后 开始健康检查# periodSeconds: 5 # 每隔5秒执行一次apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-execspec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 10; rm -rf /tmp/healthy; livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 1234567891011[root@k8s-master demo]# kubectl apply -f pod4.yaml pod/liveness-exec created# [root@k8s-master demo]# kubectl get podsNAME READY STATUS RESTARTS AGEfoo 0/1 Completed 0 105mliveness-exec 1/1 Running 1 13stomcat-67c68f7479-c68fx 1/1 Running 0 3h37mtomcat-67c68f7479-q8jrx 1/1 Running 0 3h37mtomcat-67c68f7479-v27gp 1/1 Running 0 3h37m Pod 调度约束 让pod调度到指定的节点上 比如我们有很多node节点 ，希望根据部门区分，A部门使用node 123，B部门使用node 456 默认的调度规则是根据资源利用率，做打分 pod工作流程图 用户在命令行创建pod请求发送给 apiserver, apiserver收到请求,写入到 etcd中,里面记录了用户请求要创建的pod属性, scheduler 调度器 watch 获取 etcd中有新pod需要创建， scheduler 调度器通过算法判断交给哪个节点创建,并更新给etcd,etcd记录要调配到哪个node上, kubelet 通过 watch 从etcd中,获取哪个pod要绑定到自己node中, kubelet 拿到pod要创建的信息后,通过 docker run启动容器,将启动的Pod状态更新到etcd中, 最后 kubectl get pod 请求 apiserver 从etcd中拿到 pod的状态 如果创建的 deployment 还需要控制器参与处理 调度约束 两个字段指定 121. nodeName用于将Pod调度到指定的Node名称上 2. nodeSelector用于将Pod调度到匹配Label的Node上,用于资源区分 指定node创建pod12345678910111213141516171819202122232425262728293031323334[root@k8s-master demo]# vim pod5.yamlapiVersion: v1kind: Podmetadata: name: pod-example labels: app: nginxspec: nodeName: k8s-node2 containers: - name: nginx image: nginx:1.16[root@k8s-master demo]# kubectl apply -f pod5.yaml # 调度到 k8s-node2 [root@k8s-master demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESfoo 0/1 Completed 0 135m 10.244.1.19 k8s-node2 &lt;none&gt; &lt;none&gt;pod-example 1/1 Running 0 44s 10.244.1.22 k8s-node2 &lt;none&gt; &lt;none&gt;tomcat-67c68f7479-c68fx 1/1 Running 0 4h8m 10.244.1.15 k8s-node2 &lt;none&gt; &lt;none&gt;tomcat-67c68f7479-q8jrx 1/1 Running 0 4h8m 10.244.0.25 k8s-node1 &lt;none&gt; &lt;none&gt;tomcat-67c68f7479-v27gp 1/1 Running 0 4h8m 10.244.0.24 k8s-node1 &lt;none&gt; &lt;none&gt;[root@k8s-master demo]# kubectl describe pod pod-example# 没有走调度器 直接创建 ,也就是没有default-scheduler这一步Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulled 63s kubelet, k8s-node2 Container image "nginx:1.16" already present on machine Normal Created 63s kubelet, k8s-node2 Created container nginx Normal Started 63s kubelet, k8s-node2 Started container nginx 通过yaml一次性删除所有pod12[root@k8s-master demo]# cd /opt/demo/[root@k8s-master demo]# kubectl delete -f . nodeSelector 按照标签调度 先给node打标签 node pod 都可以设计标签 12345678910111213141516171819[root@k8s-master demo]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 29h v1.16.0k8s-node2 Ready &lt;none&gt; 29h v1.16.0[root@k8s-master demo]# kubectl label nodes k8s-node1 team=anode/k8s-node1 labeled[root@k8s-master demo]# kubectl label nodes k8s-node2 team=bnode/k8s-node2 labeled[root@k8s-master demo]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSk8s-node1 Ready &lt;none&gt; 29h v1.16.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linux,team=ak8s-node2 Ready &lt;none&gt; 29h v1.16.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node2,kubernetes.io/os=linux,team=b# 前面的都是默认，最后team才是我们自己打的标签# 可以区分团队node了# env_role: dev # env_role: prod 1234567891011121314[root@k8s-master demo]# vim pod5.yaml apiVersion: v1kind: Podmetadata: name: pod-example labels: app: nginxspec: nodeSelector: team: b containers: - name: nginx image: nginx:1.16 12345678910111213141516171819202122[root@k8s-master demo]# kubectl delete -f . [root@k8s-master demo]# kubectl apply -f pod5.yaml [root@k8s-master demo]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-example 1/1 Running 0 8s 10.244.1.24 k8s-node2 &lt;none&gt; &lt;none&gt;[root@k8s-master demo]# kubectl describe pod pod-example# 走了调度器 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/pod-example to k8s-node2 Normal Pulled 37s kubelet, k8s-node2 Container image "nginx:1.16" already present on machine Normal Created 37s kubelet, k8s-node2 Created container nginx Normal Started 37s kubelet, k8s-node2 Started container nginx# 开发# 测试# 生产都在一套集群下 就可以按照标签调度# 创建一个pod 经过哪些流程 Pod 故障排查 理清思路 12# 官方手册https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ pod 的状态 1234# STATUS[root@k8s-master demo]# kubectl get podsNAME READY STATUS RESTARTS AGEpod-example 1/1 Running 0 5m49s 查看 pod 事件123456789# Pending 因为某种原因而不能顺利创建 例如 下载镜像慢或者调度不成功 # 查看 pod 事件[root@k8s-master demo]# kubectl describe pod "podname"# 镜像问题的话 会一直处于 pulling image 的状态# 调度失败在上一层Scheduled # 顺序是从上之下 先调度 再下载镜像 最后启动容器# 下载镜像慢 可以使用加速器或者内部仓库# 调度失败 看看node节点是否满足创建需求 是否不能满足 或者标签不存在 查看 pod 日志12345# kubectl logs "podname" -n "namespace"[root@k8s-master demo]# kubectl logs kube-flannel-ds-amd64-4jkk8 -n kube-system# 所以image需要做的很好 才能减少错误 进入pod中的容器123kubectl describe TYPE/NAMEkubectl logs TYPE/NAME [-c CONTAINER]kubectl exec POD [-c CONTAINER] -- COMMAND [args...] 12[root@k8s-master demo]# kubectl exec -it pod-example bashroot@pod-example:/# 12# running状态 但是服务没有正常启动 可以进入容器查看# 调度流程非常重要]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 kubectl 管理命令]]></title>
    <url>%2F2019%2F11%2F07%2Fk8s-03%2F</url>
    <content type="text"><![CDATA[kubectl 命令行管理工具 标记红色为常用 必须记住 kubectl 工具管理 集群应用 k8s 会帮我们调度到node上 k8s 镜像交付物 CI环境 打包到镜像仓库 应用程序的生命周期 - 基于镜像 创建 run12345678910111213# --replicas=3 副本个数,一般保持在2个以上[root@k8s-master1 ~]# kubectl run mynginx --replicas=3 --image=nginx:1.14 --port=80# deploy = deployment 默认创建的控制器[root@k8s-master1 ~]# kubectl get pods,deployNAME READY STATUS RESTARTS AGEpod/mynginx-559f66f86b-j4q5m 1/1 Running 0 6mpod/mynginx-559f66f86b-q5lpf 1/1 Running 0 6mpod/mynginx-559f66f86b-xltps 1/1 Running 0 6mNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/mynginx 3/3 3 3 6m 发布 expose123456# 将项目暴露出去:1. --type=NodePort 是service 的一种类型 2. --port=80 集群内容访问端口3. --target-port=80 容器端口4. --name=nginx-service service的name5. 生产环境无法使用外网，如果有外网可以用域名接下到nodeip+端口访问 123456789101112131415161718[root@k8s-master1 ~]# kubectl expose deploy/mynginx --port=80 --type=NodePort --target-port=80 --name=nginx-serviceservice/nginx-service exposed# 查看暴露的service[root@k8s-master1 ~]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/mynginx-559f66f86b-j4q5m 1/1 Running 0 10mpod/mynginx-559f66f86b-q5lpf 1/1 Running 0 10mpod/mynginx-559f66f86b-xltps 1/1 Running 0 10mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5d22hservice/nginx-service NodePort 10.0.0.190 &lt;none&gt; 80:30540/TCP 13s# 访问 任意一个node的ip+外部随机端口(30540)[root@k8s-master1 ~]# curl 172.17.70.253:30540[root@k8s-master1 ~]# curl 172.17.70.254:30540 123456789# 查看日志[root@k8s-master tmp]# kubectl get podsNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 5 5h35mnginx-59d795d786-bsfwv 1/1 Running 0 10mnginx-59d795d786-jtrzh 1/1 Running 0 10mnginx-59d795d786-nck54 1/1 Running 0 10mweb-866f97c649-mksh6 1/1 Running 0 6h1m[root@k8s-master tmp]# kubectl logs nginx-59d795d786-bsfwv 123456789# 查看事件[root@k8s-master1 ~]# kubectl describe pod mynginx-559f66f86b-xltpsEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/mynginx-559f66f86b-xltps to k8s-node2 Normal Pulled 17m kubelet, k8s-node2 Container image "nginx:1.14" already present on machine Normal Created 17m kubelet, k8s-node2 Created container mynginx Normal Started 17m kubelet, k8s-node2 Started container mynginx 更新 set ci 打包 升级 更新到最新版本，滚动更新 一个一个 回滚和更新的时候 注意pod的IP地址，每次都不一致,是因为重新创建了新的pod 但是service保持了pod的访问 1234567891011121314151617# 查看一个pod的信息 得到镜像版本[root@k8s-master1 ~]# kubectl describe pod mynginx-559f66f86b-xltps# 通过image 更新# mynginx 是容器名[root@k8s-master1 ~]# kubectl set image deployment/mynginx mynginx=nginx:1.16deployment.apps/mynginx image updated[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEmynginx-579d45d79b-4msl2 1/1 Running 0 73smynginx-579d45d79b-hb9g5 1/1 Running 0 70smynginx-579d45d79b-j426j 1/1 Running 0 71s# 滚动更新 其实就是创建了新的容器[root@k8s-master1 ~]# kubectl describe pod mynginx-579d45d79b-4msl2 回滚 rollout1234567891011121314151617181920212223242526272829# 查看当前项目的版本[root@k8s-master1 ~]# kubectl rollout history deployment/mynginxdeployment.apps/mynginx REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;# 回到上一个版本 感觉是把上一个版本重新部署一次[root@k8s-master1 ~]# kubectl rollout undo deployment/mynginxdeployment.apps/mynginx rolled back[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEmynginx-559f66f86b-6vt4z 1/1 Running 0 3smynginx-559f66f86b-7674b 1/1 Running 0 4smynginx-559f66f86b-flxn5 1/1 Running 0 2smynginx-579d45d79b-4msl2 0/1 Terminating 0 5m25smynginx-579d45d79b-hb9g5 0/1 Terminating 0 5m22smynginx-579d45d79b-j426j 0/1 Terminating 0 5m23s[root@k8s-master1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEmynginx-559f66f86b-6vt4z 1/1 Running 0 41smynginx-559f66f86b-7674b 1/1 Running 0 42smynginx-559f66f86b-flxn5 1/1 Running 0 40s# 查看镜像版本[root@k8s-master1 ~]# kubectl describe pod mynginx-559f66f86b-6vt4z 删除 delete123456789[root@k8s-master1 ~]# kubectl delete deployment/mynginxdeployment.apps "mynginx" deleted[root@k8s-master1 ~]# kubectl delete svc/nginx-serviceservice "nginx-service" deleted[root@k8s-master1 ~]# kubectl get pods,svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 5d22h kubectl 工具远程连接 K8S集群 现在所有的管理都在 master 上操作 无法在别的服务器上管理 比如node 如何在其他服务器上使用kubectl 12# 先将kubectl命令发送到 node1上[root@k8s-master1 ~]# scp /usr/local/bin/kubectl root@172.17.70.253:/usr/bin/kubectl 1234567# 测试使用# master上启动了apiserver,监听端口是 127.0.0.1:8080# 而node上没有启动无法连接apiserver服务[root@k8s-node1 ~]# kubectl get nodesThe connection to the server localhost:8080 was refused - did you specify the right host or port?[root@k8s-master1 ~]# netstat -tnlp | grep 8080 1234# 一般情况下，在k8smaster节点上集群管理工具kubectl是连接的本地http8080端口和apiserver进行通讯的,# 当然也可以通过https端口进行通讯前提是要生成证书。# 所以说kubectl不一定部署在master上，只要能和apiserver进行通讯，那么你可以将kubectl部署在任何一台你想连接到集群的主机上# 生成 kubeconfig配置文件，包含连接apiserver的配置文件 生成admin证书12345# 生成admin证书,我之前已经生成了ca证书[root@k8s-master1 TLS]# mkdir -p /opt/TLS/admin[root@k8s-master1 admin]# cp ../k8s/ca*.pem .[root@k8s-master1 admin]# cp ../k8s/ca-config.json . 12345678910111213141516171819202122232425262728# 证书配置cat &gt; admin-csr.json &lt;&lt;EOF&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF[root@k8s-master1 admin]# ls -ltotal 16-rw-r--r-- 1 root root 229 Nov 19 10:21 admin-csr.json-rw-r--r-- 1 root root 294 Nov 19 10:23 ca-config.json-rw------- 1 root root 1675 Nov 19 10:18 ca-key.pem-rw-r--r-- 1 root root 1359 Nov 19 10:18 ca.pem 12345678910111213141516# 生成证书[root@k8s-master1 admin]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin[root@k8s-master1 admin]# ls -ltotal 28-rw-r--r-- 1 root root 1009 Nov 19 10:23 admin.csr-rw-r--r-- 1 root root 229 Nov 19 10:21 admin-csr.json-rw------- 1 root root 1679 Nov 19 10:23 admin-key.pem-rw-r--r-- 1 root root 1399 Nov 19 10:23 admin.pem-rw-r--r-- 1 root root 294 Nov 19 10:23 ca-config.json-rw------- 1 root root 1675 Nov 19 10:18 ca-key.pem-rw-r--r-- 1 root root 1359 Nov 19 10:18 ca.pem# 拷贝证书以及相关kubectl到目标机器[root@k8s-master1 admin]# scp /usr/local/bin/kubectl root@172.17.70.253:/usr/bin/kubectl [root@k8s-master1 admin]# scp admin*.pem 172.17.70.253:/opt/kubernetes/ssl kubectl配置文件1234567891011121314151617181920# 在node上执行# 生成kubectl配置文件 地址可以是vip# ca.pem CA根证书# admin.pem kubectl的TLS认证证书，具有admin权限# admin-key.pem kubectl的TLS认证私钥# 进入证书目录[root@k8s-node1 ssl]# cd /opt/kubernetes/ssl# 设置连接api的地址 kubectl config set-cluster kubernetes --server=https://172.17.70.251:6443 --embed-certs=true --certificate-authority=ca.pem --kubeconfig=config# 设置用户项中cluster-admin用户证书认证字段kubectl config set-credentials cluster-admin --certificate-authority=ca.pem --embed-certs=true --client-key=admin-key.pem --client-certificate=admin.pem --kubeconfig=config# 设置默认上下文kubectl config set-context default --cluster=kubernetes --user=cluster-admin --kubeconfig=config# 设置当前环境的defaultkubectl config use-context default --kubeconfig=config 123456789101112# 测试[root@k8s-node1 ssl]# kubectl --kubeconfig=./config get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 5d18h v1.16.0k8s-node2 Ready &lt;none&gt; 5d18h v1.16.0# 优化[root@k8s-node1 ssl]# mv config /root/.kube/config[root@k8s-node1 ssl]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 5d18h v1.16.0k8s-node2 Ready &lt;none&gt; 5d18h v1.16.0 YAML配置文件管理资源语法格式123456789YAML 是一种简洁的非标记语言。语法格式：• 缩进表示层级关系• 不支持制表符“tab”缩进，使用空格缩进• 通常开头缩进 2 个空格• 字符后缩进 1 个空格，如冒号、逗号等• “---” 表示YAML格式，一个文件的开始• “#”注释 使用yaml文件创建资源对象 通过文件描述创建的资源 yaml可以留存复用，命令要写很多 123# 参考文档 # 去官网搜索想要的内容 如 deploymenthttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/ deployment12345678910111213141516171819202122232425262728[root@k8s-master1 demo]# mkdir -p /opt/demo/ ;cd /opt/demo/# 查看api的版本[root@k8s-master1 demo]# kubectl api-versions [root@k8s-master demo]# vim nginx-deployment.yamlapiVersion: apps/v1 # 指定api版本kind: Deployment # 资源名metadata: # 元数据信息 name: nginx-deployment # 名称 labels: app: nginx # 标签名spec: replicas: 3 # 副本数 selector: # 标签选择器 matchLabels: app: nginx # 通过app:nginx 关联pod template: # 被管理的对象,实际的容器 metadata: labels: app: nginx # 关联上面控制器的标签 spec: containers: # 容器 - name: nginx # name image: nginx:1.16 # 镜像名 ports: # 端口 - containerPort: 80 12345678910111213# 创建[root@k8s-master1 demo]# kubectl create -f nginx-deployment.yaml deployment.apps/nginx-deployment created# 查看pod[root@k8s-master1 demo]# kubectl get pod,deploy -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/nginx-deployment-594cc45b78-2clch 1/1 Running 0 5m1s 10.244.0.24 k8s-node1 &lt;none&gt; &lt;none&gt;pod/nginx-deployment-594cc45b78-h9h8b 1/1 Running 0 5m1s 10.244.0.23 k8s-node1 &lt;none&gt; &lt;none&gt;pod/nginx-deployment-594cc45b78-rqcld 1/1 Running 0 5m1s 10.244.1.19 k8s-node2 &lt;none&gt; &lt;none&gt;NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.apps/nginx-deployment 3/3 3 3 5m1s nginx nginx:1.16 app=nginx service12# 官网文档https://kubernetes.io/zh/docs/concepts/services-networking/service/ 12345678910111213141516# service 暴露[root@k8s-master1 demo]# vim nginx-service.yaml apiVersion: v1kind: Service # 资源对象metadata: # 源数据 name: nginx-service # name labels: # 标签 关联 pod app: nginxspec: type: NodePort selector: # 选择器 指定上面的service 标签 app: nginx ports: - port: 80 targetPort: 80 123456789101112131415161718[root@k8s-master1 demo]# kubectl create -f nginx-service.yamlservice/nginx-service created[root@k8s-master1 demo]# kubectl get pods,svc,deploy -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/nginx-deployment-594cc45b78-2clch 1/1 Running 0 20m 10.244.0.24 k8s-node1 &lt;none&gt; &lt;none&gt;pod/nginx-deployment-594cc45b78-h9h8b 1/1 Running 0 20m 10.244.0.23 k8s-node1 &lt;none&gt; &lt;none&gt;pod/nginx-deployment-594cc45b78-rqcld 1/1 Running 0 20m 10.244.1.19 k8s-node2 &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 8d &lt;none&gt;service/nginx-service NodePort 10.0.0.146 &lt;none&gt; 80:31291/TCP 96s app=nginxNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.apps/nginx-deployment 3/3 3 3 20m nginx nginx:1.16 app=nginxhttp://39.106.100.108:31291/http://39.106.168.181:31291/ 整合到一起 kubectl 是通过一条命令去创建资源对象 yaml文件 有利于留存复用,通过描述创建资源对象 12345678910111213141516# 删除 pod和service[root@k8s-master1 demo]# kubectl get pod,svc,deployNAME READY STATUS RESTARTS AGEpod/nginx-deployment-594cc45b78-2clch 1/1 Running 0 26mpod/nginx-deployment-594cc45b78-h9h8b 1/1 Running 0 26mpod/nginx-deployment-594cc45b78-rqcld 1/1 Running 0 26mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 8dservice/nginx-service NodePort 10.0.0.146 &lt;none&gt; 80:31291/TCP 7m49sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 3/3 3 3 26m[root@k8s-master1 demo]# kubectl delete service/nginx-service service "nginx-service" deleted[root@k8s-master1 demo]# kubectl delete deployment.apps/nginx-deployment 12345678910111213141516171819202122232425262728293031323334353637383940# 整合到一起[root@k8s-master1 demo]# vim nginx-deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service labels: app: nginxspec: type: NodePort selector: app: nginx ports: - port: 80 targetPort: 80 12345678910111213141516# 创建并查看[root@k8s-master1 demo]# kubectl create -f nginx-deployment.yaml deployment.apps/nginx-deployment createdservice/nginx-service created[root@k8s-master1 demo]# kubectl get pod,svc,deployNAME READY STATUS RESTARTS AGEpod/nginx-deployment-594cc45b78-2wc5c 1/1 Running 0 4spod/nginx-deployment-594cc45b78-mmr9p 1/1 Running 0 4spod/nginx-deployment-594cc45b78-vs7wh 1/1 Running 0 4sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 8dservice/nginx-service NodePort 10.0.0.199 &lt;none&gt; 80:30041/TCP 4sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-deployment 3/3 3 3 4s 通过命令行生成 yaml文件12345678910111213141516171819202122232425262728293031323334# --dry-run 不会实际创建 而是生成yaml# -o yaml &gt; nginx-1.14.yaml 生成配置文件[root@k8s-master demo]# kubectl run nginx --replicas=3 --image=nginx:1.14 --port=80 --dry-run -o yaml &gt; nginx-1.14.yamlkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. [root@k8s-master demo]# vim nginx-1.14.yaml apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: run: nginx name: nginxspec: replicas: 3 selector: matchLabels: run: nginx strategy: &#123;&#125; template: metadata: creationTimestamp: null labels: run: nginx spec: containers: - image: nginx:1.14 name: nginx ports: - containerPort: 80 resources: &#123;&#125;status: &#123;&#125; 生成 yaml yaml文件不容易记住,所以大多数时候我们都生成配置文件,以方便复用 run命令 生成1234567# 查看文件 只保留熟悉的kubectl create deployment java-demo --image 172.17.70.252/project/java-demo:latest --dry-run -o yaml &gt; deploy.yamlkubectl expose deployment java-demo --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml &gt; svc.yamlkubectl run mynginx --replicas=3 --image=nginx:1.14 --port=80 --dry-run -o yaml &gt; mynginx1.14.yaml# 需要先创建pod 关联好 kubectl expose deployment mynginx --port=80 --type=NodePort --target-port=80 --name=nginx-service --dry-run -o yaml &gt; mynginx1.14.svc.yaml get 命令生成123# 查看文件 只保留熟悉的kubectl get deployment/nginx-deployment --export -o yaml &gt; mynginx.yamlkubectl get svc/nginx-service --export -o yaml &gt; mynginx.svc.yaml 查看 pod 资源字段12# Pod容器的字段拼写忘记了kubectl explain pods.spec.containers systemctl restart kube-apiserversystemctl restart kube-controller-managersystemctl restart kube-scheduler]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 搭建一个完整的Kubernetes集群]]></title>
    <url>%2F2019%2F11%2F05%2Fk8s-02%2F</url>
    <content type="text"><![CDATA[生产环境K8S平台规划单Master集群 一个master,存在单点故障，master节点出现问题无法创建和管理应用,已经部署的还是可以工作 etcd可以独立部署(集群部署),只要k8s能连接etcd即可 多Master集群（HA） 多个master如何工作,前面使用负载均衡 重点在apiserver ，node改成连接LB负载均衡 所有请求不会直接到某个 apiserver中，而是先到LB再分配 测试环境 使用单master集群 单master 2个node etc集群部署分别部署在3台里 生产环境 必须使用多master,生产环境绝对不允许出现单点故障 按照下面的生产环境准备,node横向扩容 复用etcd的话 至少6台 机器多 etcd也独立部署 node 配置要高 生产环境 K8S 平台规划 服务器硬件配置推荐 node 不够可以水平扩展 master很少扩展,2-3台就足够了 集群中每台服务器至少预留30%的资源,跑满80%的资源就要考虑扩展 官方提供的三种部署方式 minikube Minikube是一个工具，可以在本地快速运行一个单点的Kubernetes，仅用于尝试Kubernetes或日常开发的用户使用。 部署地址：https://kubernetes.io/docs/setup/minikube/ kubeadm Kubeadm也是一个工具，提供kubeadm init和kubeadm join，用于快速部署Kubernetes集群。 部署地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制 推荐，从官方下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。 下载地址：https://github.com/kubernetes/kubernetes/releases 单master集群搭建系统初始化123456789101112131415161718192021222324关闭防火墙：systemctl stop firewalldsystemctl disable firewalld关闭selinux：setenforce 0 # 临时sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久关闭swap：swapoff -a # 临时vim /etc/fstab # 永久 注释掉swap那项，阿里云没有，只有ext4同步系统时间：# ntpdate time.windows.com添加hosts：# vim /etc/hosts172.17.70.245 k8s-master2172.17.70.246 k8s-master172.17.70.247 k8s-node1172.17.70.248 k8s-node2修改主机名：hostnamectl set-hostname k8s-master etcd 集群理解ssl证书 集群中的通信都是基于https进行交互 当前架构中,etcd之间需要https通信,k8s内部master(apiserver) 也需要通信, 需要两条证书 证书分为: 自签证书 和 权威机构颁发的证书(赛门铁克) 3000元 www.ctnrs.com 泛域名*.ctnrs.com 比较贵 根证书 自签证书 不受信任 内部服务之间 具备一定加密 内部之间程序调用不对外，可以使用 证书包括:crt(数字证书) key(私钥) 配置到web服务器 权威机构颁发的证书 https是绿色的安全的，自签是！感叹号不可信任 权威证书如果使用到k8s里，其实也不知道是否可行，因为证书里涉及到可信任的IP列表，可能有影响 https -&gt; CA -&gt; crt|key 生成etcd证书 可在任意节点完成以下操作。 使用cfssl工具自签证书 cfssl 与 openssl一样都可以生成证书 cfssl 使用json文件内容生成，比较直观 123[root@k8s-master opt]# lsTLS.tar.gz[root@k8s-master opt]# tar -zxvf TLS.tar.gz 123456789[root@k8s-master opt]# cd TLS[root@k8s-master TLS]# ls -ltotal 18820-rwxr-xr-x 1 root root 10376657 Oct 2 08:37 cfssl-rwxr-xr-x 1 root root 6595195 Oct 2 08:37 cfssl-certinfo-rwxr-xr-x 1 root root 2277873 Oct 2 08:37 cfssljson-rwxr-xr-x 1 root root 344 Oct 3 08:26 cfssl.shdrwxr-xr-x 2 root root 4096 Oct 3 10:46 etcd # etcd证书目录drwxr-xr-x 2 root root 4096 Oct 3 10:46 k8s # k8s证书目录 12345678910111213# 移动到可执行目录# cfssl.sh 脚本负责cp,而且他也可以下载,由于我这里已经有脚本了直接cp[root@k8s-master TLS]# cat cfssl.sh #curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl#curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson#curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfocp -rf cfssl cfssl-certinfo cfssljson /usr/local/binchmod +x /usr/local/bin/cfssl*[root@k8s-master TLS]# sh cfssl.sh [root@k8s-master TLS]# ls /usr/local/bin/cfssl cfssl-certinfo cfssljson 123456[root@k8s-master etcd]# cd /opt/TLS/etcd/[root@k8s-master etcd]# ls -l-rw-r--r-- 1 root root 287 Oct 3 13:12 ca-config.json # 相当于CA机构-rw-r--r-- 1 root root 209 Oct 3 13:12 ca-csr.json # CA的请求文件-rwxr-xr-x 1 root root 178 Oct 3 13:58 generate_etcd_cert.sh # 执行脚本-rw-r--r-- 1 root root 306 Oct 3 08:26 server-csr.json # 配置文件 1234567891011121314151617181920212223# 自建CA# 脚本里有两条命令[root@k8s-master etcd]# cat generate_etcd_cert.sh # cfssl gencert -initca ca-csr.json | cfssljson -bare ca -# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server# 单独执行 # 初始化CA -initca # ca的配置文件 ca-csr.json [root@k8s-master etcd]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2019/11/06 08:40:34 [INFO] generating a new CA key and certificate from CSR2019/11/06 08:40:34 [INFO] generate received request2019/11/06 08:40:34 [INFO] received CSR2019/11/06 08:40:34 [INFO] generating key: rsa-20482019/11/06 08:40:34 [INFO] encoded CSR2019/11/06 08:40:34 [INFO] signed certificate with serial number 319049982969134642253684014591326256636267413051# 执行后会生成两个pem文件[root@k8s-master etcd]# [root@k8s-master etcd]# ls -l *.pem-rw------- 1 root root 1675 Nov 6 08:40 ca-key.pem # ca的私钥-rw-r--r-- 1 root root 1265 Nov 6 08:40 ca.pem # ca的数字证书 可以拿着这两个去办法域名证书 12345678910111213141516171819202122232425262728# 为哪个域名去颁发证书# 通过 server-csr.json# CN:名字# hosts是最重要的 可信任的IP 包含所有etcd节点的ip# key 加密算法和长度# names 证书属性信息[root@k8s-master etcd]# vim server-csr.json &#123; "CN": "etcd", "hosts": [ "172.17.70.246", "172.17.70.247", "172.17.70.248" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing" &#125; ]&#125; 123456789101112131415161718# 向CA申请证书 # 指定好ca的证书和私钥，配置文件# 生成 server开头的证书# -config=ca-config.json 里面是ca机构的属性 比如域名证书的有效时间 默认的是10年[root@k8s-master etcd]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server# 会生成server开头的 数字证书和私钥[root@k8s-master etcd]# ls -l server*.pem-rw------- 1 root root 1679 Nov 6 08:49 server-key.pem -&gt; key-rw-r--r-- 1 root root 1338 Nov 6 08:49 server.pem -&gt; crt[root@k8s-master etcd]# ls -l *.pem-rw------- 1 root root 1675 Nov 6 08:40 ca-key.pem-rw-r--r-- 1 root root 1265 Nov 6 08:40 ca.pem-rw------- 1 root root 1679 Nov 6 08:49 server-key.pem-rw-r--r-- 1 root root 1338 Nov 6 08:49 server.pem etcd 简介 CoreOS 开源 etcd首先是一个键值存储仓库,用于配置共享和服务发现。 etcd 负责保存 Kubernetes Cluster 的配置信息和各种资源的状态信息。当数据发生变化时，etcd 会快速地通知 Kubernetes 相关组件。 官方推荐奇数节点部署,常见的有3 5 7 分别对应 1 2 3个冗余节点 3台etcd会先选举出1台为主节点,负责写消息,主节点同步给2个从节点。 当主节点挂了,两台从节点会选举出一台新的主节点 12# 介绍文档https://blog.csdn.net/bbwangj/article/details/82584988 12345# 优点1. 简单。使用Go语言编写部署简单；使用HTTP作为接口使用简单；2. 使用Raft算法保证强一致性让用户易于理解。3. 数据持久化。etcd默认数据一更新就进行持久化。4. 安全。etcd支持SSL客户端安全认证。 部署三个 etcd 节点1234# 环境172.17.70.246 k8s-master etcd172.17.70.247 k8s-node1 etcd172.17.70.248 k8s-node2 etcd 1234567891011121314# 二进制包下载地址https://github.com/etcd-io/etcd/releases# 上传etcd tar包[root@k8s-master opt]# ls -l etcd.tar.gz -rw-r--r-- 1 root root 10148977 Nov 6 09:38 etcd.tar.gz# 解压[root@k8s-master opt]# tar -zxvf etcd.tar.gz [root@k8s-master opt]# lsetcd # 服务目录 放在opt下 etcd.service # CentOS7服务启动文件 # 启动文件里面设置了etct的工作目录 /opt/etcd/bin/etcd 和 配置文件 /opt/etcd/cfg/etcd.conf 123456789101112# 查看工作目录[root@k8s-master etcd]# cd /opt/etcd# 更换etcd版本:可从etcd的github下载最新版本 替换bin下的二进制文件[root@k8s-master etcd]# ls -ldrwxr-xr-x 2 root root 4096 Oct 2 22:13 bin # etcd 可执行文件 etcdctl 客户端 drwxr-xr-x 2 root root 4096 Oct 3 08:32 cfg # 配置文件drwxr-xr-x 2 root root 4096 Oct 3 08:36 ssl # 证书文件 需要将生产的证书加入里面# 先删除掉 一会替换刚才生成的[root@k8s-master ssl]# cd /opt/etcd/ssl/[root@k8s-master ssl]# rm -rf * 123456789101112131415161718192021222324# 查看配置文件# 节点之间通信 2380 需要https通信 拥有证书# 面向客户端 2379 # 证书的配置也都在启动服务里 etcd.service 都使用一套证书# 如果要重装etcd 需要清空下面的目录 ETCD_DATA_DIR="/var/lib/etcd/default.etcd" [root@k8s-master etcd]# cd cfg/[root@k8s-master cfg]# vim etcd.conf #[Member]ETCD_NAME="etcd-1" # 集群名称 唯一ETCD_DATA_DIR="/var/lib/etcd/default.etcd" # etcd存储数据目录ETCD_LISTEN_PEER_URLS="https://172.17.70.246:2380" # etcd内部通信 IP+端口ETCD_LISTEN_CLIENT_URLS="https://172.17.70.246:2379" # 客户端监听 IP+端口 程序连接#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.17.70.246:2380" # 集群内部通信 通告地址ETCD_ADVERTISE_CLIENT_URLS="https://172.17.70.246:2379" # 集群客户端通信# 集群节点连接配置 名称=ip+端口ETCD_INITIAL_CLUSTER="etcd-1=https://172.17.70.246:2380,etcd-2=https://172.17.70.247:2380,etcd-3=https://172.17.70.248:2380"# TOKEN 用于简单认证ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"# 集群状态 新建是new 已有的加入使用 exsitingETCD_INITIAL_CLUSTER_STATE="new" 12# 将证书加入到 ssl 里[root@k8s-master ssl]# cp /opt/TLS/etcd/&#123;ca,server,server-key&#125;.pem ./ 12345678# 部署所有节点[root@k8s-master opt]# scp -r etcd root@172.17.70.247:/opt[root@k8s-master opt]# scp -r etcd root@172.17.70.248:/opt# 配置文件进入启动服务目录scp etcd.service root@172.17.70.246:/usr/lib/systemd/systemscp etcd.service root@172.17.70.247:/usr/lib/systemd/systemscp etcd.service root@172.17.70.248:/usr/lib/systemd/system 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 然后在进去修改相应的配置 主要是 ip 和 ETCD_NAME[root@k8s-node2 etcd]# vim /opt/etcd/cfg/etcd.conf #[Member]ETCD_NAME="etcd-3"ETCD_DATA_DIR="/var/lib/etcd/default.etcd"ETCD_LISTEN_PEER_URLS="https://172.17.70.248:2380"ETCD_LISTEN_CLIENT_URLS="https://172.17.70.248:2379"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://172.17.70.248:2380"ETCD_ADVERTISE_CLIENT_URLS="https://172.17.70.248:2379"ETCD_INITIAL_CLUSTER="etcd-1=https://172.17.70.246:2380,etcd-2=https://172.17.70.247:2380,etcd-3=https://172.17.70.248:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"ETCD_INITIAL_CLUSTER_STATE="new"[root@k8s-node2 etcd]# cat /usr/lib/systemd/system/etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcd.confExecStart=/opt/etcd/bin/etcd \ --name=$&#123;ETCD_NAME&#125; \ --data-dir=$&#123;ETCD_DATA_DIR&#125; \ --listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \ --initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster-state=new \ --cert-file=/opt/etcd/ssl/server.pem \ --key-file=/opt/etcd/ssl/server-key.pem \ --peer-cert-file=/opt/etcd/ssl/server.pem \ --peer-key-file=/opt/etcd/ssl/server-key.pem \ --trusted-ca-file=/opt/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 启动etcd1234567# 三台节点都需要操作# 第一个几点启动会等待 ， 启动后面的成功再来查看# 设置开机启动[root@k8s-master opt]# systemctl daemon-reload[root@k8s-master opt]# systemctl start etcd[root@k8s-master opt]# systemctl enable etcd 12# etcd 日志[root@k8s-master opt]# tailf /var/log/messages 1234567# 查看集群状态 需要指定3个证书哦 # k8s连接etcd 也需要指定这三个etcd证书/opt/etcd/bin/etcdctl \--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \--endpoints="https://172.17.70.246:2379,https://172.17.70.247:2379,https://172.17.70.248:2379" \cluster-health 12345# 端口检查[root@k8s-master opt]# netstat -tnlp|grep etcdtcp 0 0 172.17.70.246:2379 0.0.0.0:* LISTEN 1762/etcd tcp 0 0 127.0.0.1:2379 0.0.0.0:* LISTEN 1762/etcd tcp 0 0 172.17.70.246:2380 0.0.0.0:* LISTEN 1762/etcd 部署 Master Node生成apiserver证书123456789# 查看目录[root@k8s-master k8s]# cd /opt/TLS/k8s/[root@k8s-master k8s]# ls -ltotal 20-rw-r--r-- 1 root root 294 Oct 3 13:12 ca-config.json-rw-r--r-- 1 root root 263 Oct 3 13:12 ca-csr.json-rwxr-xr-x 1 root root 321 Oct 3 08:46 generate_k8s_cert.sh-rw-r--r-- 1 root root 230 Oct 3 13:12 kube-proxy-csr.json # node上的proxy组件证书-rw-r--r-- 1 root root 718 Oct 3 08:45 server-csr.json # apiserver 证书 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master k8s]# vim server-csr.json # hosts 最关键了 是否可信任 接口没有浏览器所以需要配置可信任# 应用程序(服务器IP) 访问 https API (自签证书)# 1. 证书添加IP可信任 # 2. 携带CA证书去访问(http转发怎么办)# 有谁会去访问 LB主从 vip master 写多点没有关系 根据规划 nodeip其实可以不用写&#123; "CN": "kubernetes", "hosts": [ "10.0.0.1", # service第一个IP地址,默认作为apiserver的负载均衡 "127.0.0.1", # 本地 "kubernetes", # 官方要求 内部为api指定dns名称 "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local", "172.17.70.245", "172.17.70.246", "172.17.70.247", "172.17.70.248", "172.17.70.249", "172.17.70.250", "172.17.70.251", "172.17.70.252", "172.17.70.253" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "BeiJing", "ST": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125; 12345678910# 执行脚本 生成证书[root@k8s-master k8s]# ./generate_k8s_cert.sh [root@k8s-master k8s]# ls -l *.pem-rw------- 1 root root 1675 Nov 6 11:09 ca-key.pem-rw-r--r-- 1 root root 1359 Nov 6 11:09 ca.pem-rw------- 1 root root 1679 Nov 6 11:09 kube-proxy-key.pem-rw-r--r-- 1 root root 1403 Nov 6 11:09 kube-proxy.pem-rw------- 1 root root 1675 Nov 6 11:09 server-key.pem-rw-r--r-- 1 root root 1675 Nov 6 11:09 server.pem 部署 apiserve controller-manager和scheduler 在Master节点完成以下操作 配置文件 -&gt; systemd管理组件 -&gt; 启动 二进制包下载地址 123# 下载Server Binaries node的也在里面# 页面打不开 用迅雷下载https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161 1234# 将需要使用的包 替换 如master节点[root@k8s-master bin]# cd /opt/kubernetes/bin/[root@k8s-master bin]# lskube-apiserver kube-controller-manager kubectl kube-scheduler 解压 k8s-master.tar.gz12[root@k8s-master opt]# ls -l /opt/k8s-master.tar.gz [root@k8s-master bin]# tar -zxvf k8s-master.tar.gz 123456789101112131415# 服务启动文件 注意服务配置的目录# 我这里是放在 opt下[root@k8s-master opt]# cat kube-apiserver.service [Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.confExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.target 1234567891011121314[root@k8s-master opt]# tree kubernetes/kubernetes/├── bin│ ├── kube-apiserver│ ├── kube-controller-manager│ ├── kubectl│ └── kube-scheduler├── cfg│ ├── kube-apiserver.conf│ ├── kube-controller-manager.conf│ ├── kube-scheduler.conf│ └── token.csv├── logs # 日志目录└── ssl # 证书目录 配置证书123456789# copy证书[root@k8s-master ssl]# cd /opt/kubernetes/ssl/[root@k8s-master ssl]# cp /opt/TLS/k8s/*.pem .[root@k8s-master ssl]# lsca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem# 保留server证书即可 kube-proxy 是给node用的[root@k8s-master ssl]# rm -rf kube-proxy*[root@k8s-master ssl]# lsca-key.pem ca.pem server-key.pem server.pem 配置文件12# 官方文档https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/ kube-apiserver.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master cfg]# vim kube-apiserver.conf KUBE_APISERVER_OPTS="--logtostderr=false \# 日志级别 从小到大 一般情况下2 越大信息越多--v=2 \# 日志目录--log-dir=/opt/kubernetes/logs \# 连接etcd--etcd-servers=https://192.168.31.61:2379,https://192.168.31.62:2379,https://192.168.31.63:2379 \# 监听IP地址--bind-address=192.168.31.61 \# 监听端口--secure-port=6443 \# 通告地址 让node连接 内网--advertise-address=192.168.31.61 \# 允许创建的容器权限 超级权限创建容器--allow-privileged=true \# service 为pod 做虚拟ip访问网段--service-cluster-ip-range=10.0.0.0/24 \# 插件--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \# 授权模式 --authorization-mode=RBAC,Node \# 自动请求颁发证书--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \# 暴露端口范围--service-node-port-range=30000-32767 \# 连接 kubelet 证书--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \# 配置apiserver使用https证书--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \# etcd 证书 --etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \# 日志审计 访问记录 --audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log" kube-controller-manager.conf12# 官方文档https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ 12345678910111213141516171819202122[root@k8s-master cfg]# vim kube-controller-manager.conf KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--leader-elect=true \# 连接master ip 8080是apiserver监听端口 默认的--master=127.0.0.1:8080 \# 监听本地 不对外--address=127.0.0.1 \--allocate-node-cidrs=true \# POD IP段--cluster-cidr=10.244.0.0/16 \# service IP段--service-cluster-ip-range=10.0.0.0/24 \# 给 node 证书 --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \--root-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \# 给node证书 的有效期时间--experimental-cluster-signing-duration=87600h0m0s" kube-scheduler.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@k8s-master cfg]# vim kube-scheduler.conf KUBE_SCHEDULER_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \# 参加选举--leader-elect \--master=127.0.0.1:8080 \--address=127.0.0.1"``` #### 修改好配置文件1. 只需要修改 apiserver.conf 2. 剩下的两个服务都是连接本地```html[root@k8s-master cfg]# vim kube-apiserver.conf KUBE_APISERVER_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://172.17.70.246:2379,https://172.17.70.247:2379,https://172.17.70.248:2379 \--bind-address=172.17.70.246 \--secure-port=6443 \--advertise-address=172.17.70.246 \--allow-privileged=true \--service-cluster-ip-range=10.0.0.0/24 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \--service-node-port-range=30000-32767 \--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/etcd/ssl/ca.pem \--etcd-certfile=/opt/etcd/ssl/server.pem \--etcd-keyfile=/opt/etcd/ssl/server-key.pem \--audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log" 启动服务123# 启动服务加载 [root@k8s-master opt]# cd /opt/[root@k8s-master opt]# mv kube-apiserver.service kube-controller-manager.service kube-scheduler.service /usr/lib/systemd/system 123456systemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 123456789101112131415[root@k8s-master opt]# systemctl start kube-apiserver[root@k8s-master opt]# ps -ef|grep kube# 查看日志[root@k8s-master opt]# cd /opt/kubernetes/logs/[root@k8s-master logs]# ls -ltotal 112lrwxrwxrwx 1 root root 61 Nov 6 12:07 kube-apiserver.ERROR -&gt; kube-apiserver.k8s-master.root.log.ERROR.20191106-120708.2088lrwxrwxrwx 1 root root 60 Nov 6 12:07 kube-apiserver.INFO -&gt; kube-apiserver.k8s-master.root.log.INFO.20191106-120705.2088-rw-r--r-- 1 root root 414 Nov 6 12:07 kube-apiserver.k8s-master.root.log.ERROR.20191106-120708.2088-rw-r--r-- 1 root root 90528 Nov 6 12:07 kube-apiserver.k8s-master.root.log.INFO.20191106-120705.2088-rw-r--r-- 1 root root 1356 Nov 6 12:07 kube-apiserver.k8s-master.root.log.WARNING.20191106-120706.2088lrwxrwxrwx 1 root root 63 Nov 6 12:07 kube-apiserver.WARNING -&gt; kube-apiserver.k8s-master.root.log.WARNING.20191106-120706.2088[root@k8s-master logs]# less /opt/kubernetes/logs/kube-apiserver.INFO 12# 将kubectl 放入/usr/bin 方便使用[root@k8s-master cfg]# mv /opt/kubernetes/bin/kubectl /usr/local/bin 12345678# 查看集群状态 过会回更新成功 能看到即可 [root@k8s-master local]# kubectl get csNAME AGEscheduler &lt;unknown&gt;controller-manager &lt;unknown&gt;etcd-0 &lt;unknown&gt;etcd-2 &lt;unknown&gt;etcd-1 &lt;unknown&gt; 启用 TLS Bootstrapping123456789101112131415161718[root@k8s-master cfg]# cat kube-apiserver.conf |grep bootstrap--enable-bootstrap-token-auth=true \[root@k8s-master cfg]# cat /opt/kubernetes/cfg/token.csv c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,"system:node-bootstrapper"# 格式：token,用户,uid,用户组# 当机器多的时候 会自动给 kubelet派发证书 node带着这个token 即可 # 给kubelet-bootstrap授权：kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap# 提示成功 clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created# token也可自行生成替换：head -c 16 /dev/urandom | od -An -t x | tr -d ' '但apiserver配置的token必须要与node节点bootstrap.kubeconfig配置里一致。 部署 Worker Node部署docker1Docker 二进制包下载地址：https://download.docker.com/linux/static/stable/x86_64/ 12345678910111213[root@k8s-master opt]# scp k8s-node.tar.gz root@172.17.70.247:/opt[root@k8s-node1 opt]# tar -zxvf k8s-node.tar.gz [root@k8s-node1 opt]# ls -ltotal 207880-rw-r--r-- 1 root root 36662740 Aug 15 19:33 cni-plugins-linux-amd64-v0.8.2.tgz # 插件-rw-r--r-- 1 root root 110 Oct 3 10:01 daemon.json # docker 配置文件-rw-r--r-- 1 root root 48047231 Jun 25 16:45 docker-18.09.6.tgz # docker 二进制-rw-r--r-- 1 root root 501 Oct 3 10:01 docker.service # docker 启动drwxr-xr-x 5 root root 4096 Nov 6 10:04 etcd-rw-r--r-- 1 root root 128129460 Nov 6 15:32 k8s-node.tar.gz-rw-r--r-- 1 root root 268 Oct 2 23:11 kubelet.service # 启动服务-rw-r--r-- 1 root root 253 Oct 2 23:11 kube-proxy.service # 启动服务drwxr-xr-x 6 root root 4096 Oct 2 22:14 kubernetes # node目录 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# 二进制安装docker[root@k8s-node1 opt]# tar -zxvf docker-18.09.6.tgz docker/docker/dockerdocker/docker-initdocker/ctrdocker/docker-proxydocker/runcdocker/containerddocker/dockerddocker/containerd-shim[root@k8s-node1 opt]# mv docker/* /usr/bin/[root@k8s-node1 opt]# mv docker.service /usr/lib/systemd/system# docker服务启动文件[root@k8s-node1 opt]# cat /usr/lib/systemd/system/docker.service [Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.target[Service]Type=notifyExecStart=/usr/bin/dockerdExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.target# 配置文件[root@k8s-node1 opt]# mkdir -p /etc/docker[root@k8s-node1 opt]# mv daemon.json /etc/docker/[root@k8s-node1 opt]# systemctl start docker[root@k8s-node1 opt]# systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@k8s-node1 opt]# docker infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 18.09.6Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: trueLogging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30init version: fec3683Security Options: seccomp Profile: defaultKernel Version: 3.10.0-862.14.4.el7.x86_64Operating System: CentOS Linux 7 (Core)OSType: linuxArchitecture: x86_64CPUs: 2Total Memory: 1.695GiBName: k8s-node1ID: QXH6:OCKS:MDUA:JW6M:IBUW:IWM4:JJ3L:6MLR:DCNO:755D:GEGP:NEIFDocker Root Dir: /var/lib/dockerDebug Mode (client): falseDebug Mode (server): falseRegistry: https://index.docker.io/v1/Labels:Experimental: falseInsecure Registries: 192.168.31.70 127.0.0.0/8Registry Mirrors: http://bc437cce.m.daocloud.io/Live Restore Enabled: falseProduct License: Community Engine 配置 kubelet 和 kube-proxy123456789101112131415161718[root@k8s-node1 opt]# tree kubernetes/kubernetes/├── bin│ ├── kubelet│ └── kube-proxy├── cfg│ ├── bootstrap.kubeconfig│ ├── kubelet.conf│ ├── kubelet-config.yml│ ├── kube-proxy.conf│ ├── kube-proxy-config.yml│ └── kube-proxy.kubeconfig├── logs└── ssl# conf 基本配置文件# kubeconfig 连接apiserver的配置文件# yml 主要配置文件 早起版本没有 都是conf里 动态更新配置文件beta 1234567891011121314151617181920[root@k8s-node1 cfg]# cd /opt/kubernetes/cfg[root@k8s-node1 cfg]# vim kubelet.conf KUBELET_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \# 当前注册到k8s里的名称 唯一--hostname-override=k8s-node1 \# 网络插件--network-plugin=cni \--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \--config=/opt/kubernetes/cfg/kubelet-config.yml \# 目录证书路径--cert-dir=/opt/kubernetes/ssl \# 这个是一个基础容器，每一个Pod启动的时候都会启动一个这样的容器。# 现在每个版本的Kubernetes都把这个镜像打包，你可以提前传到自己的registry上，然后再用这个参数指定。# registry.aliyuncs.com/archon/pause-amd64:3.0--pod-infra-container-image=lizhenliang/pause-amd64:3.0" 1234567891011121314151617181920212223242526# bootstrap 是 k8s为了解决kubelet 很多 手动办法证书,自动能为加入到集群的node 办法证书 # 用于连接apiserver[root@k8s-node1 cfg]# vim bootstrap.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem # master地址 server: https://172.17.70.246:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubelet-bootstrap name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kubelet-bootstrap user: # 必须和master上的token一致 cat /opt/kubernetes/cfg/token.csv # 是用这个token来请求 颁发证书 token: c47ffb939f5ca36231d9e3121a252940 123456[root@k8s-node1 cfg]# vim kube-proxy.conf KUBE_PROXY_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--config=/opt/kubernetes/cfg/kube-proxy-config.yml" 12345678910111213141516171819202122232425# 连接apiserver# 每个组件都会连接apiserver[root@k8s-node1 cfg]# vim kube-proxy.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem # apiserver地址 master地址 server: https://172.17.70.246:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kube-proxy name: defaultcurrent-context: defaultkind: Configpreferences: &#123;&#125;users:- name: kube-proxy user: client-certificate: /opt/kubernetes/ssl/kube-proxy.pem client-key: /opt/kubernetes/ssl/kube-proxy-key.pem 12345678910111213141516[root@k8s-node1 cfg]# vim kube-proxy-config.yml # 动态调整proxy配置kind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1address: 0.0.0.0metricsBindAddress: 0.0.0.0:10249clientConnection: kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfighostnameOverride: k8s-node1clusterCIDR: 10.0.0.0/24mode: ipvsipvs: scheduler: "rr"iptables: masqueradeAll: true 配置启动文件1[root@k8s-node1 opt]# mv *.service /usr/lib/systemd/system 证书拷贝123[root@k8s-master k8s]# cd /opt/TLS/k8s/[root@k8s-master k8s]# scp ca.pem ca-key.pem kube-proxy*.pem root@172.17.70.247:/opt/kubernetes/ssl/[root@k8s-master k8s]# scp ca.pem ca-key.pem kube-proxy*.pem root@172.17.70.248:/opt/kubernetes/ssl/ 启动服务1234567[root@k8s-node1 cfg]# systemctl start kubelet [root@k8s-node1 cfg]# systemctl enable kubelet systemctl start kubeletsystemctl start kube-proxysystemctl enable kubeletsystemctl enable kube-proxy 123456789101112131415# 需要允许给node办法证书[root@k8s-node1 cfg]# tail /opt/kubernetes/logs/kubelet.INFO I1106 16:28:34.120488 2153 feature_gate.go:216] feature gates: &amp;&#123;map[]&#125;I1106 16:28:34.120530 2153 feature_gate.go:216] feature gates: &amp;&#123;map[]&#125;I1106 16:28:34.549201 2153 mount_linux.go:168] Detected OS with systemdI1106 16:28:34.549324 2153 server.go:410] Version: v1.16.0I1106 16:28:34.549374 2153 feature_gate.go:216] feature gates: &amp;&#123;map[]&#125;I1106 16:28:34.549428 2153 feature_gate.go:216] feature gates: &amp;&#123;map[]&#125;I1106 16:28:34.549508 2153 plugins.go:100] No cloud provider specified.I1106 16:28:34.549519 2153 server.go:526] No cloud provider specified: "" from the config file: ""I1106 16:28:34.549545 2153 bootstrap.go:119] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig fileI1106 16:28:34.550829 2153 bootstrap.go:150] No valid private key and/or certificate found, reusing existing private key or creating a new one 允许给Node颁发证书123456789101112131415161718192021222324252627282930313233343536373839404142434445# 查看请求[root@k8s-master TLS]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-M69MAprvmbQdWfAwfsB3Zt07_TS-FlDPPiM9WNLCc4k 4m12s kubelet-bootstrap Pending# 允许[root@k8s-master TLS]# kubectl certificate approve node-csr-M69MAprvmbQdWfAwfsB3Zt07_TS-FlDPPiM9WNLCc4kcertificatesigningrequest.certificates.k8s.io/node-csr-M69MAprvmbQdWfAwfsB3Zt07_TS-FlDPPiM9WNLCc4k approved# 查看node节点# 没准备状态 node日志显示还没有部署CNI插件[root@k8s-master TLS]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 15s v1.16.0# 看下node里面的kubelet证书[root@k8s-node1 ssl]# ls -l kubelet-client*-rw------- 1 root root 1269 Nov 6 16:34 kubelet-client-2019-11-06-16-34-34.pemlrwxrwxrwx 1 root root 58 Nov 6 16:34 kubelet-client-current.pem -&gt; /opt/kubernetes/ssl/kubelet-client-2019-11-06-16-34-34.pem# 帮助我们生成kubelet.kubeconfig# 它自动生成 帮助我们连接api# 它使用的证书 就是生成的证书 [root@k8s-node1 cfg]# cat kubelet.kubeconfigapiVersion: v1clusters:- cluster: certificate-authority: /opt/kubernetes/ssl/ca.pem server: https://172.17.70.246:6443 name: default-clustercontexts:- context: cluster: default-cluster namespace: default user: default-auth name: default-contextcurrent-context: default-contextkind: Configpreferences: &#123;&#125;users:- name: default-auth user: client-certificate: /opt/kubernetes/ssl/kubelet-client-current.pem client-key: /opt/kubernetes/ssl/kubelet-client-current.pem 123456789101112131415161718192021222324# 别忘记还要部署另外一套节点 node2# 流程一致# 主要修改的地方: # IP地址 bootstrap.kubeconfig kube-proxy.kubeconfig[root@k8s-node2 cfg]# grep 246 *bootstrap.kubeconfig: server: https://172.17.70.246:6443kube-proxy.kubeconfig: server: https://172.17.70.246:6443# 文件中主机名 kubelet.conf kube-proxy-config.yml[root@k8s-node2 cfg]# grep hostname *kubelet.conf:--hostname-override=k8s-node2 \kube-proxy-config.yml:hostnameOverride: k8s-node2[root@k8s-master k8s]# kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-M69MAprvmbQdWfAwfsB3Zt07_TS-FlDPPiM9WNLCc4k 28m kubelet-bootstrap Approved,Issued # 已通过node-csr-_9w0NFGK2oLMrMRWyZbs0UymYNcEtZsNd6imJJmXak4 19s kubelet-bootstrap Pending # 请求[root@k8s-master k8s]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 NotReady &lt;none&gt; 26m v1.16.0k8s-node2 NotReady &lt;none&gt; 3s v1.16.0 TLS Bootstrapping 机制流程（kubelet） 部署 CNI 网络 (Flannel)1# 二进制包下载地址：https://github.com/containernetworking/plugins/releases 12345[root@k8s-node1 kubernetes]# tail /opt/kubernetes/logs/kubelet.INFO E1106 17:05:08.212430 2153 kubelet.go:2187] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedW1106 17:05:09.879829 2153 cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d网络没有发现 网络信息在 /etc/cni/net.d下 1234567891011121314[root@k8s-node1 opt]# mkdir -p /opt/cni/bin # 二进制文件目录 kubelet使用 为pod创建网络[root@k8s-node1 opt]# mkdir -p /etc/cni/net.d # 生成配置信息目录 [root@k8s-node1 opt]# tar -zxvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin# copy给node2 别忘记创建目录[root@k8s-node1 opt]# scp -r /opt/cni root@172.17.70.248:/opt[root@k8s-node2 bin]# mkdir -p /etc/cni/net.d # 确保启用cni [root@k8s-node1 opt]# cat /opt/kubernetes/cfg/kubelet.conf | grep cni--network-plugin=cni \ # cni只是第三方网络接口 看看这些网络都由哪些https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 部署 flannel123# 在master部署# 下载 wget https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml 123456789# 上传[root@k8s-master opt]# vim kube-flannel.yaml "Network": "10.244.0.0/16", 必须要和 cat /opt/kubernetes/cfg/kube-controller-manager.conf 的 一致--cluster-cidr=10.244.0.0/16 \# 使用宿主机上的网络# 关键点 他需要是从网上找镜像 image: lizhenliang/flannel:v0.11.0-amd64# 我们可以去网上下载好这个镜像 部署到自己仓库中 12345678910111213141516171819202122232425262728# 启动 pod 查看pod是否启动# 会从 lizhenliang/flannel:v0.11.0-amd64 下载镜像有点慢# 离线部署 提前下载好 导到每个node上本地要是由就不会拉取了[root@k8s-master opt]# kubectl apply -f kube-flannel.yaml [root@k8s-master opt]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-nnrkz 0/1 Init:0/1 0 35skube-flannel-ds-amd64-qf5bh 0/1 Init:0/1 0 35s# 查看pod事件状态 如果是aliyun主机 别忘记node也要出公网啊[root@k8s-master opt]# kubectl describe pod kube-flannel-ds-amd64-nnrkz -n kube-system[root@k8s-master opt]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-nnrkz 1/1 Running 0 5mkube-flannel-ds-amd64-qf5bh 1/1 Running 0 5m# 没起来试试 [root@k8s-node2 bin]# systemctl restart kube-proxy# 查看 node工作状态[root@k8s-master opt]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 79m v1.16.0k8s-node2 Ready &lt;none&gt; 52m v1.16.0 123456789101112131415161718192021# 查看pod日志 [root@k8s-master opt]# kubectl logs kube-flannel-ds-amd64-nnrkz -n kube-systemError from server (Forbidden): Forbidden (user=kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-flannel-ds-amd64-nnrkz)# 没有权限 需要授权 通过logs命令可以查看pod日志[root@k8s-master opt]# vim apiserver-to-kubelet-rbac.yaml # 集群角色授权 resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics - pods/log# 创建[root@k8s-master opt]# kubectl apply -f apiserver-to-kubelet-rbac.yaml clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet createdclusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created# 查看pod 日志[root@k8s-master opt]# kubectl logs kube-flannel-ds-amd64-nnrkz -n kube-system 12345678# 每个node上都会启动个pod[root@k8s-master opt]# kubectl get pods -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-flannel-ds-amd64-nnrkz 1/1 Running 0 14m 172.17.70.247 k8s-node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-qf5bh 0/1 CrashLoopBackOff 6 14m 172.17.70.248 k8s-node2 &lt;none&gt; &lt;none&gt;# node上会增加flannel 虚拟网卡# pod通过flannel 跨主机访问 传输数据 创建个nginx pod 试试1234567891011121314151617181920212223242526272829303132333435[root@k8s-master opt]# kubectl create deployment web --image=nginx:1.16deployment.apps/web created[root@k8s-master opt]# kubectl get podsNAME READY STATUS RESTARTS AGEweb-866f97c649-hft4w 1/1 Running 0 14s# 看他分配到哪个节点[root@k8s-master opt]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-866f97c649-hft4w 1/1 Running 0 45s 10.244.0.2 k8s-node1 &lt;none&gt; &lt;none&gt;# 当创建pod后 会出现cni网桥 虚拟交换机 pod都会加入到这里# 暴露 到集群外部[root@k8s-master opt]# kubectl expose deployment web --port=80 --type=NodePortservice/web exposed[root@k8s-master opt]# kubectl get pods,svcNAME READY STATUS RESTARTS AGEpod/web-866f97c649-hft4w 1/1 Running 0 5m4sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 6h6mservice/web NodePort 10.0.0.161 &lt;none&gt; 80:31618/TCP 46s# 由于我使用的aliyun 所有我得开端口 # 为什么两个nodeIP都可以访问呢http://39.106.100.108:31618/http://123.56.14.192:31618/[root@k8s-master opt]# curl 172.17.70.247:31618[root@k8s-master opt]# curl 172.17.70.248:31618# 都能访问说明集群网络都正常 单master正常 部署K8S内部DNS服务（CoreDNS） 作用: 对service 提供dns解析 12345678[root@k8s-master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 60mweb NodePort 10.0.0.213 &lt;none&gt; 80:32023/TCP 16m1. 部署dns后就可以通过名称访问 转发到pod2. 没有dns服务 就只能通过内部IP 程序里可能要写死 对变动有影响3. coredns 是默认dns kubeadm默认安装， 二进制需要单独安装 12345678# 固定的IP # 他也是个service[root@k8s-master ~]# cat coredns.yaml |grep clusterIP clusterIP: 10.0.0.2 # 他与node节点上的 地址一致[root@k8s-node1 cfg]# cat kubelet-config.yml | grep clusterDNS:# 如果写错了 pod的dns解析有问题 12345678# 部署[root@k8s-master ~]# kubectl apply -f coredns.yaml # 查看pods 命名空间在 kube-system[root@k8s-master ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-6d8cfdd59d-vfzxs 1/1 Running 0 48skube-flannel-ds-amd64-4jkk8 1/1 Running 7 45mkube-flannel-ds-amd64-vxfdb 1/1 Running 0 45m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 创建busybox容器[root@k8s-master ~]# vim bs.yaml apiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - image: busybox:1.28.4 command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always # 部署[root@k8s-master ~]# kubectl apply -f bs.yaml pod/busybox created# 查看pods[root@k8s-master ~]# kubectl get podsNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 0 11sweb-866f97c649-mksh6 1/1 Running 0 25m# 看下service[root@k8s-master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.0.0.1 &lt;none&gt; 443/TCP 71mweb NodePort 10.0.0.213 &lt;none&gt; 80:32023/TCP 27m # 进入容器 通过servic name 可以ping通,以后就可以直接解析name了 [root@k8s-master ~]# kubectl exec -it busybox sh/ # ping 10.0.0.213PING 10.0.0.213 (10.0.0.213): 56 data bytes64 bytes from 10.0.0.213: seq=0 ttl=64 time=0.075 ms64 bytes from 10.0.0.213: seq=1 ttl=64 time=0.038 ms--- 10.0.0.213 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.038/0.056/0.075 ms/ # nslookup webServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: webAddress 1: 10.0.0.213 web.default.svc.cluster.local/ # ping webPING web (10.0.0.213): 56 data bytes64 bytes from 10.0.0.213: seq=0 ttl=64 time=0.034 ms/ # nslookup kubernetesServer: 10.0.0.2Address 1: 10.0.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local# 程序访问service 就可以写service name K8S 高可用介绍 123456LB : Nginx LVS HaProxy4层负载 端口转发 stream TCP 性能更好7层负载 业务转发 HTTP 应用层协议满足更多数据转发高可用软件: keepalived 创建vip 健康检查和故障转移 用户访问vipnode 连接 vip 部署master21234172.17.70.245 k8s-master2172.17.70.246 k8s-master172.17.70.247 k8s-node1172.17.70.248 k8s-node2 123456789# 将kubernetes 传过去[root@k8s-master ~]# scp -r /opt/kubernetes root@172.17.70.245:/opt# 管理服务的service文件[root@k8s-master ~]# scp /usr/lib/systemd/system/&#123;kube-apiserver,kube-controller-manager,kube-scheduler&#125;.service root@172.17.70.245:/usr/lib/systemd/system# 拷贝etcd证书目录[root@k8s-master2 ssl]# mkdir -p /opt/etcd[root@k8s-master ~]# scp -r /opt/etcd/ssl root@172.17.70.245:/opt/etcd 12345# 修改配置文件# 修改apiserver配置文件为本地IP：[root@k8s-master2 cfg]# cat kube-apiserver.conf | grep 245--bind-address=172.17.70.245 \--advertise-address=172.17.70.245 \ 12# 传递 kubectl[root@k8s-master ~]# scp /usr/local/bin/kubectl root@172.17.70.245:/usr/bin/ 12345678# 启动服务[root@k8s-node1 cfg]# systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 12345678910# 查看资源 有数据即为正常[root@k8s-master2 ssl]# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 109m v1.16.0k8s-node2 Ready &lt;none&gt; 107m v1.16.0[root@k8s-master2 ssl]# kubectl get podsNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 0 55mweb-866f97c649-mksh6 1/1 Running 0 80m 部署 Nginx 负载均衡123456172.17.70.245 k8s-master2172.17.70.246 k8s-master172.17.70.247 k8s-node1172.17.70.248 k8s-node2172.17.70.249 nginx-master172.17.70.250 nginx-backup 1234567# 下载rpm包nginx rpm包：http://nginx.org/packages/rhel/7/x86_64/RPMS/[root@k8s-master tmp]# wget http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.16.1-1.el7.ngx.x86_64.rpm[root@k8s-master tmp]# scp nginx-1.16.1-1.el7.ngx.x86_64.rpm root@172.17.70.249:/root[root@k8s-master tmp]# scp nginx-1.16.1-1.el7.ngx.x86_64.rpm root@172.17.70.250:/root# 安装[root@nginx-master ~]# rpm -ivh nginx-1.16.1-1.el7.ngx.x86_64.rpm 1234567891011121314151617181920# 两台nginx 都要完成# 修改配置文件 增加4层端口搭理到master[root@nginx-backup ~]# vi /etc/nginx/nginx.conf stream &#123; log_format main '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent'; access_log /var/log/nginx/k8s-access.log main; upstream k8s-apiserver &#123; server 172.17.70.246:6443; server 172.17.70.245:6443; &#125; server &#123; listen 6443; proxy_pass k8s-apiserver; &#125;&#125; 123456# 启动nginx -tsystemctl start nginxsystemctl enable nginxsystemctl status nginx 安装 keepalived 心跳检测1[root@nginx-master ~]# yum install keepalived -y 1234567891011121314151617181920212223242526272829303132333435363738394041# 编辑配置文件[root@nginx-master keepalived]# cd /etc/keepalived/[root@nginx-master keepalived]# mv keepalived.conf keepalived.conf_bak[root@nginx-master keepalived]# vim keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_MASTER&#125;vrrp_script check_nginx &#123; script "/etc/keepalived/check_nginx.sh"&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 100 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.70.252/24 # vip &#125; track_script &#123; check_nginx &#125;&#125; 1234567891011121314151617# 检查脚本 # keepalived 对 脚本返回状态码做判断 0 和 1 0为正常[root@nginx-master ~]# vim check_nginx.sh #!/bin/bashcount=$(ps -ef |grep nginx |egrep -cv "grep|$$")if [ "$count" -eq 0 ];then exit 1else exit 0fi[root@nginx-backup keepalived]# cp /root/check_nginx.sh .[root@nginx-backup keepalived]# chmod +x check_nginx.sh 1234567891011121314151617181920212223242526272829303132333435363738# 备机配置文件[root@nginx-backup keepalived]# vim keepalived.confglobal_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id NGINX_BACKUP&#125;vrrp_script check_nginx &#123; script "/etc/keepalived/check_nginx.sh"&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 priority 90 # 优先级，备服务器设置 90 advert_int 1 # 指定VRRP 心跳包通告间隔时间，默认1秒 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.70.252/24 &#125; track_script &#123; check_nginx &#125;&#125; 123456789# 启动keepalived systemctl start keepalivedsystemctl enable keepalivedsystemctl status keepalived[root@nginx-master keepalived]# ssh root@172.17.70.252# 阿里云还是用自己的负载均衡吧# 关闭nginx测试是否偏移到备节点。 修改Node连接VIP12345678910# 将Node连接VIP： # cd /opt/kubernetes/cfg# grep 172 *bootstrap.kubeconfig: server: https://172.17.70.246:6443kubelet.kubeconfig: server: https://172.17.70.246:6443kube-proxy.kubeconfig: server: https://172.17.70.246:6443# 批量修改：改成vip地址sed -i 's#172.17.70.246#172.17.70.252#g' * 123456789101112131415161718192021222324252627282930313233343536# 重启node服务 systemctl restart kubeletsystemctl restart kube-proxy# 看看nginx有没有新日志请求tailf /var/log/nginx/k8s-access.log # 总共四个组件进来 四次请求# 再任意主节点测试 虽然是本地测试kubectl get node # 找其他公网机器cat /opt/kubernetes/cfg/token.csv curl -k --header "Authorization: Bearer c47ffb939f5ca36231d9e3121a252940" https://172.17.70.252:6443/version# 单master 测试[root@nginx-backup ~]# curl -k --header "Authorization: Bearer c47ffb939f5ca36231d9e3121a252940" https://172.17.70.249:6443/version&#123; "major": "1", "minor": "16", "gitVersion": "v1.16.0", "gitCommit": "2bd9643cee5b3b3a5ecbd3af49d09018f0773c77", "gitTreeState": "clean", "buildDate": "2019-09-18T14:27:17Z", "goVersion": "go1.12.9", "compiler": "gc", "platform": "linux/amd64"# 可以关闭一个nginxsystemctl stop nginx# 查看 vip是否便宜# 最后再查看是否可以正常访问curl -k --header "Authorization: Bearer c47ffb939f5ca36231d9e3121a252940" https://172.17.70.252:6443/version# 可以的话 就为正常 任意一个master无法访问 也可以访问到另外一台 nginx挂了 也可以通过另外的nginx负载访问]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 Kubernetes 概述]]></title>
    <url>%2F2019%2F11%2F05%2Fk8s-01%2F</url>
    <content type="text"><![CDATA[Kubernetes 是什么• Kubernetes是Google在2014年开源的一个容器集群管理系统，Kubernetes简称K8S。• K8S用于容器化应用程序的部署，扩展和管理。• K8S提供了容器编排，资源调度，弹性伸缩，部署管理，服务发现等一系列功能。• Kubernetes目标是让部署容器化应用简单高效。• 官方网站: http://www.kubernetes.io• 官方网站: https://kubernetes.io/zh/ 12# 概念 https://kubernetes.io/zh/docs/concepts/# 参考 https://kubernetes.io/zh/docs/reference/ 121. k8s 是一个容器平台2. k8s 是微服务平台 Kubernetes 特性• 自我修复在节点故障时重新启动失败的容器，替换和重新部署，保证预期的副本数量；杀死健康检查失败的容器，并且在未准备好之前不会处理客户端请求，确保线上服务不中断。 • 弹性伸缩使用命令、UI或者基于CPU使用情况自动快速扩容和缩容应用程序实例，保证应用业务高峰并发时的高可用性；业务低峰时回收资源，以最小成本运行服务。 • 自动部署和回滚K8S采用滚动更新策略更新应用，一次更新一个Pod，而不是同时删除所有Pod，如果更新过程中出现问题，将回滚更改，确保升级不受影响业务。 • 服务发现和负载均衡K8S为多个容器提供一个统一访问入口（内部IP地址和一个DNS名称），并且负载均衡关联的所有容器，使得用户无需考虑容器IP问题。 • 机密和配置管理管理机密数据和应用程序配置，而不需要把敏感数据暴露在镜像里，提高敏感数据安全性。并可以将一些常用的配置存储在K8S中，方便应用程序使用。 • 存储编排挂载外部存储系统，无论是来自本地存储，公有云（如AWS），还是网络存储（如NFS、GlusterFS、Ceph）都作为集群资源的一部分使用，极大提高存储使用灵活性。 • 批处理提供一次性任务，定时任务；满足批量数据处理和分析的场景。 1231. 负载均衡 : 用户实际访问 -&gt; service统一入口 -&gt; 负载容器2. docker : 适合单机3. k8s : 适合集群 Kubernetes 集群架构与组件 12345678910111. 两个角色: - master主控制节点:管控整个集群，调度 - node工作节点:跑任务，创建容器 2. 用户部署应用 先发送请求到 -&gt; API Server 协调创建部署3. API Server 发送给scheduler调度，根据资源利用率，看部署到哪个节点4. 如果需要创建3个副本 会交给 controller-manager 控制器 去完成5. 操作信息记录到etcd中6. master全部完成后 会交给node工作节点，node节点创建容器 和 网络规则(怎么去访问这组容器) -&gt; docker 创建容器7. pod 一个或多个容器组成8. etcd 可以部署在master 也可以独立部署 Master核心组件API Server Kubernetes API，集群的统一入口，各组件协调者，以RESTful API提供接口服务， 所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。 scheduler 根据调度算法为新创建的Pod选择一个Node节点， 可以任意部署,可以部署在同一个节点上,也可以部署在不同的节点上。 controller-manager 处理集群中常规后台任务，一个资源对应一个控制器， ControllerManager就是负责管理这些控制器的。 etcd 分布式键值存储系统。用于保存集群状态数据， 比如Pod、Service等对象信息。 Node核心组件kubelet kubelet是Master在Node节点上的Agent， 管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。 kubelet将每个Pod转换成一组容器。 kube-proxy 在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。 docker 或 rocket 容器引擎，运行容器，常用docker。 Kubernetes 核心概念Pod• 最小部署单元• 一组容器的集合,里面可能是1个、3个、多个• 一个Pod中的容器共享网络命名空间,也可以挂载数据卷让数据共享• Pod是短暂的,服务更新或者重新发布就会销毁 Controllers• 针对不同的应用场景,分为不同的控制器• ReplicaSet ： 确保预期的Pod副本数量,创建pod 有几个副本 如果少一个会帮我们启动,多了会删除,不会直接接触• Deployment ： 无状态应用部署，一般使用该控制器,实际上就是 ReplicaSet• StatefulSet ： 有状态应用部署，无状态N1节点的pod 偏移到n2节点,还能继续使用,有状态就有很多限制,偏移过去后无法继续使用,比如有存储,或者网络ID唯一，zookeeper集群,mysql主从，有状态需要考虑存储和网络ID。• DaemonSet ： 确保所有Node运行同一个Pod 适合每个node节点上都运行一个应用 监控agent• Job ： 一次性任务 就执行一次 比如备份或者离线任务• Cronjob ： 定时任务，类似crontab 定时周期去执行 比如数据备份• Controllers是更高级层次对象，用于部署和管理Pod Service• 防止Pod失联• 定义一组Pod的访问策略• 找到一组pod,负载均衡 部署应用必须掌握11. Controllers 部署应用 -&gt; 创建一组Pod -&gt; Service暴露应用,让外部访问 Label 标签，在k8s中,所有的关联和筛选都是通过标签去做，每个资源都会打标签 比如 Service 如何找到这一组Pod，通过标签关联 标签，附加到某个资源上，用于关联对象、查询和筛选 Namespace 命名空间，将对象逻辑上隔离 逻辑概念, 用户可以访问某一个命名空间，看到的是指定的命名空间,针对团队项目创建不同的命名空间,管控权限 每天5分钟玩转k8s重要概念梳理Master Master 是 Cluster 的大脑，它的主要职责是调度，即决定将应用放在哪里运行。 Master 运行 Linux 操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个 Master。 Node Node 的职责是运行容器应用。Node 由 Master 管理，Node 负责监控并汇报容器的状态，并根据 Master 的要求管理容器的生命周期。 Node 运行在 Linux 操作系统，可以是物理机或者是虚拟机。 Pod Pod 是 Kubernetes 的最小工作单元。 每个 Pod 包含一个或多个容器。 Pod 中的容器会作为一个整体被 Master 调度到一个 Node 上运行。 Pod 作用 可管理性: 有些容器天生就是需要紧密联系，一起工作。Pod 提供了比容器更高层次的抽象，将它们封装到一个部署单元中。Kubernetes 以 Pod 为最小单位进行调度、扩展、共享资源、管理生命周期。 通信和资源共享: Pod 中的所有容器使用同一个网络 namespace，即相同的 IP 地址和 Port 空间。它们可以直接用 localhost 通信。同样的，这些容器可以共享存储，当 Kubernetes 挂载 volume 到 Pod，本质上是将 volume 挂载到 Pod 中的每一个容器。 Pods 两种使用方式 运行单一容器: one-container-per-Pod 是 Kubernetes 最常见的模型，这种情况下，只是将单个容器简单封装成 Pod。 即便是只有一个容器，Kubernetes 管理的也是 Pod 而不是直接管理容器。 运行多个容器: 但问题在于：哪些容器应该放到一个 Pod 中？ 答案是：这些容器联系必须 非常紧密，而且需要 直接共享资源。 Tomcat 从 MySQL 读取数据，它们之间需要协作，但还不至于需要放到一个 Pod 中一起部署，一起启动，一起停止。同时它们是之间通过 JDBC 交换数据，并不是直接共享存储，所以放到各自的 Pod 中更合适。 Controller Kubernetes 通常不会直接创建 Pod，而是通过 Controller 来管理 Pod 的。 Controller 中定义了 Pod 的部署特性，比如有几个副本，在什么样的 Node 上运行等。 为了满足不同的业务场景，Kubernetes 提供了多种 Controller，包括 Deployment、ReplicaSet、DaemonSet、StatefuleSet、Job 等。 Deployment 是最常用的 Controller。Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。 ReplicaSet 实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。 DaemonSet 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。 StatefuleSet 能够保证 Pod 的每个副本在整个生命周期中名称是不变的。 而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。 同时 StatefuleSet 会保证副本按照固定的顺序启动、更新或者删除。 Job 用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。 Service Deployment 可以部署多个副本，每个 Pod 都有自己的 IP，外界如何访问这些副本呢？ 通过 Pod 的 IP 吗？要知道 Pod 很可能会被频繁地销毁和重启，它们的 IP 会发生变化，用 IP 来访问不太现实。 Kubernetes Service 定义了外界访问一组特定 Pod 的方式。Service 有自己的 IP 和端口，Service 为 Pod 提供了负载均衡。 Kubernetes 运行容器（Pod）与访问容器（Pod）这两项任务分别由 Controller 和 Service 执行。 Namespace 如果有多个用户或项目组使用同一个 Kubernetes Cluster，如何将他们创建的 Controller、Pod 等资源分开呢？ Namespace 可以将一个物理的 Cluster 逻辑上划分成多个虚拟 Cluster，每个 Cluster 就是一个 Namespace。 不同 Namespace 里的资源是完全隔离的。 Kubernetes 默认创建了两个 Namespace。 default – 创建资源时如果不指定，将被放到这个 Namespace 中。 kube-system – Kubernetes 自己创建的系统资源将放到这个 Namespace 中。 可以使用 kubectl get namespace 查看当前集群里所有的命名空间。 Kubernetes 架构 Kubernetes Cluster 由 Master 和 Node 组成，节点上运行着若干 Kubernetes 服务。 Master 节点 Master 是 Kubernetes Cluster 的大脑，运行着如下 Daemon 服务：kube-apiserver、kube-scheduler、kube-controller-manager、etcd 和 Pod 网络（例如 flannel）。 API Server（kube-apiserver） API Server 提供 HTTP/HTTPS RESTful API，即 Kubernetes API。 API Server 是 Kubernetes Cluster 的前端接口，各种客户端工具（CLI 或 UI）以及 Kubernetes 其他组件可以通过它管理 Cluster 的各种资源。 Scheduler（kube-scheduler） Scheduler 负责决定将 Pod 放在哪个 Node 上运行。 Scheduler 在调度时会充分考虑 Cluster 的拓扑结构，当前各个节点的负载，以及应用对高可用、性能、数据亲和性的需求。 Controller Manager（kube-controller-manager） Controller Manager 负责管理 Cluster 各种资源，保证资源处于预期的状态。 Controller Manager 由多种 controller 组成，包括 replication controller、endpoints controller、namespace controller、serviceaccounts controller 等。 不同的 controller 管理不同的资源。例如 replication controller 管理 Deployment、StatefulSet、DaemonSet 的生命周期，namespace controller 管理 Namespace 资源。 etcd etcd 负责保存 Kubernetes Cluster 的配置信息和各种资源的状态信息。 当数据发生变化时，etcd 会快速地通知 Kubernetes 相关组件。 Pod 网络 Pod 要能够相互通信，Kubernetes Cluster 必须部署 Pod 网络，flannel 是其中一个可选方案。 以上是 Master 上运行的组件 Node Node 是 Pod 运行的地方，Kubernetes 支持 Docker、rkt 等容器 Runtime。 Node上运行的 Kubernetes 组件有 kubelet、kube-proxy 和 Pod 网络（例如 flannel）。 kubelet kubelet 是 Node 的 agent，当 Scheduler 确定在某个 Node 上运行 Pod 后，会将 Pod 的具体配置信息（image、volume 等）发送给该节点的 kubelet， kubelet 根据这些信息创建和运行容器，并向 Master 报告运行状态。 kube-proxy service 在逻辑上代表了后端的多个 Pod，外界通过 service 访问 Pod。 service 接收到的请求是如何转发到 Pod 的呢？这就是 kube-proxy 要完成的工作。 每个 Node 都会运行 kube-proxy 服务，它负责将访问 service 的 TCP/UPD 数据流转发到后端的容器。 如果有多个副本，kube-proxy 会实现负载均衡。 Pod 网络 Pod 要能够相互通信，Kubernetes Cluster 必须部署 Pod 网络，flannel 是其中一个可选方案。 完整的架构图 为什么 k8s-master 上也有 kubelet 和 kube-proxy 呢？ 这是因为 Master 上也可以运行应用，即 Master 同时也是一个 Node。 几乎所有的 Kubernetes 组件本身也运行在 Pod 里，执行如下命令： kubectl get pod –all-namespaces -o wide Kubernetes 的系统组件都被放到 kube-system namespace 中。 这里有一个 kube-dns 组件，它为 Cluster 提供 DNS 服务，我们后面会讨论。 kube-dns是在执行 kubeadm init 时（第 ⑤ 步）作为附加组件安装的。 通过例子理解 k8s 架构]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 构建持续集成环境]]></title>
    <url>%2F2019%2F11%2F01%2Fdocker-base09%2F</url>
    <content type="text"><![CDATA[CI/CD介绍 CI（持续集成）/CD（持续交付/持续部署） 发布流程设计 基础环境准备docker服务器 与 Harbor123Jenkins服务器 172.17.70.243Docker服务器 172.17.70.244Git/Harbor 172.17.70.242 docker 服务器 能够访问 Harbor仓库 12[root@Jenkins ~]# docker login 172.17.70.242[root@Docker ~]# docker login 172.17.70.242 每台服务器安装JAVA环境 123456789101112131415161718# 解压[root@server2 ~]# tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/local/[root@server2 ~]# ls -l /usr/local/jdk1.8.0_231/# 配置环境变量[root@server2 local]# vim /etc/profileexport JAVA_HOME=/usr/local/jdk1.8.0_231export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH[root@server2 local]# source /etc/profile# 让设置生效[root@server2 local]# java -versionjava version "1.8.0_231"Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode) 部署 Git 服务器 部署仓库 12345678910111213141516171. 安装Gityum install git2. 创建Git用户useradd gitpasswd git3. 创建仓库su - gitmkdir solo.git cd solo.git/# 初始化仓库git --bare init[git@Git-Harbor solo.git]$ lsbranches config description HEAD hooks info objects refs 部署本地git 提交solo项目到git仓库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455561. Jenkins服务器上安装git,下载solo项目有到服务器上,模拟开发上传到git仓库再[root@Jenkins opt]# yum install git2. 下载solo项目[root@Jenkins opt]# git clone https://github.com/b3log/solo.git# 本地工程路径: /opt/solo[root@Jenkins tmp]# git clone https://github.com/b3log/solo-skins.git[root@Jenkins tmp]# cd solo-skins/[root@Jenkins skins]# cd /opt/test/solo/src/main/webapp/skins# 复制所有皮肤到本地目录[root@Jenkins skins]# cp -r /tmp/solo-skins/* .3. 创建本地git目录 test[root@Jenkins opt]# mkdir test[root@Jenkins opt]# cd test/# 把git服务器上solo项目拉过来[root@Jenkins test]# git clone root@172.17.70.242:/home/git/solo.git# 在把本地写好的solo项目 git上去[root@Jenkins solo]# cd /opt/test/solo/[root@Jenkins test]# cp ../solo/* solo/ -rf[root@Jenkins solo]# cd /opt/test/solo[root@Jenkins solo]# ls -ltotal 428-rw-r--r-- 1 root root 141981 Nov 3 17:58 CHANGE_LOGS.html-rw-r--r-- 1 root root 519 Nov 3 17:58 Dockerfile-rw-r--r-- 1 root root 4486 Nov 3 17:58 gulpfile.js-rw-r--r-- 1 root root 34522 Nov 3 17:58 LICENSE-rw-r--r-- 1 root root 938 Nov 3 17:58 package.json-rw-r--r-- 1 root root 211486 Nov 3 17:58 package-lock.json-rw-r--r-- 1 root root 14647 Nov 3 17:58 pom.xml-rw-r--r-- 1 root root 1390 Nov 3 17:58 README.mddrwxr-xr-x 2 root root 4096 Nov 3 17:58 scriptsdrwxr-xr-x 4 root root 4096 Nov 3 17:58 src# 提交到本地[root@Jenkins solo]# git config user.name "leo"[root@Jenkins solo]# git config user.email "365042337@qq.com"[root@Jenkins solo]# git add .[root@Jenkins solo]# git commit -m "all"# psuh到 git 服务器[root@Jenkins solo]# git push origin masterroot@172.17.70.242's password: Counting objects: 655, done.Delta compression using up to 2 threads.Compressing objects: 100% (641/641), done.Writing objects: 100% (655/655), 5.14 MiB | 9.80 MiB/s, done.Total 655 (delta 240), reused 0 (delta 0)To root@172.17.70.242:/home/git/solo.git * [new branch] master -&gt; master 创建 自定义虚拟网络 solo12# 在docker服务器上创建[root@Docker opt]# docker network create solo solo 部署 mysql服务器12345docker run -d \--name solo_mysql \--net solo \-e MYSQL_ROOT_PASSWORD=123456 \-e MYSQL_DATABASE=solo mysql:5.6 --character-set-server=utf8 修改 solo 源码中的数据库连接配置123456789101112# 使用mysql容器名访问[root@Jenkins resources]# cd /opt/test/solo/src/main/resources[root@Jenkins resources]# sed -i 's#localhost#solo_mysql#g' local.properties # 提交到本地[root@Jenkins solo]# git config user.name "leo"[root@Jenkins solo]# git config user.email "365042337@qq.com"[root@Jenkins solo]# git add .[root@Jenkins solo]# git commit -m "all"# psuh到 git 服务器[root@Jenkins solo]# git push origin master 部署 Harbor 镜像仓库11. 如果不使用https,docker服务器上别忘记配置信任 Jenkins 安装12345# 配置jenkins 服务器 与git服务器免交互[root@Jenkins jenkins]# ssh-keygen [root@Jenkins jenkins]# ssh-copy-id git@172.17.70.242# 不用输入密码 即可git[root@Jenkins jenkins]# git clone git@172.17.70.242:/home/git/solo.git 12345678910111213141516171819202122232425262728293031323334353637383940# 使用jenkins容器[root@Jenkins solo]# cd /opt/[root@Jenkins opt]# mkdir jenkins# git jenkins容器使用git拉取镜像# libltdl-dev jenkins使用宿主机的docker引擎# 先拉取最新jenkins版本[root@Jenkins jenkins]# docker pull jenkins/jenkins[root@Jenkins jenkins]# vim Dockerfile FROM jenkins/jenkinsUSER root# RUN echo '' &gt; /etc/apt/sources.list.d/jessie-backports.list &amp;&amp; \# wget http://mirrors.163.com/.help/sources.list.jessie -O /etc/apt/sources.list# COPY sources.list.jessie /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y git libltdl-dev [root@Jenkins jenkins]# docker build -t jenkins:v1 .# 启动[root@Jenkins jenkins]# docker run -d --name jenkins -p 8080:8080 -v /var/jenkins_home/:/var/jenkins_home -v /usr/local/apache-maven-3.5.0:/usr/local/maven -v /usr/local/jdk1.8.0_231/:/usr/local/jdk -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/usr/bin/docker -v ~/.ssh:/root/.ssh jenkins:v1[root@Jenkins jenkins]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1e37fe659cc2 jenkins:v1 "/bin/tini -- /usr/l…" 42 seconds ago Up 41 seconds 0.0.0.0:8080-&gt;8080/tcp, 50000/tcp jenkins# 安装 apache-maven-3.5.0-bin.tar[root@Jenkins opt]# tar -zxvf apache-maven-3.5.0-bin.tar.gz -C /usr/local/# 安装jenkins解锁[root@Jenkins secrets]# cat /var/jenkins_home/secrets/initialAdminPassword b4b2de84e81f45caae80a105a9460eba# 选择插件安装 构建基础镜像12345678910111213[root@Docker solo]# mkdir -p /opt/solo/[root@Docker solo]# vim Dockerfile FROM centos:7MAINTAINER leo 365042337@qq.comRUN yum install unzip iproute -yENV JAVA_HOME /usr/local/jdkADD apache-tomcat-8.5.47.tar.gz /usr/local/RUN mv /usr/local/apache-tomcat-8.5.47 /usr/local/tomcat &amp;&amp; \ sed -i '1a JAVA_OPTS="-Djava.security.egd=file:/dev/./urandom"' /usr/local/tomcat/bin/catalina.shWORKDIR /usr/local/tomcatEXPOSE 8080ENTRYPOINT ["./bin/catalina.sh", "run"] 1234# 构建[root@Docker solo]# docker build -t 172.17.70.242/test/tomcat:v1 .# 推送 项目会基于这个tomcat构建服务[root@Docker solo]# docker push 172.17.70.242/test/tomcat:v1 Jenkins基本配置全局工具配置 git jdk maven 系统配置12345678# jenkins 安装ssh插件# 配置 SSH remote hosts# jenkins配置免交互 访问 docker主机[root@Jenkins solo]# ssh-copy-id root@172.17.70.244# 添加用户凭据 credentials -&gt; 全局 -&gt; 加入公钥[root@Jenkins solo]# cat /root/.ssh/id_rsa 1别忘记保存 Jenkins创建项目安装 Maven 插件1Maven Integration 创建sola项目123456781. 配置好源码管理 jenkins到git 免交互访问git@172.17.70.242:/home/git/solo.git2. 每10分钟拉取一次代码*/30 * * * * 3. 跳过项目的测试用例clean package -Dmaven.test.skip=true 立即构建 测试一次123456789cd /var/jenkins_home/workspace/solo_blog/target[root@Jenkins target]# ls -ltotal 21880drwxr-xr-x 3 root root 4096 Nov 4 15:42 classesdrwxr-xr-x 3 root root 4096 Nov 4 15:42 generated-sourcesdrwxr-xr-x 2 root root 4096 Nov 4 15:42 maven-archiverdrwxr-xr-x 3 root root 4096 Nov 4 15:42 maven-statusdrwxr-xr-x 12 root root 4096 Nov 4 15:42 solo-rw-r--r-- 1 root root 22380877 Nov 4 15:42 solo.war # 实际要部署的包 增加构建后操作 把项目打包到镜像 推送到镜像仓库 12345678910111213141516171819202122# 构建后 到当前目录 写dockerfile文件 构建镜像并推送到公共仓库cd $WORKSPACEdocker login -u admin -p 123456 172.17.70.242cat &gt; Dockerfile &lt;&lt; EOF# BASEFROM 172.17.70.242/test/tomcat:v1# WHOMAINTAINER leo 365042337@qq.com# COPYCOPY target/solo.war /tmp/ROOT.war# RUN RUN rm -rf /usr/local/tomcat/webapps/* &amp;&amp; \ unzip /tmp/ROOT.war -d /usr/local/tomcat/webapps/ROOT &amp;&amp; \ rm -f /tmp/ROOT.war ENTRYPOINT ["/usr/local/tomcat/bin/catalina.sh", "run"]EOFdocker build -t 172.17.70.242/test/solo:v1 .docker login -u admin -p 123456 172.17.70.242docker push 172.17.70.242/test/solo:v1 项目镜像传给 docker服务器 运行 使用项目构建后操作 1234567891011# 远程登录docker服务器,然后拉取项目镜像 run# 1. 考虑镜像需要清理 容器需要清理 再拉取运行# 2. 挂上jdk 从而减少容器的大小# 3. 后期要考虑项目版本构建 每次的构建版本不同# 4. jenkins 判断状态码 删除没有的容器 echo$? = 1 所以加上 | true# --net solo 指定自定义网络是因为solo需要mysql 在docker服务上启动了一个docker rmi -f 172.17.70.242/test/solo:v1 | truedocker rm -f solo | truedocker login -u admin -p 123456 172.17.70.242docker run -d -it --name solo --net solo -p 88:8080 -v /usr/local/jdk1.8.0_231:/usr/local/jdk 172.17.70.242/test/solo:v1 构建后查看1234[root@Docker ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMEScc910d9d1bb9 172.17.70.242/test/solo:v1 "/usr/local/tomcat/b…" 8 seconds ago Up 7 seconds 0.0.0.0:88-&gt;8080/tcp solo8395404a43b5 mysql:5.6 "docker-entrypoint.s…" 13 hours ago Up 11 minutes 3306/tcp solo_mysql 测试maven 构建 solo项目1234# 仅仅是测试[root@Jenkins solo]# /usr/local/apache-maven-3.5.0/bin/mvn clean package -Dmaven.test.skip=true# 依赖是再 pom.xml 文件中指定好的 第一次拉取需要下载 形成缓存 使用 pipeline 构建项目把整体工作放在流水线中 基础使用 阶段演示 123456789101112131415node &#123; stage('git check') &#123; sh "echo 1" &#125; stage('build') &#123; sh "echo 2" &#125; stage('deploy') &#123; sh "echo 3" &#125; &#125; 使用pipeline的优点 项目发布可视化，明确阶段，方便问题处理 jenkinsfile文件管理整个项目的生命周期 jenkinsfile可以放到整个项目代码中版本管理 使用pipeline 是ci/cd 最佳实践 123451. 构建过程更加清晰可见2. 每个阶段都可以查看过程日志,可以更清晰的定位问题3. 通过脚本实现各个环节4. 可以放到git仓库中,从仓库中后去直接构建5. 可以与项目放在一起 项目周期一致 配置 solo pipeline丢弃旧的构建 代码版本参数 maven的配置123451. 我这个jenkins是容器启动的 挂载了本地的maven环境2. -v /usr/local/apache-maven-3.5.0:/usr/local/maven3. maven是要用的的是容器路径 /usr/local/maven4. jdk也是 /usr/local/jdk5. 这些环境与jenkins的全局工具配置路径一致就可以找到 流水线脚本12345678# 这里我发现 我的jenkins是容器安装的...没法指定node也没办法远程 我就吧docker先部署到本地了 ，后面我在想办法改回来[root@Jenkins local]# docker network create solodocker run -d \--name solo_mysql \--net solo \-e MYSQL_ROOT_PASSWORD=123456 \-e MYSQL_DATABASE=solo mysql:5.6 --character-set-server=utf8 12345678910111213141516171819202122232425262728293031323334353637383940node &#123; // 指定Slave标签 我这用的master 上面有git和maven环境 // 拉取代码 stage('Git Checkout') &#123; // 别忘记设置ssh免登录 访问到git服务器 Tag是根据版本号码进行构建自定义传值 checkout([$class: 'GitSCM', branches: [[name: '$Tag']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: 'git@172.17.70.242:/home/git/solo.git']]]) &#125; // 代码编译 stage('Maven Build') &#123; sh ''' export JAVA_HOME=/usr/local/jdk /usr/local/maven/bin/mvn clean package -Dmaven.test.skip=true ''' &#125; // 项目打包到镜像并推送到镜像仓库 // REPOSITORY = 镜像仓库地址 项目名/版本号 stage('Build and Push Image') &#123;sh '''REPOSITORY=172.17.70.242/test/solo:$&#123;Tag&#125;cat &gt; Dockerfile &lt;&lt; EOFFROM 172.17.70.242/test/tomcat:v1MAINTAINER leo 365042337@qq.comRUN rm -rf /usr/local/tomcat/webapps/*COPY target/solo.war /usr/local/tomcat/webapps/ROOT.warENTRYPOINT ["/usr/local/tomcat/bin/catalina.sh", "run"]EOFdocker build -t $REPOSITORY .docker login -u admin -p 123456 172.17.70.242docker push $REPOSITORY''' &#125; // 部署到Docker主机 stage('Deploy to Docker') &#123; sh ''' REPOSITORY=172.17.70.242/test/solo:$&#123;Tag&#125; docker rm -f blog-solo |true docker image rm $REPOSITORY |true docker login -u admin -p 123456 172.17.70.242 docker run -d --name blog-solo --net solo -p 88:8080 -v /usr/local/jdk1.8.0_231:/usr/local/jdk $REPOSITORY ''' &#125;&#125; 1234567891011121314151617# 查看git配置[root@Jenkins solo]# cd /opt/test/solo/[root@Jenkins solo]# cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true[remote "origin"] url = root@172.17.70.242:/home/git/solo.git fetch = +refs/heads/*:refs/remotes/origin/*[branch "master"] remote = origin merge = refs/heads/master[user] name = leo email = 365042337@qq.com 模拟版本发布123456789101112131415161718192021222324# 创建个文件 形成新版本代码[root@Jenkins solo]# touch src/main/webapp/test01# 提交到本地[root@Jenkins solo]# git add .[root@Jenkins solo]# git commit -m "test01"[master 2602e3a] test01 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 src/main/webapp/test01# 打tag[root@Jenkins solo]# git tag 1.0.3# push 到主分支[root@Jenkins solo]# git push origin 1.0.3root@172.17.70.242's password: Counting objects: 10, done.Delta compression using up to 2 threads.Compressing objects: 100% (5/5), done.Writing objects: 100% (6/6), 438 bytes | 0 bytes/s, done.Total 6 (delta 3), reused 0 (delta 0)To root@172.17.70.242:/home/git/solo.git * [new tag] 1.0.3 -&gt; 1.0.3 开始构建 123456# 容器启动[root@Jenkins local]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES46bdaf3cb6cc 172.17.70.242/test/solo:1.0.3 "/usr/local/tomcat/b…" 21 seconds ago Up 20 seconds 0.0.0.0:88-&gt;8080/tcp blog-solo6c856f6c9f08 mysql:5.6 "docker-entrypoint.s…" 3 minutes ago Up 3 minutes 3306/tcp solo_mysql8bfb5bbc017e jenkins:v1 "/sbin/tini -- /usr/…" 19 hours ago Up 4 hours 0.0.0.0:8080-&gt;8080/tcp, 50000/tcp jenkins 1234# 进入容器 查看是否存在新版本文件[root@Jenkins local]# docker exec -it blog-solo bash[root@46bdaf3cb6cc tomcat]# cat webapps/ROOT/test01 Consul 容器服务自动注册与发现安装与部署123456789101112131415161、介绍Consul是一个分布式、高可用性，在基础设施中发现和配置服务的工具。 2、安装下载二进制Consul包：https://www.consul.io/downloads.html# unzip consul_0.9.2_linux_amd64.zip# mv consul /usr/bin3、部署consul agent -server -bootstrap -ui -data-dir=/var/lib/consul-data -bind=172.17.70.243 -client=0.0.0.0 -node=server01# server 模式启动 集群的主节点 存储数据 处理请求 # ui web管理系统# data-dir= 数据目录# bind 绑定地址# node 节点名 1234[root@Jenkins opt]# ls -l consul_0.9.2_linux_amd64.zip [root@Jenkins opt]# unzip consul_0.9.2_linux_amd64.zip [root@Jenkins opt]# mv consul /usr/bin 123456# 后台启动 端口8500 [root@Jenkins opt]# nohup consul agent -server -bootstrap -ui -data-dir=/var/lib/consul-data -bind=172.17.70.243 -client=0.0.0.0 -node=server01 &amp;&gt;/var/log/caonsul.log &amp;[root@Jenkins opt]# tailf /var/log/caonsul.log # web访问http://39.106.100.108:8500/ui/dc1/services 基础使用1234567891011121314151617查看集群信息：consul membersconsul info |grep leaderconsul catalog services通过HTTP API获取集群信息：curl 127.0.0.1:8500/v1/status/peers # 集群server成员curl 127.0.0.1:8500/v1/status/leader # 集群Raft leadercurl 127.0.0.1:8500/v1/catalog/services # 注册的所有服务curl 127.0.0.1:8500/v1/catalog/services/nginx # 服务信息curl 127.0.0.1:8500/v1/catalog/nodes # 集群节点详细信息服务注册：curl -X PUT -d \'&#123;"id": "jetty","name": "service_name","address": "192.168.0.212","port": 8080,"tags": ["test"],"checks": [&#123;"http": "http://192.168.0.212:8080/","interval": "5s"&#125;]&#125;' \http://192.168.0.211:8500/v1/agent/service/register 实现容器服务自动加入Nginx集群 Docker + Registrator + Consul consul-template：一个守护程序，用于实时查询consul集群数据，并更新文件系统上的任意数量的指定模板，生成配置文件，更新完成后可以选择运行任何Shell命令。 让它生成nginx配置文件 从集群中拿去 再去放入nginx里 执行重载 gliderlabs/registrator：检查容器运行状态自动注册和注销Docker容器的服务到服务配置中心。目前支持Consul、etcd和SkyDNS2。 检查容器服务状态、注销和配置服务中心 运行在docker主机，如果创建新的docker就捕捉出来，注册到Consul 12https://github.com/hashicorp/consul-templatehttps://releases.hashicorp.com/consul-template/0.19.3/consul-template_0.19.3_linux_amd64.zip 模拟在一台docker主机上跑多个nginx服务 让他们自动加入一个nginx LB上 registrator 容器部署123456# Docker主机启动注册器# 172.17.70.244# registrator# docker服务# docker run -d --name=registrator --net=host -v /var/run/docker.sock:/tmp/docker.sock --restart=always gliderlabs/registrator:latest -ip=172.17.70.244 consul://172.17.70.243:8500 12345678[root@Docker ~]# docker run -d --name=registrator --net=host --volume=/var/run/docker.sock:/tmp/docker.sock gliderlabs/registrator:latest consul://172.17.70.243:8500[root@Docker ~]# docker logs registrator[root@Docker ~]# docker run -it -d -p 88:80 nginx:1.16[root@Docker ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES27bc99e5993e nginx:1.16 "nginx -g 'daemon of…" About a minute ago Up About a minute 0.0.0.0:88-&gt;80/tcp cranky_rosalinddeca47a38063 gliderlabs/registrator:latest "/bin/registrator co…" 3 minutes ago Up 3 minutes registrator 他会自动将本地的容器 注册到 consul中 安装 nginx12345678# 172.17.70.243 # consul# nginx# consul-template 需要与nginx负载节点一起 本地配置文件生成 加载nginx [root@Jenkins ~]# yum install nginx# 修改配置文件 先把/etc/nginx/nginx.conf下的80监听注释掉 安装 consul-template 需要与nginx同一台机器 12345678910111213141516171819202122[root@Jenkins opt]# unzip consul-template_0.19.3_linux_amd64.zip [root@Jenkins opt]# mv consul-template /usr/bin/[root@Jenkins consul_nginx]# vim nginx.ctmplupstream http_backend &#123; # ip_hash; &#123;&#123;range service "nginx"&#125;&#125; server &#123;&#123; .Address &#125;&#125;:&#123;&#123; .Port &#125;&#125;; &#123;&#123; end &#125;&#125;&#125;server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125;&#125;# &#123;&#123;range service "nginx"&#125;&#125; 获取 consul 下 nginx名字 循环获取IP地址和端口 生成配置文件 并 reload nginx1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@Jenkins consul_nginx]# cd /opt/consul_nginx[root@Jenkins consul_nginx]# consul-template -consul-addr 39.106.100.108:8500 -template "./nginx.ctmpl:/etc/nginx/conf.d/site.conf:nginx -s reload"# 自定生成了配置文件 并加入了 docker的nginx地址 [root@Jenkins ~]# vim /etc/nginx/conf.d/site.conf upstream http_backend &#123; # ip_hash; server 172.17.70.243:88;&#125;server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125;&#125;# 再启动一个docker[root@Docker ~]# docker run -it -d -p 89:80 nginx:1.16# 查看配置文件是否自动添加[root@Jenkins nginx]# cat /etc/nginx/conf.d/site.conf upstream http_backend &#123; # ip_hash; server 172.17.70.243:88; server 172.17.70.243:89; &#125;server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://http_backend; &#125; &#125;]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 项目打包部署]]></title>
    <url>%2F2019%2F11%2F01%2Fdocker-base08%2F</url>
    <content type="text"><![CDATA[Docker Compose介绍 Compose是一个定义和管理多容器的工具，使用Python语言编写。 使用Compose配置文件描述多个容器应用的架构，比如使用什么镜像、数据卷、网络、映射端口等； 然后一条命令管理所有服务，比如启动、停止、重启等。 安装1234567891011121314# 安装 docker compose 单机编排https://docs.docker.com/compose/install/# 下载curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose# 增加执行权限[root@Docker opt]# chmod +x /usr/local/bin/docker-compose # 测试[root@Docker opt]# docker-compose versiondocker-compose version 1.24.1, build 4667896bdocker-py version: 3.7.3CPython version: 3.6.8OpenSSL version: OpenSSL 1.1.0j 20 Nov 2018 12yum install pippip install docker-compose 定义容器环境 根据yanml文件启动容器 YAML文件格式及编写注意事项YAML是一种标记语言很直观的数据序列化格式，可读性高。类似于XML数据描述语言，语法比XML简单的很多。YAML数据结构通过缩进来表示，连续的项目通过减号来表示，键值对用冒号分隔，数组用中括号括起来，hash用花括号括起来。YAML文件格式注意事项： 不支持制表符tab键缩进，需要使用空格缩进 通常开头缩进2个空格 字符后缩进1个空格，如冒号、逗号、横杆 用井号注释 如果包含特殊字符用单引号引起来 布尔值（true、false、yes、no、on、off）必须用引号括起来，这样分析器会将他们解释为字符串。 配置文件常用字段1官方文档：https://docs.docker.com/compose/compose-file 常用命令 Docker Compose 一键部署 LNMP 网站平台dockerfile nginx1234567891011121314151617181920212223242526272829303132[root@Docker-2 nginx]# mkdir -p /opt/compose_lnmp/nginx/[root@Docker-2 nginx]# cd /opt/compose_lnmp/nginx/[root@Docker-2 nginx]# vim Dockerfile # BASEFROM centos:7# WHOMAINTAINER leo 365042337@qq.com# YUM-REPOCOPY nginx.repo /etc/yum.repos.d/# PKGRUN cd /etc/yum.repos.d;mkdir bak;mv *.repo bak/ &amp;&amp; \ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \ curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo &amp;&amp; \ yum install pcre-devel zlib-devel openssl-devel autoconf nginx -y &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/*# nginx-confCOPY default.conf /etc/nginx/conf.d/default.confCOPY nginx.conf /etc/nginx/nginx.conf# PROTEXPOSE 80# CMDCMD ["nginx", "-g", "daemon off;"] 12345678[root@Docker-2 compose_lnmp]# tree nginx/nginx/├── CentOS-Base.repo├── default.conf├── Dockerfile├── epel.repo├── nginx.conf└── nginx.repo 123[root@Docker-2 compose_lnmp]# tree wwwroot/wwwroot/└── index.html dockerfile php1234567891011121314151617181920212223242526272829303132[root@Docker-2 php]# vim Dockerfile # BASEFROM centos:7# WHOMAINTAINER leo 365042337@qq.com# YUMRUN cd /etc/yum.repos.d;mkdir bak;mv *.repo bak/ &amp;&amp; \ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \ curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo &amp;&amp; \ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm &amp;&amp; \ rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm &amp;&amp; \ yum -y install php72w php72w-cli php72w-common php72w-devel \ php72w-embedded php72w-gd php72w-mbstring php72w-pdo php72w-xml php72w-fpm php72w-mysqlnd php72w-opcache &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/*RUN useradd nginx# CONFCOPY php.ini /etc/COPY php-fpm.conf /etc/COPY www.conf /etc/php-fpm.d/# PROTEXPOSE 9000# CMDCMD ["php-fpm"] 123456[root@Docker-2 compose_lnmp]# tree phpphp├── Dockerfile├── php-fpm.conf├── php.ini└── www.conf 1234[root@Docker-2 compose_lnmp]# tree wwwroot/wwwroot/├── index.html└── test.php mysql mysql 使用官方镜像 12345678910111213141516mysql: hostname: mysql image: mysql:5.6 ports: - 3306:3306 networks: - lnmp volumes: - ./mysql/conf:/etc/mysql/conf.d # 挂载本地的配置文件 - ./mysql/data:/var/lib/mysql # 挂载本地目录 command: --character-set-server=utf8 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: wordpress MYSQL_USER: user MYSQL_PASSWORD: 123456 docker-compose.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@Docker-2 compose_lnmp]# vim docker-compose.yml version: '3'services: nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - 88:80 networks: - lnmp volumes: - ./wwwroot:/wwwroot php: hostname: php build: context: ./php dockerfile: Dockerfile networks: - lnmp volumes: - ./wwwroot:/wwwroot mysql: hostname: mysql image: mysql:5.6 ports: - 3306:3306 networks: - lnmp volumes: - ./mysql/conf:/etc/mysql/conf.d - ./mysql/data:/var/lib/mysql command: --character-set-server=utf8 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: wordpress MYSQL_USER: user MYSQL_PASSWORD: 123456networks: lnmp: 编排12345678910111213141516[root@Docker-2 compose_lnmp]# docker-compose -f docker-compose.yml up -dCreating compose_lnmp_mysql_1 ... doneCreating compose_lnmp_nginx_1 ... doneCreating compose_lnmp_php_1 ... done[root@Docker-2 compose_lnmp]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0b0d94961312 compose_lnmp_php "php-fpm" 5 minutes ago Up 5 minutes 9000/tcp compose_lnmp_php_1246c13ae9fef compose_lnmp_nginx "nginx -g 'daemon of…" 5 minutes ago Up 5 minutes 0.0.0.0:88-&gt;80/tcp compose_lnmp_nginx_1ba5bee314099 mysql:5.6 "docker-entrypoint.s…" 5 minutes ago Up 5 minutes 0.0.0.0:3306-&gt;3306/tcp compose_lnmp_mysql_1[root@Docker-2 compose_lnmp]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcompose_lnmp_php latest d14d23830b03 5 minutes ago 371MBcompose_lnmp_nginx latest 372095ee6c06 12 minutes ago 310MBmysql 5.6 b3983abaa3fe 2 weeks ago 302MB 部署 wordpress 上传 wordpress-4.7.4-zh_CN.tar 1[root@Docker-2 wwwroot]# tar -zxvf wordpress-4.9.4-zh_CN.tar.gz 配置 12http://39.106.100.108:88/wordpress/# 数据库为容器hostname 部署 ngixn + tomcat 集群dockerfile tomcat123456789101112131415161718192021222324252627[root@Docker-2 tomcat]# vim Dockerfile # BASEFROM centos:7# WHOMAINTAINER leo 365042337@qq.com# tomcatCOPY apache-tomcat-8.5.47.tar.gz /opt/RUN yum install java-1.8.0-openjdk wget curl unzip net-tools -y &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/* &amp;&amp; \ cd /opt/ &amp;&amp; \ tar zxf apache-tomcat-8.5.47.tar.gz -C /usr/local/ &amp;&amp; \ mv /usr/local/apache-tomcat-8.5.47 /usr/local/tomcat &amp;&amp; \ rm -rf apache-tomcat-8.5.47.tar.gz /usr/local/tomcat/webapps/* &amp;&amp; \ sed -i '1a JAVA_OPTS="-Djava.security.egd=file:/dev/./urandom"' /usr/local/tomcat/bin/catalina.shENV PATH $PATH:/usr/local/tomcat/binWORKDIR /usr/local/tomcatEXPOSE 8080CMD ["/usr/local/tomcat/bin/catalina.sh", "run"] 1234[root@Docker-2 compose_nginx_tomcat]# tree tomcat/tomcat/├── apache-tomcat-8.5.47.tar.gz└── Dockerfile nginx 动静分离123456789101112131415161718192021222324252627282930[root@Docker-2 nginx]# vim Dockerfile # BASEFROM centos:7# WHOMAINTAINER leo 365042337@qq.com# YUM-REPOCOPY nginx.repo /etc/yum.repos.d/# PKGRUN cd /etc/yum.repos.d;mkdir bak;mv *.repo bak/ &amp;&amp; \ curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \ curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo &amp;&amp; \ yum install pcre-devel zlib-devel openssl-devel autoconf nginx -y &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/*# nginx-confCOPY default.conf /etc/nginx/conf.d/default.confCOPY nginx.conf /etc/nginx/nginx.confCOPY proxy_params /etc/nginx/proxy_params# PROTEXPOSE 80# CMD CMD ["nginx", "-g", "daemon off;"] 123456789[root@Docker-2 compose_nginx_tomcat]# tree nginx/nginx/├── CentOS-Base.repo├── default.conf├── Dockerfile├── epel.repo├── nginx.conf├── nginx.repo└── proxy_params mysql12345[root@Docker-2 compose_nginx_tomcat]# tree mysql/mysql/├── conf│ └── my.cnf└── data docker-compose.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@Docker-2 compose_nginx_tomcat]# vim docker-compose.yml version: '3'services: nginx: hostname: nginx build: context: ./nginx dockerfile: Dockerfile ports: - 88:80 networks: - lnmt volumes: - ./www:/opt/www tomcat01: hostname: tomcat01 build: ./tomcat networks: - lnmt volumes: - ./app:/usr/local/tomcat/webapps tomcat02: hostname: tomcat02 build: ./tomcat networks: - lnmt volumes: - ./app02:/usr/local/tomcat/webapps mysql: hostname: mysql image: mysql:5.6 ports: - 3307:3306 networks: - lnmt volumes: - ./mysql/conf:/etc/mysql/conf.d - ./mysql/data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: user123networks: lnmt: 目录结构 与 动态静态文件1234567891011121314151617181920212223242526272829[root@Docker-2 opt]# tree compose_nginx_tomcat/compose_nginx_tomcat/├── app│ └── test│ └── java_test.jsp├── app02│ └── test│ └── java_test.jsp├── docker-compose.yml├── mysql│ ├── conf│ │ └── my.cnf│ └── data├── nginx│ ├── CentOS-Base.repo│ ├── default.conf│ ├── Dockerfile│ ├── epel.repo│ ├── nginx.conf│ ├── nginx.repo│ └── proxy_params├── tomcat│ ├── apache-tomcat-8.5.47.tar.gz│ └── Dockerfile└── www ├── images │ └── nginx.png ├── index.html └── mysite.html 1234567891011121314[root@Docker-2 test]# cat java_test.jsp &lt;%@ page language="java" import="java.util.*" pageEncoding="utf-8"%&gt;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;JSP Test Page&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;% Random rand = new Random(); out.println("&lt;h1&gt;Random number from tomcat01:&lt;/h1&gt;"); out.println(rand.nextInt(99)+100); %&gt; &lt;/BODY&gt;&lt;/HTML&gt; 123456789101112131415161718192021222324252627[root@Docker-2 www]# cat mysite.html &lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8" /&gt; &lt;title&gt;测试ajax和跨域访问&lt;/title&gt; &lt;script src="http://libs.baidu.com/jquery/2.1.4/jquery.min.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;script type="text/javascript"&gt;$(document).ready(function()&#123; $.ajax(&#123; type: "GET", url: "http://39.106.100.108:88/test/java_test.jsp", success: function(data) &#123; $("#get_data").html(data) &#125;, error: function() &#123; alert("fail!!,请刷新再试!"); &#125; &#125;);&#125;);&lt;/script&gt; &lt;body&gt; &lt;h1&gt;测试动静分离&lt;/h1&gt; &lt;img src="http://39.106.100.108:88/images/nginx.png"&gt; &lt;div id="get_data"&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 1[root@proxy ~]# wget -O /tmp/nginx.png http://nginx.org/nginx.png 编排与测试123456[root@Docker-2 compose_nginx_tomcat]# docker-compose -f docker-compose.yml up -dCreating network "compose_nginx_tomcat_lnmt" with the default driverCreating compose_nginx_tomcat_tomcat02_1 ... doneCreating compose_nginx_tomcat_mysql_1 ... doneCreating compose_nginx_tomcat_tomcat01_1 ... doneCreating compose_nginx_tomcat_nginx_1 ... done 1234567[root@Docker-2 compose_nginx_tomcat]# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------------------------compose_nginx_tomcat_mysql_1 docker-entrypoint.sh mysqld Up 0.0.0.0:3307-&gt;3306/tcpcompose_nginx_tomcat_nginx_1 nginx -g daemon off; Up 0.0.0.0:88-&gt;80/tcp compose_nginx_tomcat_tomcat01_1 /usr/local/tomcat/bin/cata ... Up 8080/tcp compose_nginx_tomcat_tomcat02_1 /usr/local/tomcat/bin/cata ... Up 8080/tcp 123456789101112# docker-compose 命令 一定要进入 项目的目录里去执行[root@Docker-2 compose_nginx_tomcat]# docker-compose downStopping compose_nginx_tomcat_nginx_1 ... doneStopping compose_nginx_tomcat_mysql_1 ... doneStopping compose_nginx_tomcat_tomcat01_1 ... doneStopping compose_nginx_tomcat_tomcat02_1 ... doneRemoving compose_nginx_tomcat_nginx_1 ... doneRemoving compose_nginx_tomcat_mysql_1 ... doneRemoving compose_nginx_tomcat_tomcat01_1 ... doneRemoving compose_nginx_tomcat_tomcat02_1 ... doneRemoving network compose_nginx_tomcat_lnmt 1# 刷新整合页面测试 两台tomcat轮询显示]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 快速入门]]></title>
    <url>%2F2019%2F10%2F29%2Fdocker-base07%2F</url>
    <content type="text"><![CDATA[Docker 概述Docker 是什么 使用最广泛的开源容器引擎 一种操作系统级的虚拟化技术 依赖于Linux内核特性：Namespace（资源隔离）和Cgroups（资源限制） 一个简单的应用程序打包工具 Docker 设计目标 提供简单的应用程序打包工具 开发人员和运维人员职责逻辑分离 开发人员 如何使用docker部署 运维人员 如何管理容器 多环境保持一致性 构建:镜像产出 线上线下环境一致 运输:处处运行 不需要配置和修改 减少环境不一致的问题 构建,运输,随处运行 (Build,Ship and Run any App,Angwhere) Docker 基本组成 Docker Client：客户端 Ddocker Daemon：守护进程 Docker Images：镜像 Docker Container：容器 Docker Registry：镜像仓库 容器 vs 虚拟机 Docker 应用场景 应用程序打包和发布 应用程序隔离 持续集成 部署微服务 快速搭建测试环境 提供PaaS产品（平台即服务） Docker 安装Docker版本 社区版（Community Edition，CE） 企业版（Enterprise Edition，EE） 支持平台 CentOS7.x 安装 Docker123# 官方文档https://docs.docker.com/https://docs.docker.com/v18.03/ 123456789101112131415161718192021222324252627282930313233343536# 关闭方后墙和selinux[root@docker ~]# systemctl status firewalld[root@docker ~]# systemctl disable firewalld[root@docker ~]# getenforce Disabled# 安装依赖包yum install -y yum-utils device-mapper-persistent-data lvm2# 添加Docker官方源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# 查看仓库中所有docker安装版本 docker-ce-stableyum list docker-ce --showduplicates | sort -r# 安装指定版本 Docker CE# yum install docker-ce-&lt;VERSION STRING&gt;yum install docker-ce-18.06.3.ce# 默认安装是最新版本yum install docker-ce -y # 启动dockersystemctl start dockersystemctl enable docker.service# 通过运行hello world 镜像验证Docker是否正确安装。docker run hello-world# 查看docker 版本信息docker info# 卸载 Docker CEyum remove docker-cerm -rf /var/lib/docker Docker 镜像管理镜像是什么123451. 一个分层存储的文件2. 一个软件的环境3. 一个镜像可以创建N个容器4. 一种标准化的交付1. 5. 一个不包含Linux内核而又精简的Linux操作系统 镜像不是一个单一的文件，而是有多层构成。 我们可以通过docker history &lt;ID/NAME&gt; 查看镜像中各层内容及大小， 每层对应着Dockerfile中的一条指令。 Docker镜像默认存储在/var/lib/docker/\&lt;storage-driver>中。 镜像仓库 Docker Hub是由Docker公司负责维护的公共注册中心，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像。 地址：https://hub.docker.com/explore 镜像加速器 时速云1234https://www.daocloud.io/mirrorcurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.iosystemctl restart docker 镜像与容器联系 容器其实是在镜像的最上面加了一层读写层，在运行容器里文件改动时， 会先从镜像里要写的文件复制到容器自己的文件系统中（读写层）。 如果容器删除了，最上面的读写层也就删除了，改动也就丢失了。所以无论多 少个容器共享一个镜像，所做的写操作都是从镜像的文件系统中复制过来操作的，并不会修改镜像的源文件， 这种方式提高磁盘利用率。 若想持久化这些改动，可以通过docker commit 将容器保存成一个新镜像。 查看读写层123456789101112131415[root@docker ~]# docker run -it -d --name web nginx:1.16[root@docker ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd09ffde2ee5e nginx:1.16 "nginx -g 'daemon of…" 29 seconds ago Up 27 seconds 80/tcp web[root@docker ~]# docker inspect d09ffde2ee5e[root@docker 0b6de35669d9b5a5ae37514e3236083b3fed6d8f5b1f4ad9fd1c2ba50fdc96ff]# lsdiff link lower merged work# 开启个端口 添加个文件[root@docker ~]# docker exec -it d09ffde2ee5e bashroot@d09ffde2ee5e:/# touch test.log[root@docker merged]# cd /var/lib/docker/overlay2/0b6de35669d9b5a5ae37514e3236083b3fed6d8f5b1f4ad9fd1c2ba50fdc96ff/merged/[root@docker merged]# ls -l 镜像常用命令 镜像离线导出和导入12345678910111213141516171819202122232425262728# 离线传镜像 导出和导入[root@docker ~]# docker pull busyboxUsing default tag: latestlatest: Pulling from library/busybox7c9d20b9b6cd: Pull complete Digest: sha256:be38efcd3a8289ff66e60c5652beed36968eeb856ae5d90f3cf02ea81e4a57daStatus: Downloaded newer image for busybox:latest[root@docker ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 19485c79a9bb 7 weeks ago 1.22MB[root@docker ~]# docker image save busybox &gt; busybox.tar[root@docker ~]# docker image rm busyboxUntagged: busybox:latestUntagged: busybox@sha256:be38efcd3a8289ff66e60c5652beed36968eeb856ae5d90f3cf02ea81e4a57daDeleted: sha256:19485c79a9bbdca205fce4f791efeaa2a103e23431434696cc54fdd939e9198dDeleted: sha256:6c0ea40aef9d2795f922f4e8642f0cd9ffb9404e6f3214693a1fd45489f38b44[root@docker ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 540a289bab6c 6 days ago 126MBhello-world latest fce289e99eb9 10 months ago 1.84kB[root@docker ~]# docker image load &lt; busybox.tar 6c0ea40aef9d: Loading layer [==================================================&gt;] 1.437MB/1.437MBLoaded image: busybox:latest 查看镜像版本12# 官方镜像版本查找 筛选想要找到的镜像名称,选择tags查看https://hub.docker.com/_/nginx Docker 容器管理创建容器常用选项 123# 容器退出时重启策略# 镜像名称在最后,最后可以加cmddocker run -d --name web3 --restart always -P nginx 容器资源限制 1234# OOM Killer 操作系统在cpu不够用时 会删除使用率最高的进程# 当容器没有限制的时候 会使用所有的cpu和内存# 如果 --memory-swap = --memory 那么容器使用swap 为0 # 不设置 --memory-swap = 默认是内存的2倍 1234567[root@docker merged]# docker run -it -d --name web05 --cpus=".5" --memory="500m" --memory-swap="600m" busybox03d354261b2a3d0c2f51c3f5557f6c3ce7be2fbd2ac1b68e9a51f32c372c4f52[root@docker merged]# docker stats --no-stream web05CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS03d354261b2a web05 0.00% 1.074MiB / 500MiB 0.21% 0B / 0B 0B / 0B 1# 多个容器的资源限制要做 担心黑客攻击、搞瘫宿主机 管理容器常用命令 12# 列出最新创建的容器[root@docker ~]# docker ps -l 12# 显示一个容器运行的进程[root@docker merged]# docker top 03d354261b2a Docker 数据管理 容器是即行即用的 设计目标其中一条就是,不要轻易的进入容器,修改容器 容器删除后里面的数据就不会存在,不能每次都commit构建一个新的镜像吧… 那我们程序中经常需要变动数据 如web的根目录,或者是mysql的data目录,怎么办呢? 需要专门的存储方式,不能存在镜像里 将这些经常变动的数据目录,放在宿主机存储,删除容器数据也不会丢失,下次使用的时候挂载宿主机里面的数据即可 将数据从宿主机挂载到容器中的三种方式 Docker提供三种方式将数据从宿主机挂载到容器中: volumes： Docker管理宿主机文件系统的一部分（/var/lib/docker/volumes）。保存数据的最佳方式。 把数据存储在宿主机上 bind mounts： 将宿主机上的任意位置的文件或者目录挂载到容器中。 常用于 容器想使用宿主机上的目录文件 tmpfs： 挂载存储在主机系统的内存中，而不会写入主机的文件系统。如果不希望将数据持久存储在任何位置，可以使用 tmpfs，同时避免写入容器可写层提高性能。 Volume123456注意：1. 如果没有指定卷，自动创建。2. 建议使用--mount，更通用。文档:https://docs.docker.com/engine/admin/volumes/volumes/#start-a-container-with-a-volume 管理数据卷 123456# 创建docker volume create nginx-vol# 列出当前docker volume ls# 查看卷docker volume inspect nginx-vol 用卷创建一个容器： 12docker run -d --name=nginx-test --mount src=nginx-vol,dst=/usr/share/nginx/html nginxdocker run -d --name=nginx-test -v nginx-vol:/usr/share/nginx/html nginx 清理： 123# docker stop nginx-test# docker rm nginx-test # docker volume rm nginx-vol 使用: 12345678910111213141516171819202122232425262728[root@docker merged]# docker volume create nginx_volnginx_vol# 这个数据卷就可以让多个容器 共享数据[root@docker merged]# docker volume lsDRIVER VOLUME NAMElocal nginx_vol[root@docker merged]# docker volume inspect nginx_vol[ &#123; "CreatedAt": "2019-10-29T12:12:35+08:00", "Driver": "local", "Labels": &#123;&#125;, "Mountpoint": "/var/lib/docker/volumes/nginx_vol/_data", # 宿主机的挂载点 "Name": "nginx_vol", "Options": &#123;&#125;, "Scope": "local" &#125;][root@docker _data]# docker run -d --name web -p 80:80 --mount src=nginx_vol,dst=/usr/share/nginx/html nginx[root@docker merged]# docker inspect web[root@docker merged]# cd /var/lib/docker/volumes/nginx_vol/_data[root@docker _data]# echo "hello nginx" &gt; index.html # 再启动一个容器 挂载同一个数据卷 查看数据是否共享[root@docker _data]# docker run -d --name web2 -p 88:80 --mount src=nginx_vol,dst=/usr/share/nginx/html nginx 如果我们的某一个容器坏了,比如web2,如果不使用数据卷,里面的数据就看不到了 如果使用数据卷，直接挂载到nginx_vol就能看到了 删除容器,数据卷没有被删除掉,数据还在，除非也要删除数据卷 123# 停掉web02 再启动一个容器 把名字改了 端口不变 继续使用# 容器挂了没事,数据还在[root@docker _data]# docker run -d --name web3 -p 88:80 --mount src=nginx_vol,dst=/usr/share/nginx/html nginx Bind Mounts123注意：1. 如果源文件/目录没有存在，不会自动创建，会抛出一个错误。2. 如果挂载目标在容器中非空目录，则该目录现有内容将被隐藏。 12345678910用卷创建一个容器：# docker run -d -it --name=nginx-test --mount type=bind,src=/app/wwwroot,dst=/usr/share/nginx/html nginx# docker run -d -it --name=nginx-test -v /app/wwwroot:/usr/share/nginx/html nginx验证绑定：# docker inspect nginx-test清理：# docker stop nginx-test # docker rm nginx-test 123456789101112131415161718192021# 必须先要有这个目录[root@docker _data]# mkdir -p /app/www/root[root@docker _data]# docker run -d -it --name=web -p 80:80 --mount type=bind,src=/app/www/root,dst=/usr/share/nginx/html nginx9c16811aa4553e47bf3416bd3617a99ddff4f0f940e42804c76fae278a5bcb73# 挂载的目录 没有数据 和数据卷有区别,数据卷会把容器的数据拿出来[root@docker _data]# cd /app/www/root/[root@docker root]# ls -ltotal 0# 查看容器里 没有数据 挂载了宿主机的目录[root@docker ~]# docker exec -it d09ffde2ee5e bashroot@9c16811aa455:/# cd /usr/share/nginx/html/root@9c16811aa455:/usr/share/nginx/html# lsroot@9c16811aa455:/usr/share/nginx/html# # 再有新数据就会被同步过来root@9c16811aa455:/usr/share/nginx/html# echo "hello nginx" &gt; index.html[root@docker root]# lsindex.html 数据管理总结1234567Volume特点： 0 多个运行容器之间共享数据。1. 当容器停止或被移除时，该卷依然存在。2. 多个容器可以同时挂载相同的卷。3. 当明确删除卷时，卷才会被删除。4. 将容器的数据存储在远程主机或其他存储上5. 将数据从一台Docker主机迁移到另一台时，先停止容器，然后备份卷的目录（/var/lib/docker/volumes/） 12345Bind Mounts特点： 1. 从主机共享配置文件到容器。默认情况下，挂载主机/etc/resolv.conf到每个容器，提供DNS解析。2. 在Docker主机上的开发环境和容器之间共享源代码。例如，可以将Maven target目录挂载到容器中，每次在Docker主机上构建Maven项目时，容器都可以访问构建的项目包。3. 当Docker主机的文件或目录结构保证与容器所需的绑定挂载一致时 Docker 网络模式 bridge –net=bridge 默认网络，Docker启动后创建一个docker0网桥，默认创建的容器也是添加到这个网桥中。 host –net=host 容器不会获得一个独立的network namespace，而是与宿主机共用一个。这就意味着容器不会有自己的网卡信息， 而是使用宿主机的。 容器除了网络，其他都是隔离的。 none –net=none 获取独立的network namespace，但不为容器进行任何网络配置，需要我们手动配置。 container –net=container:Name/ID 与指定的容器使用同一个network namespace，具有同样的网络配置信息，两个容器除了网络，其他都还是隔离的。 自定义网络 与默认的bridge原理一样，但自定义网络具备内部DNS发现，可以通过容器名或者主机名容器之间网络通信。 bridge1[root@docker root]# docker run --rm -it --name bridge_test busybox bash host12345# 与宿主机网络配置一致# 在这个容器起的任何服务,都是使用宿主机的网络命名空间# 启动一个nginx 80端口 就真的是再使用宿主机的80端口# 容器网络没有隔离,其他隔离[root@docker root]# docker run --rm -it --net=host --name bridge_test busybox none123# 独立的网络命名空间,什么都不给你配置,需要自己配置# 自己手动配置 ip ns 这个用的很少[root@docker root]# docker run --rm -it --net=none --name bridge_test busybox container1234567891011121314# container： 让web2 与 web1 使用同一个网络命名空间[root@docker ~]# docker run -it -d --name web1 -p 88:80 busybox 0f94a0d8e173f0d724901c569f6c1df45e3e550503bc8b08ffcea231809f5c63[root@docker ~]# docker run -it -d --name web2 --net=container:web1 nginx:1.16771d7686b84e611c5058a59ad4fb238eeda5cac3ac0d1f907333a19b500a2235# 访问88 http://47.94.132.190:88/[root@docker ~]# docker exec -it web1 sh/ # netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - 自定义网络1234567891011121314151617181920212223242526# 默认创建的容器之间网络是隔离的,无法互相访问[root@docker ~]# docker network create bs-test[root@docker ~]# docker network lsNETWORK ID NAME DRIVER SCOPEc21a41553f06 bridge bridge local4ea4e3d78a78 bs-test bridge localc86b096a9bad host host local8ac4d9069089 none null local# 创建两台容器 指定自定义网络 --net=bs-test[root@docker ~]# docker run -it -d --name web1 --net=bs-test busyboxa4de56fe66deb842e15df0fc5e3729337a73da76171eee462987b187163a6a39[root@docker ~]# docker run -it -d --name web2 --net=bs-test busybox9b2d3aac32d33c34c579eccf063639cf81cc12618885da0b7da4f408b2b86aff# 测试互相访问/ # ping 172.20.0.3PING 172.20.0.3 (172.20.0.3): 56 data bytes64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.093 ms/ # ping web2PING web2 (172.20.0.3): 56 data bytes64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.061 ms 自定义网段网络 创建网络的时候 指定网段 启动容器 指定IP 自动dns 这个解析 只对用户自定义网络有效 1234567891011121314151617181920212223242526272829303132[root@docker ~]# docker network create --subnet 172.22.16.0/24 --gateway 172.22.16.1 my-net e9a2a92c293b2b08d5a3bd1344867cddf53c455d235b2fd960ddc10b5f09c871[root@docker ~]# docker network lsNETWORK ID NAME DRIVER SCOPEc21a41553f06 bridge bridge locala736c14402e0 bs-test bridge localc86b096a9bad host host locale9a2a92c293b my-net bridge local8ac4d9069089 none null local# 启动容器的时候 指定创建的网络 和 IP地址[root@docker ~]# docker run -it -d --network=my-net --ip 172.22.16.100 --name web3 busybox29747c8bb924b2aa0fc6e838614c21eef243df0015fb27d903f6a00abc6e019a[root@docker ~]# docker run -it -d --network=my-net --ip 172.22.16.200 --name web4 busybox9ce7e2a67a6c8d0dc900fa4985fe81e66a62f5c13ce5112164550fc12814be4a[root@docker ~]# docker exec -it web3 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever56: eth0@if57: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:16:10:64 brd ff:ff:ff:ff:ff:ff inet 172.22.16.100/24 brd 172.22.16.255 scope global eth0 valid_lft forever preferred_lft forever/ # ping 172.22.16.200PING 172.22.16.200 (172.22.16.200): 56 data bytes64 bytes from 172.22.16.200: seq=0 ttl=64 time=0.114 ms64 bytes from 172.22.16.200: seq=1 ttl=64 time=0.066 ms 把容器加入到自定义网络1234567891011121314151617181920212223242526272829303132333435363738# 创建一个基础容器[root@docker ~]# docker run -it -d --name web05 busybox55920dacacf33aa03b7000c098a88750e0efb771c8e2eac53864d5d22a573b24[root@docker ~]# docker exec -it web05 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever60: eth0@if61: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever# 加入到自定义网络 相当于添加了一块网卡[root@docker ~]# docker network connect my-net web05# 新增了网络配置[root@docker ~]# docker exec -it web05 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever60: eth0@if61: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever62: eth1@if63: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:16:10:02 brd ff:ff:ff:ff:ff:ff inet 172.22.16.2/24 brd 172.22.16.255 scope global eth1 valid_lft forever preferred_lft forever# 测试互通/ # ping 172.22.16.100PING 172.22.16.100 (172.22.16.100): 56 data bytes64 bytes from 172.22.16.100: seq=0 ttl=64 time=0.129 ms 容器网络访问原理veth pair(对) Docker安装完成时会创建一个 命名为 docker0 的 linux bridge 安装网桥工具 1234567891011yum install bridge-utils# 新建一台容器 默认使用bridge连接方式[root@docker ~]# docker run -it -d --name web busyboxf60c49b81814a74d0ae7c06f9cd3f14e5ae2f57b7876afc2a1478f9f9bf73d22# 查看网桥信息# 一个新的网络接口 veth808f338 被挂到了 docker0 上，veth808f338 就是新创建容器的虚拟网卡。[root@docker ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242c21ccdde no veth808f338 容器的网络配置 12容器有一个网卡 eth0@if65。大家可能会问了，为什么不是 veth808f338 呢？实际上 eth0@if65 和 veth808f338 是一对 veth pair(对)。 veth pair 是一种成对出现的特殊网络设备，可以把它们想象成由一根虚拟网线连接起来的一对网卡， 网卡的一头（eth0@if65）在容器中，另一头（veth808f338）挂在网桥 docker0 上， 其效果就是将 eth0@if65 也挂在了 docker0 上。 eth0@if65 已经配置了 IP 172.18.0.2，为什么是这个网段呢？ 12345[root@docker ~]# docker network inspect bridgebridge 网络配置的 subnet 就是 172.18.0.0/16，并且网关是 172.18.0.1这个网关在哪儿呢？大概你已经猜出来了，就是 docker0容器创建时，docker 会自动从 172.18.0.0/16 中分配一个 IP，这里 16 位的掩码保证有足够多的 IP 可以供容器使用。 Docker 提供三种 用户自定义 网络驱动：bridge, overlay 和 macvlan。 overlay 和 macvlan 用于创建跨主机的网络，我们可通过 bridge 驱动创建类似前面默认的 bridge 网络。 123456789101112131415161718192021222324docker network create --subnet 172.22.16.0/24 --gateway 172.22.16.1 my-net # 这里我们创建了新的 bridge 网络 my-net，网段为 172.22.16.0/24，网关为 172.22.16.1。# 与前面一样，网关在 my-net 对应的网桥 br-5d863e9f78b6 上：[root@docker ~]# brctl showbridge name bridge id STP enabled interfacesbr-78039658d427 8000.0242f6ae7c81 no docker0 8000.0242c21ccdde no veth808f338[root@docker ~]# ifconfig[root@docker ~]# docker network inspect my-net# 只有使用 --subnet 创建的网络才能指定静态 IP。docker run -it -d --network=my-net --ip 172.22.16.100 --name web3 busybox[root@docker ~]# docker exec -it web3 sh# veth pair(对) vethd795390 eth0@if65[root@docker ~]# brctl showbridge name bridge id STP enabled interfacesbr-78039658d427 8000.0242f6ae7c81 no vethd795390docker0 8000.0242c21ccdde no veth808f338 数据包是怎样到达外网的12345# 查看iptables [root@docker ~]# iptables -t nat -S-A POSTROUTING -s 172.22.16.0/24 ! -o br-78039658d427 -j MASQUERADE-A POSTROUTING -s 172.18.0.0/16 ! -o docker0 -j MASQUERADE 如果网桥 docker0 收到来自 172.18.0.0/16 网段的外出包，把它交给 MASQUERADE 处理。而 MASQUERADE 的处理方式是将包的源地址替换成 host 的地址发送出去，即做了一次网络地址转换（NAT）。 tcpdump 查看地址是如何转换的 1234567[root@docker ~]# ip rdefault via 172.17.79.253 dev eth0 169.254.0.0/16 dev eth0 scope link metric 1002 172.17.64.0/20 dev eth0 proto kernel scope link src 172.17.70.238 172.18.0.0/16 dev docker0 proto kernel scope link src 172.18.0.1 172.22.16.0/24 dev br-78039658d427 proto kernel scope link src 172.22.16.1 123456789101112131415161718192021222324252627282930# 监控 docker0 和 eth0 上的 icmp（ping）数据包当 busybox ping www.baidu.com 时，tcpdump 输出如下：/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever64: eth0@if65: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever/ # ping www.baidu.comPING www.baidu.com (220.181.38.149): 56 data bytes64 bytes from 220.181.38.149: seq=0 ttl=51 time=4.998 ms64 bytes from 220.181.38.149: seq=1 ttl=51 time=5.050 ms[root@docker ~]# tcpdump -i docker0 -n icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on docker0, link-type EN10MB (Ethernet), capture size 262144 bytes21:23:21.775420 IP 172.18.0.2 &gt; 220.181.38.149: ICMP echo request, id 6400, seq 0, length 6421:23:21.780433 IP 220.181.38.149 &gt; 172.18.0.2: ICMP echo reply, id 6400, seq 0, length 64 [root@docker ~]# tcpdump -i eth0 -n icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes21:23:21.775433 IP 172.17.70.238 &gt; 220.181.38.149: ICMP echo request, id 6400, seq 0, length 6421:23:21.780418 IP 220.181.38.149 &gt; 172.17.70.238: ICMP echo reply, id 6400, seq 0, length 64 流程总结 1234567891. busybox 发送 ping 包：172.18.0.2 &gt; www.baidu.com。docker0 收到包，发现是发送到外网的，交给 NAT 处理。NAT 将源地址换成 eth0 的 IP：172.17.70.238 &gt;www.baidu.com。ping 包从 eth0 发送出去，到达 www.baidu.com。通过 NAT，docker 实现了容器对外网的访问。 外网是如何访问容器的 数据包发送给宿主机后 通过DNAT 转给容器 123456[root@docker ~]# docker run -it -d -p 88:80 --name web6 nginx:1.1651972606abafed3a064986f6460ad794c839b62723ba9a48a51ebc5bfcbf7de9[root@docker ~]# iptables-save# 端口转发 -A DOCKER ! -i docker0 -p tcp -m tcp --dport 88 -j DNAT --to-destination 172.18.0.3:80 DockerfileDockerfile 格式 123456789101112FROM # 指定基础镜像FROM centosMAINTAINER # 指定维护者信息，可以没有RUN # 在命令前面加上RUN即可 RUN yum install httpd -yADD # 复制文件，与COPY不同的是,如果 src 是归档文件（tar, zip, tgz, xz 等），文件会被自动解压到 destWORKDIR # 设置当前工作目录VOLUME # 设置卷，挂载主机目录EXPOSE # 指定对外的端口CMD # 容器启动时执行的命令CMD [“/bin/bash”],只有最后一个生效。CMD 可以被 docker run 之后的参数替换。COPY # 将文件从 build context 复制到镜像。COPY src destENV # 环境变量ENTRYPOINT # 容器启动后执行的命令,只有最后一个生效（无法被替换，CMD 或 docker run 之后的参数会被当做参数传递给 ENTRYPOINT) Build镜像 快速部署 LNMP 网站平台 生产上应该注意到的 12341. 编译安装 (1.configure 2.make 3.make install) 还是 yum安装 (yum源) RUN 2. 编译安装需要启用哪些模块 RUN 3. nginx初始化 RUN 4. 启动 CMD 121. 可以先自己准备一个centos环境 然后自己在上面操作一次2. 都成功后 再写 Dockerfile 1231. 让自己生成的镜像越小越好2. 使用 &amp;&amp; 拼接命令3. 清理yum缓存，删除源码包 1231. 按照项目写dockerfile2. 底层是操作系统3. 以操作系统为最底层一层一层构建 目录规划 和 系统基础镜像12345678910# 按项目创建目录分层[root@docker ~]# mkdir -p /opt/game/&#123;base,runtime&#125;[root@docker ~]# mkdir -p /opt/game/base/centos[root@docker ~]# tree /opt/game/ /opt/game/ # 项目├── base # 基础环境 │ └── centos # centos 基础镜像└── runtime # 运行环境 构建 centos:7 基础镜像 由于官方提供的centos镜像有很多基础命令没有安装，所有我们自己来构建一个基础base 12345678910111213141516171819[root@docker centos]# vim Dockerfile# BASE FROM centos:7# WHOMAINTAINER leo 365042337@qq.com# EPELCOPY epel.repo /etc/yum.repos.d/COPY CentOS-Base.repo /etc/yum.repos.d/# EPEL And PKGRUN echo "==&gt; Install curl and helper tools..." &amp;&amp; \ yum install -y net-tools wget vim tree lsof tupdump gcc glibc gcc-c++ make &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/* 1234# 也可以下载#cd /etc/yum.repos.d;mkdir bak;mv *.repo bak/ &amp;&amp; \#curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \#curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo &amp;&amp; \ 123456# 构建镜像 game/centos:7[root@docker centos]# docker build -t game/centos:7 .[root@docker centos]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/centos 7 11810c6646f1 About a minute ago 381MB 构建 Nginx 基础镜像1234567891011[root@docker nginx]# mkdir -p /opt/game/runtime/nginx[root@docker nginx]# cd /opt/game/runtime/nginx# 使用nginx官方yum源 安装最新稳定版[root@docker nginx]# vim nginx.repo[nginx]name=nginx stable repobaseurl=http://nginx.org/packages/centos/7/$basearch/gpgcheck=0enabled=1 12345678910111213141516171819202122[root@docker nginx]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# YUM-REPOCOPY nginx.repo /etc/yum.repos.d/# PKGRUN yum install pcre-devel zlib-devel openssl-devel autoconf nginx -y &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/*# PROTEXPOSE 80# CMDCMD ["nginx", "-g", "daemon off;"] 12345678910111213141516# 构建[root@docker nginx]# docker build -t game/nginx:1.16 .# 查看镜像[root@docker nginx]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/nginx 1.16 b3fe0ded9499 8 minutes ago 437MB# 启动测试容器[root@docker nginx]# docker run -it -d --name test -p 80:80 game/nginx:1.16# 查看进程[root@docker nginx]# docker top test# 公网测试访问http://47.94.221.247/ 1234567891011121314151617181920212223242526272829# 传个测试首页测试 构建个项目# 项目镜像基于基础镜像打包[root@docker nginx]# echo "hello nginx" &gt; index.html[root@docker nginx]# vim Dockerfile-test # BASEFROM game/nginx:1.16# WHOMAINTAINER leo 365042337@qq.com# COPYCOPY index.html /usr/share/nginx/html/# 构建成项目镜像[root@docker nginx]# docker build -t game/app:v1 -f Dockerfile-test .[root@docker nginx]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/app v1 3cba5a6f4f7b 4 seconds ago 437MBgame/nginx 1.16 b3fe0ded9499 24 minutes ago 437MB# 启动容器测试 [root@docker nginx]# docker run -it -d --name v1 -p 88:80 game/app:v175c0c995be3e04a3cd2e8f413a5a95222f89837c74440f7483e259b2f3e8cd0b# 访问测试http://47.94.221.247:88/ 构建 PHP 基础镜像 创建构建目录 1[root@docker runtime]# mkdir -p /opt/game/runtime/php/ 编写php运行环境dockerfile 123456789101112131415161718192021222324252627# 提前准备好优化的配置文件[root@docker php]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# CMDRUN rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm &amp;&amp; \ rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm &amp;&amp; \ yum install php72w php72w-cli php72w-common php72w-devel \ php72w-embedded php72w-gd php72w-mbstring php72w-pdo php72w-xml \ php72w-fpm php72w-mysqlnd php72w-opcache -y &amp;&amp; \# CONFCOPY php.ini /etc/COPY php-fpm.conf /etc/COPY php-fpm.conf /etc/php-fpm.d/# PROTEXPOSE 9000# CMDCMD ["php-fpm"] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 编译安装[root@docker php]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# CMDRUN yum install -y wget vim pcre pcre-devel openssl openssl-devel libicu-devel gcc gcc-c++ \ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel \ zlib zlib-devel glibc glibc-devel glib2 glib2-devel ncurses ncurses-devel curl curl-devel \ krb5-devel libidn libidn-devel openldap openldap-devel nss_ldap jemalloc-devel cmake \ boost-devel bison automake libevent libevent-devel gd gd-devel libtool* libmcrypt libmcrypt-devel \ mcrypt mhash libxslt libxslt-devel readline readline-devel gmp gmp-devel libcurl libcurl-devel openjpeg-devel &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/*# COPYCOPY php-7.2.24.tar.gz /opt/RUN cd /opt/ &amp;&amp; groupadd www &amp;&amp; useradd -g www www &amp;&amp; tar -zxf php-7.2.24.tar.gz &amp;&amp; cd php-7.2.24 &amp;&amp; cp -frp /usr/lib64/libldap* /usr/lib/ &amp;&amp; \ ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --enable-fpm \ --with-fpm-user=www --with-fpm-group=www --enable-mysqlnd --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --enable-mysqlnd-compression-support \ --with-iconv-dir --with-freetype-dir --with-jpeg-dir --with-png-dir --with-zlib --with-libxml-dir --enable-xml --disable-rpath --enable-bcmath \ --enable-shmop --enable-sysvsem --enable-inline-optimization --with-curl --enable-mbregex --enable-mbstring --enable-intl --with-mcrypt --with-libmbfl \ --enable-ftp --with-gd --enable-gd-jis-conv --enable-gd-native-ttf --with-openssl --with-mhash --enable-pcntl --enable-sockets --with-xmlrpc --enable-zip \ --enable-soap --with-gettext --disable-fileinfo --enable-opcache --with-pear --enable-maintainer-zts --with-ldap=shared --without-gdbm &amp;&amp; \ make -j 4 &amp;&amp; make install &amp;&amp; \ mkdir -p /var/log/php-fpm &amp;&amp; \ cd /opt/ &amp;&amp; rm -rf php* \# CONFCOPY php.ini /usr/local/php/etc/COPY php-fpm.conf /usr/local/php/etc/COPY www.conf /usr/local/php/etc/php-fpm.d/ENV PATH $PATH:/usr/local/php/sbinWORKDIR /usr/local/php# PROTEXPOSE 9000# CMDCMD ["php-fpm"] 123456# 编译[root@docker php]# docker build -t game/php:v2 .[root@docker php]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/php v2 02d40f53884b 13 minutes ago 880MB 12345678910# 测试[root@docker php]# docker run -d --name php-test game/php:v2[root@docker php]# docker exec -it php-test bash[root@8b89eebb72ad php]# php-fpm -vPHP 7.2.24 (fpm-fcgi) (built: Oct 30 2019 06:48:25)Copyright (c) 1997-2018 The PHP GroupZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies 构建 yum版 PHP 基础镜像12345678910111213141516[root@Docker php]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# YUMRUN rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm &amp;&amp; \ rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm &amp;&amp; \ yum -y install php72w php72w-cli php72w-common php72w-devel \ php72w-embedded php72w-gd php72w-mbstring php72w-pdo php72w-xml php72w-fpm php72w-mysqlnd php72w-opcache &amp;&amp; \ echo "==&gt; Clean up..." &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/* 1[root@Docker php]# docker build -t game/php:7.2 . 123[root@Docker php]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/php 7.2 a2219aedc08d 6 seconds ago 514MB 123456789101112131415161718192021222324252627# 添加优化配置文件 构建成可用的php镜像# 注意: # 1. 由于我的nginx也是yum安装的，所以用户是启动用户也是nginx php应该使用和nginx同一用户，需要useradd nginx# 2. ; Default Value: any 一定要注释 使用默认值any 允许访问FastCGI进程的IP，any不限制 ; listen.allowed_clients = 127.0.0.1# 3. daemonize = no[root@Docker php]# vim Dockerfile # BASEFROM game/php:7.2# WHOMAINTAINER leo 365042337@qq.comRUN useradd nginx# CONFCOPY php.ini /etc/COPY php-fpm.conf /etc/COPY www.conf /etc/php-fpm.d/# PROTEXPOSE 9000# CMDCMD ["php-fpm"] 构建 TOMCAT 基础镜像1234567891011121314151617181920212223242526272829303132[root@docker runtime]# mkdir -p /opt/game/runtime/tomcat[root@docker tomcat]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# tomcatCOPY apache-tomcat-8.5.47.tar.gz /opt/# JDKRUN yum install java-1.8.0-openjdk wget curl unzip net-tools -y &amp;&amp; \ yum clean all &amp;&amp; \ rm -rf /var/cache/yum/* &amp;&amp; \ cd /opt/ &amp;&amp; \ tar zxf apache-tomcat-8.5.47.tar.gz -C /usr/local/ &amp;&amp; \ mv /usr/local/apache-tomcat-8.5.47 /usr/local/tomcat &amp;&amp; \ rm -rf apache-tomcat-8.5.47.tar.gz /usr/local/tomcat/webapps/* &amp;&amp; \ mkdir -p /usr/local/tomcat/webapps/test &amp;&amp; \ echo "hello tomcat" &gt; /usr/local/tomcat/webapps/test/status.html &amp;&amp; \ sed -i '1a JAVA_OPTS="-Djava.security.egd=file:/dev/./urandom"' /usr/local/tomcat/bin/catalina.shENV PATH $PATH:/usr/local/tomcat/binWORKDIR /usr/local/tomcatEXPOSE 8080CMD ["/usr/local/tomcat/bin/catalina.sh", "run"] 1234567891011# 测试[root@docker tomcat]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/tomcat v1 164bc8a196b9 2 minutes ago 637MBgame/php v2 02d40f53884b About an hour ago 880MBgame/app v1 3cba5a6f4f7b 7 hours ago 437MB# 启动容器[root@docker tomcat]# docker run -d --name tom1 -p 88:8080 game/tomcat:v1 http://47.94.221.247:88/test/status.html 123456789# 测试个小项目[root@docker tomcat]# vim Dockerfile-testapp FROM game/tomcat:v1COPY jenkins.war /usr/local/tomcat/webapp/ROOT.war# 构建 # 容器运行 快速部署LNMP网站平台构建 自定义网络 lnmp12# 构建后在lnmp网络的容器可以互相访问docker network create lnmp 构建 php 容器12345docker run -d --name lnmp_php --net lnmp --mount src=wwwroot,dst=/wwwroot game/app-php:7.2 [root@Docker runtime]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES49c19215722f game/app-php:7.2 "php-fpm" 2 seconds ago Up 1 second 9000/tcp lnmp_php 1234567891011121314151617181920212223查看创建的 wwwroot 数据卷 docker inspect wwwroot[root@Docker runtime]# docker inspect wwwroot[ &#123; "CreatedAt": "2019-10-31T09:49:13+08:00", "Driver": "local", "Labels": null, "Mountpoint": "/var/lib/docker/volumes/wwwroot/_data", "Name": "wwwroot", "Options": null, "Scope": "local" &#125;]# 进入数据卷 创建一个 phpinfo文件 用于测试nginx-&gt;php[root@Docker runtime]# cd /var/lib/docker/volumes/wwwroot/_data[root@docker _data]# vim test.php&lt;?php phpinfo();?&gt; 构建 nginx 容器123456789101112131415161718192021222324252627282930# yum 安装的nginx 配置文件在 conf.d/里 把里面的default.conf 覆盖下# lnmp_php 是通过自定义网络dns找到lnmp_php容器[root@Docker opt]# vim default.confserver &#123; server_name location; listen 80; root /wwwroot; index index.php index.html; location ~ \.php$ &#123; fastcgi_pass lnmp_php:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /wwwroot$fastcgi_script_name; include fastcgi_params; &#125;&#125;[root@Docker opt]# docker run -d --name lnmp_nginx --net lnmp -p 88:80 \&gt; --mount type=bind,src=/opt/default.conf,dst=/etc/nginx/conf.d/default.conf --mount src=wwwroot,dst=/wwwroot game/nginx:1.16[root@Docker opt]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd9e9535d8e0b game/nginx:1.16 "nginx -g 'daemon of…" 6 seconds ago Up 6 seconds 0.0.0.0:88-&gt;80/tcp lnmp_nginx49c19215722f game/app-php:7.2 "php-fpm" 11 minutes ago Up 11 minutes 9000/tcp lnmp_php# 创建一个index.html一会用于测试[root@Docker runtime]# cd /var/lib/docker/volumes/wwwroot/_data/[root@Docker _data]# echo "hello nginx" &gt;&gt; index.html 测试 nginx 与 php的互通性12345# 访问公网88端口http://60.205.217.112:88/# 访问测试php页面http://60.205.217.112:88/test.php 创建 mysql 容器1234567891011121314# 使用官方的mysql镜像# -e 传入变量 mysql容器会帮忙执行 创建数据库 和 设置密码 mysql:5.7 帮我们做了很多设置docker run -d \--name lnmp_mysql \--net lnmp \--mount src=mysql-vol,dst=/var/lib/mysql \-e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress mysql:5.7 --character-set-server=utf8[root@Docker opt]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6b769d4ef1b0 mysql:5.7 "docker-entrypoint.s…" 11 seconds ago Up 10 seconds 3306/tcp, 33060/tcp lnmp_mysqld9e9535d8e0b game/nginx:1.16 "nginx -g 'daemon of…" 16 minutes ago Up 16 minutes 0.0.0.0:88-&gt;80/tcp lnmp_nginx49c19215722f game/app-php:7.2 "php-fpm" 27 minutes ago Up 27 minutes 9000/tcp lnmp_php 123456789101112131415161718192021222324# mysql容器的数据在数据款中 mysql_vol[root@Docker _data]# cd /var/lib/docker/volumes/mysql-vol/_data/[root@Docker _data]# ls -ltotal 188480-rw-r----- 1 polkitd input 56 Oct 31 10:16 auto.cnf-rw------- 1 polkitd input 1676 Oct 31 10:16 ca-key.pem-rw-r--r-- 1 polkitd input 1112 Oct 31 10:16 ca.pem-rw-r--r-- 1 polkitd input 1112 Oct 31 10:16 client-cert.pem-rw------- 1 polkitd input 1680 Oct 31 10:16 client-key.pem-rw-r----- 1 polkitd input 1346 Oct 31 10:17 ib_buffer_pool-rw-r----- 1 polkitd input 79691776 Oct 31 10:17 ibdata1-rw-r----- 1 polkitd input 50331648 Oct 31 10:17 ib_logfile0-rw-r----- 1 polkitd input 50331648 Oct 31 10:16 ib_logfile1-rw-r----- 1 polkitd input 12582912 Oct 31 10:17 ibtmp1drwxr-x--- 2 polkitd input 4096 Oct 31 10:16 mysqldrwxr-x--- 2 polkitd input 4096 Oct 31 10:16 performance_schema-rw------- 1 polkitd input 1676 Oct 31 10:16 private_key.pem-rw-r--r-- 1 polkitd input 452 Oct 31 10:16 public_key.pem-rw-r--r-- 1 polkitd input 1112 Oct 31 10:16 server-cert.pem-rw------- 1 polkitd input 1676 Oct 31 10:16 server-key.pemdrwxr-x--- 2 polkitd input 12288 Oct 31 10:16 sysdrwxr-x--- 2 polkitd input 4096 Oct 31 10:17 wordpress 部署 wordpress php版123456789101112[root@Docker _data]# cd /var/lib/docker/volumes/wwwroot/_data/[root@Docker _data]# wget https://cn.wordpress.org/wordpress-4.9.4-zh_CN.tar.gz[root@Docker _data]# tar -zxvf wordpress-4.9.4-zh_CN.tar.gz# 访问 wordpress# 跳转了话自己加下端口http://60.205.217.112:88/wordpress/ http://60.205.217.112:88/wordpress/wp-admin/setup-config.php# 按照步骤安装 # 安装后的首页http://60.205.217.112:88/wordpress/ 1234[root@Docker wordpress]# cp wp-config-sample.php wp-config.php[root@Docker wordpress]# &gt;wp-config.php [root@Docker wordpress]# vim wp-config.php # 把页面上的内容粘贴进来即可 12345# 看日志[root@Docker wordpress]# docker exec -it lnmp_nginx bash[root@d9e9535d8e0b nginx]# cd /var/log/nginx/[root@d9e9535d8e0b nginx]# tail -200 access.log 镜像仓库 Harbor介绍 Habor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展， 从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ， AD/LDAP集成以及审计日志等，足以满足基本企业需求。 12345678# githubhttps://github.com/goharbor/harbor# 安装文档https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md# 下载 https://github.com/goharbor/harbor/releases 离线安装123456789101112131415# 安装 docker compose 单机编排https://docs.docker.com/compose/install/# 下载curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose# 增加执行权限[root@Docker opt]# chmod +x /usr/local/bin/docker-compose # 测试[root@Docker opt]# docker-compose versiondocker-compose version 1.24.1, build 4667896bdocker-py version: 3.7.3CPython version: 3.6.8OpenSSL version: OpenSSL 1.1.0j 20 Nov 2018 12345678910# 解压安装包,修改配置文件[root@Docker opt]# tar -xf harbor-offline-installer-v1.8.4.tgz [root@Docker opt]# cd harbor[root@Docker harbor]# vim harbor.yml # 修改主机名和管理员密码、数据库密码hostname: 172.17.70.239 # httpharbor_admin_password: 123456 # 访问密码database: password: 123456 123456# 准备[root@Docker harbor]# ./prepare# 安装[root@Docker harbor]# ./install.sh# web访问http://60.205.217.112 12345678910111213141516171819# 列出[root@Docker harbor]# docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------harbor-core /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcpharbor-portal nginx -g daemon off; Up (healthy) 80/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:80-&gt;80/tcp redis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp registryctl /harbor/start.sh Up (healthy) [root@Docker harbor]# vim docker-compose.yml # 看看是如何编排的# 每个功能都用到一个容器 免https使用123456789101112131415161718192021222324[root@Docker harbor]# vim /etc/docker/daemon.json # 写入进项仓库 IP+port&#123; "registry-mirrors": ["http://f1361db2.m.daocloud.io"], "insecure-registries": ["172.17.70.239"]&#125;# 重启docker systemctl daemon-reloadsystemctl restart docker.service# 重启harbor仓库# cd 到 harbor的安装目录cd /opt/harbor# 执行命令docker-compose stopdocker-compose up -d[root@Docker nginx]# docker infoInsecure Registries: 172.17.70.239 127.0.0.0/8 创建项目管理员用户 登录镜像仓库 123[root@Docker nginx]# docker login 172.17.70.239Username: leoPassword: 推送镜像123456789101112131415161718# 在项目中标记镜像：# docker tag SOURCE_IMAGE[:TAG] 172.17.70.239/library/IMAGE[:TAG][root@Docker harbor]# docker tag game/nginx:1.16 172.17.70.239/library/nginx:1.16[root@Docker harbor]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE172.17.70.239/library/nginx 1.16 15d5ee003c55 4 hours ago 420MB# 推送[root@Docker harbor]# docker push 172.17.70.239/library/nginx:1.16The push refers to repository [172.17.70.239/library/nginx]180f55013204: Pushed 1439954d81bb: Pushed f7c2c1cea963: Pushed 63406e473c91: Pushed 877b494a9f30: Pushed 1.16: digest: sha256:4505325827b9b4d8d9164c7617280792b2b1e05a0b185ed3e1d2d7fccad0e2f7 size: 1368 创建一个新项目123456789101112131415# 私有项目需要认证才能下载# 添加成员# 管理员 所有权限 对项目上传 下载# 开发者 对项目# 访客 只读[root@Docker harbor]# docker tag game/app-php:7.2 172.17.70.239/mygame/app-php:v1[root@Docker harbor]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE172.17.70.239/library/nginx 1.16 15d5ee003c55 4 hours ago 420MBgame/nginx 1.16 15d5ee003c55 4 hours ago 420MB172.17.70.239/mygame/app-php v1 16d838c461cd 4 hours ago 515MB[root@Docker harbor]# docker push 172.17.70.239/mygame/app-php:v1 123456789101112131415161718192021222324# 权限演示:# 退出仓库[root@Docker harbor]# docker logout 172.17.70.239Removing login credentials for 172.17.70.239# 删除本地的镜像 [root@Docker harbor]# docker rmi 172.17.70.239/library/nginx:1.16[root@Docker harbor]# docker rmi 172.17.70.239/mygame/app-php:v1# 下载公有项目镜像[root@Docker harbor]# docker pull 172.17.70.239/library/nginx:1.161.16: Pulling from library/nginxDigest: sha256:4505325827b9b4d8d9164c7617280792b2b1e05a0b185ed3e1d2d7fccad0e2f7Status: Downloaded newer image for 172.17.70.239/library/nginx:1.16# 下载私有项目镜像[root@Docker harbor]# docker pull 172.17.70.239/mygame/app-php:v1Error response from daemon: pull access denied for 172.17.70.239/mygame/app-php, repository does not exist or may require 'docker login'[root@Docker harbor]# docker login 172.17.70.239Username: leoPassword: [root@Docker harbor]# docker pull 172.17.70.239/mygame/app-php:v1 1234567# 启动一个容器[root@Docker harbor]# docker run -it -d --name mynginx 172.17.70.239/library/nginx:1.16[root@Docker harbor]# docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2f96c5d40fe9 172.17.70.239/library/nginx:1.16 "nginx -g 'daemon of…" 6 seconds ago Up 5 seconds 80/tcp mynginx 图形页面管理 Portainer shipyard 已经不提供更新 Portainer 是一个开源、轻量级Docker管理用户界面,基于Docker API，可管理Docker主机或Swarm模式 123RancherDocker UIPortainer # https://www.portainer.io/ 1234# 数据卷docker volume create portainer_data# 启动Portainer容器docker run -d -p 8000:8000 -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer 12345# 访问界面[root@Docker-2 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9b7a7bbcddd0 portainer/portainer "/portainer" 16 minutes ago Exited (1) 11 minutes ago hardcore_bhaskara[root@Docker-2 ~]# docker start 9b7a7bbcddd0 Prometheus 监控基础为什么要监控 对系统不间断实时监控 实时反馈系统当前状态 保证业务持续性运行 监控维度 Prometheus 概述 Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目， 拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算 基金会（CNCF），成为继Kubernetes之后的第二个托管项目。 12https://prometheus.iohttps://github.com/prometheus Prometheus 特点 多维数据模型：由度量名称和键值对标识的时间序列数据 PromSQL：一种灵活的查询语言，可以利用多维数据完成复杂的查询 不依赖分布式存储，单个服务器节点可直接工作 基于HTTP的pull方1. 采集时间序列数据 推送时间序列数据通过PushGateway组件支持 通过服务发现或静态配置发现目标 多种图形模式及仪表盘支持（grafana） Prometheus 架构 Prometheus Server：收集指标和存储时间序列数据，并提供查询接口 ClientLibrary：客户端库 Push Gateway：短期存储指标数据。主要用于临时性的任务 Exporters：采集已有的第三方服务监控指标并暴露metrics Alertmanager：告警 Web UI：简单的Web控制台 实例 和 作业 实例：可以抓取的目标称为实例（Instances） -&gt; 客户端 作业：具有相同目标的实例集合称为作业（Job） -&gt; 实例分类 通过容器 安装部署12Docker部署: https://prometheus.io/docs/prometheus/latest/installation/访问Web: http://localhost:9090 prometheus的监控大多数都是通过这个yml文件配置 12docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \ prom/prometheus 下载镜像 1234[root@Docker-2 ~]# docker pull prom/prometheus[root@Docker-2 opt]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEprom/prometheus latest 2c8e464e47f4 13 days ago 129MB 下载二进制包 12# 用里面的 prometheus.yml文件wget https://github.com/prometheus/prometheus/releases/download/v2.13.1/prometheus-2.13.1.linux-amd64.tar.gz 启动容器 123# 数据在容器中docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \ prom/prometheus 123# 持久化数据到宿主机上docker run -p 9090:9090 -v /prometheus-data \ prom/prometheus --config.file=/prometheus-data/prometheus.yml 123456789# 后台启动[root@Docker-2 opt]# docker run -d -p 9090:9090 -v /opt/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus[root@Docker-2 opt]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES294ee83dfb5c prom/prometheus "/bin/prometheus --c…" 5 seconds ago Up 4 seconds 0.0.0.0:9090-&gt;9090/tcp jolly_wiles# 访问http://39.106.100.108:9090/graph cAdvisor（Container Advisor） 用于收集正在运行的容器资源使用和性能信息。 cAdvisor 不具备存储 只具备采集 数据存储可持久访问的容器中 1https://github.com/google/cadvisor 12345678910sudo docker run \ --volume=/:/rootfs:ro \ --volume=/var/run:/var/run:ro \ --volume=/sys:/sys:ro \ --volume=/var/lib/docker/:/var/lib/docker:ro \ --volume=/dev/disk/:/dev/disk:ro \ --publish=8080:8080 \ --detach=true \ --name=cadvisor \ google/cadvisor:latest 1234[root@Docker-2 opt]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESff52b0f280c8 google/cadvisor:latest "/usr/bin/cadvisor -…" 4 seconds ago Up 2 seconds 0.0.0.0:8080-&gt;8080/tcp cadvisor294ee83dfb5c prom/prometheus "/bin/prometheus --c…" 12 minutes ago Up 12 minutes 0.0.0.0:9090-&gt;9090/tcp jolly_wiles 12345# webhttp://39.106.100.108:8080/containers/# 数据接口 http://39.106.100.108:8080/metrics 数据接口采集的数据,交给prometheus解析，遵循他的数据格式 拿这个接口 就可以告诉prometheus，我当前docker主机上的所有容器指标，他已经帮忙采集到了 Grafana 是一个开源的度量分析和可视化系统。 12https://grafana.com/grafana/downloadhttps://grafana.com/dashboards/193 12# 容器安装docker run -d --name=grafana -p 3000:3000 grafana/grafana 123# web # 默认用户名和密码 admin/admin 进去自己修改http://39.106.100.108:3000/login 添加 prometheus 数据源 1# 1. 如果访问不了 会提示网络连接有问题 修改 prometheus 数据文件12345678# prometheus 去找 cAdvisor 拿监控docker的数据# static_configs: 主动的配置被监控端# ,分割 可以写多个被监控端[root@Docker-2 opt]# vim /opt/prometheus.yml - job_name: 'docker' static_configs: - targets: ['172.17.70.240:8080'] 1234567891011[root@Docker-2 opt]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf239732834d3 grafana/grafana "/run.sh" 18 minutes ago Up 18 minutes 0.0.0.0:3000-&gt;3000/tcp grafanaff52b0f280c8 google/cadvisor:latest "/usr/bin/cadvisor -…" 29 minutes ago Up 29 minutes 0.0.0.0:8080-&gt;8080/tcp cadvisor294ee83dfb5c prom/prometheus "/bin/prometheus --c…" 41 minutes ago Up 41 minutes 0.0.0.0:9090-&gt;9090/tcp jolly_wiles# 重启下 prom[root@Docker-2 opt]# docker restart 294ee83dfb5c# 去看看有没有 cadvisor_version_info 和 container 的数据 http://39.106.100.108:9090 添加 Grafana 仪表盘 12345671. 导入一个别人写好的 2. 193是别人写好的ID3. 可以去官网看 那些人家写好的 下载使用数高的 兼容性好https://grafana.com/grafana/dashboards4. 可以选择监控指标的edit 拿到 rate(container_cpu_user_seconds_total&#123;image!=""&#125;[5m]) * 1005. 去prometheus里面执行查看数据,这是promsql 123# 再启动一个容器 [root@Docker-2 opt]# docker run -it -d --name=mynginx nginx:1.16# 查看监控]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09 ELK 收集Nginx访问日志实战案例]]></title>
    <url>%2F2019%2F10%2F28%2Felk-base09%2F</url>
    <content type="text"><![CDATA[Nginx的日志格式与日志变量 Nginx跟Apache一样，都支持自定义输出日志格式，在进行Nginx日志格式定义前，先来了解一下关于多层代理获取用户真实IP的几个概念。 remote_addr：表示客户端地址，但有个条件，如果没有使用代理，这个地址就是客户端的真实IP，如果使用了代理，这个地址就是上层代理的IP。 X-Forwarded-For：简称XFF，这是一个HTTP扩展头，格式为 X-Forwarded-For: client, proxy1, proxy2， 如果一个HTTP请求到达服务器之前，经过了三个代理 Proxy1、Proxy2、Proxy3，IP 分别为 IP1、IP2、IP3，用户真实IP为 IP0， 那么按照 XFF标准，服务端最终会收到以下信息：X-Forwarded-For: IP0, IP1, IP2 由此可知，IP3这个地址X-Forwarded-For并没有获取到，而remote_addr刚好获取的就是IP3的地址。 1231. $remote_addr：此变量如果走代理访问，那么将获取上层代理的IP，如果不走代理，那么就是客户端真实IP地址。2. $http_x_forwarded_for：此变量获取的就是X-Forwarded-For的值。3. $proxy_add_x_forwarded_for：此变量是$http_x_forwarded_for和$remote_addr两个变量之和。 自定义Nginx日志格式 掌握了Nginx日志变量的含义后，接着开始对它输出的日志格式进行改造，这里我们仍将Nginx日志输出设置为json格式， 下面仅列出Nginx配置文件nginx.conf中日志格式和日志文件定义部分，定义好的日志格式与日志文件如下： 12345678910111213141516171819# 当 $http_x_forwarded_for == "" 时,就把$remote_addr 赋值给$clientRealIp# 当不为空时,~^(?P&lt;firstAddr&gt;[0-9\.]+),?.*$ 匹配输出 取出第一个IP值 交给$firstAddr再交给$clientRealIp# accessip_list 此变量是$http_x_forwarded_for和$remote_addr两个变量之和# client_ip map里面截取的真实IP[root@filebeat1 nginx]# vim nginx.conf...http &#123; ... map $http_x_forwarded_for $clientRealIp &#123; "" $remote_addr; ~^(?P&lt;firstAddr&gt;[0-9\.]+),?.*$ $firstAddr; &#125; log_format nginx_log_json '&#123;"accessip_list":"$proxy_add_x_forwarded_for","client_ip":"$clientRealIp","http_host":"$host","@timestamp":"$time_iso8601","method":"$request_method","url":"$request_uri","status":"$status","http_referer":"$http_referer","body_bytes_sent":"$body_bytes_sent","request_time":"$request_time","http_user_agent":"$http_user_agent","total_bytes_sent":"$bytes_sent","server_ip":"$server_addr"&#125;'; access_log /var/log/nginx/access.log nginx_log_json; ... 2层的代理配置123本机 172.17.70.235代理1 172.17.70.236代理2 172.17.70.230 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@filebeat1 conf.d]# vim /etc/nginx/conf.d/test.conf server &#123; listen 80; server_name 172.17.70.235; root /data/; index index.html;&#125;[root@filebeat1 conf.d]# systemctl start nginx# 配置1级代理 172.17.70.236[root@logstash nginx]# vim conf.d/proxy.conf upstream test &#123; server 172.17.70.235:80;&#125;server &#123; listen 80; server_name 172.17.70.236; location / &#123; proxy_pass http://test; include proxy_params; &#125;&#125;[root@logstash nginx]# vim proxy_params proxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k;[root@logstash nginx]# curl 172.17.70.236 123456789101112131415161718# 2级代理 172.17.70.230[root@server2 conf.d]# vim proxy.conf upstream test &#123; server 172.17.70.236:80;&#125;server &#123; listen 80; server_name 172.17.70.230; location / &#123; proxy_pass http://test; include proxy_params; &#125;&#125;[root@server2 conf.d]# curl 172.17.70.230/index.html 观察日志12345[root@filebeat1 nginx]# tail -200 /var/log/nginx/access.log &#123;"accessip_list":"172.17.70.235","client_ip":"172.17.70.235","http_host":"172.17.70.235","@timestamp":"2019-10-28T15:05:39+08:00","method":"GET","url":"/","status":"200","http_referer":"-","body_bytes_sent":"11","request_time":"0.000","http_user_agent":"curl/7.29.0","total_bytes_sent":"246","server_ip":"172.17.70.235"&#125;&#123;"accessip_list":"172.17.70.236, 172.17.70.236","client_ip":"172.17.70.236","http_host":"172.17.70.236","@timestamp":"2019-10-28T15:05:45+08:00","method":"GET","url":"/","status":"200","http_referer":"-","body_bytes_sent":"11","request_time":"0.000","http_user_agent":"curl/7.29.0","total_bytes_sent":"241","server_ip":"172.17.70.235"&#125;&#123;"accessip_list":"172.17.70.230, 172.17.70.230, 172.17.70.236","client_ip":"172.17.70.230","http_host":"172.17.70.230","@timestamp":"2019-10-28T15:05:51+08:00","method":"GET","url":"/index.html","status":"200","http_referer":"-","body_bytes_sent":"11","request_time":"0.000","http_user_agent":"curl/7.29.0","total_bytes_sent":"241","server_ip":"172.17.70.235"&#125;&#123;"accessip_list":"61.51.152.214, 172.17.70.230, 172.17.70.236","client_ip":"61.51.152.214","http_host":"60.205.217.112","@timestamp":"2019-10-28T15:06:59+08:00","method":"GET","url":"/","status":"304","http_referer":"-","body_bytes_sent":"0","request_time":"0.000","http_user_agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36","total_bytes_sent":"173","server_ip":"172.17.70.235"&#125; 在这个输出中，可以看到，client_ip和accessip_list输出的异同，client_ip字段输出的就是真实的客户端IP地址， 而accessip_list输出是代理叠加而成的IP列表， 第一条日志，是直接访问http://1172.17.70.235 不经过任何代理得到的输出日志， 第二条日志，是经过一层代理访问172.17.70.236 而输出的日志， 第三条日志，是经过二层代理访问172.17.70.230 得到的日志输出。 最后是通过本地公网访问。 Nginx中获取客户端真实IP的方法很简单，无需做特殊处理，这也给后面编写logstash的事件配置文件减少了很多工作量。 配置 filebeat filebeat是安装在Nginx服务器上的 1234567891011121314151617181920212223242526272829# 将之前的apache修改一下即可# 一个事监控文件路径改成 nginx# 一个事topic名称 nginxlogs[root@filebeat1 filebeat]# vim filebeat.yml filebeat.inputs:- type: log enabled: true paths: - /var/log/nginx/access.log fields: log_topic: nginxlogsoutput.kafka: enabled: true hosts: ["172.17.70.232:9092", "172.17.70.232:9092", "172.17.70.232:9092"] version: "0.10" topic: '%&#123;[fields][log_topic]&#125;' partition.round_robin: reachable_only: true worker: 2 required_acks: 1 compression: gzip max_message_bytes: 10000000#processors:#- drop_fields:# fields: ["beat", "input", "source", "offset","prospector"]logging.level: debugname: "172.17.70.235" kafka 测试接收1234567891011121314# 启动filebeat# filebeat也会增加数据,没有过滤的话 只能在logstash里面过滤[root@filebeat1 filebeat]# ./filebeat -e -c filebeat.yml # 查看topic[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list__consumer_offsetsapachelogsnginxlogsosmessages# 消费数据# 刷新下公网页面 2级代理访问[root@kafkazk1 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic nginxlogs 配置 logstash 由于在Nginx输出日志中已经定义好了日志格式，因此在logstash中就不需要对日志进行过滤和分析操作了， 下面直接给出logstash事件配置文件kafka_nginx_into_es.conf的内容： apache需要自己定义获取真实客户端IP，Nginx通过map实现了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@logstash logstash]# vim kafka_nginx_into_es.confinput &#123; kafka &#123; bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092" topics =&gt; "nginxlogs" group_id =&gt; "logstash" codec =&gt; json &#123; charset =&gt; "UTF-8" &#125; add_field =&gt; &#123; "[@metadata][myid]" =&gt; "nginxaccess_log" &#125; &#125;&#125;filter &#123; if [@metadata][myid] == "nginxaccess_log" &#123; mutate &#123; # 将message字段内容中UTF-8单字节编码做替换处理，这是为了应对URL有中文出现的情况。 gsub =&gt; ["message", "\\x", "\\\x"] &#125; # 如果message字段中有HEAD请求，就删除此条信息。 if ( 'method":"HEAD' in [message] ) &#123; drop &#123;&#125; &#125; json &#123; source =&gt; "message" remove_field =&gt; "prospector" remove_field =&gt; "beat" remove_field =&gt; "source" remove_field =&gt; "input" remove_field =&gt; "offset" remove_field =&gt; "fields" remove_field =&gt; "host" remove_field =&gt; "@version" remove_field =&gt; "message" &#125; &#125;&#125;output &#123; if [@metadata][myid] == "nginxaccess_log" &#123; stdout &#123; codec =&gt; "rubydebug" &#125; &#125;&#125; 12[root@logstash logstash]# bin/logstash -f kafka_nginx_into_es.conf # 访问 产生日志 输出到 ES集群12345678910111213141516171819202122# 用于判断，跟上面input中[@metadata][myid]对应，当有多个输入源的时候，# 可根据不同的标识，指定到不同的输出地址。# 指定输出到elasticsearch，并指定elasticsearch集群的地址。# 指定nginx日志在elasticsearch中索引的名称，这个名称会在Kibana中用到。# 索引的名称推荐以logstash开头，后面跟上索引标识和时间。output &#123; if [@metadata][myid] == "nginxaccess_log" &#123; elasticsearch &#123; hosts =&gt; ["172.17.70.229:9200","172.17.70.230:9200","172.17.70.231:9200"] index =&gt; "logstash_nginxlogs-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125;[root@logstash logstash]# bin/logstash -f kafka_nginx_into_es.conf # 刷新下数据# http://60.205.217.112/# 去配置索引http://60.205.217.112:5601/ 123# 后台运行[root@logstashserver ~]# cd /usr/local/logstash[root@logstash logstash]# nohup bin/logstash -f kafka_nginx_into_es.conf &amp;]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08 ELK 收集Apache访问日志实战案例]]></title>
    <url>%2F2019%2F10%2F27%2Felk-base08%2F</url>
    <content type="text"><![CDATA[ELK 收集日志的几种方式 ELK收集日志常用的有两种方式，分别是： 不修改源日志的格式，而是通过logstash的grok方式进行过滤、清洗，将原始无规则的日志转换为规则的日志。 修改源日志输出格式，按照需要的日志格式输出规则日志，logstash只负责日志的收集和传输，不对日志做任何的过滤清洗。 这两种方式各有优缺点， 第一种方式不用修改原始日志输出格式，直接通过logstash的grok方式进行过滤分析， 好处是对线上业务系统无任何影响，缺点是logstash的grok方式在高压力情况下会成为性能瓶颈，如果要分析的日志量超大时，日志过滤分析可能阻塞正常的日志输出。 因此，在使用logstash时，能不用grok的，尽量不使用grok过滤功能。 第二种方式缺点是需要事先定义好日志的输出格式，这可能有一定工作量， 但优点更明显，因为已经定义好了需要的日志输出格式，logstash只负责日志的收集和传输，这样就大大减轻了logstash的负担，可以更高效的收集和传输日志。 另外，目前常见的web服务器，例如apache、nginx等都支持自定义日志输出格式。因此，在企业实际应用中，第二种方式是首选方案。 12主要看系统是否已上线,如果没有上线可与研发技术确认,使用定义好的日志输出格式,减少logstash压力,如果已上线,先看看日志的输出格式,能不用的话尽量不用grok,用了就加大资源配置,尤其是cpu ELK 收集Apache访问日志应用架构 ELK+Filebeat+Kafka+ZooKeeper构建大数据日志 前端是apache服务器 apache 的日志格式与日志变量 apache支持自定义输出日志格式，但是，apache有很多日志变量字段，所以在收集日志前，需要首先确定哪些是我们需要的日志字段，然后将日志格式定下来。 要完成这个工作，需要了解apache日志字段定义的方法和日志变量的含义，在apache配置文件httpd.conf中，对日志格式定义的配置项为LogFormat， 默认的日志字段定义为如下内容： 1LogFormat "%h %l %u %t \"%r\" %&gt;s %b \"%&#123;Referer&#125;i\" \"%&#123;User-Agent&#125;i\"" combined apache 的日志格式 自定义apache日志格式 这里定义将apache日志输出为json格式，下面仅列出apache配置文件httpd.conf中日志格式和日志文件定义部分，定义好的日志格式与日志文件如下： 12345678[root@filebeat1 ~]# vim /etc/httpd/conf/httpd.conf LogFormat "&#123;\"@timestamp\":\"%&#123;%Y-%m-%dT%H:%M:%S%z&#125;t\",\"client_ip\":\"%&#123;X-Forwarded-For&#125;i\",\"direct_ip\": \"%a\",\"request_time\":%T,\"status\":%&gt;s,\"url\":\"%U%q\",\"method\":\"%m\",\"http_host\":\"%&#123;Host&#125;i\",\"server_ip\":\"%A\",\"http_referer\":\"%&#123;Referer&#125;i\",\"http_user_agent\":\"%&#123;User-agent&#125;i\",\"body_bytes_sent\":\"%B\",\"total_bytes_sent\":\"%O\"&#125;" access_log_json CustomLog logs/access.log access_log_json 这里通过LogFormat指令定义了日志输出格式，在这个自定义日志输出中，定义了13个字段，定义方式为：字段名称:字段内容，字段名称是随意指定的， 能代表其含义即可，字段名称和字段内容都通过双引号括起来，而双引号是特殊字符，需要转移，因此，使用了转移字符“\”，每个字段之间通过逗号分隔。 此外，还定义了一个时间字段 @timestamp，这个字段的时间格式也是自定义的，此字段记录日志的生成时间，非常有用。 CustomLog指令用来指定日志文件的名称和路径。 需要注意的是，上面日志输出字段中用到了body_bytes_sent和total_bytes_sent发送字节数统计字段，这个功能需要apache加载mod_logio.so模块， 如果没有加载这个模块的话，需要安装此模块并在httpd.conf文件中加载一下即可。 1234567891011121314151617181920212223# 重启服务# 配置个访问页面cd /var/www/htmlecho "hello apache" &gt;&gt;index.html# 重启服务systemctl status httpd# 到其他服务器curl测试[root@server2 html]# curl 172.17.70.235hello apache# 查看日志[root@filebeat1 httpd]# cd /etc/httpd/logs/[root@filebeat1 logs]# cat access.log &#123;"@timestamp":"2019-10-28T08:21:21+0800","client_ip":"-","direct_ip": "172.17.70.230","request_time":0,"status":200,"url":"/index.html","method":"GET","http_host":"172.17.70.235","server_ip":"172.17.70.235","http_referer":"-","http_user_agent":"curl/7.29.0","body_bytes_sent":"13","total_bytes_sent":"253"&#125;&#123;"@timestamp":"2019-10-28T08:25:10+0800","client_ip":"-","direct_ip": "172.17.70.230","request_time":0,"status":200,"url":"/index.html","method":"GET","http_host":"172.17.70.235","server_ip":"172.17.70.235","http_referer":"-","http_user_agent":"curl/7.29.0","body_bytes_sent":"13","total_bytes_sent":"253"&#125;# 日志生成格式已经改变好# apache的简单反向代理 验证日志输出 apache的日志格式配置完成后，重启apache，然后查看输出日志是否正常，如果能看到类似如下内容，表示自定义日志格式输出正常： 在这个输出中，可以看到，client_ip和direct_ip输出的异同， client_ip字段对应的变量为“%{X-Forwarded-For}i”，它的输出是代理叠加而成的IP列表， 而direct_ip对应的变量为%a，表示不经过代理访问的直连IP，当用户不经过任何代理直接访问apache时，client_ip和direct_ip应该是同一个IP。 配置 filebeat修改配置文件 filebeat是安装在apache服务器上 123456789101112131415161718192021222324252627282930313233343536[root@filebeat1 logs]# cd /usr/local/filebeat/[root@filebeat1 filebeat]# vim filebeat.yml filebeat.inputs:- type: log enabled: true paths: - /var/log/httpd/access.log fields: log_topic: apachelogs filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false name: "172.17.70.235"#============================= kafka output ==================================output.kafka: enabled: true hosts: ["172.17.70.232:9092", "172.17.70.232:9092", "172.17.70.232:9092"] version: "0.10" topic: '%&#123;[fields][log_topic]&#125;' partition.round_robin: reachable_only: true worker: 2 required_acks: 1 compression: gzip max_message_bytes: 10000000 #processors:#- drop_fields:# fields: ["beat", "input", "source", "offset","prospector"]logging.level: debug 121. 这个配置文件中，是将apache的访问日志/var/log/httpd/access.log内容实时的发送到kafka集群topic为apachelogs中。2. 需要注意的是filebeat输出日志到kafka中配置文件的写法。 测试并启动12345678910111213141516171819202122# 启动# filebeat的职责是监控收集日志,发送给kafka[root@filebeat1 filebeat]#./filebeat -e -c filebeat.yml# debug模式# 访问一下 产生点内容[root@server2 html]# curl 172.17.70.235hello apache# 去kafka查看[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list__consumer_offsetsapachelogsosmessages# 启动消费数据bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic apachelogs# 消息已可正常发送到kafka# 使用logstash消费数据 并进行清洗数据 配置 logstash logstash事件配置文件kafka_apache_into_es.conf 1[root@logstash logstash]# cd /usr/local/logstash/ 输入部分1234567891011121314151617# 输入部分input &#123; kafka &#123; # 指定输入源中kafka集群的地址。 bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092" #指定输入源中需要从哪个topic中读取数据。 topics =&gt; "apachelogs" group_id =&gt; "logstash" codec =&gt; json &#123; # 将输入的json格式进行UTF8格式编码 # 从kafka取出数据也必须是json格式 charset =&gt; "UTF-8" &#125; # 增加一个字段，用于标识和判断，在output输出中会用到,用作标识判断,有标识就输出 add_field =&gt; &#123; "[@metadata][tagid]" =&gt; "apacheaccess_log" &#125; &#125;&#125; 过滤部分1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 过滤部分# 由于我们在apache日志已经自定义了格式,所有过滤的工作减少了不少,否则要自制写正则过滤filter &#123; if [@metadata][myid] == "apacheaccess_log" &#123; # mutate 数据修改 # 这里的message就是message字段，也就是日志的内容。 # 这个插件的作用是将message字段内容中UTF-8单字节编码做替换处理，这是为了应对URL有中文出现的情况。 mutate &#123; gsub =&gt; ["message","\\x","\\\x"] &#125; # 如果message字段中有HEAD请求，就删除此条信息。get/post留下。 if ('method":"HEAD' in [message]) &#123; drop &#123;&#125; &#125; # 启用json解码插件 json &#123; # 为什么要删除 message字段,是因为json插件解码后,已经保留了13个字段，而message就没有用了 source =&gt; "message" # 这里添加一个字段，用于后面的判断。 add_field =&gt; &#123; "[@metadata][direct_ip]" =&gt; "%&#123;direct_ip&#125;"&#125; # 移除不需要的字段，九个字段都是filebeat传输日志时添加的，没什么用处，所以需要移除。 remove_field =&gt; "@version" remove_field =&gt; "prospector" remove_field =&gt; "beat" remove_field =&gt; "source" remove_field =&gt; "input" remove_field =&gt; "offset" remove_field =&gt; "fields" remove_field =&gt; "host" # 因为json格式中已经定义好了每个字段，那么输出也是按照每个字段输出的， # 因此就不需要message字段了，这里是移除message字段。 remove_field =&gt; "message" &#125; # 这是对client_ip这个字段按逗号进行分组切分，因为在多级代理情况下，client_ip获取到的IP可能是IP列表， # 如果是单个ip的话，也会进行分组，只不过是分一个组而已。 mutate &#123; split =&gt; ["client_ip", ","] &#125; # 将切分出来的第一个分组赋值给client_ip，因为client_ip是IP列表的情况下，第一个IP才是客户端真实的IP。 mutate &#123; replace =&gt; &#123; "client_ip" =&gt; "%&#123;client_ip[0]&#125;" &#125; &#125; # if判断，主要用来判断当client_ip为"-"的情况下， # 当direct_ip不为"-"的情况下，就将direct_ip的值赋给client_ip。因为在client_ip为"-"的情况下，都是直接不经过代理的访问， # 此时direct_ip的值就是客户端真实IP地址，所以要进行一下替换。 # not in ["%&#123;direct_ip&#125;","-"] 这个判断的意思是如果direct_ip非空。 if [client_ip] == "-" &#123; if [@metadata][direct_ip] not in ["%&#123;direct_ip&#125;","-"] &#123; mutate &#123; replace =&gt; &#123; "client_ip" =&gt; "%&#123;direct_ip&#125;" &#125; &#125; &#125; &#125;else&#123; drop&#123;&#125; &#125; # direct_ip只是一个过渡字段，主要用于在某些情况下将值传给client_ip， # 因此传值完成后，就可以删除direct_ip字段了。 #mutate &#123; # remove_field =&gt; "direct_ip" #&#125; &#125;&#125; 输出到终端测试12345678# 放到终端输出测试output &#123; if [@metadata][myid] == "apacheaccess_log" &#123; stdout &#123; codec =&gt; "rubydebug" &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758### 完整版本[root@logstash logstash]# vim kafka_apache_into_es.conf mutate &#123; gsub =&gt; ["message","\\x","\\\x"] &#125; if ('method":"HEAD' in [message]) &#123; drop &#123;&#125; &#125; json &#123; source =&gt; "message" add_field =&gt; &#123; "[@metadata][direct_ip]" =&gt; "%&#123;direct_ip&#125;"&#125; remove_field =&gt; "@version" remove_field =&gt; "prospector" remove_field =&gt; "beat" remove_field =&gt; "source" remove_field =&gt; "input" remove_field =&gt; "offset" remove_field =&gt; "fields" remove_field =&gt; "host" remove_field =&gt; "message" &#125; mutate &#123; split =&gt; ["client_ip", ","] &#125; mutate &#123; replace =&gt; &#123; "client_ip" =&gt; "%&#123;client_ip[0]&#125;" &#125; &#125; if [client_ip] == "-" &#123; if [@metadata][direct_ip] not in ["%&#123;direct_ip&#125;","-"] &#123; mutate &#123; replace =&gt; &#123; "client_ip" =&gt; "%&#123;direct_ip&#125;" &#125; &#125; &#125; &#125;else&#123; drop&#123;&#125; &#125; #mutate &#123; # remove_field =&gt; "direct_ip" #&#125; &#125;&#125;output &#123; if [@metadata][myid] == "apacheaccess_log" &#123; stdout &#123; codec =&gt; "rubydebug" &#125; &#125;&#125; 123456# 放到终端测试# 启动logstash[root@logstash logstash]# ./bin/logstash -f kafka_apache_into_es.conf # 测试[root@server2 html]# curl 172.17.70.235/index.htmlhello apache 输出到 ES集群1234567891011121314151617output &#123; # 用于判断，跟上面input中[@metadata][myid]对应，当有多个输入源的时候， # 可根据不同的标识，指定到不同的输出地址。 if [@metadata][myid] == "apacheaccess_log" &#123; elasticsearch &#123; # 这是指定输出到elasticsearch，并指定elasticsearch集群的地址。 hosts =&gt; ["172.17.70.229:9200","172.17.70.230:9200","172.17.70.231:9200"] # 指定apache日志在elasticsearch中索引的名称，这个名称会在Kibana中用到。 # 索引的名称推荐以logstash开头，后面跟上索引标识和时间。 index =&gt; "logstash_apachelogs-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125;[root@logstash logstash]# ./bin/logstash -f kafka_apache_into_es.conf http://60.205.217.112:5601/app/kibana#/home?_g=() klbana 展示数据 filebeat收集数据到kafka，然后logstash从kafka拉取数据，如果数据能够正确发送到elasticsearch， 我们就可以在Kibana中配置索引了。 登录Kibana，首先配置一个index_pattern，点击kibana左侧导航中的Management菜单， 然后选择右侧的Index Patterns按钮，最后点击左上角的Create index pattern。 创建索引12# 有数据才有索引# -* 索引会以日期命名,每天一个索引 索引以什么排序12以时间字段@timestamp排序创建成功字段全部都正常先收拾 展示]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07 ELK Logstash 配置语法详解]]></title>
    <url>%2F2019%2F10%2F27%2Felk-base07%2F</url>
    <content type="text"><![CDATA[Logstash基本语法组成 logstash之所以功能强大和流行，还与其丰富的过滤器插件是分不开的，过滤器提供的并不单单是过滤的功能， 还可以对进入过滤器的原始数据进行复杂的逻辑处理，甚至添加独特的事件到后续流程中。 Logstash配置文件有如下三部分组成，其中input、output部分是必须配置， filter部分是可选配置，而filter就是过滤器插件，可以在这部分实现各种日志过滤功能。 123456789input &#123; #输入插件&#125;filter &#123; #过滤匹配插件&#125;output &#123; #输出插件&#125; Logstash 输入插件(Input)读取文件(File) logstash使用一个名为filewatch的ruby gem库来监听文件变化,并通过一个叫.sincedb的数据库文件来记录被监听的日志文件的读取进度（时间戳）， 这个sincedb数据文件的默认路径在 &lt;path.data&gt;/plugins/inputs/file下面， 文件名类似于.sincedb_452905a167cf4509fd08acb964fdb20c， 而&lt;path.data&gt;表示logstash插件存储目录，默认是LOGSTASH_HOME/data。 123456789101112input &#123; file &#123; path =&gt; ["/var/log/messages"] type =&gt; "system" start_position =&gt; "beginning" &#125;&#125;output &#123; stdout&#123; codec=&gt;rubydebug &#125;&#125; 这个配置是监听并接收本机的/var/log/messages文件内容， start_position表示按时间戳记录的地方开始读取，如果没有时间戳则从头开始读取，有点类似cat命令， 默认情况下，logstash会从文件的结束位置开始读取数据，也就是说logstash进程会以类似tail -f命令的形式逐行获取数据。 type用来标记事件类型,通常会在输入区域通过type标记事件类型。 1看业务需求,是否需要重新读取日志中所有内容,需要就加上 start_position =&gt; "beginning" 标准输入(Stdin) stdin是从标准输入获取信息 123456789101112input&#123; stdin&#123; add_field=&gt;&#123;"key"=&gt;"iivey"&#125; tags=&gt;["add1"] type=&gt;"test1" &#125;&#125;output &#123; stdout&#123; codec=&gt;rubydebug &#125;&#125; 读取 Syslog日志 6.0版本叫做rsyslog 如何将rsyslog收集到的日志信息发送到logstash中，这里以centos7.5为例，需要做如下两个步骤的操作： 首先，在 需要收集日志 的 服务器 上找到rsyslog的配置文件/etc/rsyslog.conf,添加如下内容： 1*.* @@172.16.213.120:5514 其中，172.16.213.120是logstash服务器的地址。5514是logstash启动的监听端口。 接着，重启rsyslog服务： 然后，在logstash服务器上创建一个事件配置文件，内容如下： 12345678910111213input &#123; syslog &#123; port =&gt; "5514" &#125;&#125;output &#123; stdout&#123; codec=&gt;rubydebug &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash2.conf 123456# 去要收集的服务器上配置[root@filebeat1 ~]# vim /etc/rsyslog.conf *.* @@172.17.70.236:5514[root@filebeat1 ~]# systemctl restart rsyslog 读取TCP网络数据 下面的事件配置文件就是通过”LogStash::Inputs::TCP”和”LogStash::Filters::Grok”配合实现syslog功能的例子， 这里使用了logstash的TCP/UDP插件读取网络数据： grok 正则抓取 123456789101112131415161718192021222324input &#123; tcp &#123; port =&gt; "5514" &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;SYSLOGLINE&#125;" &#125; &#125;&#125;output &#123; stdout&#123; codec=&gt;rubydebug &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash3.conf # 造个登录失败日志# 还可以导入日志[root@filebeat1 ~]# nc 172.17.70.236 5514 &lt; /var/log/secure Logstash 编码插件(Codec) 其实我们就已经用过编码插件codec了，也就是这个rubydebug，它就是一种codec，虽然它一般只会用在stdout插件中，作为配置测试或者调试的工具。 编码插件(Codec)可以在logstash输入或输出时处理不同类型的数据， 因此，Logstash不只是一个input–&gt;filter–&gt;output的数据流，而是一个input–&gt;decode–&gt;filter–&gt;encode–&gt;output的数据流。 Codec支持的编码格式常见的有plain、json、json_lines等。下面依次介绍。 codec插件之 plain plain是一个空的解析器，它可以让用户自己指定格式，也就是说输入是什么格式，输出就是什么格式。 下面是一个包含plain编码的事件配置文件： 123456789input&#123; stdin&#123; &#125;&#125;output&#123; stdout&#123; codec =&gt; "plain" &#125;&#125; codec插件之 json、json_lines 如果发送给logstash的数据内容为json格式,可以在input字段加入codec=&gt;json来进行解析，这样就可以根据具体内容生成字段，方便分析和储存。 如果想让logstash输出为json格式，可以在output字段加入codec=&gt;json，下面是一个包含json编码的事件配置文件： json每个字段是key:values格式，多个字段之间通过逗号分隔。有时候，如果json文件比较长，需要换行的话，那么就要用json_lines编码格式了。 数据分析 大多数使用json格式 数据非常大需要换行,就改成json_lines 123456789input &#123; stdin &#123; &#125; &#125;output &#123; stdout &#123; codec =&gt; json &#125;&#125; Logstash 过滤器插件(Filter) filter插件对字段的分割、转换和赋值 Grok 正则捕获 grok是一个十分强大的logstash filter插件，他可以通过正则解析任意文本，将非结构化日志数据弄成结构化和方便查询的结构。 他是目前logstash 中解析非结构化日志数据最好的方式。 Grok 的语法规则是： %{语法: 语义} “语法”指的就是匹配的模式，例如使用NUMBER模式可以匹配出数字，IP模式则会匹配出127.0.0.1这样的IP地址： 例如输入的内容为： 172.16.213.132 [07/Feb/2018:16:24:19 +0800] “GET / HTTP/1.1” 403 5039 123%&#123;IP:clientip&#125;匹配模式将获得的结果为： clientip: 172.16.213.132%&#123;HTTPDATE:timestamp&#125;匹配模式将获得的结果为： timestamp: 07/Feb/2018:16:24:19 +0800%&#123;QS:referrer&#125;匹配模式将获得的结果为： referrer: "GET / HTTP/1.1" 匹配规则方法路径查看 12345# 匹配模式[root@logstash test]# cd /usr/local/logstash/vendor/bundle/jruby/2.3.0/gems/logstash-patterns-core-4.1.2/patterns/# 基础匹配模式[root@logstash patterns]# more grok-patterns 调试grok正则表达式工具： 实际使用过滤先通过调试工具获取方法 12345678http://grokdebug.herokuapp.comhttp://120.203.18.89:6969/130/grok-debug-elk-%E5%9C%A8%E7%BA%BF%E8%B0%83%E8%AF%95grok%E5%B7%A5%E5%85%B7/# 转义\空格\[ ... \]# 语法:语义语义用来标志截取出来的数据是做什么用的 123172.16.213.132 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125; 通过上面这个组合匹配模式，我们将输入的内容分成了五个部分，即五个字段， 将输入内容分割为不同的数据字段，这对于日后解析和查询日志数据非常有用，这正是使用grok的目的。 Logstash默认提供了近200个匹配模式（其实就是定义好的正则表达式）让我们来使用，可以在logstash安装目录下， 例如这里是/usr/local/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.1.2/patterns目录里面查看， 基本定义在grok-patterns文件中。 1234567# 从这些定义好的匹配模式中，可以查到上面使用的四个匹配模式对应的定义规则匹配模式 正则定义规则NUMBER (?:%&#123;BASE10NUM&#125;)HTTPDATE %&#123;MONTHDAY&#125;/%&#123;MONTH&#125;/%&#123;YEAR&#125;:%&#123;TIME&#125; %&#123;INT&#125;IP (?:%&#123;IPV6&#125;|%&#123;IPV4&#125;)QS %&#123;QUOTEDSTRING&#125; 使用 Grok 正则捕获123456789101112131415[root@logstash test]# vim logstash6.confinput&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125; 123[root@logstash test]# ../bin/logstash -f logstash6.conf # 输入内容：172.16.213.132 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 进一步过滤 messages 既然已经把messages字段过滤分割5段,那么已经不需要messages整体这个字段了,需要把它删除 两个时间字段 日志收集时间和日志产生时间 不一致,主要是我们在后面展示的时候,只想展示 日志的产生时间，需要替换赋值 我们自己定义的时间字段也可删除,需要放在赋值之后,要不然赋值没了 1remove_field =&gt; ["message"] 删除字段中的某一列 12345678910111213141516171819[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] remove_field =&gt; [ "message" ] &#125; date&#123; match =&gt; ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125; 12345678910111213141516171819202122232425262728[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] remove_field =&gt; ["message"] &#125; date&#123; match =&gt; ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"] &#125; mutate&#123; remove_field =&gt; ["timestamp"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash6.conf # 输入内容：172.16.213.132 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 时间处理 (Date) date插件是对于排序事件和回填旧数据尤其重要，它可以用来转换日志记录中的时间字段，变成LogStash::Timestamp对象， 然后转存到@timestamp字段里，这在之前已经做过简单的介绍。 12345678filter &#123; grok &#123; match =&gt; ["message", "%&#123;HTTPDATE:timestamp&#125;"] &#125; date &#123; match =&gt; ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"] &#125;&#125; 数据修改 (Mutate)gsup 正则表达式替换匹配字段 gsub可以通过正则表达式替换字段中匹配到的值，只对字符串字段有效 123456# filed_name_1 是要替换的字段名filter &#123; mutate &#123; gsub =&gt; ["filed_name_1", "/" , "_"] &#125;&#125; 12345678910111213141516171819202122232425[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date&#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate&#123; # remove_field =&gt; ["timestamp"] gsub =&gt; ["message","/","_"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125; split 分隔符分割字符串为数组 split可以通过指定的分隔符分割字段中的字符串为数组 1234567# 将filed_name_2字段以"|"为区间分隔为数组。filter &#123; mutate &#123; split =&gt; ["filed_name_2", "|"] &#125;&#125; 123456789101112131415161718192021222324252627282930[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date&#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate&#123; # remove_field =&gt; ["timestamp"] # gsub =&gt; ["message","/","_"] split =&gt; ["message","|"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash6.conf # 输入内容172.16.213.132|[07/Feb/2018:16:24:19 +0800]|"GET / HTTP/1.1"|403|5039 rename 重命名字段 rename可以实现重命名某个字段的功能 1234567# 这个示例表示将字段old_field重命名为new_field。filter &#123; mutate &#123; rename =&gt; &#123; "old_field" =&gt; "new_field" &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date&#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate&#123; # remove_field =&gt; ["timestamp"] # gsub =&gt; ["message","/","_"] # split =&gt; ["message","|"] rename =&gt; &#123; "message" =&gt; "new_message" &#125; &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash6.conf # 输入内容172.16.213.132 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 remove_field 删除字段 remove_field可以实现删除某个字段的功能 1234567# 将字段timestamp删除filter &#123; mutate &#123; remove_field =&gt; ["timestamp"] &#125;&#125; 数据修改(Mutate) 完整示例12345678910111213141516171819202122232425262728293031323334# gsub =&gt; ["referrer","\"",""]# ["referrer","\"",""] -&gt; \" 转换成空[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok&#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date&#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate&#123; rename =&gt; &#123; "response" =&gt; "new_response" &#125; gsub =&gt; ["referrer","\"",""] split =&gt; ["clientip","."] remove_field =&gt; ["timestamp"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash6.conf # 输入内容172.16.213.132 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 GeoIP 地址查询归类 GeoIP是最常见的免费IP地址归类查询库，当然也有收费版可以使用。 GeoIP库可以根据IP 地址提供对应的地域信息，包括国别，省市，经纬度等，此插件对于可视化地图和区域统计非常有用。 下面是一个关于GeoIP插件的简单示例（仅列出filter部分）：： 1234567# 其中，ip_field字段是输出IP地址的一个字段。filter &#123; geoip &#123; source =&gt; "ip_field" &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok &#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date &#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate &#123; #rename =&gt; &#123; "response" =&gt; "new_response" &#125; #gsub =&gt; ["referrer","\"",""] #split =&gt; ["clientip","."] remove_field =&gt; ["timestamp"] &#125; geoip &#123; source =&gt; "clientip" &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash6.conf 114.114.114.114 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 1234567891011121314151617181920212223242526272829303132333435# 精简 经度 纬度 时区[root@logstash test]# vim logstash6.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok &#123; match =&gt; ["message","%&#123;IP:clientip&#125;\ \[%&#123;HTTPDATE:timestamp&#125;\]\ %&#123;QS:referrer&#125;\ %&#123;NUMBER:response&#125;\ %&#123;NUMBER:bytes&#125;"] # remove_field =&gt; ["message"] &#125; date &#123; match =&gt; ["timestamp", "dd/MM/yyyy:HH:mm:ss Z"] &#125; mutate &#123; #rename =&gt; &#123; "response" =&gt; "new_response" &#125; #gsub =&gt; ["referrer","\"",""] #split =&gt; ["clientip","."] remove_field =&gt; ["timestamp"] &#125; geoip &#123; source =&gt; "clientip" fields =&gt; [ "city_name", "region_name", "country_name", "ip" , "latitude", "longitude" , "timezone" ] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125; filter插件综合应用实例 下面给出一个业务系统输出的日志格式，由于业务系统输出的日志格式无法更改， 因此就需要我们通过logstash的filter过滤功能以及grok插件来获取需要的数据格式， 此业务系统输出的日志内容以及原始格式如下： 12018-02-09T10:57:42+08:00|~|123.87.240.97|~|Mozilla/5.0 (iPhone; CPU iPhone OS 11_2_2 like Mac OS X) AppleWebKit/604.4.7 Version/11.0 Mobile/15C202 Safari/604.1|~|http://m.sina.cn/cm/ads_ck_wap.html|~|1460709836200|~|DF0184266887D0E 123# 分析 先看什么分割的这段日志都是以“|~|”为区间进行分隔的,那么刚好我们就以“|~|”为区间分隔符将这段日志内容分割为6个字段。这里通过grok插件进行正则匹配组合就能完成这个功能。 内容分割1234567# 把内容分成了6个字段# GREEDYDATA 用来匹配一段所有内容,\|\~\|用来分割一直匹配得到最后,所有每个分割都要加好# (%&#123;GREEDYDATA:http_user_agent&#125;)\|\~\| # 调试grok正则表达式工具：http://grokdebug.herokuapp.comhttp://120.203.18.89:6969/130/grok-debug-elk-%E5%9C%A8%E7%BA%BF%E8%B0%83%E8%AF%95grok%E5%B7%A5%E5%85%B7/ 配置过滤检测1234567# 流程# 1. 日志分割 按照特有的间隔:\|\~\|filter - grok - match =&gt; &#123;"message" =&gt; "..."&#125;# 2. message 分割好后就没用了 可以remove_field# 3. data 日志产生时间格式化,再赋值给@timestamp,原本的locatime也没用了 删除# 4. 2018-02-09T10:57:42+08:00 时间格式要正确 1234567891011121314151617181920212223242526272829303132[root@logstash test]# vim logstash7.conf input&#123; stdin&#123;&#125;&#125;filter&#123; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;TIMESTAMP_ISO8601:localtime&#125;\|\~\|%&#123;IPORHOST:clientip&#125;\|\~\|(%&#123;GREEDYDATA:http_user_agent&#125;)\|\~\|(%&#123;DATA:http_referer&#125;)\|\~\|%&#123;GREEDYDATA:mediaid&#125;\|\~\|%&#123;GREEDYDATA:osid&#125;" &#125; remove_field =&gt; ["message"] &#125; date &#123; match =&gt; ["localtime", "yyyy-MM-dd'T'HH:mm:ssZZ"] target =&gt; "@timestamp" &#125; mutate &#123; remove_field =&gt; ["localtime"] &#125;&#125;output&#123; stdout&#123; codec =&gt; "rubydebug" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash7.conf # 输入内容:2018-02-09T10:57:42+08:00|~|123.87.240.97|~|Mozilla/5.0 (iPhone; CPU iPhone OS 11_2_2 like Mac OS X) AppleWebKit/604.4.7 Version/11.0 Mobile/15C202 Safari/604.1|~|http://m.sina.cn/cm/ads_ck_wap.html|~|1460709836200|~|DF0184266887D0E 1ELK的难点就在于 日志的过滤,清晰 Logstash 输出插件(output) output是Logstash的最后阶段，一个事件可以经过多个输出，而一旦所有输出处理完成，整个事件就执行完成。 一些常用的输出包括： 1231. file: 表示将日志数据写入磁盘上的文件。2. elasticsearch: 表示将日志数据发送给Elasticsearch。Elasticsearch可以高效方便和易于查询的保存数据。3. graphite: 表示将日志数据发送给graphite，graphite是一种流行的开源工具，用于存储和绘制数据指标。 Logstash还支持输出到nagios、hdfs、email（发送邮件）和Exec（调用命令执行） 12# 官方文档https://www.elastic.co/guide/en/logstash/6.3/output-plugins.html 输出到标准输出(stdout) stdout与之前介绍过的stdin插件一样，它是最基础和简单的输出插件 12345output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 保存为文件（file） file插件可以将输出保存到一个文件中 很棒,相当于文件的实时备份 1234567# 使用了变量匹配，用于自动匹配时间和主机名，这在实际使用中很有帮助。# /data/log3 这段目录必须存在,我测试用了tmpoutput &#123; file &#123; path =&gt; "/data/log3/%&#123;+yyyy-MM-dd&#125;/%&#123;host&#125;_%&#123;+HH&#125;.log" &#125; 123456789101112131415161718[root@logstash test]# vim logstash8.conf input &#123; stdin &#123;&#125;&#125;output &#123; file &#123; path =&gt; "/tmp/%&#123;+yyyy-MM-dd&#125;/%&#123;host&#125;_%&#123;+HH&#125;.log" &#125;&#125;[root@logstash test]# ../bin/logstash -f logstash8.confhello[root@logstash test]# cat /tmp/2019-10-27/logstash_14.log&#123;"@timestamp":"2019-10-27T14:48:25.284Z","@version":"1","host":"logstash","message":"hello"&#125; 完整同步日志12345678910111213141516[root@logstash test]# vim logstash8.conf input &#123; stdin &#123;&#125;&#125;output &#123; file &#123; path =&gt; "/tmp/%&#123;+yyyy-MM-dd&#125;/%&#123;host&#125;_%&#123;+HH&#125;.log" codec =&gt; line &#123; format =&gt; "%&#123;message&#125;"&#125; &#125;&#125;114.114.114.114 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039[root@logstash test]# cat /tmp/2019-10-27/logstash_15.log 114.114.114.114 [07/Feb/2018:16:24:19 +0800] "GET / HTTP/1.1" 403 5039 输出到 elasticsearch Logstash将过滤、分析好的数据输出到elasticsearch中进行存储和查询，是最经常使用的方法。 12345678 output &#123; elasticsearch &#123; host =&gt; ["172.16.213.37:9200","172.16.213.77:9200","172.16.213.78:9200"] index =&gt; "logstash-%&#123;+YYYY.MM.dd&#125;" manage_template =&gt; false template_name =&gt; "template-web_access_log" &#125;&#125; 上面配置中每个配置项含义如下： 123456789101112host： - 是一个数组类型的值，后面跟的值是elasticsearch节点的地址与端口，默认端口是9200。可添加多个地址。index： - 写入elasticsearch的索引的名称，这里可以使用变量。 - Logstash提供了%&#123;+YYYY.MM.dd&#125;这种写法。在语法解析的时候，看到以+ 号开头的，就会自动认为后面是时间格式， - 尝试用时间格式来解析后续字符串。这种以天为单位分割的写法，可以很容易的删除老的数据或者搜索指定时间范围内的数据。 - 此外，注意索引名中不能有大写字母。manage_template: - 用来设置是否开启logstash自动管理模板功能，如果设置为false将关闭自动管理模板功能。 - 如果我们自定义了模板，那么应该设置为false。template_name: - 这个配置项用来设置在Elasticsearch中模板的名称。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06 ELK+Filebeat+Kafka+ZooKeeper 构建大数据日志分析平台]]></title>
    <url>%2F2019%2F10%2F25%2Felk-base06%2F</url>
    <content type="text"><![CDATA[ELK 应用案例 典型 ELK 应用架构此架构稍微有些复杂，因此，这里做一下架构解读。 这个架构图从左到右，总共分为5层，每层实现的功能和含义分别介绍如下： 第一层. 数据采集层 数据采集层位于最左边的业务服务器集群上，在每个业务服务器上面安装了filebeat做日志收集，然后把采集到的原始日志发送到Kafka+zookeeper集群上。 filebeat 只能做简单的数据过滤,数据此时还是原始数据。 第二层. 消息队列层 原始日志发送到Kafka+zookeeper集群上后，会进行集中存储，此时，filbeat是消息的生产者，存储的消息可以随时被消费。 通过zookeeper调度和协调kafka工作，比如主节点挂掉了,选取主节点等 第三层. 数据分析层 Logstash作为消费者，会去Kafka+zookeeper集群节点实时拉取原始日志，然后将获取到的原始日志根据规则进行分析、清洗、过滤，最后将清洗好的日志转发至Elasticsearch集群。 如数据量过大,或者考虑性能,Logstash可以为多台。 第四层. 数据持久化存储 Elasticsearch集群在接收到logstash发送过来的数据后，执行写磁盘，建索引库等操作，最后将结构化的数据存储到Elasticsearch集群上。 第五层. 数据查询、展示层 Kibana是一个可视化的数据展示平台，当有数据检索请求时，它从Elasticsearch集群上读取数据，然后进行可视化出图和多维度分析。\ 环境与角色说明服务器环境与角色 操作系统统一采用Centos7.5版本，各个服务器角色如下表所示：(我使用阿里云服务器,操作系统可能会是7.6) 软件环境与版本 下表详细说明了本节安装软件对应的名称和版本号，其中，ELK三款软件推荐选择一样的版本，这里选择的是6.3.2版本。 安装JDK以及设置环境变量 选择合适版本并下载JDK Zookeeper 、elasticsearch和Logstash都依赖于Java环境，并且elasticsearch和Logstash要求JDK版本至少在JDK1.7或者以上。 安装过程 1234567891011121314151617181920212223[root@server2 ~]# lsjdk-8u231-linux-x64.tar.gz# 解压[root@server2 ~]# tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/local/[root@server2 ~]# ls -l /usr/local/jdk1.8.0_231/# 配置环境变量[root@server2 local]# vim /etc/profileexport JAVA_HOME=/usr/local/jdk1.8.0_231export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH[root@server2 local]# source /etc/profile# 让设置生效[root@server2 local]# java -versionjava version "1.8.0_231"Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)# 所有的ES环境都需要安装 安装并配置 elasticsearch 集群elasticsearch 集群的架构与角色 在ElasticSearch的架构中，有三类角色，分别是Client Node、Data Node和Master Node， 搜索查询的请求一般是经过Client Node来向 Data Node 获取数据， 而索引查询首先请求 Master Node 节点，然后 Master Node 将请求分配到多个Data Node节点完成一次索引查询。 集群中每个角色的含义介绍如下： master node： 可以理解为主节点，主要用于元数据(metadata)的处理，比如索引的新增、删除、分片分配等，以及管理集群各个节点的状态。 elasticsearch集群中可以定义多个主节点,但是，在同一时刻，只有一个主节点起作用，其它定义的主节点，是作为主节点的候选节点存在。 当一个主节点故障后，集群会从候选主节点中选举出新的主节点。 data node： 数据节点，这些节点上保存了数据分片。它负责数据相关操作，比如分片的CRUD、搜索和整合等操作。 数据节点上面执行的操作都比较消耗 CPU、内存和I/O资源，因此数据节点服务器要选择较好的硬件配置，才能获取高效的存储和分析性能。 client node： 客户端节点，属于可选节点，是作为任务分发用的，它里面也会存元数据，但是它不会对元数据做任何修改。 client node存在的好处是可以分担data node的一部分压力，因为elasticsearch的查询是两层汇聚的结果， 第一层是在data node上做查询结果汇聚，然后把结果发给client node，client node接收到data node发来的结果后再做第二次的汇聚， 然后把最终的查询结果返回给用户。这样，client node就替data node分担了部分压力。 安装 elasticsearch集群1234# 环境介绍server1 172.17.70.229 ES Master、ES NataNode server2 172.17.70.230 ES Master、Kibanaserver3 172.17.70.231 ES Master、ES NataNode 下载ES 1234https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gzhttps://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gzhttps://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.3.2-linux-x86_64.tar.gzhttps://artifacts.elastic.co/downloads/logstash/logstash-6.3.2.tar.gz 123456789[root@server2 ~]# mkdir -p /app/elk[root@server2 elk]# ls -ltotal 632464-rw-r--r-- 1 root root 91452574 Jul 24 2018 elasticsearch-6.3.2.tar.gz-rw-r--r-- 1 root root 12483335 Oct 25 11:02 filebeat-6.3.2-linux-x86_64.tar.gz-rw-r--r-- 1 root root 194151339 Oct 25 10:24 jdk-8u231-linux-x64.tar.gz-rw-r--r-- 1 root root 205331616 Oct 25 10:51 kibana-6.3.2-linux-x86_64.tar.gz-rw-r--r-- 1 root root 144211416 Oct 25 11:02 logstash-6.3.2.tar.gz 1234# 解压安装es[root@server2 elk]# tar -zxvf elasticsearch-6.3.2.tar.gz -C /usr/local# 修改目录名[root@server2 local]# mv /usr/local/elasticsearch-6.3.2 /usr/local/elasticsearch 12345678910111213# 目录说明[root@server2 local]# cd elasticsearch/[root@server2 elasticsearch]# ls -ltotal 460drwxr-xr-x 3 root root 4096 Oct 25 11:03 bin # 启动服务脚本 和 服务drwxr-xr-x 2 root root 4096 Jul 20 2018 config # 配置文件drwxr-xr-x 2 root root 4096 Jul 20 2018 lib-rw-r--r-- 1 root root 13675 Jul 20 2018 LICENSE.txtdrwxr-xr-x 2 root root 4096 Jul 20 2018 logsdrwxr-xr-x 17 root root 4096 Jul 20 2018 modules-rw-r--r-- 1 root root 416018 Jul 20 2018 NOTICE.txtdrwxr-xr-x 2 root root 4096 Jul 20 2018 plugins # 插件-rw-r--r-- 1 root root 8511 Jul 20 2018 README.textile 12345678[root@server2 elasticsearch]# cd config/ -rw-rw---- 1 root root 2853 Jul 20 2018 elasticsearch.yml # 主配置文件-rw-rw---- 1 root root 2937 Jul 20 2018 jvm.options # JVM内存配置-rw-rw---- 1 root root 6380 Jul 20 2018 log4j2.properties-rw-rw---- 1 root root 473 Jul 20 2018 role_mapping.yml-rw-rw---- 1 root root 197 Jul 20 2018 roles.yml-rw-rw---- 1 root root 0 Jul 20 2018 users-rw-rw---- 1 root root 0 Jul 20 2018 users_roles 增加es用户授权12345678#增加用户组groupadd elasticsearch#增加用户，并规定所属用户组和密码useradd elasticsearch -g elasticsearch# 递归更改文件的拥有者chown -R elasticsearch:elasticsearch /usr/local/elasticsearch 操作系统调优 操作系统以及JVM调优主要是针对安装elasticsearch的机器。对于操作系统，需要调整几个内核参数，将下面内容添加到/etc/sysctl.conf文件中： fs.file-max主要是配置系统最大打开文件描述符数，建议修改为655360或者更高， vm.max_map_count影响Java线程数量，用于限制一个进程可以拥有的VMA(虚拟内存区域)的大小，系统默认是65530，建议修改成262144或者更高。 123456[root@server2 config]# vim /etc/sysctl.conf fs.file-max=655360vm.max_map_count = 262144# 启用生效[root@server2 config]# sysctl -p 调整进程最大打开文件描述符（nofile）、最大用户进程数（nproc）和最大锁定内存地址空间（memlock），添加如下内容到/etc/security/limits.conf文件中： 12345678[root@server2 config]# vim /etc/security/limits.conf * soft nofile 204800* hard nofile 204800* soft nproc 655350* hard nproc 655350* soft memlock unlimited* hard memlock unlimited 最后，还需要修改/etc/security/limits.d/20-nproc.conf文件（centos7.x系统） 1234567[root@server2 config]# vim /etc/security/limits.d/20-nproc.conf* soft nproc 655350# 启用生效重新打开终端即可[root@server2 ~]# ulimit -a JVM调优 JVM调优主要是针对elasticsearch的JVM内存资源进行优化，elasticsearch的内存资源配置文件为jvm.options， 此文件位于/usr/local/elasticsearch/config目录下，打开此文件,修改如下内容： 1234567[root@server2 ~]# cd /usr/local/elasticsearch/config/[root@server2 config]# vim jvm.options -Xms1g-Xmx1g# 我的阿里云主机是2G内存# 可根据服务器内存大小，修改为合适的值。一般设置为服务器物理内存的一半最佳。 配置 elasticsearch elasticsearch的配置文件均在elasticsearch根目录下的config文件夹，这里是/usr/local/elasticsearch/config目录， 主要有jvm.options、elasticsearch.yml和log4j2.properties三个主要配置文件。这里重点介绍elasticsearch.yml一些重要的配置项及其含义。 这里配置的elasticsearch.yml文件内容如下： 12345678910111213141516171819[root@server2 config]# cd /usr/local/elasticsearch/config[root@server2 config]# vim elasticsearch.yml cluster.name: elkbigdata # 集群名称node.name: server1 # 节点名称 配置文件这里都不一致node.master: true # 是否有权利成为master 默认是true 默认第一台启动的es就是masternode.data: true # 是否是一个数据节点path.data: /data1/elasticsearch,/data2/elasticsearch # 索引数据的存储路径path.logs: /usr/local/elasticsearch/logs # 日志bootstrap.memory_lock: true # 锁住物理内存 不使用swapnetwork.host: 0.0.0.0http.port: 9200 # 对外提供http服务端口 通过该端口查看返回的数据信息discovery.zen.minimum_master_nodes: 1 # 最少master节点数 集群中不能低于这个数discovery.zen.ping.timeout:3s # 默认 新增节点集群会去ping 3秒ping不到就认为故障 网络丢包就加大discovery.zen.ping.unicast.hosts: ["172.17.70.229:9300", "172.17.70.230:9300", "172.17.70.231:9300"]# master初始化列表# 9300 是es集群互相通信端口 注意node.name肯定不能相同，其他可以相同 cluster.name: elkbigdata 配置elasticsearch集群名称，默认是elasticsearch。这里修改为elkbigdata，elasticsearch会自动发现在同一网段下的集群名为elkbigdata的主机。 node.name: server1 节点名，任意指定一个即可，这里是server1，我们这个集群环境中有三个节点，分别是server1、server2和server3，记得根据主机的不同，要修改相应的节点名称。 node.master: true 指定该节点是否有资格被选举成为master，默认是true，elasticsearch集群中默认第一台启动的机器为master角色，如果这台服务器宕机就会重新选举新的master。 node.data: true 指据，默认为true，表示数据存储节点，如果节点配置node.master:false并且node.data: false，则该节点就是client node。 这个client node类似于一个“路由器”，负责将集群层面的请求转发到主节点，将数据相关的请求转发到数据节点。 path.data:/data1/elasticsearch,/data2/elasticsearch 设置索引数据的存储路径，默认是elasticsearch根目录下的data文件夹，这里自定义了两个路径，可以设置多个存储路径，用逗号隔开。 path.logs: /usr/local/elasticsearch/logs 设置日志文件的存储路径，默认是elasticsearch根目录下的logs文件夹 bootstrap.memory_lock: true 此配置项一般设置为true用来锁住物理内存。linux下可以通过“ulimit -l” 命令查看最大锁定内存地址空间（memlock）是不是unlimited network.host: 0.0.0.0 此配置项用来设置elasticsearch提供服务的IP地址，默认值为0.0.0.0，此参数是在elasticsearch新版本中增加的，此值设置为服务器的内网IP地址即可。 http.port: 9200 设置elasticsearch对外提供服务的http端口，默认为9200。其实，还有一个端口配置选项transport.tcp.port，此配置项用来设置节点间交互通信的TCP端口，默认是9300。 discovery.zen.minimum_master_nodes: 1 配置当前集群中最少的master节点数，默认为1，也就是说，elasticsearch集群中master节点数不能低于此值，如果低于此值，elasticsearch集群将停止运行。在三个以上节点的集群环境中，建议配置大一点的值，推荐2至4个为好。 discovery.zen.ping.unicast.hosts: [“172.17.70.229:9300”, “172.17.70.230:9300”,”172.17.70.231:9300”] 设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。这里需要注意，master节点初始列表中对应的端口是9300。即为集群交互通信端口。 创建data目录123456# /data1/elasticsearch,/data2/elasticsearch[root@server2 config]# mkdir -p /data1/elasticsearch[root@server2 config]# mkdir -p /data2/elasticsearch[root@server2 config]# chown -R elasticsearch:elasticsearch /data1/elasticsearch/[root@server2 config]# chown -R elasticsearch:elasticsearch /data2/elasticsearch/ 使用普通用户启动ES服务1234567891011# es5版本后 禁止root用户启动es# 启动elasticsearch服务需要在一个普通用户下完成，如果通过root用户启动elasticsearch的话，可能会收到如下错误：# java.lang.RuntimeException: can not run elasticsearch as root# “-d”参数的意思是将elasticsearch放到后台运行。[root@server2 config]# su - elasticsearch[elasticsearch@server2 elasticsearch]$ cd /usr/local/elasticsearch/[elasticsearch@server2 elasticsearch]$ bin/elasticsearch -d [elasticsearch@server2 logs]$ netstat -ntlp [elasticsearch@server2 logs]$ ps -aux |grep java 验证elasticsearch集群的正确性1234567891011121314151617181920212223# 将所有elasticsearch节点的服务启动后，在任意一个节点执行如下命令：[elasticsearch@server2 logs]$ curl http://172.17.70.230:9200&#123; "name" : "server2", "cluster_name" : "elkbigdata", "cluster_uuid" : "Qxj3RgRARVCylLtalVOZFA", "version" : &#123; "number" : "6.3.2", "build_flavor" : "default", "build_type" : "tar", "build_hash" : "053779d", "build_date" : "2018-07-20T05:20:23.451332Z", "build_snapshot" : false, "lucene_version" : "7.3.1", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125;# 同时查看另外两台节点curl http://172.17.70.229:9200curl http://172.17.70.231:9200 安装与配置 zookeeper 集群 对于集群模式下的ZooKeeper部署，官方建议至少要三台服务器，关于服务器的数量，推荐是奇数个（3、5、7、9等等），以实现ZooKeeper集群的高可用，这里使用三台服务器进行部署 下载与安装zookeeper ZooKeeper是用Java编写的，需要安装Java运行环境，可以从zookeeper官网https://zookeeper.apache.org/获取zookeeper安装包，这里安装的版本是zookeeper-3.4.13.tar.gz。 将下载下来的安装包直接解压到一个路径下即可完成zookeeper的安装， 123[root@server2 elk]# tar -zxvf zookeeper-3.4.13.tar.gz -C /usr/local[root@server2 elk]# mv /usr/local/zookeeper-3.4.13 /usr/local/zookeeper[root@server2 elk]# cd /usr/local/zookeeper/ 配置zookeeper zookeeper 安装到了/usr/local目录下，因此，zookeeper的配置模板文件/usr/local/zookeeper/conf/zoo_sample.cfg， 拷贝zoo_sample.cfg并重命名为zoo.cfg，重点配置如下内容： 12[root@server2 conf]# cd /usr/local/zookeeper/conf/[root@server2 conf]# cp zoo_sample.cfg zoo.cfg 123456789# 配置属性参数tickTime=2000 # 控制心跳超时 2000毫秒 度量单位initLimit=10 # Follower服务器初始化连接到Leader时 最长能忍受多少个心跳 10*2000=20秒syncLimit=5 # Leader与Follower之间发送消息，请求和应答时间长度 5*2000dataDir=/data/zookeeper # 必须配 存储快照 不配置log 也会放在这里clientPort=2181 # 监听端口server.1=172.16.213.51:2888:3888 # 集群服务器信息 .1代表第几台服务器 server.2=172.16.213.109:2888:3888 # 2888 是与 Leader 通信的端口server.3=172.16.213.75:2888:3888 # 3888 选举时服务器之间通信端口 123456789[root@kafkazk1 conf]# grep ^'[a-Z]' zoo.cfg tickTime=2000initLimit=10syncLimit=5dataDir=/data/zookeeperclientPort=2181server.1=172.17.70.232:2888:3888server.2=172.17.70.233:2888:3888server.3=172.17.70.234:2888:3888 每个配置项含义如下 tickTime：zookeeper使用的基本时间度量单位，以毫秒为单位，它用来控制心跳和超时。2000表示2 tickTime。更低的tickTime值可以更快地发现超时问题。 initLimit：这个配置项是用来配置Zookeeper集群中Follower服务器初始化连接到Leader时，最长能忍受多少个心跳时间间隔数（也就是tickTime） syncLimit：这个配置项标识Leader与Follower之间发送消息，请求和应答时间长度最长不能超过多少个tickTime的时间长度 dataDir：必须配置项，用于配置存储快照文件的目录。需要事先创建好这个目录，如果没有配置dataLogDir，那么事务日志也会存储在此目录。 clientPort：zookeeper服务进程监听的TCP端口，默认情况下，服务端会监听2181端口。 server.A=B:C:D：其中A是一个数字，表示这是第几个服务器；B是这个服务器的IP地址；C表示的是这个服务器与集群中的Leader服务器通信的端口；D 表示如果集群中的Leader服务器宕机了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 myid 文件 除了修改zoo.cfg配置文件外，集群模式下还要配置一个文件myid， 这个文件需要放在dataDir配置项指定的目录下，这个文件里面只有一个数字，如果要写入1，表示第一个服务器，与zoo.cfg文本中的server.1中的1对应，以此类推， 在集群的第二个服务器zoo.cfg配置文件中dataDir配置项指定的目录下创建myid文件，写入2，这个2与zoo.cfg文本中的server.2中的2对应。 Zookeeper在启动时会读取这个文件，得到里面的数据与zoo.cfg里面的配置信息比较，从而判断每个zookeeper server的对应关系。 为了保证zookeeper集群配置的规范性，建议将zookeeper集群中每台服务器的安装和配置文件路径都保存一致。 123server1 myid 1server2 myid 2server3 myid 3 1234567[root@kafkazk1 conf]# mkdir -p /data/zookeeper[root@kafkazk1 conf]# cd /data/zookeeper[root@kafkazk1 zookeeper]# vim myid[root@kafkazk1 zookeeper]# cat /data/zookeeper/myid 1# 3台集群同样操作 启动 zookeeper1234567# 三台机器一起启动[root@kafkazk1 conf]# cd /usr/local/zookeeper/bin[root@kafkazk1 bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 1234567891011121314151617# 查看启动是否成功1. Zookeeper启动后，通过jps命令（jdk内置命令）可以看到有一个QuorumPeerMain标识，2. 这个就是Zookeeper启动的进程，前面的数字是Zookeeper进程的PID。[root@kafkazk1 bin]# jps1334 QuorumPeerMain1359 Jps# 日志文件 会在启动的路径下[root@kafkazk3 bin]# tail -200 zookeeper.out# 端口[root@kafkazk3 bin]# netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 1334/java tcp 0 0 172.17.70.234:3888 0.0.0.0:* LISTEN 1334/java 1234567# 有时候为了启动Zookeeper方面，也可以添加zookeeper环境变量到系统的/etc/profile中，# 这样，在任意路径都可以执行“zkServer.sh start”命令了，添加环境变量的内容为：vim /etc/profileexport ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile 安装并配置 Kafka Broker 集群 这里将kafka和zookeeper部署在一起了。另外，由于是部署集群模式的kafka，因此下面的操作需要在每个集群节点都执行一遍。 下载与安装Kafka12# 选择kafka版本 需要与 filebeat 需求的对应https://www.elastic.co/guide/en/beats/filebeat/6.3/kafka-output.html 可以从kafka官网 https://kafka.apache.org/downloads 获取kafka安装包，这里推荐的版本是kafka_2.10-0.10.0.1.tgz， 将下载下来的安装包直接解压到一个路径下即可完成kafka的安装，这里统一将kafka安装到/usr/local目录下，基本操作过程如下： 12[root@kafkazk1 elk]# tar -zxvf kafka_2.10-0.10.0.1.tgz -C /usr/local[root@kafkazk1 elk]# mv /usr/local/kafka_2.10-0.10.0.1 /usr/local/kafka 12345678[root@kafkazk1 kafka]# ls -ldrwxr-xr-x 3 root root 4096 Aug 4 2016 bindrwxr-xr-x 2 root root 4096 Aug 4 2016 configdrwxr-xr-x 2 root root 4096 Oct 26 11:50 libs-rw-r--r-- 1 root root 28824 Aug 4 2016 LICENSE-rw-r--r-- 1 root root 336 Aug 4 2016 NOTICEdrwxr-xr-x 2 root root 4096 Aug 4 2016 site-docs 1234# 主配置文件[root@kafkazk1 kafka]# ls -l config/server.properties # kafka他也自带了zookeeper,但是我们用的是自己的所有不用关心他的配置文件 配置 kafka集群12345678910111213141516# 常用配置broker.id=1 # 节点在集群中的唯一标识 每个节点需要修改listeners=PLAINTEXT://172.16.213.51:9092 # kafka监听地址和端口 9092默认 每个节点需要修改log.dirs=/usr/local/kafka/logs # 日志文件 关键是数据num.partitions=6 # topic有多少个分区 &gt;= 消费者数 保证每个消费者都能得到数据num.recovery.threads.per.data.dir=1 # 可减少启动时 日志的加载时间log.retention.hours=60 # 日志保持时间 60小时 log.segment.bytes=1073741824 # partition中每个segment数据文件的大小，默认是1Gzookeeper.connect=172.16.213.51:2181,172.16.213.75:2181,172.16.213.109:2181# zookeeper 所在的地址 三个zookeeper节点log.retention.check.interval.ms=300000 # 日志检查时间 zookeeper.connection.timeout.ms=6000 # zookeeper 连接超时时间auto.create.topics.enable=true # 是否自动创建topic delete.topic.enable=true # 设置可以物理删除topic 每个配置项含义如下123456789101112131415161718192021222324252627282930313233343536373839404142434445461. broker.id： - 每一个broker在集群中的唯一表示，要求是正数。 - 当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况。 2. listeners： - 设置kafka的监听地址与端口，可以将监听地址设置为主机名或IP地址，这里将监听地址设置为IP地址。3. log.dirs： - 这个参数用于配置kafka保存数据的位置，kafka中所有的消息都会存在这个目录下。 - 可以通过逗号来指定多个路径， kafka会根据最少被使用的原则选择目录分配新的parition。 - 需要注意的是，kafka在分配parition的时候选择的规则不是按照磁盘的空间大小来定的，而是根据分配的 parition的个数多小而定。4. num.partitions： - 这个参数用于设置新创建的topic有多少个分区，可以根据消费者实际情况配置，配置过小会影响消费性能。 - 这里配置6个。5. log.retention.hours： - 这个参数用于配置kafka中消息保存的时间，还支持log.retention.minutes和 log.retention.ms配置项。 - 这三个参数都会控制删除过期数据的时间，推荐使用log.retention.ms。如果多个同时设置，那么会选择最小的那个。6. log.segment.bytes： - 配置partition中每个segment数据文件的大小，默认是1GB，超过这个大小会自动创建一个新的segment file。7. zookeeper.connect： - 这个参数用于指定zookeeper所在的地址，它存储了broker的元信息。 - 这个值可以通过逗号设置多个值，每个值的格式均为：hostname:port/path， - 每个部分的含义如下： - hostname：表示zookeeper服务器的主机名或者IP地址，这里设置为IP地址。 - port： 表示是zookeeper服务器监听连接的端口号。 - /path：表示kafka在zookeeper上的根目录。如果不设置，会使用根目录。8. auto.create.topics.enable： - 这个参数用于设置是否自动创建topic，如果请求一个topic时发现还没有创建， kafka会在broker上自动创建一个topic， - 如果需要严格的控制topic的创建，那么可以设置auto.create.topics.enable为false，禁止自动创建topic。9. delete.topic.enable： - 在0.8.2版本之后，Kafka提供了删除topic的功能，但是默认并不会直接将topic数据物理删除。 - 如果要从物理上删除（即删除topic后，数据文件也会一同删除），就需要设置此配置项为true。 本次配置 kafka集群都要修改配置文件 12345678910111213141516171819[root@kafkazk1 config]# grep ^'[a-Z]' server.properties broker.id=1listeners=PLAINTEXT://172.17.70.232:9092num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/usr/local/kafka/logsnum.partitions=6num.recovery.threads.per.data.dir=1log.retention.hours=60log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181zookeeper.connection.timeout.ms=6000auto.create.topics.enable=truedelete.topic.enable=true 启动kafka集群 在启动kafka集群前，需要确保ZooKeeper集群已经正常启动。 依次在kafka各个节点上执行如下命令即可 12345678910# nohup 后台启动# &amp; 输出nohup.out 日志在本地[root@kafkazk1 kafka]# cd /usr/local/kafka[root@kafkazk1 kafka]# nohup bin/kafka-server-start.sh config/server.properties &amp;[1] 1766[root@kafkazk1 kafka]# nohup: ignoring input and appending output to ‘nohup.out’[root@kafkazk1 kafka]# lsbin config libs LICENSE logs nohup.out NOTICE site-docs 12345678这里将kafka放到后台运行，启动后，会在启动kafka的当前目录下生成一个nohup.out文件，1. 可通过此文件查看kafka的启动和运行状态。2. 通过jps指令，可以看到有个Kafka标识，这是kafka进程成功启动的标志。[root@kafkazk1 kafka]# jps2032 Jps1334 QuorumPeerMain1766 Kafka 123[root@kafkazk2 kafka]# tail -200 nohup.out [root@kafkazk1 kafka]# ps -ef|grep java[root@kafkazk1 kafka]# netstat -tnlp | grep 9092 kafka 集群基本命令操作 kefka提供了多个命令用于查看、创建、修改、删除topic信息， 也可以通过命令测试如何生产消息,消费消息等,这些命令位于kafka安装目录的bin目录下,这里是/usr/local/kafka/bin。 登录任意一台kafka集群节点，切换到此目录下，即可进行命令操作。 下面列举kafka的一些常用命令的使用方法: 1234567# 务必掌握1. 显示topic列表2. 创建一个topic，并指定topic属性（副本数、分区数等）3. 查看某个topic的状态4. 生产消息5. 消费消息6. 删除topic 123# 显示当前kafka集群中的topic列表[root@kafkazk1 kafka]# cd /usr/local/kafka[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list 12345678910# 创建一个topic[root@kafkazk1 kafka]# bin/kafka-topics.sh --create --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --replication-factor 1 --partitions 3 --topic mytopicCreated topic "mytopic".--replication-factor 1 # 副本信息 保存1份--partitiions3 # partitiions数量 和消费者数量有关--topic mytopic # topic 名称[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --listmytopic 123456789# 查看某个topic的属性信息[root@kafkazk1 kafka]# bin/kafka-topics.sh --describe --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopicTopic:mytopic PartitionCount:3 ReplicationFactor:1 Configs: Topic: mytopic Partition: 0 Leader: 1 Replicas: 1 Isr: 1 Topic: mytopic Partition: 1 Leader: 2 Replicas: 2 Isr: 2 Topic: mytopic Partition: 2 Leader: 3 Replicas: 3 Isr: 3# Replicas 对应副本# Isr: 1 活动状态 12345678# 生产消息# --broker-list 指定broker信息# 生产消息指定broker地址+端口 kafka集群的节点信息# --topic 指定消息生产在哪个topic# 交互命令 敲完命令 回车 输入消息[root@kafkazk1 kafka]# bin/kafka-console-producer.sh --broker-list 172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092 --topic mytopictest data kafka 123456 12345678910# 消费消息 # 查看消息 就是消费消息# 在开个终端查看消息# 消息是实时消费[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopicheeloabcdefg# 通过键盘输入消息内容，消费者马上可以看到# Crtl + c 退出 12345678# 从头开始接收 查看所有消息# 从头开始接收消息并没有顺序 ，只有实时消费查看 是按照生产的顺序[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic mytopic --from beginningheelotest data kafkaabcdefg123456leo 实际生产机制,并不是键盘生产消息，而是通过第三方软件，比如logstash或者filebeat,向kafka生产数据 消费者 可以使ES 也可以使logstash，最终实现数据传递 kafka就是生产和消费数据的中介,他实现数据的传递,消息队列,持久化缓存数据,作用于消息传输和保存数据 1234567891011# 删除一个topic[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --delete --topic mytopicTopic mytopic is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true.# 如果没有设置 delete.topic.enable=true 那么就是标记删除(逻辑删除),设置了就是物理删除# 再次查看就没有了[root@kafkazk1 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --list# 调试消费数据会经常用到,确认数据是否到kafa 安装并配置 Filebeat为什么要使用 filebeat Logstash功能虽然强大，但是它依赖java、在数据量大的时候，Logstash进程会消耗过多的系统资源，这将严重影响业务系统的性能， 而filebeat就是一个完美的替代者，filebeat是Beat成员之一，基于Go语言，没有任何依赖，配置文件简单，格式明了， 同时，filebeat比logstash更加轻量级，所以占用系统资源极少，非常适合安装在生产机器上。 下载与安装 filebeat 由于filebeat基于go语言开发,无其他任何依赖,因而安装非常简单, 可以从elastic官网https://www.elastic.co/downloads/beats/filebeat 获取filebeat安装包， 这里下载的版本是filebeat-6.3.2-linux-x86_64.tar.gz。 将下载下来的安装包直接解压到一个路径下即可完成filebeat的安装。 根据前面的规划，将filebeat安装到filebeat server主机上，这里设定将filebeat安装到/usr/local目录下， 基本操作过程如下： 1234567[root@filebeat1 ~]# mkdir -p /app/elk[root@filebeat1 ~]# cd /app/elk/[root@filebeat1 elk]# lsfilebeat-6.3.2-linux-x86_64.tar.gz[root@filebeat1 elk]# tar -zxvf filebeat-6.3.2-linux-x86_64.tar.gz -C /usr/local[root@filebeat1 elk]# mv /usr/local/filebeat-6.3.2-linux-x86_64 /usr/local/filebeat 12345678910111213[root@filebeat1 filebeat]# cd /usr/local/filebeat/[root@filebeat1 filebeat]# ls -l-rw-r--r-- 1 root root 55717 Jul 20 2018 fields.yml-rwxr-xr-x 1 root root 47593843 Jul 20 2018 filebeat # 启动文件-rw-r----- 1 root root 58886 Jul 20 2018 filebeat.reference.yml-rw------- 1 root root 7230 Jul 20 2018 filebeat.yml # 配置文件drwxrwxr-x 4 1000 1000 4096 Jul 20 2018 kibana-rw-r--r-- 1 root root 13675 Jul 20 2018 LICENSE.txtdrwxr-xr-x 16 1000 1000 4096 Jul 20 2018 moduledrwxr-xr-x 2 root root 4096 Jul 20 2018 modules.d # 模块文件 快速配置filebeat-rw-r--r-- 1 root root 143351 Jul 20 2018 NOTICE.txt-rw-r--r-- 1 root root 802 Jul 20 2018 README.md 123# 配置方法1. 手动配置 filebeat.yml 能更清楚运行机制2. 模块化配置 modules.d 下的已经写好的配置 , 重命名就可以使用 配置 filebeat filebeat的配置文件目录为/usr/local/filebeat/filebeat.yml，这里仅列出常用的配置项，内容如下： 123456789101112131415161718192021222324filebeat.inputs:- type: log # 输入log日志类型 enabled: true # 默认是走模块方式,手工需要配置为true 模块就不启作用了 paths: # 指定要收集的日志 可以是完整路径,也可以是模糊匹配 - /var/log/messages - /var/log/secure fields: # log_topic: osmessages 就是我们定义的topic 的名称 所以搜集的日志会放入 log_topic: osmessages #exclude_files: ['.gz$'] # 过滤 排除.gz结尾的文件 name: "172.16.213.157" # 服务器的标识 为空就是主机名output.kafka: # kafka 配置 enabled: true hosts: ["172.16.213.51:9092", "172.16.213.75:9092", "172.16.213.109:9092"] # kafka集群 version: "0.10" # 版本 topic: '%&#123;[fields][log_topic]&#125;' # 指定输出到哪个topic partition.round_robin: # rr轮询 reachable_only: true worker: 2 required_acks: 1 # 最大限度保证数据写入 leader确定后再进行发送下一条 compression: gzip # 数据压缩 max_message_bytes: 10000000 # 单条消息的最大长度 10Mlogging.level: debug # 消息级别 生产的时候可以改成警告或者info 123456789101112131415161718192021222324# 实验版本filebeat.inputs:- type: log enabled: true paths: - /var/log/messages - /var/log/secureoutput.kafka: enabled: true hosts: ["172.17.70.232:9092", "172.17.70.232:9092", "172.17.70.232:9092"] version: "0.10" topic: '%&#123;[fields][log_topic]&#125;' partition.round_robin: reachable_only: true worker: 2 required_acks: 1 compression: gzip max_message_bytes: 10000000processors:- drop_fields: fields: ["beat", "input", "source", "offset","prospector","host"]logging.level: debugname: "172.17.70.235" 123# 官方文档支持https://www.elastic.co/guide/en/beats/filebeat/6.3/configuration-filebeat-options.htmlhttps://www.elastic.co/guide/en/beats/filebeat/6.3/configuring-output.html 1# 所有输出的选项 都需要注释 默认的是es 1收费插件 Xpack 配置项的含义介绍如下：123456789101112131415161718192021222324252627282930313233341. filebeat.inputs： - 用于定义数据原型。 2. type： - 指定数据的输入类型，这里是log，即日志，是默认值，还可以指定为stdin，即标准输入。参考官方文档 3. enabled: true： - 启用手工配置filebeat，而不是采用模块方式配置filebeat。4. paths： - 用于指定要监控的日志文件，可以指定一个完整路径的文件，也可以是一个模糊匹配格式，例如： - - /data/nginx/logs/nginx_*.log，该配置表示将获取/data/nginx/logs目录下的所有以.log结尾的文件，注意这里有个破折号“-”， - 要在paths配置项基础上进行缩进，不然启动filebeat会报错，另外破折号前面不能有tab缩进，建议通过空格方式缩进。 - - /var/log/*.log，该配置表示将获取/var/log目录的所有子目录中以”.log”结尾的文件，而不会去查找/var/log目录下以”.log”结尾的文件。5. name: - 设置filebeat收集的日志中对应主机的名字，如果配置为空，则使用该服务器的主机名。这里设置为IP，便于区分多台主机的日志信息。\ 6. output.kafka： - filebeat支持多种输出，支持向kafka，logstash，elasticsearch输出数据，这里的设置是将数据输出到kafka。7. enabled： - 表明这个模块是启动的。 8. host: - 指定输出数据到kafka集群上，地址为kafka集群IP加端口号。9. topic： - 指定要发送数据给kafka集群的哪个topic，若指定的topic不存在，则会自动创建此topic。 - 注意topic的写法，在filebeat6.x之前版本是通过“%&#123;[type]&#125;”来自动获取document_type配置项的值。 - 而在filebeat6.x之后版本是通过'%&#123;[fields][log_topic]&#125;'来获取日志分类的。10. logging.level： - 定义filebeat的日志输出级别，有critical、error、warning、info、debug五种级别可选，在调试的时候可选择debug模式。 1# 过滤 清除字段 启动 filebeat 收集日志 所有配置完成之后，就可以启动filebeat，开启收集日志进程了，启动方式如下： 123456[root@filebeat1 filebeat]# cd /usr/local/filebeat/[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;[root@filebeat1 filebeat]# tail -200 nohup.out # 开始监控文件 12345678910# 看看要收集的日志 格式[root@filebeat1 filebeat]# tail /var/log/messagesOct 26 17:12:01 filebeat1 rsyslogd: [origin software="rsyslogd" swVersion="8.24.0" x-pid="813" x-info="http://www.rsyslog.com"] rsyslogd was HUPedOct 26 17:20:01 filebeat1 systemd: Started Session 11 of user root.Oct 26 17:20:01 filebeat1 systemd: Starting Session 11 of user root.Oct 26 17:30:01 filebeat1 systemd: Started Session 12 of user root.Oct 26 17:30:01 filebeat1 systemd: Starting Session 12 of user root.[root@filebeat1 filebeat]# tail /var/log/secure 模拟测试 filebeat 输出信息格式解读 开启filebeat的日志查看记录,有变化就会更新到日志中 1[root@filebeat1 filebeat]# tailf nohup.out 模拟一个失败的登录,日志产生到 /var/log/secure 中 123456789101112131415161718192021222324252627282930313233343536373839# 找一台服务器访问filebeat主机,故意失败登录,故意输错密码[root@server2 elk]# ssh 172.17.70.235root@172.17.70.235's password: Permission denied, please try again.[root@filebeat1 filebeat]# tailf nohup.out # json格式的输出..."@timestamp": "2019-10-26T09:39:23.831Z", # 收集到日志的具体时间"source": "/var/log/secure", # 收集的哪个日志"offset": 151, # 偏移量,消费信息的时候可以使用 "prospector": &#123; # 收集类型 和 输入日志类型 "type": "log" &#125;, "input": &#123; "type": "log" &#125;, "fields": &#123; # 自定义的列 "log_topic": "osmessages" &#125;, "beat": &#123; # 收集信息 "name": "172.17.70.235", "hostname": "filebeat1", "version": "6.3.2" &#125;, "host": &#123; # 主机信息 "name": "172.17.70.235" &#125;, # 收集的数据信息 "message": "Oct 26 17:39:14 filebeat1 sshd[1527]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.17.70.230 user=root",... filebeat输出信息格式解读 从这个输出可以看到，输出日志被修改成了JSON格式，日志总共分为10个字段， 分别是 “@timestamp”、”@metadata”、”beat”、”host”、”source”、“offset”、”message”、”prospector”、”input”和”fields”字段， 每个字段含义如下： 123456789101. @timestamp：时间字段，表示读取到该行内容的时间。2. @metadata：元数据字段，此字段只有是跟Logstash进行交互使用。3. beat：beat属性信息，包含beat所在的主机名、beat版本等信息。4. host： 主机名字段，输出主机名，如果没主机名，输出主机对应的IP。5. source： 表示监控的日志文件的全路径。6. offset： 表示该行日志的偏移量。7. message： 表示真正的日志内容。8. prospector：filebeat对应的消息类型。9. input：日志输入的类型，可以有多种输入类型，例如Log、Stdin、redis、Docker、TCP/UDP等10.fields：topic对应的消息字段或自定义增加的字段。 过滤字段 日志输出格式介绍和字段删减方法 filebeat收集了这么多字段的数据,所有我们要做一个简单的过滤 再交给后面的程序 通过filebeat接收到的内容，默认增加了不少字段，但是有些字段对数据分析来说没有太大用处， 所以有时候需要删除这些没用的字段，在filebeat配置文件中添加如下配置，即可删除不需要的字段： 这个设置表示删除”beat”、”input”、”source”、”offset” 四个字段，其中， @timestamp 和@metadata字段是不能删除的，就算加上也过滤不了。 做完这个设置后，再次查看kafka中的输出日志，已经不再输出这四个字段信息了。 后面可以通过logstash进行更好的过滤,不用担心 123processors:- drop_fields: fields: ["beat", "input", "source", "offset"] 1234# 重启filebeat [root@filebeat1 filebeat]# pgrep -f filebeat1514[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat` 1# 减少输出后 查看kafka集群有没有收到日志 登录到任意的kafka集群节点 消费topic osmessages就行 12345678910# 先看看有没有创建 我们定义的topic[root@kafkazk2 kafka]# bin/kafka-topics.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --listosmessages# 开启一个消费[root@kafkazk2 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic osmessages# 再模拟一次数据查看 数据有消费# filebeat 实时查询并生产数据 可以正常发送给 kafka# logstash 可以进行更好的过滤将想要的数据 匹配给不同的字段 1234# 时间概念:"@timestamp":"2019-10-26T10:32:52.317Z" # 收集日志时间"message":"Oct 26 18:32:51 # 输出信息:系统日志打印的真实时间 安装并配置 Logstash 服务下载与安装 Logstash 可以从elastic官网 https://www.elastic.co/downloads/logstash 获取logstash安装包，这里下载的版本是logstash-6.3.2.tar.gz。 将下载下来的安装包直接解压到一个路径下即可完成logstash的安装。根据前面的规划， 将logstash安装到logstash server主机上，这里统一将logstash安装到/usr/local目录下，基本操作过程如下： 1234567[root@filebeat1 ~]# mkdir -p /app/elk[root@filebeat1 ~]# cd /app/elk[root@logstash elk]# tar -zxvf logstash-6.3.2.tar.gz -C /usr/local[root@logstash elk]# mv /usr/local/logstash-6.3.2 /usr/local/logstash[root@logstash elk]# cd /usr/local/logstash/ Logstash 是怎么工作的 Logstash是一个开源的、服务端的数据处理pipeline（管道），它可以接收多个源的数据、然后对它们进行转换、最终将它们发送到指定类型的目的地。 Logstash是通过 插件机制 实现各种功能的，可以在https://github.com/logstash-plugins 下载各种功能的插件，也可以自行编写插件。 Logstash实现的功能主要分为接收数据、解析过滤并转换数据、输出数据三个部分，对应的插件依次是input插件、filter插件、output插件， 其中，filter插件是可选的，其它两个是必须插件。也就是说在一个完整的Logstash配置文件中，必须有input插件和output插件。 12# 官方文档学习https://www.elastic.co/guide/en/logstash/6.3/input-plugins.html 常用的 input 插件 input插件主要用于接收数据，Logstash支持接收多种数据源，常用的有如下几种： file: 读取一个文件，这个读取功能有点类似于linux下面的tail命令，一行一行的实时读取。 syslog: 监听系统514端口的syslog messages，并使用RFC3164格式进行解析。 redis: Logstash 可以从redis服务器读取数据，此时redis类似于一个消息缓存组件。 kafka： Logstash 也可以从kafka集群中读取数据，kafka加Logstash的架构一般用在数据量较大的业务场景，kafka可用作数据的缓冲和存储。 filebeat： filebeat是一个文本日志收集器，性能稳定，并且占用系统资源很少，Logstash可以接收filebeat发送过来的数据。 常用的 filter filter插件主要用于数据的过滤、解析和格式化，也就是将非结构化的数据解析成结构化的、可查询的标准化数据。常见的filter插件有如下几个： grok: 正则捕获 grok是Logstash最重要的插件，可解析并结构化任意数据，支持正则表达式，并提供了很多内置的规则和模板可供使用。 使用最多，但也最复杂。 mutate: 数据修改 提供了丰富的基础类型数据处理能力。包括类型转换，字符串处理和字段处理等。 date：时间处理 可以用来转换你的日志记录中的时间字符串。 GeoIP：地址查询 可以根据IP地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。 常用的 output output插件用于数据的输出，一个Logstash事件可以穿过多个output，直到所有的output处理完毕，这个事件才算结束。输出插件常见的有如下几种： elasticsearch: 发送数据到elasticsearch。 file： 发送数据到文件中。 redis： 发送数据到redis中，从这里可以看出，redis插件既可以用在input插件中，也可以用在output插件中。 kafka： 发送数据到kafka中，与redis插件类似，此插件也可以用在Logstash的输入和输出插件中。 Logstash 配置文件入门 /usr/local/logstash/config/, jvm.options是设置JVM内存资源的配置文件，logstash.yml是logstash全局属性配置文件， 另外还需要自己创建一个logstash事件配置文件，这里介绍下logstash事件配置文件的编写方法和使用方式。 在介绍Logstash配置之前，先来认识一下logstash是如何实现输入和输出的。 Logstash提供了一个shell脚本/usr/local/logstash/bin/logstash, 可以方便快速的启动一个logstash进程，在Linux命令行下，运行如下命令启动Logstash进程： 12[root@logstash elk]# cd /usr/local/logstash/[root@logstash logstash]# bin/logstash -e 'input&#123;stdin&#123;&#125;&#125; output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;' 123451. -e代表执行的意思。2. input即输入的意思，input里面即是输入的方式，这里选择了stdin，就是标准输入（从终端输入）。3. output即输出的意思，output里面是输出的方式，这里选择了stdout，就是标准输出（输出到终端）。4. 这里的codec是个插件，表明格式。这里放在stdout中，表示输出的格式，5. rubydebug是专门用来做测试的格式，一般用来在终端输出JSON格式。 123451. 这就是logstash的输出格式。Logstash在输出内容中会给事件添加一些额外信息。2. 比如"@version"、"host"、"@timestamp" 都是新增的字段， 3. 而最重要的是@timestamp ，用来标记事件的发生时间。4. 由于这个字段涉及到Logstash内部流转，如果给一个字符串字段重命名为@timestamp的话，Logstash就会直接报错。5. 另外，也不能删除这个字段。 编写事件文件 在logstash的输出中，常见的字段还有type，表示事件的唯一类型, tags，表示事件的某方面属性，我们可以随意给事件添加字段或者从事件里删除字段。 使用-e参数在命令行中指定配置是很常用的方式，但是如果logstash需要配置更多规则的话，就必须把配置固化到文件里，这就是logstash事件配置文件 如果把上面在命令行执行的logstash命令，写到一个配置文件logstash-1.conf中，就变成如下内容： 12345678910111213141516[root@logstash logstash]# vim logstash-1.conf input &#123; stdin&#123;&#125;&#125;output &#123; stdout&#123;codec =&gt; rubydebug&#125;&#125;[root@logstash logstash]# bin/logstash -f logstash-1.conf# 通过这种方式也可以启动logstash进程，不过这种方式启动的进程是在前台运行的，要放到后台运行，# 可通过nohup命令实现，操作如下：[root@logstash logstash]# nohup bin/logstash -f logstash-simple.conf &amp;# 这样，logstash进程就放到了后台运行了，在当前目录会生成一个nohup.out文件，# 可通过此文件查看 logstash 进程的启动状态。 input 输入插件 file logstash启动后会去监控messages文件 123456789101112131415161718[root@logstash logstash]# vim logstash-1.conf input &#123; file&#123; path =&gt; "/var/log/messages" &#125;&#125;output &#123; stdout&#123;codec =&gt; rubydebug&#125;&#125;[root@logstash logstash]# bin/logstash -f logstash-1.conf# 我们发现日志产生的时间是世界时间,不是东八区,后续我们会用方法修改# file用来获取文件懂得输入,多了path文件路径字段,实时监控信息# 对于output插件，这里仍然采用rubydebug的JSON输出格式，这对于调试logstash输出信息是否正常非常有用。# 如果需要监控多个文件，可以通过逗号分隔即可，例如：# path =&gt; ["/var/log/*.log","/var/log/message","/var/log/secure"] output 输出插件 输出到kafka12345678910111213[root@logstash logstash]# vim logstash_in_kafka.confinput&#123; file &#123; path =&gt; ["/var/log/messages","/var/log/secure"] &#125;&#125;output &#123; kafka &#123; bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092" topic_id =&gt; "osmessages" &#125;&#125; 123451. 这个配置文件中，输入input仍然是file，重点看输出插件，这里定义了output的输出源为kafka，2. 通过bootstrap_servers选项指定了kafka集群的IP地址和端口。3. 特别注意这里IP地址的写法，每个IP地址之间通过逗号分隔。4. output输出中的topic_id选项，是指定输出到kafka中的哪个topic下，5. 这里是osmessages，如果无此topic，会自动重建topic。 123456# kafka 消费数据[root@kafkazk1 kafka]# bin/kafka-console-consumer.sh --zookeeper 172.17.70.232:2181,172.17.70.233:2181,172.17.70.234:2181 --topic osmessages# 放到后台 持续收集[root@logstash logstash]# nohup bin/logstash -f logstash_in_kafka.conf &amp;[root@logstash logstash]# ps -ef|grep java 收集 filebeat端口发来的数据12345678910111213141516171819202122# 5044是接收端口，filebeat把数据发送到他的5044端口上,logstash收集[root@logstash logstash]# vim logstash_in_filebeat.confinput &#123; beats &#123; port =&gt; 5044 &#125;&#125;output &#123; stdout&#123; codec =&gt; rubydebug &#125;&#125;# Starting server on port: 5044 # 本地会被启动5044端口用来接收数据# 本次不用后台运行,在输出到终端[root@logstash logstash]# bin/logstash -f logstash_in_filebeat.conf[root@logstash logstash]# netstat -tnlp | grep 5044tcp 0 0 0.0.0.0:5044 0.0.0.0:* LISTEN 1267/java 123456789# 配置filebeat# 配置kafka先注释掉,输出改用 logstash# 重启filebeat[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat`[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;[root@filebeat1 filebeat]# tail -200 nohup.out # ssh连接故意输错密码，在查看logstash终端收集的信息 123456789101112131415161718192021# 把输出改成kafka# 必须指定json格式[root@logstash logstash]# vim logstash_in_filebeat.confinput &#123; beats &#123; port =&gt; 5044 &#125;&#125;output &#123; kafka &#123; codec =&gt; json bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092" topic_id =&gt; "osmessages" &#125;&#125;[root@logstash logstash]# bin/logstash -f logstash_in_filebeat.conf 配置logstash作为转发节点 上面对logstash的使用做了一个基础的介绍，现在回到本节介绍的这个案例中，在这个部署架构中， logstash是作为一个二级转发节点使用的，也就是它将kafka作为数据接收源，然后将数据发送到elasticsearch集群中， 按照这个需求，新建logstash事件配置文件 kafka_os_into_es.conf，内容如下： 1234567891011121314151617[root@logstash logstash]# vim kafka_os_into_es.confinput &#123; kafka &#123; bootstrap_servers =&gt; "172.17.70.232:9092,172.17.70.233:9092,172.17.70.234:9092" topics =&gt; ["osmessages"] codec =&gt; json &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["172.17.70.229:9200","172.17.70.230:9200","172.17.70.231:9200"] index =&gt; "osmessageslog-%&#123;+YYYY-MM-dd&#125;" &#125;&#125;[root@logstash logstash]# nohup bin/logstash -f kafka_os_into_es.conf &amp; 123# 注意topicsindex 指定存到ES里数据 索引 的名称 必须指定 页面可以通过索引查询 安装并配置Kibana展示日志数据下载与安装Kibana kibana使用Node.js(JavaScript)语言编写，安装部署十分简单，即下即用，可以从elastic官网https://www.elastic.co/cn/downloads/kibana 下载所需的版本， 这里需要注意的是Kibana与Elasticsearch的版本必须一致，另外，在安装Kibana时，要确保Elasticsearch、Logstash和kafka已经安装完毕。 这里安装的版本是kibana-6.3.2-linux-x86_64.tar.gz。将下载下来的安装包直接解压到一个路径下即可完成kibana的安装，根据前面的规划， 将kibana安装到server2主机上，然后统一将kibana安装到/usr/local目录下，基本操作过程如下： 123[root@server2 elk]# tar -zxvf kibana-6.3.2-linux-x86_64.tar.gz -C /usr/local[root@server2 elk]# mv /usr/local/kibana-6.3.2-linux-x86_64 /usr/local/kibana 配置 Kibana 由于将Kibana安装到了/usr/local目录下，因此，Kibana的配置文件为/usr/local/kibana/config/kibana.yml, Kibana 配置非常简单，这里仅列出常用的配置项，内容如下： 123456[root@server2 kibana]# vim /usr/local/kibana/config/kibana.yml[root@server2 kibana]# grep ^'[a-Z]' /usr/local/kibana/config/kibana.yml server.port: 5601server.host: "172.17.70.230"elasticsearch.url: "http://172.17.70.231:9200"kibana.index: ".kibana6" 每个配置项的含义介绍如下 阿里云服务器 用公网地址 开访问端口 5601 server.port： kibana绑定的监听端口，默认是5601。 server.host： kibana绑定的IP地址，如果内网访问，设置为内网地址即可。 elasticsearch.url： kibana访问ElasticSearch的地址，如果是ElasticSearch集群，添加任一集群节点IP即可， 官方推荐是设置为ElasticSearch集群中client node角色的节点IP。 kibana.index： 用于存储kibana数据信息的索引，这个可以在kibanaweb界面中看到。 启动Kibana服务与web配置 启动kibana服务的命令在/usr/local/kibana/bin目录下，执行如下命令启动kibana服务： 123456789[root@server2 kibana]# cd /usr/local/kibana/[root@server2 kibana]# nohup bin/kibana &amp;[root@server2 kibana]# ps -ef|grep noderoot 1625 1237 6 10:44 pts/0 00:00:01 bin/../node/bin/node --no-warnings bin/../src/cliroot 1637 1237 0 10:45 pts/0 00:00:00 grep --color=auto nodehttp://60.205.217.112:5601/app/kibana 创建索引1234之前我们在 logstash定义了 这个索引 osmessageslog-*# 根据时间排序 查看数据1以后不用再去每台服务器查看日志 12自定义 按照字段搜索message:Failed 12345# 收集的字段与我们filebeat 过滤的字段 息息相关 如果加上host 就可以知道来自哪台服务器,添加上就有了processors:- drop_fields: fields: ["beat", "input", "source", "offset","prospector","host"] 修改filebeat的过滤 1234[root@filebeat1 filebeat]# kill -9 `pgrep -f filebeat`[root@filebeat1 filebeat]# nohup ./filebeat -e -c filebeat.yml &amp;# 再去访问 添加上host字段# 在filebeat配置文件里 配置name = 服务器IP 就可现实IP地址，否则是默认主机名 调试并验证日志数据流向 经过上面的配置过程，大数据日志分析平台已经基本构建完成，由于整个配置架构比较复杂，这里来梳理下各个功能模块的数据和业务流向。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05 ELK 常见应用架构]]></title>
    <url>%2F2019%2F10%2F25%2Felk-base05%2F</url>
    <content type="text"><![CDATA[ELK 常见应用架构简单的ELK应用架构 此架构主要是将Logstash部署在各个节点上搜集相关日志、数据，并经过分析、过滤后发送给远端服务器上的Elasticsearch进行存储。 Elasticsearch再将数据以分片的形式压缩存储，并提供多种API供用户查询、操作。 用户可以通过Kibana Web直观的对日志进行查询，并根据需求生成数据报表。 此架构的优点是搭建简单，易于上手。 缺点是Logstash消耗系统资源比较大，运行时占用CPU和内存资源较高。 另外，由于没有消息队列缓存，可能存在数据丢失的风险。此架构建议供初学者或数据量小的环境使用。 典型ELK架构 此架构主要特点是引入了消息队列机制，位于各个节点上的Logstash Agent（一级Logstash，主要用来传输数据）先将数据传递给消息队列（常见的有Kafka、Redis等）， 接着，Logstash server（二级Logstash，主要用来拉取消息队列数据，过滤并分析数据）将格式化的数据传递给Elasticsearch进行存储。 最后，由Kibana将日志和数据呈现给用户。 由于引入了Kafka（或者Redis）缓存机制，即使远端Logstash server因故障停止运行，数据也不会丢失，因为数据已经被存储下来了。 这种架构适合于较大集群、数据量一般的应用环境，但由于二级Logstash要分析处理大量数据，同时Elasticsearch也要存储和索引大量数据，因此它们的负荷会比较重， 解决的方法是将它们配置为集群模式，以分担负载。 此架构的优点在于引入了消息队列机制，均衡了网络传输，从而降低了网络闭塞尤其是丢失数据的可能性，但依然存在Logstash占用系统资源过多的问题，在海量数据应用场景下，可能会出现性能瓶颈。 ELK集群架构 + filebeat + kafka集群 这个架构是在上面第二个架构基础上改进而来的，主要是将前端收集数据的Logstash Agent换成了filebeat，消息队列使用了kafka集群， 然后将Logstash和Elasticsearch都通过集群模式进行构建，此架构适合大型集群、海量数据的业务场景， 它通过将前端Logstash Agent替换成filebeat，有效降低了收集日志对业务系统资源的消耗。 同时，消息队列使用kafka集群架构，有效保障了收集数据的安全性和稳定性， 而后端Logstash和Elasticsearch均采用集群模式搭建，从整体上提高了ELK系统的高效性、扩展性和吞吐量。 下面我们就以此架构为主介绍如何安装、配置、构建和使用ELK大数据日志分析系统。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 filebeat 基础入门]]></title>
    <url>%2F2019%2F10%2F25%2Felk-base04%2F</url>
    <content type="text"><![CDATA[Filebeat 简介1elk官网 https://www.elastic.co/ Filebeat是一个开源的文本日志收集器,它是elastic公司Beats数据采集产品的一个子产品，采用go语言开发， 一般安装在业务服务器上作为代理来监测日志目录或特定的日志文件，并把它们发送到logstash、elasticsearch、redis或Kafka等。 可以在官方地址 https://www.elastic.co/downloads/beats 下载各个版本的Filebeat。 Filebeat 架构与运行原理 Filebeat是一个轻量级的日志监测、传输工具， 它最大的特点是性能稳定、配置简单、占用系统资源很少。 这也是强烈推荐Filebeat的原因。 下图是官方给出的Filebeat架构图： 运行原理 从图中可以看出，Filebeat主要由两个组件构成： prospector（探测器）和harvester（收集器）。这两类组件一起协作完成Filebeat的工作。 Harvester负责进行单个文件的内容收集，在运行过程中，每一个Harvester会对一个文件逐行进行内容读取，并且把读写到的内容发送到配置的output中。 当Harvester开始进行文件的读取后，将会负责这个文件的打开和关闭操作，因此，在Harvester运行过程中，文件都处于打开状态。 如果在收集过程中，删除了这个文件或者是对文件进行了重命名，Filebeat依然会继续对这个文件进行读取，这时候将会一直占用着文件所对应的磁盘空间，直到Harvester关闭。 Prospector负责管理Harvster，它会找到所有需要进行读取的数据源。然后交给Harvster进行内容收集， 如果input type配置的是log类型，Prospector将会去配置路径下查找所有能匹配上的文件，然后为每一个文件创建一个Harvster。 综上所述,filebeat的工作流程为 12345671. 当开启filebeat程序的时候,它会启动一个或多个探测器（prospector）去检测指定的日志目录或文件,在配置文件定义好2. 对于探测器找出的每一个日志文件，filebeat会启动收集进程（harvester）,3. 每一个收集进程读取一个日志文件的内容，然后将这些日志数据发送到后台处理程序（spooler）,4. 后台处理程序会集合这些事件，最后发送集合的数据到output指定的目的地。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 kafka 基础入门]]></title>
    <url>%2F2019%2F10%2F24%2Felk-base03%2F</url>
    <content type="text"><![CDATA[kafka 基本概念 Kafka是一种高吞吐量的分布式发布/订阅消息系统，这是官方对kafka的定义。 kafka是Apache组织下的一个开源系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景。 比如基于hadoop平台的数据分析、低时延的实时系统、storm/spark流式处理引擎等。 kafka现在它已被多家大型公司作为多种类型的数据管道和消息系统使用。 kafka 角色术语 在介绍架构之前，先了解下kafka中一些核心概念和各种角色。 123456781. Broker：Kafka集群包含一个或多个服务器，每个服务器被称为broker。2. Topic：每条发布到Kafka集群的消息都有一个分类，这个类别被称为Topic（主题）。 ***** 3. Producer：指消息的生产者，负责发布消息到Kafka broker。4. Consumer :指消息的消费者，从Kafka broker拉取数据，并消费这些已发布的消息。5. Partition：Parition是物理上的概念，每个Topic包含一个或多个Partition，每个partition都是一个有序的队列。 partition 中的每条消息都会被分配一个有序的id （称为offset）(号码标识)。 ***** 6. Consumer Group:消费者组，可以给每个Consumer指定消费者组，若不指定消费者组，则属于默认的group。7. Message：消息，通信的基本单位，每个producer可以向一个topic发布一些消息。 Kafka 拓扑架构 一个典型的Kafka集群包含若干Producer，若干broker、若干Consumer Group，以及一个Zookeeper集群。 Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。 Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。 Topic与 Partition Kafka中的topic是以partition的形式存放的，每一个topic都可以设置它的partition数量。 Partition的数量决定了组成topic的log的数量。 推荐partition的数量一定要大于同时运行的consumer的数量。 另外，建议partition的数量大于集群broker的数量，这样消息数据就可以均匀的分布在各个broker中。 1234561. 那么，Topic为什么要设置多个Partition呢，这是因为kafka是基于文件存储的，2. 通过配置多个partition可以将消息内容分散存储到多个broker上,这样可以避免文件尺寸达到单机磁盘的上限。3. 同时，将一个topic切分成任意多个partitions，可以保证消息存储、消息消费的效率，4. 因为越多的partitions可以容纳更多的consumer，可有效提升Kafka的吞吐率。5. 因此，将Topic切分成多个partitions的好处是可以将大量的消息分成多批数据同时写到不同节点上，将写请求分担负载到各个集群节点。 Kafka 消息发送的机制 每当用户往某个Topic发送数据时，数据会被hash到不同的partition。(也可以通过指定 写到某一个partition中) 这些partition位于不同的集群节点上，所以每个消息都会被记录一个offset消息号，就是offset号。 消费者通过这个offset号去查询读取这个消息。 发送消息流程 首先获取topic的所有Patition，如果客户端不指定Patition，也没有指定Key的话，使用自增长的数字取余数的方式实现指定的Partition。 这样Kafka将平均的向Partition中生产数据。如果想要控制发送的partition，则有两种方式， 一种是指定partition，另一种就是根据Key自己写算法。实现其partition方法。 每一条消息被发送到broker时，会根据paritition规则选择被存储到哪一个partition。 如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。 同时，每条消息被append到partition中时，是顺序写入磁盘的,因此效率非常高，经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证。 1kafka 实现高效和内存无关,依靠顺序写入磁盘 Kafka 消息消费机制 Kafka中的Producer和consumer采用的是push（推送）、pull（拉取）的模式，即Producer只是向broker push消息，consumer只是从broker pull消息， push和pull对于消息的生产和消费是异步进行的。 pull模式的一个好处是Consumer可自主控制消费消息的速率，同时Consumer还可以自己控制消费消息的方式是批量的从broker拉取数据还是逐条消费数据。 当生产者将数据发布到topic时，消费者通过pull的方式，定期从服务器拉取数据,当然在pull数据的时候，服务器会告诉consumer可消费的消息offset。 消费规则 不同 Consumer Group下的消费者可以消费partition中相同的消息，相同的Consumer Group下的消费者只能消费partition中不同的数据。 topic的partition的个数和同一个消费组的消费者个数最好一致，如果消费者个数多于partition个数，则会存在有的消费者消费不到数据。 服务器会记录每个consumer的在每个topic的每个partition下的消费的offset,然后每次去消费去拉取数据时，都会从上次记录的位置开始拉取数据。 Kafka 消息存储机制 在存储结构上，每个partition在物理上对应一个文件夹，该文件夹下存储这个partition的所有消息和索引文件， 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。 partiton命名规则为topic名称+序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 在每个partition (文件夹)中有多个大小相等的segment(段)数据文件，每个segment的大小是相同的，但是每条消息的大小可能不相同，因此segment 数据文件中消息数量不一定相等。 segment数据文件有两个部分组成，分别为index file和data file，此两个文件是一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件和数据文件。 其实Kafka最核心的思想是使用磁盘,而不是使用内存，使用磁盘操作有以下几个好处： 12341、磁盘缓存由Linux系统维护，减少了程序员的不少工作。2、磁盘顺序读写速度超过内存随机读写。3、JVM的GC效率低，内存占用大。使用磁盘可以避免这一问题。4、系统冷启动后，磁盘缓存依然可用。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 Zookeeper 基础入门]]></title>
    <url>%2F2019%2F10%2F24%2Felk-base02%2F</url>
    <content type="text"><![CDATA[ZooKeeper 概念介绍 在介绍ZooKeeper之前，先来介绍一下分布式协调技术。 所谓分布式协调技术: 主要是用来解决分布式环境当中多个进程之间的同步控制， 让他们有序的去访问某种共享资源，防止造成资源竞争（脑裂）的后果。 保障共享资源不会产生竞争 分布式系统 所谓分布式系统就是在不同地域分布的多个服务器,共同组成的一个应用系统来,为用户提供服务。 在分布式系统中最重要的是进程的调度 假设有一个分布在三个地域的服务器组成的一个应用系统，在第一台机器上挂载了一个资源，然后这三个地域分布的应用进程都要竞争这个资源。 但我们又不希望多个进程同时进行访问,这个时候就需要一个协调器,来让它们有序的来访问这个资源。 这个协调器就是分布式系统中经常提到的那个“锁”。 分布式 “锁” 例如 “进程1” 在使用该资源的时候，会先去获得这把锁。 “进程1” 获得锁以后会对该资源保持独占，此时其它进程就无法访问该资源。 “进程1” 在用完该资源以后会将该锁释放掉，以便让其它进程来获得锁。 由此可见，通过这个“锁”机制，就可以保证分布式系统中多个进程能够有序的访问该共享资源。 这里把这个分布式环境下的这个“锁”叫作分布式锁。这个分布式锁就是分布式协调技术实现的核心内容。 ZooKeeper 就是分布式 “锁” 综上所述，ZooKeeper是一种为分布式应用所设计的高可用、高性能的开源分布式协调服务。 它提供了一项基本服务：分布式锁服务。 同时，也提供了数据的维护和管理机制，如：统一命名服务、状态同步服务、集群管理、分布式消息队列、分布式应用配置项的管理等等。 ZooKeeper 应用举例 这里以ZooKeeper提供的基本服务分布式锁为例进行介绍。 在分布式锁服务中，有一种最典型应用场景，就是通过对集群进行Master角色的选举，来解决分布式系统中的单点故障问题。 所谓单点故障，就是在一个主从的分布式系统中，主节点负责任务调度分发，从节点负责任务的处理，而当主节点发生故障时，整个应用系统也就瘫痪了，那么这种故障就称为单点故障。 解决单点故障，传统的方式是采用一个备用节点，这个备用节点定期向主节点发送ping包，主节点收到ping包以后向备用节点发送回复Ack信息， 当备用节点收到回复的时候就会认为当前主节点运行正常，让它继续提供服务。 而当主节点故障时，备用节点就无法收到回复信息了，此时，备用节点就认为主节点宕机，然后接替它成为新的主节点继续提供服务。 这种传统解决单点故障的方法，虽然在一定程度上解决了问题，但是有一个隐患，就是网络问题。 可能会存在这样一种情况：主节点并没有出现故障，只是在回复ack响应的时候网络发生了故障，这样备用节点就无法收到回复。 那么它就会认为主节点出现了故障，接着，备用节点将接管主节点的服务，并成为新的主节点。 此时，分布式系统中就出现了两个主节点（双Master节点）的情况，双Master节点的出现，会导致分布式系统的服务发生混乱。 这样的话，整个分布式系统将变得不可用。为了防止出现这种情况，就需要引入ZooKeeper来解决这种问题。 ZooKeeper 工作原理 下面通过三种情形，介绍下Zookeeper是如何进行工作的。 Master 启动 在分布式系统中引入Zookeeper以后，就可以配置多个主节点，这里以配置两个主节点为例，假定它们是”主节点A”和”主节点B”。 当两个主节点都启动后，它们都会向ZooKeeper中注册节点信息。 我们假设”主节点A”锁注册的节点信息是”master00001”，”主节点B”注册的节点信息是”master00002”， 注册完以后会进行选举，选举有多种算法，这里以编号最小作为选举算法，那么编号最小的节点将在选举中获胜并获得锁成为主节点，也就是”主节点A”将会获得锁成为主节点， 然后”主节点B”将被阻塞成为一个备用节点。这样，通过这种方式Zookeeper就完成了对两个Master进程的调度。完成了主、备节点的分配和协作。 Master 故障 如果”主节点A”发生了故障，这时候它在ZooKeeper所注册的节点信息会被自动删除。 而ZooKeeper会自动感知节点的变化，发现”主节点A”故障后，会再次发出选举，这时候”主节点B”将在选举中获胜，替代”主节点A”成为新的主节点。 这样就完成了主、备节点的重新选举。 Master 恢复 如果主节点恢复了，它会再次向ZooKeeper注册自身的节点信息。 只不过这时候它注册的节点信息将会变成”master00003”，而不是原来的信息。 ZooKeeper会感知节点的变化再次发动选举，这时候”主节点B”在选举中会再次获胜继续担任”主节点”，”主节点A”会担任备用节点。 Zookeeper就是通过这样的协调、调度机制如此反复的对集群进行管理和状态同步的。 1231. 每个master节点都会在zookeeper 注册信息2. 故障了会删除,恢复后会重新注册3. 保障资源不会被竞争 Zookeeper 集群架构 Zookeeper一般是通过集群架构来提供服务的，下图是Zookeeper的基本架构图。 Zookeeper集群主要角色有Server和client，其中，Server又分为Leader、Follower和Observer三个角色，每个角色的含义如下： 123456Leader：领导者角色，主要负责投票的发起和决议，以及更新系统状态。Follower：跟随者角色，用于接收客户端的请求并返回结果给客户端，在选举过程中参与投票。Observer：观察者角色，用户接收客户端的请求，并将写请求转发给leader，同时同步leader状态，但不参与投票。Observer目的是扩展系统，提高伸缩性。Client:客户端角色，用于向Zookeeper发起请求。 123456781. Zookeeper集群中每个Server在内存中存储了一份数据，在Zookeeper启动时，将从实例中选举一个Server作为leader，Leader负责处理数据更新等操作，当且仅当大多数Server在内存中成功修改数据，才认为数据修改成功。2. Zookeeper写的流程为： - 客户端Client首先和一个Server或者Observe通信，发起写请求， - 然后Server将写请求转发给Leader, - Leader再将写请求转发给其它Server，其它Server在接收到写请求后写入数据并响应Leader， - Leader在接收到大多数写成功回应后，认为数据写成功，最后响应Client，完成一次写操作过程。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 ELK 企业常见应用架构分享以及工作流程]]></title>
    <url>%2F2019%2F10%2F24%2Felk-base01%2F</url>
    <content type="text"><![CDATA[ELK 架构介绍核心组成 ELK是一个应用套件，由Elasticsearch、Logstash和Kibana三部分组件组成，简称ELK； 它是一套开源免费、功能强大的日志分析管理系统。 ELK可以将我们的系统日志、网站日志、应用系统日志等各种日志进行收集、过滤、清洗，然后进行集中存放并可用于实时检索、分析。 这三款软件都是开源软件，通常是配合使用，而且又先后归于Elastic.co公司名下，故又被简称为ELK Stack。 下图是ELK Stack的基础组成。 Elasticsearch 介绍 Elasticsearch是一个实时的分布式搜索和分析引擎，它可以用于全文搜索，结构化搜索以及分析，采用Java语言编写。 它的主要特点如下： 12345678910# 1. 实时搜索，实时分析# 2. 分布式架构、实时文件存储，并将每一个字段都编入索引# 3. 文档导向，所有的对象全部是文档# 4. 高可用性，易扩展，支持集群（Cluster）、分片和复制（Shards和Replicas）# 5. 接口友好，支持JSO# # 发音: 简称吧 ESElasticsearch[ɪˈlæstɪk] [sɜːtʃ]一咧四第可 色吃 Elasticsearch支持集群架构，典型的集群架构如下图所示： 从图中可以看出，Elasticsearch集群中有Master Node和Slave Node两种角色，其实还有一种角色Client Node，这在后面会做深入介绍。 123# Master Node 主节点 用于节点的协调和调度# Slave Node 从节点 存储数据# 客户端 -&gt; Master 调度 -&gt; Slave -&gt; 数据读取或者写入 Logstash 介绍 Logstash是一款轻量级的、开源的日志收集处理框架，它可以方便的把分散的、多样化的日志搜集起来，并进行自定义过滤分析处理，然后传输到指定的位置，比如某个服务器或者文件。Logstash采用JRuby语言编写。 它的主要特点如下： 1234# Logstash的理念很简单，从功能上来讲，它只做三件事情：# 1. input：数据收集# 2. filter：数据加工，如过滤，改写等# 3. output：数据输出,日志可以传到 服务器、ELK、kafka、redis 别看它只做三件事，但通过组合输入和输出，可以变幻出多种架构实现多种需求。Logstash内部运行逻辑如下图所示： 123456Shipper：主要用来收集日志数据，负责监控本地日志文件的变化，及时把日志文件的最新内容收集起来，然后经过加工、过滤，输出到Broker。 Shipper 相当于客户端,用来收集数据。Broker：相当于日志Hub，用来连接多个Shipper和多个Indexer。Indexer：从Broker读取文本，经过加工、过滤，输出到指定的介质（可以是文件、网络、elasticsearch等）中。 123Redis服务器是logstash官方推荐的broker，这个broker起数据缓存的作用。通过这个缓存器可以提高Logstash shipper发送日志到Logstash indexer的速度，同时避免由于突然断电等导致的数据丢失。可以实现broker功能的还有很多软件，例如kafka等。 12345这里需要说明的是，在实际应用中，LogStash自身并没有什么角色，只是根据不同的功能、不同的配置给出不同的称呼而已，无论是Shipper还是Indexer，始终只做前面提到的三件事。这里需要重点掌握的是logstash中Shipper和Indexer的作用，因为这两个部分是logstash功能的核心，在下面的介绍中，会陆续介绍到这两个部分实现的功能细节。 kibana 介绍 Kibana是一个开源的数据分析可视化平台。 使用Kibana可以为Logstash和ElasticSearch提供的日志数据进行高效的搜索、可视化汇总和多维度分析。 还可以与Elasticsearch搜索引擎之中的数据进行交互。它基于浏览器的界面操作可以快速创建动态仪表板，实时监控ElasticSearch的数据状态与更改。 kibana使用的是nodejs开发。 ELK 工作流程 在需要收集日志的所有服务上部署logstash，作为logstash shipper用于监控并收集、过滤日志。 接着，将过滤后的日志发送给Broker。 然后，Logstash Indexer将存放在Broker中的数据再写入Elasticsearch。 Elasticsearch对这些数据创建索引。 最后由Kibana对其进行各种分析并以图表的形式展示。 1234561. 有些时候，如果收集的日志量较大，为了保证日志收集的性能和数据的完整性，logstash shipper和logstash indexer之间的缓冲器（Broker）也经常采用kafka来实现。2. kafka 可以将数据永久保存下来,多了一份安全性。3. 在这个图中，要重点掌握的是ELK架构的数据流向，以及logstash、Elasticsearch和Kibana组合实现的功能细节。]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13 Nginx 常见问题]]></title>
    <url>%2F2019%2F10%2F24%2Fnginx-base13%2F</url>
    <content type="text"><![CDATA[多Server优先级1同一IP 按照server文件名,顺序匹配 多Location优先级1一个server出现多个location 1234567891011121314151617181920[root@Nginx conf.d]# cat testserver.conf server &#123; listen 80; server_name 192.168.69.113; root /soft; index index.html; location = /code1/ &#123; rewrite ^(.*)$ /code1/index.html break; &#125; location ~ /code* &#123; rewrite ^(.*)$ /code3/index.html break; &#125; location ^~ /code &#123; rewrite ^(.*)$ /code2/index.html break; &#125;&#125; 12345678910[root@Nginx conf.d]# curl http://192.168.69.113/code1/&lt;h1&gt;Code 1&lt;/h1&gt;//注释掉精确匹配=, 重启Nginx[root@Nginx ~]# curl http://192.168.69.113/code1/&lt;h1&gt;Code 2&lt;/h1&gt;//注释掉^~, 重启Nginx[root@Nginx ~]# curl http://192.168.69.113/code1/&lt;h1&gt;Code 3&lt;/h1&gt; 12345location 优先级 = 精确匹配 ^~ 以什么开头 ~ 匹配区分大小写 ~* 不区分大小写 try_files的使用1nginx的try_files按顺序检查文件是否存在 1234567location /&#123; try_files $uri $uri/ /index.php;&#125;# 1.检查用户请求的uri内容是否存在本地,存在则解析# 2.将请求加/, 类似于重定向处理 # 3.最后交给index.php处理 123456789# 演示环境准备[root@Nginx ~]# echo "Try-Page" &gt; /soft/code/index.html[root@Nginx ~]# echo "Tomcat-Page" &gt; /soft/app/apache-tomcat-9.0.7/webapps/ROOT/index.html//启动tomcat[root@Nginx ~]# sh /soft/app/apache-tomcat-9.0.7/bin/startup.sh//检查tomcat端口[root@Nginx ~]# netstat -lntp|grep 8080tcp6 0 0 :::8080 :::* LISTEN 104952/java 1234567891011121314151617# 配置Nginx的tryfiles[root@Nginx ~]# cat /etc/nginx/conf.d/try.conf server &#123; listen 80; server_name 192.168.69.113; root /soft/code; index index.html; location / &#123; try_files $uri @java_page; &#125; location @java_page &#123; proxy_pass http://127.0.0.1:8080; &#125;&#125;//重启Nginx[root@Nginx ~]# nginx -s reload 12345678910# 测试tryfiles[root@Nginx ~]# curl http://192.168.69.113/index.htmlTry-Page//将/soft/code/index.html文件移走[root@Nginx ~]# mv /soft/code/&#123;index.html,index.html_bak&#125;//发现由Tomcat吐回了请求[root@Nginx ~]# curl http://192.168.69.113/index.html Tomcat-Page root与alias区别12 1234567891011121314151617181920# root 路径配置[root@Nginx ~]# mkdir /local_path/code/request_path/code/ -p[root@Nginx ~]# echo "Root" &gt; /local_path/code/request_path/code/index.html//Nginx的root配置[root@Nginx ~]# cat /etc/nginx/conf.d/root.conf server &#123; listen 80; index index.html; location /request_path/code/ &#123; root /local_path/code/; &#125;&#125;//请求测试[root@Nginx conf.d]# curl http://192.168.69.113/request_path/code/index.htmlRoot//实际请求本地文件路径为/local_path/code/'request_path/code'/index.html 1234567891011121314151617181920# alias路径配置[root@Nginx ~]# mkdir /local_path/code/request_path/code/ -p[root@Nginx ~]# echo "Alias" &gt; /local_path/code/index.html//配置文件[root@Nginx ~]# cat /etc/nginx/conf.d/alias.conf server &#123; listen 80; index index.html; location /request_path/code/ &#123; alias /local_path/code/; &#125;&#125;//测试访问[root@Nginx ~]# curl http://192.168.69.113/request_path/code/index.htmlAlias//实际访问本地路径/local_path/code/'index.html' 12root 会帮你拼接alias 直接替换路径 获取用户真实IP12$remote_addr # 只能获取到最近一台服务器访问IPx_forwarded_for # 头部信息容易被篡改 http返回状态码12345678910200 正常请求301 永久跳转302 临时跳转400 请求参数错误401 账户密码错误(authorization required)403 权限被拒绝(forbidden)404 文件没找到(Not Found)413 用户上传文件大小限制(Request Entity Too Large)502 后端服务无响应(boy gateway)504 后端服务执行超时(Gateway Time-out) 网站访问原理12345网站相关术语: 如果一栋大厦里所有工作人员通过1个IP公网接口上网, 总共100个设备, 当所有人同时请求一个网站, 并且刷新了5次, 那么请求pv、ip、uv分别是多少 pv:页面浏览量 500uv:唯一设备 100ip:唯一出口 1 123456piwiki 开源统计百度统计站长分析谷歌统计公司开发写的统计脚本awk 统计 取值-排序-去重-统计-html文件 网站访问流程123456789101112131415161718191.DNS流程 1.查询本地Hosts 2.请求本地localDNS 请求根 3.返回对应的IP2.HTTP连接 1.建立TCP三次握手，发送请求内容, 请求头、请求的行、请求的主体 2.将请求传递给负载均衡, 负载均衡做对应的调度 3.如果请求的是静态页面, 那么调度至对应的静态集群组即可 4.如果请求的是动态页面, 将请求调度至动态集群组 1.如果仅仅是请求页面, 可能会经过Opcache缓存返回 2.如果请求页面需要查询数据库, 或者是往数据库插入内容 3.检查对应的操作是查询还是写入, 如果是查询数据库 4.检查查询的内容是否有被缓存, 如有缓存则返回 5.检查查询语句, 将查询结果返回 6.内存缓存Redis缓存对应的查询结果 7.返回对应客户端请求的内容至于WEB节点 8.WEB节点收到请求后返回内容至负载均衡 9.负载均衡返回客户端内容, TCP四次断开3.HTTP断开连接 如何优化 Nginx Nginx架构总结基于Nginx中间件的架构]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12 Nginx 性能优化与压测工具]]></title>
    <url>%2F2019%2F10%2F24%2Fnginx-base12%2F</url>
    <content type="text"><![CDATA[性能 优化概述 在做性能优化前, 我们需要对如下进行考虑 12345678910111.当前系统结构瓶颈 观察指标 压力测试 其实压力测试精准度 并不是很高2.了解业务模式 接口业务类型 系统层次化结构 SOA 松耦合3.性能与安全 性能好安全弱 安全好性能低 压力测试工具 ab安装压力测试工具ab1[root@proxy code]# yum install httpd-tools -y 123456# 了解压测工具使用方式[root@proxy code]# ab -n 200 -c 2 http://127.0.0.1/# -n总的请求次数# -c并发请求数# -k是否开启长连接 压测宝123# 调取全国浏览 检测你的网站http://www.yacebao.com/trial.shtml 12345678910111213141516171819server &#123; listen 80; server_name localhost; set $ip 0; if ($http_x_forward_for ~ 211.161.160.201)&#123; set $ip 1; &#125; if ($remote_addr ~ 211.161.160.201)&#123; set $ip 1; &#125; # 如果$ip值为0,则返回403, 否则允许访问 location /hello &#123; if ($ip = "0")&#123; return 403; &#125; default_type application/json; return 200 '&#123;"status":"success"&#125;'; &#125; tomcat 处理静态比nginx处理 弱很多123456789101112131415161718192021# ab只能参考[root@nginx-lua conf.d]# cat jsp.conf server &#123; server_name localhost; listen 80; location / &#123; root /soft/code; try_files $uri @java_page; index index.jsp index.html; &#125; location @java_page&#123; proxy_pass http://192.168.56.20:8080; &#125;&#125;//分别给Nginx准备静态网站[root@nginx-lua ~]# cat /soft/code/bgx.html &lt;h1&gt; Ab Load &lt;/h1&gt;//给Tomcat准备静态网站文件[root@tomcat-node1-20 ROOT]# cat /soft/tomcat-8080/webapps/ROOT/bgx.html &lt;h1&gt; Ab Load &lt;/h1&gt; 1234567891011121314151617181920212223242526272829# 使用ab工具进行压力测试//进行压力测试[root@Nginx conf.d]# ab -n2000 -c2 http://127.0.0.1/bgx.html...Server Software: nginx/1.12.2Server Hostname: 127.0.0.1Server Port: 80Document Path: /bgx.htmlDocument Length: 19 bytesConcurrency Level: 200# 总花费总时长Time taken for tests: 1.013 seconds# 总请求数Complete requests: 2000# 请求失败数Failed requests: 0Write errors: 0Total transferred: 510000 bytesHTML transferred: 38000 bytes# 每秒多少请求/s(总请求出/总共完成的时间)Requests per second: 9333.23 [#/sec] (mean)# 客户端访问服务端, 单个请求所需花费的时间Time per request: 101.315 [ms] (mean)# 服务端处理请求的时间Time per request: 0.507 [ms] (mean, across all concurrent requests)# 判断网络传输速率, 观察网络是否存在瓶颈Transfer rate: 491.58 [Kbytes/sec] received 12345678910111213# 将nginx下的bgx文件移走, 再次压测会由tomcat进行处理Concurrency Level: 200Time taken for tests: 1.028 secondsComplete requests: 2000Failed requests: 0Write errors: 0Total transferred: 510000 bytesHTML transferred: 38000 bytesRequests per second: 1945.09 [#/sec] (mean)Time per request: 102.823 [ms] (mean)Time per request: 0.514 [ms] (mean, across all concurrent requests)Transfer rate: 484.37 [Kbytes/sec] received 影响性能的指标123456789101112131415161718192021影响性能方便整体关注1.网络 网络的流量 每秒带宽 : 1M带宽 / 8 : 流量峰值200M 网络是否丢包 这些会影响http的请求与调用2.系统 硬件有没有磁盘损坏,磁盘速率 系统负载、内存、系统稳定性 -&gt; zabbix 3.服务 连接优化、请求优化 -&gt; Nginx 根据业务形态做对应的服务设置 4.程序 接口性能 处理速度 程序执行效率5.数据库 慢查询 读写性能 数据缓存每个架构服务与服务之间都或多或少有一些关联, 我们需要将整个架构进行分层, 找到对应系统或服务的短板, 然后进行优化 系统性能优化12345678910文件句柄, Linux一切皆文件，文件句柄可以理解为就是一个索引 文件句柄会随着我们进程的调用频繁增加 系统默认对文件句柄有限制，不能让一个进程无限的调用 需要限制每个进程和每个服务使用多大的文件句柄 文件句柄是必须要调整的优化参数 系统层面必须优化设置方式 系统全局性修改 用户局部性修改 进程局部性修改 12345678910111213141516vim /etc/security/limits.conf# 针对root用户root soft nofile 65535root hard nofile 65535# 所有用户, 全局* soft nofile 25535* hard nofile 25535# 对于Nginx进程worker_rlimit_nofile 45535;# root用户 # soft提醒# hard限制 # nofile文件数配置项# 65535最大大小 1234567891011121314# 虚拟机查看[root@linux-node1 ~]# ulimit -n1024[root@linux-node1 ~]# vim /etc/security/limits.confroot soft nofile 65535root hard nofile 65535* soft nofile 25535* hard nofile 25535[root@linux-node1 ~]# source /etc/security/limits.conf# 重新连接[root@linux-node1 ~]# ulimit -n65535 1234567891011121314151617181920212223242526272829303132333435# 阿里云服务器[root@proxy code]# cat /etc/security/limits.conf# End of fileroot soft nofile 65535root hard nofile 65535* soft nofile 65535* hard nofile 65535# 其中, nofile是文件句柄数量，线程句柄也属于它。nproc是进程数量。[root@proxy code]# ps -aux |grep nginxroot 6389 0.0 0.1 64524 3444 ? Ss 10:39 0:00 nginx: master process /etc/nginx/sbin/nginxnobody 19303 0.0 0.1 65168 3416 ? S 15:14 0:00 nginx: worker processroot 19392 0.0 0.0 112660 968 pts/0 R+ 16:28 0:00 grep --color=auto nginx[root@proxy code]# cat /proc/6389/limits Limit Soft Limit Hard Limit Units Max cpu time unlimited unlimited seconds Max file size unlimited unlimited bytes Max data size unlimited unlimited bytes Max stack size 8388608 unlimited bytes Max core file size 0 unlimited bytes Max resident set unlimited unlimited bytes Max processes 7283 7283 processes Max open files 65535 65535 files Max locked memory 65536 65536 bytes Max address space unlimited unlimited bytes Max file locks unlimited unlimited locks Max pending signals 7283 7283 signals Max msgqueue size 819200 819200 bytes Max nice priority 0 0 Max realtime priority 0 0 Max realtime timeout unlimited unlimited us Nginx性能优化cpu 亲和1CPU亲和, 减少进程之间不断频繁迁移, 减少性能损耗 123456781.查看当前CPU物理状态[root@nginx ~]# lscpu |grep "CPU(s)"CPU(s): 24On-line CPU(s) list: 0-23NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23# 2颗物理cpu,每颗cpu12核心, 总共24核心 12345672. 将Nginx worker进程绑到不同的核心上//启动多少worker进程, 官方建议和cpu核心一致, 第一种绑定组合方式#worker_processes 24;# 最佳方式绑定方式 1.9版本之后推出worker_processes auto;worker_cpu_affinity auto; 123456[root@linux-node1 ~]# ps -ef|grep nginx[root@linux-node1 ~]# ps -eo pid,args,psr|grep [n]ginx 1436 nginx: master process /usr/ 0 1458 nginx: worker process 0 1459 nginx: worker process 1 通用Nginx配置模板文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Nginx通用优化配置文件[root@nginx ~]# cat nginx.confuser nginx;worker_processes auto;worker_cpu_affinity auto;error_log /var/log/nginx/error.log warn;pid /run/nginx.pid;#调整至1w以上,负荷较高建议2-3w以上worker_rlimit_nofile 35535;events &#123; use epoll;# 限制每个进程能处理多少个连接请求,10240x16(worker_processes) # 每个work进程接收连接请求10240 worker_connections 10240;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream;# 统一使用utf-8字符集 charset utf-8; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; # http层配置 每个server都会打印 , 每个server可以加上对应的打印 # 写日志会占用读写I/O access_log /var/log/nginx/access.log main;# Core module 文件的高效传输 sendfile on;# 静态资源服务器建议打开 tcp_nopush on;# 动态资源服务建议打开,需要打开keepalived tcp_nodelay on; keepalive_timeout 65;# Gzip module gzip on; gzip_disable "MSIE [1-6]\."; # IE6浏览器版本不支压缩 关闭 gzip_http_version 1.1;# Virtal Server include /etc/nginx/conf.d/*.conf;&#125; 12345proxy-cache浏览器缓存server-hasp 有的优化 不能在全局,需要单独server进行 参考优化大全]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11 Nginx + Lua]]></title>
    <url>%2F2019%2F10%2F24%2Fnginx-base11%2F</url>
    <content type="text"><![CDATA[Lua 脚本基础语法1Lua 是一种简洁、轻量、可扩展的脚本语言 12345# Nginx+Lua优势充分的结合Nginx的并发处理epool优势和Lua的轻量实现简单的功能且高并发的场景统计IP统计用户信息安全WAF 安装 lua12# CentOS 7.4 默认安装好[root@proxy conf.d]# yum install lua lua 的运行方式1234561. 交互式[root@proxy conf.d]# luaLua 5.1.4 Copyright (C) 1994-2008 Lua.org, PUC-Rio&gt; print('hello world')hello world 123456782. 非交互式,文件执行[root@proxy conf.d]# vim test.lua#!/usr/bin/luaprint("hello world")[root@proxy conf.d]# lua test.lua hello world lua 的注释1234567891011121314151617# -- 行注释#!/usr/bin/lua-- print("hello world")print("hello leo")# 块注释 --[[ ... --]]#!/usr/bin/lua--[[print("hello world")print("hello leo")--]]print("hello lex") 变量1234567891011121314151617181920[root@proxy conf.d]# vim test.lua #!/usr/bin/lua--[[print("hello world")print("hello leo")--]]print("hello lex")name = 'leo'age = 28print("My name is:",name)print(age)# 布尔类型只有nil和false# 数字0,空字符串 都是true# lua中的变量如果没有特殊说明, 全是全局变量 while 循环语句12345# 语法while ... do...end# Lua没有++或是+=这样的操作 123456789101112131415[root@proxy conf.d]# vim while100.lua#!/usr/bin/luasum = 0num = 1while num &lt;= 100 do sum = sum + num num = num + 1endprint(sum)# 运行[root@proxy conf.d]# lua while100.lua 5050 for 循环1234# 语法for ... do...end 1234567891011121314[root@proxy conf.d]# vim for100.lua#!/usr/bin/luasum = 0for i = 1,100 do sum = sum + iendprint(sum)[root@proxy conf.d]# lua for100.lua 5050 if 判断语句12345if...thenelseif...thenelse...end 123# ~= 不等于# 字符串的拼接操作符 ".."# io库的分别从stdin和stdout读写，read和write函数 1234567891011121314151617[root@proxy conf.d]# vim ifelse.lua#!/usr/bin/luanum = 30s='aspython'if num &gt; 50 then print('&gt; 40')elseif s ~= 'aspython' then -- ~= 表示 != io.write('s is not aspython') -- 风骚的标准输出else thisIsGlobal = 5 -- 驼峰式命名 local line = io.read() -- .. 作为字符串连接符 print('凛冬将至' .. line)end Nginx 加载 Lua环境1231. 阿里运测试环境 更换系统盘2. 默认情况下 Nginx 不支持 Lua 模块, 需要安装 LuaJIT 解释器, 并且需要重新编译 Nginx , 建议使用 openrestry3. LuaJIT Ngx_devel_kit 和 lua-nginx-module Nginx 编译安装环境准备1[root@proxy etc]# yum -y install gcc gcc-c++ make pcre-devel zlib-devel openssl-devel 12345# 下载最新的 luajit 和 ngx_devel_kit 以及 lua-nginx-module[root@proxy ~]# mkdir -p /soft/src &amp;&amp; cd /soft/src[root@proxy ~]# wget http://luajit.org/download/LuaJIT-2.0.4.tar.gz[root@proxy ~]# wget https://github.com/simpl/ngx_devel_kit/archive/v0.2.19.tar.gz[root@proxy ~]# wget https://github.com/openresty/lua-nginx-module/archive/v0.10.13.tar.gz 解压 ngx_devel_kit 和 lua-nginx-module1234# 解压后为ngx_devel_kit-0.2.19[root@nginx ~]# tar xf v0.2.19.tar.gz # 解压后为lua-nginx-module-0.9.16[root@nginx ~]# tar xf v0.10.13.tar.gz 安装LuaJIT Luajit是Lua即时编译器。123456[root@nginx ~]# tar zxvf LuaJIT-2.0.4.tar.gz [root@nginx ~]# cd LuaJIT-2.0.4[root@nginx ~]# make &amp;&amp; make install# 成功==== Successfully installed LuaJIT 2.0.4 to /usr/local ==== 安装 Nginx 并加载模块1234[root@nginx ~]# cd /soft/src[root@nginx ~]# wget http://nginx.org/download/nginx-1.16.1.tar.gz[root@nginx ~]# tar xf nginx-1.16.1.tar.gz[root@nginx ~]# cd nginx-1.16.1 1234567# 编译安装./configure --prefix=/etc/nginx --with-http_ssl_module \--with-http_stub_status_module --with-http_dav_module \--add-module=../ngx_devel_kit-0.2.19/ \--add-module=../lua-nginx-module-0.10.13 [root@proxy nginx-1.16.1]# make &amp;&amp; make install 12# 建立软链接, 不建立会出现share object错误[root@proxy lib64]# ln -s /usr/local/lib/libluajit-5.1.so.2 /lib64/libluajit-5.1.so.2 验证 nginx+lua123456789101112131415161718192021[root@proxy conf]# vim /etc/nginx/conf/nginx.conf ... location / &#123; root html; index index.html index.htm; &#125; location /test &#123; default_type text/html; content_by_lua_block &#123; ngx.say('hello world') &#125; &#125;[root@proxy conf]# /etc/nginx/sbin/nginx[root@proxy conf]# /etc/nginx/sbin/nginx -t[root@proxy conf]# /etc/nginx/sbin/nginx -s reload# 访问 http://60.205.217.112/test 直接部署春哥的开源项目OpenResty1234567891011121314151617181920212223//安装依赖包 编译完成后就可以使用 # yum install -y readline-devel pcre-devel openssl-devel# cd /soft/src下载并编译安装openresty# wget https://openresty.org/download/ngx_openresty-1.9.3.2.tar.gz# tar zxf ngx_openresty-1.9.3.2.tar.gz# cd ngx_openresty-1.9.3.2# ./configure --prefix=/soft/openresty-1.9.3.2 \--with-luajit --with-http_stub_status_module \--with-pcre --with-pcre-jit# gmake &amp;&amp; gmake install# ln -s /soft/openresty-1.9.3.2/ /soft/openresty//测试openresty安装# vim /soft/openresty/nginx/conf/nginx.confserver &#123; location /hello &#123; default_type text/html; content_by_lua_block &#123; ngx.say("HelloWorld") &#125; &#125;&#125; Nginx 调用 Lua指令1Nginx调用Lua模块指令, Nginx的可插拔模块加载执行, 共11个处理阶段 Nginx+Lua实现代码灰度发布1234使用Nginx结合lua实现代码灰度发布,按照一定的关系区别，分不同的代码进行上线，使代码的发布能平滑过渡上线1. 用户的信息cookie等信息区别2. 根据用户的ip地址, 颗粒度更广 1234567执行过程：1.用户请求到达前端代理Nginx, 内嵌的lua模块会解析Nginx配置文件中Lua脚本2.Lua脚本会获取客户端IP地址,查看Memcached缓存中是否存在该键值3.如果存在则执行@java_test,否则执行@java_prod4.如果是@java_test, 那么location会将请求转发至新版代码的集群组5.如果是@java_prod, 那么location会将请求转发至原始版代码集群组6.最后整个过程执行后结束 实践环境准备123CentOS7 Nginx+Lua+Memcached 172.17.70.227CentOS7 Tomcat集群 8080 prod 172.17.70.228CentOS7 Tomcat集群 9090 test 172.17.70.226 安装两台服务器Tomcat,分别启动8080和9090端口12345678910[root@tomcat-node1-20 ~]# yum install java -y[root@tomcat-node1-20 ~]# mkdir /soft/src -p[root@tomcat-node1-20 ~]# cd /soft/src[root@nginx ~]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.7/bin/apache-tomcat-9.0.7.tar.gz[root@tomcat-node1-20 src]# tar xf apache-tomcat-9.0.7.tar.gz -C /soft[root@tomcat-node1-20 soft]# cp -r apache-tomcat-9.0.7/ tomcat-8080[root@tomcat-node1-20 bin]# /soft/tomcat-8080/bin/startup.sh//注意tomcat默认监听在8080端口, 如果需要启动9090端口需要修改server.xml配置文件 1234567891011121314151617181920212223# 先搞定8080 然后传过去[root@tomcat-node1 soft]# scp -r tomcat-8080 root@172.17.70.226:/soft# 修改端口[root@tomcat-node2 soft]# mv tomcat-8080 tomcat-9090[root@tomcat-node2 soft]# vim /soft/tomcat-9090/conf/server.xml &lt;Connector port="9090" protocol="HTTP/1.1" [root@tomcat-node2 soft]# sh tomcat-9090/bin/startup.shUsing CATALINA_BASE: /soft/tomcat-9090Using CATALINA_HOME: /soft/tomcat-9090Using CATALINA_TMPDIR: /soft/tomcat-9090/tempUsing JRE_HOME: /usrUsing CLASSPATH: /soft/tomcat-9090/bin/bootstrap.jar:/soft/tomcat-9090/bin/tomcat-juli.jarTomcat started. [root@tomcat-node2 soft]# netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1108/sshd tcp 0 0 0.0.0.0:9090 0.0.0.0:* LISTEN 1533/java tcp 0 0 127.0.0.1:8005 0.0.0.0:* LISTEN 1533/java tcp 0 0 0.0.0.0:8009 0.0.0.0:* LISTEN 1533/java 123456789101112131415161718[root@tomcat-node1 ROOT]# cd /soft/tomcat-8080/webapps/ROOT/[root@tomcat-node1 ROOT]# rm -rf *[root@tomcat-node1 ROOT]# vim test.jsp &lt;%@ page language="java" import="java.util.*" pageEncoding="utf-8"%&gt;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;JSP 8080-Prod&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;h1&gt; JSP 8080-Prod &lt;/h1&gt; &lt;% Random rand = new Random(); out.println("&lt;h1&gt;Random number:&lt;/h1&gt;"); out.println(rand.nextInt(99)+100); %&gt; &lt;/BODY&gt;&lt;/HTML&gt; 123456789101112131415161718[root@tomcat-node2 ROOT]# cd /soft/tomcat-9090/webapps/ROOT/[root@tomcat-node2 ROOT]# rm -rf *[root@tomcat-node2 ROOT]# vim test.jsp &lt;%@ page language="java" import="java.util.*" pageEncoding="utf-8"%&gt;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;JSP 9090-Test&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;h1&gt; JSP 9090-Test &lt;/h1&gt; &lt;% Random rand = new Random(); out.println("&lt;h1&gt;Random number:&lt;/h1&gt;"); out.println(rand.nextInt(99)+100); %&gt; &lt;/BODY&gt;&lt;/HTML&gt; 配置 Memcached 并让其支持 Lua 调用123456789# 安装memcached服务[root@proxy conf]# yum install memcached -y# 配置memcached支持lua[root@proxy conf]# cd /soft/src[root@proxy src]# wget https://github.com/agentzh/lua-resty-memcached/archive/v0.11.tar.gz[root@proxy src]# mkdir -p /etc/nginx/lua[root@proxy src]# cp -r lua-resty-memcached-0.11/lib/resty/memcached.lua /etc/nginx/lua/[root@proxy src]# ls -l /etc/nginx/lua/memcached.lua 1234# 启动memcached[root@proxy src]# systemctl start memcached[root@proxy src]# systemctl enable memcached[root@proxy src]# systemctl status memcached 配置 负载均衡调度123456# 编译安装的 增加引用[root@proxy conf]# vim /etc/nginx/conf/nginx.confinclude /etc/nginx/conf.d/*.conf;# 去掉默认的server cryl+v 选取删除行 shift+i # esc[root@proxy conf]# /etc/nginx/sbin/nginx -t 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@proxy conf]# mkdir -p /etc/nginx/conf.d[root@proxy conf.d]# vim lua.conf # httplua_package_path "/etc/nginx/lua/memcached.lua";upstream java_prod &#123; server 172.17.70.228:8080;&#125;upstream java_test &#123; server 172.17.70.226:9090;&#125;server &#123; listen 80; server_name 60.205.217.112; location /hello &#123; default_type 'text/plain'; content_by_lua 'ngx.say("hello ,lua scripts")'; &#125; location /myip &#123; default_type 'text/plain'; content_by_lua ' clientIP = ngx.req.get_headers()["x_forwarded_for"] ngx.say("Forwarded_IP:",clientIP) if clientIP == nli then clientIP = ngx.var.remote_addr ngx.say("Remote_IP:",clientIP) end '; &#125; location / &#123; default_type 'text/plain'; content_by_lua_file /etc/nginx/lua/dep.lua; &#125; location @java_prod &#123; proxy_pass http://java_prod; include /etc/nginx/conf.d/proxy_params; &#125; location @java_test &#123; proxy_pass http://java_test; include /etc/nginx/conf.d/proxy_params; &#125;&#125; 1234567891011121314151617# nginx反向代理tomcat,必须配置头部信息否则返回400错误[root@proxy conf.d]# vim /etc/nginx/conf.d/proxy_params proxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 30;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k; 12[root@proxy conf]# /etc/nginx/sbin/nginx -t[root@proxy conf]# /etc/nginx/sbin/nginx -s reload 12http://60.205.217.112/testhttp://60.205.217.112/myip 编写 Nginx 调用灰度发布Lua 脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 编写 Nginx 调用灰度发布Lua 脚本# /etc/nginx/lua/dep.lua[root@proxy lua]# vim dep.lua --获取x-real-ipclientIP = ngx.req.get_headers()["X-Real-IP"]--如果IP为空-取x_forwarded_forif clientIP == nil then clientIP = ngx.req.get_headers()["x_forwarded_for"]end--如果IP为空-取remote_addrif clientIP == nil then clientIP = ngx.var.remote_addrend--定义本地,加载memcached local memcached = require "resty.memcached"--实例化对象 local memc, err = memcached:new()--判断连接是否存在错误 if not memc then ngx.say("failed to instantiate memc: ", err) return end--建立memcache连接 local ok, err = memc:connect("127.0.0.1", 11211)--无法连接往前端抛出错误信息 if not ok then ngx.say("failed to connect: ", err) return end--获取对象中的ip-存在值赋给res local res, flags, err = memc:get(clientIP)-- --ngx.say("value key: ",res,clientIP) if err then ngx.say("failed to get clientIP ", err) return end--如果值为1则调用local-@java_test if res == "1" then ngx.exec("@java_test") return end--否则调用local-@java_prod ngx.exec("@java_prod") return 123[root@proxy lua]# /etc/nginx/sbin/nginx -s reloadhttp://60.205.217.112/test.jsp 灰度发布123456789101112# 把IP 传给 memcached# 生产换将 需要得到IP地址库 用memcached管理地址页 导入[root@proxy logs]# telnet 127.0.0.1 11211Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is '^]'.set 125.34.39.133 0 0 11STOREDhttp://60.205.217.112/test.jsp 1234567# 剔除get 125.34.39.133VALUE 125.34.39.133 0 11ENDdelete 125.34.39.133DELETED Nginx+Lua 实现WAF应用防火墙123451.常见的恶意行为 1.爬虫行为和恶意抓取，资源盗取防护手段 1. 基础防盗链功能不让恶意用户能够轻易的爬取网站对外数据 2. access_moudle-&gt;对后台，部分用户服务的数据提供IP防护 1234567892.常见的攻击手段 1. 后台密码撞库，通过猜测密码字典不断对后台系统登陆性尝试，获取后台登陆密码防护手段 1.后台登陆密码复杂度 2.使用access_module-对后台提供IP防控 3.预警机制文件上传漏洞,利用上传接口将恶意代码植入到服务器中，再通过url去访问执行代码执行方式 bgx.com/1.jpg/1.php 1234563.常见的攻击手段利用未过滤/未审核的用户输入进行Sql注入的攻击方法, 让应用运行本不应该运行的SQL代码防护手段 1.php配置开启安全相关限制 2.开发人员对sql提交进行审核,屏蔽常见的注入手段 3.Nginx+Lua构建WAF应用层防火墙, 防止Sql注入 模拟SQL注入攻击 快速安装lnmp架构 1[root@proxy ~]# yum install mariadb mariadb-server php php-fpm php-mysql -y 1234567891011121314151617181920212223242526# 配置MySQL[root@nginx ~]# systemctl start mariadb[root@proxy ~]# mysql -uroot MariaDB [(none)]&gt; create database info;MariaDB [(none)]&gt; use info;MariaDB [info]&gt; create table user(id int(11),username varchar(64), password varchar(64), email varchar(64));MariaDB [info]&gt; desc user;+----------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+-------------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || username | varchar(64) | YES | | NULL | || password | varchar(64) | YES | | NULL | || email | varchar(64) | YES | | NULL | |+----------+-------------+------+-----+---------+-------+//插入数据MariaDB [info]&gt; insert into user (id,username,password,email) values(1,'bgx',('123'),'bgx@foxmail.com');MariaDB [info]&gt; select * from info.user;+------+----------+----------------------------------+-----------------+| id | username | password | email |+------+----------+----------------------------------+-----------------+| 1 | bgx | 123 | bgx@foxmail.com |+------+----------+----------------------------------+-----------------+1 row in set (0.00 sec) 123456789101112131415161718192021222324252627282930# 配置php代码[root@proxy ~]# mkdir -p /soft/code[root@proxy ~]# vim /soft/code/login.html&lt;html&gt;&lt;head&gt; &lt;title&gt; Sql注入演示场景 &lt;/title&gt; &lt;meta http-equiv="content-type"content="text/html;charset=utf-8"&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="sql.php" method="post"&gt;&lt;table&gt; &lt;tr&gt; &lt;td&gt; 用 户: &lt;/td&gt; &lt;td&gt;&lt;input type="text" name="username"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; 密 码: &lt;/td&gt; &lt;td&gt;&lt;input type="text" name="password"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="submit" value="提交"&gt;&lt;/td&gt; &lt;td&gt;&lt;input type="reset" value="重置"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617181920212223# 被html调用的sql.php文件[root@proxy ~]# vi /soft/code/sql.php&lt;?php $conn = mysql_connect("localhost",'root','') or die("数据库连接失败！"); mysql_select_db("info",$conn) or die ("您选择的数据库不存在"); $name=$_POST['username']; $pwd=$_POST['password']; $sql="select * from user where username='$name' and password='$pwd'"; echo $sql."&lt;br /&gt;"; $query=mysql_query($sql); $arr=mysql_fetch_array($query); if($arr)&#123; echo "login success!&lt;br /&gt;"; echo $arr[1]; echo $arr[3]."&lt;br /&gt;&lt;br /&gt;"; &#125;else&#123; echo "login failed!"; &#125;?&gt;[root@proxy conf.d]# systemctl start php-fpm 12345678910111213141516# 配置Nginx + php[root@proxy conf.d]# vim phpserver.conf server &#123; server_name 60.205.217.112; root /soft/code; index index.html index.php; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /soft/code/$fastcgi_script_name; include fastcgi_params; &#125;&#125; 12345http://60.205.217.112/sql.php bgx / 123# sql注入# 用户名 ' or 1=1#'# pymysql时候遇到过,问题是由于自己拼接sql语句导致,可以用提供的拼接sql方法避免 使用lua解决此类安全问题 部署Waf相关防护代码1234567# https://github.com/loveshell/ngx_lua_waf[root@proxy src]# yum install -y git[root@proxy src]# cd /soft/src/# 把ngx_lua_waf复制到nginx的目录下,解压命名为waf[root@proxy src]# cp -r ngx_lua_waf /etc/nginx/waf 123456789101112131415161718192021//在nginx.conf的http段添加[root@proxy conf.d]# vim phpserver.conf lua_package_path "/etc/nginx/waf/?.lua";lua_shared_dict limit 10m;init_by_lua_file /etc/nginx/waf/init.lua;access_by_lua_file /etc/nginx/waf/waf.lua;server &#123; server_name 60.205.217.112; root /soft/code; index index.html index.php; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /soft/code/$fastcgi_script_name; include fastcgi_params; &#125;&#125; 12[root@proxy conf.d]# /etc/nginx/sbin/nginx -t[root@proxy conf.d]# /etc/nginx/sbin/nginx -s reload 12345678# 配置config.lua里的waf规则目录(一般在waf/conf/目录下)[root@proxy waf]# vim /etc/nginx/waf/config.lua RulePath = "/etc/nginx/waf/wafconf/"attacklog = "on"logdir = "/etc/nginx/logs/hack/"# 绝对路径如有变动，需对应修改, 然后重启nginx即可 12[root@proxy conf.d]# /etc/nginx/sbin/nginx -t[root@proxy conf.d]# /etc/nginx/sbin/nginx -s reload Nginx + lua防止Sql注入12345# 添加规则vim /etc/nginx/waf/wafconf/post\sor\s+\s = 空格 or 空格 的 防止CC攻击12345678[root@nginx ~]# vim /etc/nginx/waf/config.luaCCDeny="on"CCrate="100/60" --设置cc攻击频率，单位为秒. --默认1分钟同一个IP只能请求同一个地址100次 [root@proxy conf.d]# /etc/nginx/sbin/nginx -t[root@proxy conf.d]# /etc/nginx/sbin/nginx -s reload 123456# 开个本地虚拟机 测试一下[root@linux-node1 ~]# yum install httpd-tools[root@linux-node1 ~]# ab -n 2000 -c 200 http://60.205.217.112/login.html# 本地再访问 超过限制就被拦截 其他IP没有影响# 手机还可以访问 说明其他IP 没有影响 nginx+lua 学习1https://github.com/loveshell/ngx_lua_waf]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10 Nginx 构建动态网站架构 lnmt]]></title>
    <url>%2F2019%2F10%2F24%2Fnginx-base10%2F</url>
    <content type="text"><![CDATA[安装 LNMT 架构下载 tomcat1234567# CentOS7 修改主机名[root@nginx-node2 ~]# hostnamectl set-hostname tomcat-node1[root@nginx-node2 ~]# bash# 下载tomcat# wget https://www-us.apache.org/dist/tomcat/tomcat-9/v9.0.27/bin/apache-tomcat-9.0.27.tar.gz# wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.7/bin/apache-tomcat-9.0.7.tar.gz 安装 JDK123456[root@tomcat-node1 code]# yum install java -y[root@tomcat-node1 code]# java -versionopenjdk version "1.8.0_232"OpenJDK Runtime Environment (build 1.8.0_232-b09)OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode) 安装 tomcat1234[root@proxy tmp]# mkdir -p /soft/tools[root@tomcat-node1 tools]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.7/bin/apache-tomcat-9.0.7.tar.gz[root@tomcat-node1 tools]# tar -zxvf apache-tomcat-9.0.27.tar.gz -C /soft[root@tomcat-node1 soft]# cp -r apache-tomcat-9.0.27/ tomcat-8080 解决 CentOS7 启动tomcat慢12yum install rng-tools systemctl start rngd 启动 tomcat1234[root@tomcat-node1 bin]# cd /soft/tomcat-8080/bin/[root@tomcat-node1 bin]# sh startup.sh[root@tomcat-node1 bin]# netstat -tnlp 配置 Nginx proxy 负载均衡12345678910111213141516[root@proxy conf.d]# vim jsp.conf upstream jsp_pool &#123; server 172.17.70.228:8080;&#125;server &#123; listen 80; server_name 60.205.217.112; location / &#123; proxy_pass http://jsp_pool; include proxy_params; &#125;&#125; 123456789101112131415[root@proxy conf.d]# cat /etc/nginx/proxy_paramsproxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k; 12[root@proxy conf.d]# nginx -t[root@proxy conf.d]# nginx -s reload JVM故障排查思路]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09 Nginx 构建动态网站架构 lnmp]]></title>
    <url>%2F2019%2F10%2F23%2Fnginx-base09%2F</url>
    <content type="text"><![CDATA[安装 LNMP 架构1yum安装 nginx php7.2 mysql5.7 安装 Nginx12# 基础软件yum install -y net-tools wget vim lrzsz tree screen lsof tupdump nc mtr nmap gcc glibc gcc-c++ make 123# 1. 使用Nginx官方提供的rpm包http://nginx.org/en/download.htmlhttp://nginx.org/en/linux_packages.html#RHEL-CentOS 1234567[root@linux-node1 ~]# vim /etc/yum.repos.d/nginx.repo[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/7/$basearch/gpgcheck=0enabled=1 12345# 安装启动[root@linux-node1 ~]# yum install -y nginx[root@linux-node1 ~]# systemctl start nginx[root@linux-node1 ~]# systemctl enable nginx[root@linux-node1 ~]# systemctl status nginx 安装 php7.212# 移除旧版php [root@linux-node1 ~]# yum remove php-mysql-5.4 php php-fpm php-common 123# 1. 安装扩展源[root@linux-node1 ~]# rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm[root@linux-node1 ~]# rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 123# php72w-fpm php72w-mysqlnd php72w-opcache 重要必须装[root@linux-node1 yum.repos.d]# yum -y install php72w php72w-cli php72w-common php72w-devel \php72w-embedded php72w-gd php72w-mbstring php72w-pdo php72w-xml php72w-fpm php72w-mysqlnd php72w-opcache 1234# 启动php[root@linux-node1 yum.repos.d]# systemctl start php-fpm[root@linux-node1 yum.repos.d]# systemctl enable php-fpm[root@linux-node1 yum.repos.d]# systemctl status php-fpm 安装 MySQL5.7123456789101112131415# 下载官方扩展源,扩展源集成mysql5.6、5.7、8.0,仅5.7仓库是开启# http://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/[root@linux-node1 ~]# rpm -ivh http://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/mysql57-community-release-el7-10.noarch.rpm[root@linux-node1 ~]# yum install mysql-community-server -y[root@linux-node1 ~]# systemctl start mysqld[root@linux-node1 ~]# systemctl enable mysqld# 查看安装的初始密码[root@linux-node1 ~]# grep 'temporary password' /var/log/mysqld.log2019-10-23T08:33:30.968573Z 1 [Note] A temporary password is generated for root@localhost: k&amp;ruTxuGO03## 登录修改密码[root@linux-node1 ~]# mysql -uroot -p'k&amp;ruTxuGO03#'mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass1!'; Nginx vim 语法高亮1234567891011# 创建目录 下载配置文件[root@linux-node1 syntax]# mkdir -p ~/.vim/syntax[root@linux-node1 syntax]# cd ~/.vim/syntax[root@linux-node1 syntax]# wget http://www.vim.org/scripts/download_script.php?src_id=14376 -O nginx.vim# 添加[root@linux-node1 syntax]# vi ~/.vim/filetype.viau BufRead,BufNewFile /etc/nginx* set ft=nginx# 查看vim /etc/nginx/nginx.conf 配置 LNMP 架构配置Nginx 实现动态请求转发至php123456789101112131415[root@linux-node1 conf.d]# vim php.conf server &#123; server_name 10.0.0.156; listen 80; root /soft/code; index index.php index.html; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /soft/code$fastcgi_script_name; include fastcgi_params; &#125;&#125; 添加 php 测试页面1234567[root@linux-node1 code]# vim /soft/code/info.php&lt;?php phpinfo();?&gt;[root@linux-node1 code]# echo "&lt;h1&gt;hello&lt;/h1&gt;" &gt;&gt; index.html 12345# 测试访问[root@linux-node1 conf.d]# nginx -t[root@linux-node1 conf.d]# nginx -s reloadhttp://10.0.0.156/info.php mysqli模块测试连接 mysql123456789101112131415161718[root@linux-node1 code]# vim /soft/code/mysqli.php&lt;?php $servername = "localhost"; $username = "root"; $password = "MyNewPass1!"; // 创建连接 $conn = mysqli_connect($servername, $username, $password); // 检测连接 if (!$conn) &#123; die("Connection failed: " . mysqli_connect_error()); &#125; echo "连接成功";?&gt;http://10.0.0.156/mysqli.php pdo模块测试连接 mysql123456789101112131415161718[root@linux-node1 code]# vim /soft/code/mysqlpdo.php&lt;?php $servername = "localhost"; $username = "root"; $password = "MyNewPass1!"; try &#123; $conn = new PDO("mysql:host=$servername;dbname=test", $username, $password); echo "连接成功"; &#125; catch(PDOException $e) &#123; echo $e-&gt;getMessage(); &#125;?&gt;http://10.0.0.156/mysqlpdo.php Nginx 与 PHP 原理Nginx FastCGI的运行原理 PHP 配置文件优化php-ini 优化12345678910111213141516171819202122232425262728293031321. php-ini php懂得配置文件,调整php解析器# 打开php的安全模式,控制php执行危险函数, 默认是Off,改为Onsql.safe_mode = Off# 关闭php头部信息, 隐藏版本号, 默认是On,该为Offexpose_php = On# 错误信息输出控制display_error = Offerror_reporting = E_WARNING &amp; E_ERROR# 记录错误日志至后台, 方便追溯log_errors = Onerror_log = /var/log/php_error.log# 每个脚本时间最大内存memory_limit = 128M# 上传文件最大许可,默认2M, 建议调整为16,32Mupload_max_filesize = 2M# 禁止远程执行phpshell,默认On, 建议Offallow_url_fopen = On# 时区调整,默认PRC, 建议调整为Asia/Shanghaidate.timezone = PRC# 整体优化后配置文件sql.safe_mode = Offexpose_php = Offdisplay_error = Offerror_reporting = E_WARNING &amp; E_ERRORlog_errors = Onerror_log = /var/log/php_error.logupload_max_filesize = 50Mallow_url_fopen = Offdate.timezone = Asia/Shanghai php-fpm 优化123456789101. php-fpm 监听9000端口[root@linux-node1 conf.d]# ps -ef|grep phproot 940 1 0 16:34 ? 00:00:00 php-fpm: master process (/etc/php-fpm.conf)[root@linux-node1 conf.d]# vim /etc/php-fpm.conf[root@linux-node1 conf.d]# include=/etc/php-fpm.d/*.conf # 引入了文件[root@linux-node1 conf.d]# cd /etc/php-fpm.d/[root@linux-node1 php-fpm.d]# lswww.conf 1234567891. 优化该配置文件 https://www.xuliangwei.com/bgx/1024.html2. 备份[root@linux-node1 php-fpm.d]# cd /etc/php-fpm.d/[root@linux-node1 php-fpm.d]# mv www.conf www.conf.bak3. 建立日志目录[root@linux-node1 php-fpm.d]# mkdir -p /var/log/php/ 12345678910111213141516171819202122232425262728293031323334353637383940414. 更新配置文件# PHP-FPM配置文件 4核16G、8核16G[root@linux-node1 php-fpm.d]# vim www.conf[global]pid = /var/run/php-fpm.pid# php-fpm程序错误日志error_log = /var/log/php/php-fpm.loglog_level = warningrlimit_files = 655350events.mechanism = epoll[www]user = nginxgroup = nginxlisten = 127.0.0.1:9000listen.owner = wwwlisten.group = wwwlisten.mode = 0660listen.allowed_clients = 127.0.0.1# 动态pm = dynamicpm.max_children = 512# 启动 1个php-fpm进程占用至少8M内存空间pm.start_servers = 10pm.min_spare_servers = 10pm.max_spare_servers = 30pm.process_idle_timeout = 15s;pm.max_requests = 2048# php-www模块错误日志php_flag[display_errors] = offphp_admin_value[error_log] = /var/log/php/php-www.logphp_admin_flag[log_errors] = on# php慢查询日志request_slowlog_timeout = 5sslowlog = /var/log/php/php-slow.log 1235. 重启php 进程个数已增加为动态[root@linux-node1 php-fpm.d]# systemctl restart php-fpm[root@linux-node1 php-fpm.d]# systemctl status php-fpm PHP5-FPM 配置纤细解释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[global]#pid设置, 记录程序启动后pidpid = /var/run/php-fpm.pid#php-fpm程序启动错误日志路径error_log = /soft/log/php/php-fpm_error.log# 错误级别. 可用级别为: alert（必须立即处理）,error（错误情况）, warning（警告情况）, notice（一般重要信息）, debug（调试信息）. 默认: notice.log_level = warning#设置文件打开描述符的rlimit限制.rlimit_files = 65535events.mechanism = epoll#启动进程的用户和组[www]user = wwwgroup = www# fpm监听端口listen = 127.0.0.1:9000# unix socket设置选项，如果使用tcp方式访问，这里注释即可。listen.owner = wwwlisten.group = www# 允许访问FastCGI进程的IP，any不限制listen.allowed_clients = 127.0.0.1# pm设置动态调度pm = dynamic# 同一时刻最大的php-fpm子进程数量pm.max_children = 200# 动态方式下的起始php-fpm进程数量pm.start_servers = 20# 动态方式下服务器空闲时最小php-fpm进程数量pm.min_spare_servers = 10# 动态方式下服务器空闲时最大php-fpm进程数量pm.max_spare_servers = 30# 最大请求pm.max_requests = 1024pm.process_idle_timeout = 15s;# FPM状态页面,用于监控php-fpm状态使用pm.status_path = /status# 错误日志php_flag[display_errors] = offphp_admin_value[error_log] = /soft/log/php/php-www_error.logphp_admin_flag[log_errors] = on# 配置php慢查询, 以及慢查询记录日志位置request_slowlog_timeout = 5sslowlog = /soft/log/php/php-slow.log# 启动 1个php-fpm 进程占用至少8M内存空间]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08 Nginx构建Https加密传输网站 (基于IOS苹果要求)]]></title>
    <url>%2F2019%2F10%2F23%2Fnginx-base08%2F</url>
    <content type="text"><![CDATA[HTTPS 基本概述121.传输数据被中间⼈盗⽤,信息泄露2.数据内容劫持,篡改 HTTPS 配置语法 1231. 启动2. 证书签名文件3. 证书秘钥文件 HTTPS 配置场景 检查当前环境1234561. 检查openssl 1.0.2版本 CentOS7 默认达到[root@proxy conf.d]# openssl versionOpenSSL 1.0.2k-fips 26 Jan 20172. nginx必须有ssl模块[root@proxy conf.d]# nginx -V 内部生成 只能用于测试12[root@proxy conf.d]# mkdir /etc/nginx/ssl_key -p[root@proxy conf.d]# cd /etc/nginx/ssl_key 1234567891011121314# 创建私钥[root@proxy ssl_key]# openssl genrsa -idea -out server.key 2048Generating RSA private key, 2048 bit long modulus.............................................................+++.....................................+++e is 65537 (0x10001)Enter pass phrase for server.key:Verifying - Enter pass phrase for server.key:# 密码123456[root@proxy ssl_key]# ls -l-rw-r--r-- 1 root root 1747 Oct 23 09:31 server.key 1234567891011121314151617181920212223242526# 生成使用签名请求证书和私钥生成自签证书[root@proxy ssl_key]# openssl req -days 36500 -x509 -sha256 -nodes -newkey rsa:2048 -keyout server.key -out server.crt# 10年# 加密方式: sha256 rsa:2048# 输入的内容随意填写Generating a 2048 bit RSA private key................+++...+++writing new private key to 'server.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:LeoLocality Name (eg, city) [Default City]:bjOrganization Name (eg, company) [Default Company Ltd]:SAOrganizational Unit Name (eg, section) []:SACommon Name (eg, your name or your server's hostname) []:SAEmail Address []:SA@qq.com 配置 Nginx123456789101112131415161718192021222324252627282930[root@proxy conf.d]# vim ssl.conf server &#123; #listen 80; listen 443 ssl; server_name 60.205.217.112; index index.html index.htm; # ssl on; # ssl_session_cache share:SSL:10m; ssl_session_timeout 10m; ssl_certificate ssl_key/server.crt; ssl_certificate_key ssl_key/server.key; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; root /soft/code; access_log /var/log/nginx/ssl.log main; &#125;&#125;[root@proxy conf.d]# cat /soft/code/index.html&lt;h1&gt;https index hello&lt;/h1&gt;[root@proxy conf.d]# ls -l /etc/nginx/ssl_key/-rw-r--r-- 1 root root 1326 Oct 23 09:35 server.crt-rw-r--r-- 1 root root 1704 Oct 23 09:35 server.key 1# 测试访问 需要 加上https 公有云配置苹果要求 Https加密证书123456# 免费证书 有几个二级域名 就要申请几个证书nginx.bjstack.com httpswww.bjstack.com https# 泛域名 通用型 绑定几个二级域名都行*.bjstack.com https]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07 Nginx Rewrite 跳转规则与实践]]></title>
    <url>%2F2019%2F10%2F22%2Fnginx-base07%2F</url>
    <content type="text"><![CDATA[Rewrite 基本概述 rewrite 主要实现 url 地址重写, 以及重定向. Rewrite使⽤场景: 12341. URL访问跳转: 支持开发设计, 页面跳转, 兼容性支持, 展示效果2. SEO优化: 依赖于url路径,以便⽀持搜索引擎录入3. 维护: 后台维护, 流量转发等4. 安全: 伪静态,真实动态页面进行伪装 Rewrite 配置语法 正则表达式 正则表达式 终端测试工具1234567891011[root@proxy ~]# yum install -y pcre-tools[root@proxy ~]# pcretestPCRE version 8.32 2012-11-30 re&gt; /(\d+)\.(\d+)\.(\d+)\.(\d+)/data&gt; 192.168.1.100 0: 192.168.1.100 1: 192 2: 168 3: 1 4: 100 Rewrite 标记Flag break 与 last1234567891011121314151617181920212223242526[root@proxy conf.d]# vim rewrite.conf server &#123; listen 80; server_name 60.205.217.112; root /soft/code; location ~ ^/break &#123; rewrite ^/break /test/ break; &#125; location ~ ^/last &#123; rewrite ^/last /test/ last; &#125; location /test &#123; default_type application/json; return 200 '&#123;"status":"success"&#125;'; &#125;&#125;[root@proxy conf.d]# nginx -t[root@proxy conf.d]# nginx -s reloadhttp://60.205.217.112/test/ 1测试 break 1测试 last last 与 break 对比总结12345last 会新建一个请求 请求 域名+/testhttp://60.205.217.112/last -&gt; http://60.205.217.112/testbreak匹配后不会进行匹配,会查找对应root站点目录下包含的/test目录,由于我没有test目录 就报错了http://60.205.217.112/break -&gt; http://60.205.217.112/test 得有/test目录 redirect 与 permanent123456789101112[root@proxy conf.d]# vim rewrite.confserver &#123; listen 80; server_name 60.205.217.112; root /soft/code; location ~ ^/bgx &#123; rewrite ^/bgx http://kt.xuliangwei.com redirect; # 302 临时跳转 关闭nginx后 不会跳转 rewrite ^/bgx http://kt.xuliangwei.com permanent; # 301 永久跳转 关闭nginx后 还可以跳转，客户端浏览器需要清理缓存 &#125;&#125; Rewrite 使用场景重写URL地址12345678910111213[root@proxy conf.d]# mkdir -p /soft/code/course/11/22[root@proxy conf.d]# echo "&lt;h1&gt;Nginx&lt;/h1&gt;" &gt;&gt; /soft/code/course/11/22/course_33.html[root@proxy conf.d]# vim rewrite.confserver &#123; listen 80; server_name 60.205.217.112; index index.html; location / &#123; root /soft/code; &#125;&#125; 12# 路径较深http://60.205.217.112/course/11/22/course_33.html 1234567891011121314151617181920# 跳转 正则匹配[root@proxy 22]# vim /etc/nginx/conf.d/rewrite.confserver &#123; listen 80; server_name 60.205.217.112; root /soft/code; index index.html; location / &#123; rewrite ^/course-(\d+)-(\d+)-(\d+)\.html /course/$1/$2/course_$3.html break; &#125;&#125;# 如果有人输入 /course-(数字)-(数字)-(数字)\.html # 会被转化成 /course/$1/$2/course_$3.html 页面http://60.205.217.112/course-11-22-33.html-&gt;http://60.205.217.112/course/11/22/course_33.html 实现跳转123if ($http_user_agent ~* Chrome)&#123; rewrite ^/nginx http://kt.xuliangwei.com/index.html redirect;&#125; Rewrite 额外补充12345# Rewrite 匹配优先级1. 执⾏server块的rewrite指令2. 执⾏location匹配3. 执⾏选定的location中的rewrite 1# Rewrite 优雅书写]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06 Nginx Proxy_cache 缓存服务]]></title>
    <url>%2F2019%2F10%2F22%2Fnginx-base06%2F</url>
    <content type="text"><![CDATA[Nginx 缓存服务 常见缓存类型 缓存配置语法 缓存配置实践12CentOS7.4 Nginx Proxy 172.17.70.227CentOS7.4 Nginx Web 172.17.70.226 web 节点准备12345678910111213141516# 目录建立[root@nginx soft]# mkdir -p /soft/code&#123;1..3&#125;# html⽂件[root@nginx code]# for i in &#123;1..3&#125;;do echo Code1-Url$i &gt; /soft/code1/url$i.html;done[root@nginx code]# for i in &#123;1..3&#125;;do echo Code2-Url$i &gt; /soft/code2/url$i.html;done[root@nginx code]# for i in &#123;1..3&#125;;do echo Code3-Url$i &gt; /soft/code3/url$i.html;done[root@nginx code1]# ls -ltotal 12-rw-r--r-- 1 root root 11 Oct 22 16:47 url1.html-rw-r--r-- 1 root root 11 Oct 22 16:47 url2.html-rw-r--r-- 1 root root 11 Oct 22 16:47 url3.html[root@nginx code1]# cat url1.html Code1-Url1 启动三个端口监听1234567891011121314151617181920212223242526[root@nginx conf.d]# vim web_node.conf server &#123; listen 8081; server_name 172.17.70.226; index index.html; root /soft/code1;&#125;server &#123; listen 8082; server_name 172.17.70.226; index index.html; root /soft/code2;&#125;server &#123; listen 8083; server_name 172.17.70.226; index index.html; root /soft/code3;&#125;[root@nginx conf.d]# nginx -t[root@nginx conf.d]# nginx -s reload 123456[root@nginx code1]# curl 172.17.70.226:8081/url1.htmlCode1-Url1[root@nginx code1]# curl 172.17.70.226:8081/url2.htmlCode1-Url2[root@nginx code1]# curl 172.17.70.226:8081/url3.htmlCode1-Url3 代理配置缓存123456789101112131415161718192021222324252627282930313233343536373839404142# 创建缓存目录[root@proxy ~]# mkdir -p /soft/cache# 配置文件[root@proxy conf.d]# vim proxy_cache.confproxy_cache_path /soft/cache levels=1:2 keys_zone=code_cache:10m max_size=10g inactive=60m use_temp_path=off;# 缓存路径 proxy_cache_path /soft/cache# 目录结构级别 levels=1:2 一般都是2层 多了效率会底下 按照两层⽬录分级# 空间名称+10M keys_zone=code_cache:10m # 最大缓存文件只能是10G 超过就把最弱剔除 max_size=10g# 60分钟没有被访问缓存会被清理 inactive=60m# 临时⽂件,会影响性能,建议关闭 use_temp_path=offupstream cache &#123; server 172.17.70.226:8081; server 172.17.70.226:8082; server 172.17.70.226:8083;&#125;server &#123; listen 80; server_name 60.205.217.112; index index.html; location / &#123; proxy_pass http://cache; proxy_cache code_cache; proxy_cache_valid 200 304 12h; proxy_cache_valid any 10m; add_header Nginx-Cache "$upstream_cache_status"; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; include proxy_params; &#125;&#125;# proxy_cache # 开启缓存# proxy_cache_valid # 状态码200|304的过期为12h,其余状态码10分钟过期# add_header # 可以看response头部信息 看是否命中# proxy_next_upstream # 超时和500+请求 就会被直接跳转到第二台请求 12[root@proxy conf.d]# nginx -t[root@proxy conf.d]# nginx -s reload 客户端测试12 123456789101112# 查看缓存内容:[root@proxy conf.d]# cd /soft/cache/[root@proxy cache]# ls7 8 d# 关闭缓存 回归正常轮询proxy_cache off;nginx -s reload# 清理缓存[root@proxy cache]# rm -rf *# 页面会重新开始缓存,第一次都是MISS Nginx 清理缓存方式12[root@proxy /]# cd /soft/cache/[root@proxy cache]# rm -rf * 123# curl 测试缓存[root@proxy conf.d]# curl -I http://60.205.217.112/url3.html | grep "Nginx-Cache"Nginx-Cache: HIT ngx_cache_purge 扩展模块清理部分页面 不缓存指定部分页面 不参加 proxy_Cache 缓存123456789101112131415161718192021222324252627282930313233[root@proxy conf.d]# vim proxy_cache.conf proxy_cache_path /soft/cache levels=1:2 keys_zone=code_cache:10m max_size=10g inactive=60m use_temp_path=off;upstream cache &#123; server 172.17.70.226:8081; server 172.17.70.226:8082; server 172.17.70.226:8083;&#125;server &#123; listen 80; server_name 60.205.217.112; index index.html; location / &#123; if ($request_uri ~ ^/(url3|login|register|password)) &#123; set $cookie_nocache 1; &#125; proxy_pass http://cache; proxy_cache code_cache; proxy_cache_valid 200 304 12h; proxy_cache_valid any 10m; proxy_no_cache $cookie_nocache $arg_nocache $arg_comment; proxy_no_cache $http_pargma $http_authorization; add_header Nginx-Cache "$upstream_cache_status"; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; include proxy_params; &#125;&#125;[root@proxy conf.d]# nginx -s reload 1234567891011121314# 清理缓存rm -rf /soft/cache/*[root@proxy soft]# curl -I http://60.205.217.112/url1.html | grep "Nginx-Cache"Nginx-Cache: MISS[root@proxy soft]# curl -I http://60.205.217.112/url1.html | grep "Nginx-Cache"Nginx-Cache: HIT[root@proxy soft]# curl -I http://60.205.217.112/url3.html | grep "Nginx-Cache"Nginx-Cache: MISS[root@proxy soft]# curl -I http://60.205.217.112/url3.html | grep "Nginx-Cache"Nginx-Cache: MISS 缓存日志 记录统计12345# 修改/etc/nginx/nginx.conf中log_format格式http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"' '"$upstream_cache_status"'; 1234# 修改proxy_cache.conf,在server标签新增access⽇志# 只记录该sever的访问日志access_log /var/log/nginx/proxy_cache.log main; 12345[root@proxy soft]# nginx -s reload[root@proxy soft]# curl -I http://60.205.217.112/url1.html | grep "Nginx-Cache"[root@proxy soft]# curl -I http://60.205.217.112/url2.html | grep "Nginx-Cache"[root@proxy soft]# curl -I http://60.205.217.112/url3.html | grep "Nginx-Cache" nginx cache 查看命中率1http://www.361way.com/nginx-cache/2665.html Nginx 缓存总结12345678910111. 缓存的分类: 1. 客户端缓存 2. 代理缓存 3. 服务端缓存 2. Nginx 配置缓存 1. proxy_cache3. Nginx 如何清理缓存 1. 直接删除缓存目录 2. 使用ngx_cache_purge 编译后使用第三方模块4. Nginx 可以指定页面不参与缓存]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05 Nginx 负载均衡]]></title>
    <url>%2F2019%2F10%2F21%2Fnginx-base05%2F</url>
    <content type="text"><![CDATA[负载均衡的应用 提升吞吐率, 提升请求性能, 提高容灾 负载均衡按范围划分: GSLB全局负载均衡 SLB 4层代理 端口转发 通过公网访问内部主机,访问公网 8080端口 转发 内网服务器 8080端口 4层代理 还有 haproxy lvs Nginx 负载均衡配置 Nginx 实现负载均衡用到了 proxy_pass 代理模块核心配置, 将客户端请求代理转发给一组 upstream 虚拟服务池 通过端口 实验负载均衡创建对应 html文件12345678910111213141516171819202122232425261. 创建对应 html 文件[root@nginx soft]# mkdir /soft/&#123;code1,code2,code3&#125; -p[root@nginx soft]# cat /soft/code1/index.html &lt;html&gt; &lt;title&gt;Coder1&lt;/title&gt; &lt;body bgcolor="red"&gt; &lt;h1&gt; Code1-8081 &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;[root@nginx soft]# cat /soft/code2/index.html &lt;html&gt; &lt;title&gt;Coder2&lt;/title&gt; &lt;body bgcolor="blue"&gt; &lt;h1&gt; Code1-8082 &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;[root@nginx soft]# cat /soft/code3/index.html &lt;html&gt; &lt;title&gt;Coder3&lt;/title&gt; &lt;body bgcolor="green"&gt; &lt;h1&gt; Code1-8083 &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 创建对应的 server文件1234567891011121314151617181920212223242526272829303132333435363738[root@nginx conf.d]# cat node1.conf server &#123; listen 8081; server_name 172.17.70.224; root /soft/code1; index index.html;&#125;[root@nginx conf.d]# cat node2.conf server &#123; listen 8082; server_name 172.17.70.224; root /soft/code2; index index.html;&#125;[root@nginx conf.d]# cat node3.conf server &#123; listen 8083; server_name 172.17.70.224; root /soft/code3; index index.html;&#125;[root@nginx conf.d]# nginx -t[root@nginx conf.d]# nginx -s reload[root@nginx conf.d]# netstat -tnlpProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 10539/nginx: master tcp 0 0 0.0.0.0:8081 0.0.0.0:* LISTEN 10539/nginx: master tcp 0 0 0.0.0.0:8082 0.0.0.0:* LISTEN 10539/nginx: master tcp 0 0 0.0.0.0:8083 0.0.0.0:* LISTEN 10539/nginx: master [root@nginx conf.d]# curl -I 172.17.70.224:8081 配置负载均衡123456789101112131415161718192021222324252627# proxy [root@proxy ~]# vim /etc/nginx/conf.d/proxy.conf upstream node &#123; server 172.17.70.224:8081; server 172.17.70.224:8082; server 172.17.70.224:8083;&#125;server&#123; listen 80; server_name localhost; location / &#123; proxy_pass http://node; include proxy_params; &#125;&#125;[root@proxy ~]# nginx -t[root@proxy ~]# nginx -s reload# 会轮询访问 3个端口服务http://47.95.120.246/ Nginx 负载均衡状态模块 12345678backup 预留的,只有其他的节点挂掉了 才能被访问max_fails=3 fail_timeout=10s; 允许失败3次 失败后暂停10秒 3次后还不行会被踢掉upstream node &#123; server 172.17.70.224:8081 backup; server 172.17.70.224:8082 max_fails=3 fail_timeout=10s; server 172.17.70.224:8083 max_fails=3 fail_timeout=10s;&#125; Nginx 负载均衡调度策略 Nginx 4层负载均衡 TCP配置 12345678910111213141516171819202122232425262728293031[root@proxy ~]# vim /etc/nginx/nginx.conf# For more information on configuration, see:# * Official English Documentation: http://nginx.org/en/docs/# * Official Russian Documentation: http://nginx.org/ru/docs/user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;stream &#123; upstream ssh_proxy &#123; hash $remote_addr consistent; server 172.17.70.224:22; &#125; server &#123; listen 6666; proxy_pass ssh_proxy; proxy_connect_timeout 1s; proxy_timeout 300s; &#125;&#125; 阿里云 SLB实践 HTTP与TCP场景 解绑弹性IP 够买负载均衡SLB 分区一定要和 实例在一起 华北2 配置后端服务器 1可以先测试一下 ping 112.126.119.100 / telnet 112.126.119.100 9999 推荐使用 虚拟服务器组 1虚拟机服务组 对应每台主机的 实际端口 7层负载 Nginx 动静分离 Nginx 动静分离应用案例 1231. CentOS7.4 172.17.70.227 proxy2. CentOS7.4 172.17.70.226 tomcat 动态jsp3. CentOS7.4 172.17.70.226 nginx 静态图片 12345678910# 基本流程1. 配置静态nginx 使用公网访问nginx.png 2. 配置动态nginx 本地验证是否打开 3. 定义负载均衡 4. 新增ajax文件 调度jsp文件 调度nginx静态图片5. 验证 如果tomcat服务终止,jsp无法响应 但是静态图片还在nginx+tomcat静态 js css png -&gt; nginx动态 jsp -&gt; tomcat 准备静态资源123456# 准备静态图片[root@nginx soft]# mkdir -p /soft/code/images# 只有proxy有公网可以下载,下载好后传过去[root@proxy ~]# wget -O /tmp/nginx.png http://nginx.org/nginx.png[root@proxy tmp]# scp nginx.png root@172.17.70.226:/soft/code/images/ 123456789101112131415161718192021# nginx配置[root@nginx soft]# yum install -y nginx[root@nginx conf.d]# vim static.conf[root@nginx conf.d]# cd /etc/nginx/conf.d/server &#123; listen 80; server_name 172.17.70.226; root /soft/code; index index.html; location ~ .*\.(jpg|css|gif|png)$ &#123; gzip on; root /soft/code/images; &#125;&#125;[root@nginx conf.d]# nginx -t[root@nginx conf.d]# nginx -s reload 12# proxy 访问测试curl 172.17.70.226/nginx.png 使用proxy 调度访问静态图片12345678910111213141516171819202122# 配置 proxy[root@proxy conf.d]# cd /etc/nginx/conf.d/[root@proxy conf.d]# vim proxy.confupstream static &#123; server 172.17.70.226:80;&#125;server &#123; listen 80; server_name 60.205.217.112; location / &#123; root /soft/code; index index.html; &#125; location ~ .*\.(png|jpg|gif)$ &#123; proxy_pass http://static; include proxy_params; &#125;&#125; 123456789101112131415161718# proxy_params[root@proxy conf.d]# vim /etc/nginx/proxy_paramsproxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k; 1测试访问: http://60.205.217.112/nginx.png 准备动态资源1234567891011121314151617181920212223[root@nginx bin]# yum install java -y[root@proxy tmp]# scp apache-tomcat-8.5.47.tar.gz root@172.17.70.226:/tmp[root@nginx ~]# mkdir -p /soft/app[root@nginx app]# cp /tmp/apache-tomcat-8.5.47.tar.gz ./[root@nginx app]# tar -zxvf apache-tomcat-8.5.47.tar.gz [root@nginx app]# cd apache-tomcat-8.5.47/webapps/ROOT/[root@nginx ROOT]# vi java_test.jsp &lt;%@ page language="java" import="java.util.*" pageEncoding="utf-8"%&gt;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;JSP Test Page&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;% Random rand = new Random(); out.println("&lt;h1&gt;Random number:&lt;/h1&gt;"); out.println(rand.nextInt(99)+100); %&gt; &lt;/BODY&gt;&lt;/HTML&gt; 123# 解决tomcat8 在centos7 启动慢yum install rng-tools systemctl start rngd 123# 启动tomcat服务[root@nginx ROOT]# cd /soft/app/apache-tomcat-8.5.47/bin/[root@nginx bin]# sh startup.sh 12345678910111213# 配置本地动态访问[root@proxy code]# curl http://172.17.70.226:8080/java_test.jsp&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;JSP Test Page&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;h1&gt;Random number:&lt;/h1&gt;116 &lt;/BODY&gt;&lt;/HTML&gt; 使用proxy 调度动态jsp页面12345678910111213141516171819202122232425262728293031323334[root@proxy conf.d]# vim proxy.conf upstream static &#123; server 172.17.70.226:80;&#125;upstream jsp &#123; server 172.17.70.226:8080;&#125;server &#123; listen 80; server_name 60.205.217.112; location / &#123; root /soft/code; index index.html; &#125; location ~ .*\.(png|jpg|gif)$ &#123; proxy_pass http://static; include proxy_params; &#125; location ~ .*\.jsp$ &#123; proxy_pass http://jsp; include proxy_params; &#125;&#125;[root@proxy conf.d]# nginx -t[root@proxy conf.d]# nginx -s reloadhttp://60.205.217.112/java_test.jsp 编写动静整合 html 文件12345678910111213141516171819202122232425262728[root@proxy conf.d]# vim /soft/code/mysite.html&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8" /&gt; &lt;title&gt;测试ajax和跨域访问&lt;/title&gt; &lt;script src="http://libs.baidu.com/jquery/2.1.4/jquery.min.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;script type="text/javascript"&gt;$(document).ready(function()&#123; $.ajax(&#123; type: "GET", url: "http://60.205.217.112/java_test.jsp", success: function(data) &#123; $("#get_data").html(data) &#125;, error: function() &#123; alert("fail!!,请刷新再试!"); &#125; &#125;);&#125;);&lt;/script&gt; &lt;body&gt; &lt;h1&gt;测试动静分离&lt;/h1&gt; &lt;img src="http://60.205.217.112/nginx.png"&gt; &lt;div id="get_data"&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 12# 测试访问http://60.205.217.112/mysite.html 测试动静分离整合12当停止tomcat时,静态图片依旧可以访问到[root@nginx bin]# sh shutdown.sh 12# 恢复[root@nginx bin]# sh startup.sh 按手机类型调度不同后端节点 根据user_agent判断 准备web站点123[root@nginx soft]# mkdir -p /soft/&#123;android,iphone&#125;[root@nginx soft]# echo "&lt;h1&gt;Android&lt;/h1&gt;" &gt;&gt; /soft/android/index.html[root@nginx soft]# echo "&lt;h1&gt;Iphone&lt;/h1&gt;" &gt;&gt; /soft/iphone/index.html 配置 nginx 站点12345678910111213141516171819202122232425262728[root@nginx soft]# cd /etc/nginx/conf.d/[root@nginx conf.d]# vim phone.confserver &#123; listen 8081; server_name 172.17.70.227; index index.html; location / &#123; root /soft/android; index index.html; &#125;&#125;server &#123; listen 8082; server_name 172.17.70.227; index index.html; location / &#123; root /soft/iphone; index index.html; &#125;&#125;[root@nginx conf.d]# nginx -t[root@nginx conf.d]# nginx -s reload 12345# 测试[root@proxy conf.d]# curl 172.17.70.226:8081&lt;h1&gt;Android&lt;/h1&gt;[root@proxy conf.d]# curl 172.17.70.226:8082&lt;h1&gt;Iphone&lt;/h1&gt; 根据 user_agent 分配页面123456# = 开头表示精确匹配# ^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。以xx开头# ~ 开头表示区分大小写的正则匹配 以xx结尾# ~* 开头表示不区分大小写的正则匹配 以xx结尾#!~和!~*分别为区分大小写不匹配及不区分大小写不匹配 的正则# / 通用匹配，任何请求都会匹配到。 1234567891011121314151617181920212223242526272829[root@proxy conf.d]# vim proxy_phone.confupstream iphone &#123; server 172.17.70.226:8082;&#125;upstream android &#123; server 172.17.70.226:8081;&#125;server &#123; listen 80; server_name 60.205.217.112; index index.html; location / &#123; if ($http_user_agent ~* "android")&#123; proxy_pass http://android; &#125; if ($http_user_agent ~* "iphone")&#123; proxy_pass http://iphone; &#125; &#125;&#125;[root@proxy conf.d]# nginx -t[root@proxy conf.d]# nginx -s reload 1234# 发给测试一下http://60.205.217.112/# 可能专门为安卓 或者 iphone 专门提供了页面# 手机不一样 型号不一样 样式不一样 按浏览器调度不同后端节点 按不同目录路径 调度不同后端节点 Nginx 负载知识总结]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 Nginx 代理服务]]></title>
    <url>%2F2019%2F10%2F21%2Fnginx-base04%2F</url>
    <content type="text"><![CDATA[Nginx 代理基本概述在互联网请求里, 客户端⽆法直接向服务端发起请求, 那么就需要用到代理服务, 来实现客户端和服务端通信。 Nginx 作为代理服务可以实现很多的协议代理, 我们主要以 http 代理为主 正向代理(内部上⽹) 客户端代理-&gt;服务端 反向代理 客户端-&gt;代理服务端 代理区别 区别在于代理的对象不一样 # 翻墙 正向代理代理的对象是客户端 反向代理代理的对象是服务端 Nginx 代理配置语法 proxy_pass 1234567891011121314151617# 编译统一配置 引用文件[root@nginx-node1 code]# vim /etc/nginx/proxy_paramsproxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k; 1234567891011121314# 具体location实现引用,不要每个location下都实现一次[root@nginx-node1 conf.d]# vim game.conf [root@nginx-node1 conf.d]# vim mygame.conf server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://127.0.0.1:8080; include proxy_params; &#125; 123456789101112131415161718192021222324252627282930313233# 参数解释proxy_set_header # 获取真实用户IP 日志如果想拿到 必须这样传 不传拿到的一直是代理服务器IPproxy_connect_timeout : # nginx与upstream server的连接超时时间proxy_send_timeout : # nginx发送数据至upstream server超时, 默认60s, 如果连续的60s内没有发送1个字节, 连接关闭proxy_read_timeout : # nginx接收upstream server数据超时, 默认60s, 如果连续的60s内没有收到1个字节, 连接关闭如果proxy_buffering开启,nginx假定被代理的后端服务器会以最快速度响应,并把内容保存在由指令 proxy_buffer_size 和 proxy_buffers 指定的缓冲区里边.1. 如果响应body无法放在内存里边,那么部分内容会被写到磁盘上。2. 如果proxy_buffering被关闭了,那么响应body会按照获取body的多少立刻同步传送到客户端。 - nginx不尝试计算被代理服务器整个响应body的大小, - nginx能从服务器接受的最大数据,是由指令 proxy_buffer_size 指定的。3. 对于基于长轮询(long-polling)的Comet 应用来说,关闭 proxy_buffering 是重要的,不然异步响应将被缓存导致Comet无法工作。 - 但是无论proxy_buffering是否开启，proxy_buffer_size都是生效的proxy_buffer_size 32k;# 设置代理服务器（nginx）从后端realserver读取并保存用户头信息的缓冲区大小，默认与proxy_buffers大小相同，其实可以将这个指令值设的小一点proxy_buffers 4 128k;# proxy_buffers缓冲区，nginx针对单个连接缓存来自后端realserver的响应，网页平均在128k以下的话，这样设置# 开辟4个长度为128k大小的read_buf用来存储bodyproxy_busy_buffers_size 256k;# 高负荷下缓冲大小（proxy_buffers*2）proxy_max_temp_file_size 256k;# 当 proxy_buffers 放不下后端服务器的响应内容时，会将一部分保存到硬盘的临时文件中，这个值用来设置最大临时文件大小，默认1024M，它与 proxy_cache 没有关系。大于这个值，将从upstream服务器传回。设置为0禁用。proxy_temp_file_write_size 64k# 当缓存被代理的服务器响应到临时文件时，这个选项限制每次写临时文件的大小。proxy_temp_path（可以在编译的时候）指定写到哪那个目录。 Nginx 反向代理 准备服务器121. 够买两台华北的aliyun主机,一台可以上外网,一台无法出外网2. 让可以出外网的80作为nginx主机,剩下一台作为web主机,用 ssh 内网地址登录 1234# 使用默认的yum源 安装nginxyum install -y systemctl start nginx Nginx反向代理配置实例反向道理配置 web配置 123456789101112131415161718192021222324252627282930313233343536373839404142# 在web服务器上创建目录 并 添加nginx配置 [root@nginx ~]# mkdir -p /soft/code/images[root@nginx ~]# vim /etc/nginx/conf.d/images.confserver&#123; listen 80; server_name localhost; root /soft/code; location / &#123; root /soft/code; index index.html; &#125; location ~ .*\.(png|jpg|gif)$ &#123; gzip on; root /soft/code/images; &#125;&#125;# rz 存储[root@nginx images]# nginx -t图片进入目录[root@nginx images]# yum install -y lrzsz[root@nginx images]# ls -l[root@nginx images]# ls -l-rw-r--r-- 1 root root 2103 Oct 21 12:43 nginx.png-rw-r--r-- 1 root root 173005 Oct 12 18:00 timg_02.jpg# 注释默认配置文件crtl + v 按下选中要注释的行数shift + i 再输入# 最后按ESC 多行被注释# 启动[root@nginx images]# nginx -t[root@nginx images]# nginx[root@nginx images]# curl http://127.0.0.1/nginx.png proxy配置 1234567891011121314151617# proxy配置[root@proxy ~]# vim /etc/nginx/proxy_paramsproxy_redirect default;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_connect_timeout 60;proxy_send_timeout 60;proxy_read_timeout 60;proxy_buffer_size 32k;proxy_buffering on;proxy_buffers 4 128k;proxy_busy_buffers_size 256k;proxy_max_temp_file_size 256k; 12345678910111213141516171819202122# proxy 指向 172.17.70.224 web服务端IP[root@proxy ~]# vim /etc/nginx/conf.d/proxy.confserver&#123; listen 80; server_name localhost; index index.html; location / &#123; proxy_pass http://172.17.70.224; include proxy_params; &#125;&#125;[root@proxy ~]# nginx -t[root@proxy ~]# nginx -s reloadhttp://47.95.120.246/nginx.png # 看到图片 代理成功 123456# 日志# proxy114.243.26.3 - - [21/Oct/2019:13:08:24 +0800] "GET /nginx.png HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "-"# web172.17.70.225 - - [21/Oct/2019:13:08:24 +0800] "GET /nginx.png HTTP/1.0" 304 0 "-" "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "114.243.26.3" 1234# 加个首页测试代理也会转发 注意server_name 使用域名的话 需要一致[root@nginx code]# vim /soft/code/index.html&lt;h1&gt;Nginx Web Index&lt;/h1&gt;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 Nginx 静态资源 web服务]]></title>
    <url>%2F2019%2F10%2F19%2Fnginx-base03%2F</url>
    <content type="text"><![CDATA[静态资源概述Nginx 作为静态资源 Web 服务器部署配置,传输非常高效,常常用于 静态资源处理请求,动静分离。 静态资源配置语法文件读取高效 sendfile1开启后,文件不需要通过 用户空间 传输文件,直接在内核中copy 提高网络传输效率 nopush1看业务是要求 实时性 还是 传输效率 静态资源 文件压缩 gzip1231. 压缩 相当于 把碎文件 打了一个包整体传输,而且包的大小会变小,减少带宽2. 相对应会增加服务器的cpu性能消耗3. 服务端压缩,浏览器会帮我们解压 压缩图片121. 创建图片目录[root@nginx-node1 src]# mkdir /soft/code/images 1234567891011121314151617181920212. 配置文件# /nginx/nginx.conf 下别忘记优化# sendfile on; # 文件读取高效# tcp_nopush on; # 数据包累计到一定大小发送# tcp_nodelay on; # 尽快发送数据包[root@nginx-node1 conf.d]# vim image.conf server &#123; listen 80; server_name localhost; location ~ .*\.(jpg|gif|png)$ &#123; # gzip off; # gzip_http_version 1.1; # gzip_comp_level 2; # gzip_types text/plain application/json application/x-javascript application/css application/xml # application/xml+rss text/javascript application/x-httpd-php image/jpeg image/gif image/png; root /soft/code/images; &#125;&#125; 在没有优化gzip压缩的时候,访问的图片大小 1234567891011121314# 压缩类型在 vim /etc/nginx/mime.types 对应添加,没有的可以自己补上server &#123; listen 80; server_name localhost; location ~ .*\.(jpg|gif|png)$ &#123; gzip on; # 开启压缩 gzip_http_version 1.1; # http 版本 gzip_comp_level 2; # 压缩级别 2-6 gzip_types text/plain application/json application/x-javascript application/css application/xml application/xml+rss text/javascript application/x-httpd-php image/jpeg image/gif image/png; # 压缩级别 root /soft/code/images; &#125;&#125; 启用 gzip 压缩图片后(由于图片之前压缩过, 所以压缩比率不太明显) 压缩文本12345678910111213141516171819202122232425[root@nginx-node1 conf.d]# mkdir -p /soft/code/docserver &#123; listen 80; server_name localhost; location ~ .*\.(jpg|gif|png)$ &#123; gzip on; gzip_http_version 1.1; gzip_comp_level 2; gzip_types text/plain application/json application/x-javascript application/css application/xml application/xml+rss text/javascript application/x-httpd-php image/jpeg image/gif image/png; root /soft/code/images; &#125; location ~ .*\.(txt|xml)$ &#123; gzip on; gzip_http_version 1.1; gzip_comp_level 2; gzip_types text/plain text/xml image/jpeg image/gif image/png; root /soft/code/doc; &#125;&#125;# 源XML文件从260KB 压缩到 12.5KB 浏览器 缓存概述 浏览器缓存 实战案例 123456789101112131415161718# 图片缓存30天，css 和 js文件缓存1小时 location ~ .*\.(jpg|gif|png)$ &#123; gzip on; gzip_http_version 1.1; gzip_comp_level 2; root /soft/code/images; expires 30d; &#125; location ~ .*\.(css|js)$ &#123; gzip on; gzip_http_version 1.1; gzip_comp_level 2; root /soft/code/images; expires 1h; &#125;# 以下面开头的目录下面的文件 都存在缓存时间 静态资源 防盗链 防止资源被盗用 防盗链设置思路: 区别哪些请求是否正常用户请求 基于 http_refer 防盗链配置模块,判断如果不是我公司的域名访问图片,不允许调用 123456789101. 页面调用本地的图片资源&lt;html&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;pachong&lt;title&gt; &lt;/head&gt; &lt;body style="background-color:red;"&gt; &lt;img src="http://47.56.134.107/timg_01.jpg"&gt; &lt;/body&gt;&lt;/html&gt; 123456789102.启动防盗链# 支持 IP、域名、正则方式# 一般情况是域名 因为会用到CDN cdn.leo.comlocation ~ .*\.(jpg|gif|png)$ &#123; valid_referers none blocked 47.56.134.107; if ($invalid_referer) &#123; return 403; &#125; root /soft/code/images;&#125; 静态资源小结 Nginx 做静态web站点 html、css、js、png、jpg、视频、文本… 压缩 浏览器缓存 跨域访问 防止图片]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 Nginx 常用模块]]></title>
    <url>%2F2019%2F10%2F19%2Fnginx-base02%2F</url>
    <content type="text"><![CDATA[Nginx 日志模块 http 简单的请求过程 1[root@nginx-node1 ~]# curl -v www.baidu.com Nginx 日志配置规范 可自己编排,调换位置 全局配置,只能放在 http区域,不能配置在 server里 123log_format main &apos;[$time_local] &apos; &apos;$remote_addr - $remote_user &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; 1234567891011# Nginx 日志变量$remote_addr # 表示客户端地址$remote_user # http 客户端请求nginx认证用户名$time_local # Nginx 的时间$request # Request 请求行, GET、POST方法、http协议版本$status # respoence 返回状态码$body_bytes_sent # 从服务端响应给客户端body信息大小$http_referer # http 上一级页面,防盗链、用户行为分析$http_user_agent # http 头部信息,客户端访问设备,安卓,苹果,pc 适配$http_x_forwarded_for # http 请求携带的http信息,真实IP地址,用户可能是通过代理过来的 Nginx 状态监控 –with-http_stub_status_module 记录 Nginx 客户端基本访问状态信息 重启ngxin服务 会重新计数 12Syntax: stub_status;Context: server, location 1234567# 不是每个server一个,而是统计整个nginxlocation /mystatus &#123; stub_status on; allow 127.0.0.1; # 允许自己访问 监控时 脚本只自己访问自己 deny all;&#125; 12345678910111213141516171819202122# Nginx_status 概述Active connections:2 # Nginx当前活跃连接数server accepts handled requests16 16 19# server 表示 Nginx 处理接收握手总次数 -- 建立连接总数# accepts 表示 Nginx 处理接收总连接数。 -- 成功连接总数请求丢失数=(握手数-连接数)如果一致,说明没有丢失请求。# handled requests 表示总共处理了19次请求。 -- 处理请求总数,连接打开网页,刷新一次为一个请求# Reading Nginx读取数据# Writing Nginx写的情况# Waiting Nginx开启keep-alive长连接情况下,既没有读也没有写,建立连接情况# 开启keep-alive的情况下,这个值等于 active – (reading + writing),意思就是Nginx已经处理完成,正在等候下一次请求指令的驻留连接.# 所以,在访问效率高,请求很快被处理完毕的情况下,Waiting数比较多是正常的.如果reading +writing数较多,则说明并发访问量非常大,正在处理过程中. Nginx 下载站点 Nginx默认是不允许列出整个⽬录浏览下载。 1234Syntax: autoindex on|off;Default: autoindex off;Context: http, server, location 123456789101112# autoindex 常用参数autoindex_exact_size off;默认为on 显示文件的确切大小 单位是bytes。修改为off 显示文件的大概大小 单位是kB或者MB或者GB。autoindex_localtime on;默认为off 显示文件时间为GMT时间。修改为on 显示文件时间为文件的服务器时间。charset utf-8,gbk;默认中文目录乱码，添加上解决乱码。 12345678# 配置目录浏览功能location /down &#123; root /soft/package/src; autoindex on; autoindex_exact_size off; autoindex_localtime on; charset utf-8,gbk;&#125; 1234cd /soft/package/src/down[root@nginx-node1 down]# ls2000.png sound1.mp3 Nginx 访问限制12连接频率限制 limit_conn_module # 针对TCP请求频率限制 limit_req_module http协议的连接与请求 12345HTTP是建⽴在TCP, 在完成HTTP请求需要先建⽴TCP三次握⼿（称为TCP连接）,在连接的基础上在HTTP请求。1. 连接限制是TCP,一个连接可以有多个请求2. 请求是HTTP Nginx 请求限制配置 虚拟机只能测试出 请求的限制,公有用可以配置连接的设置 limit_req zone=req_zone; 放在location 里面只针对location,如果放在location外面,针对整个server,放在http 所有的都会被限制 对请求比对连接的限制 更有效 12345678910# Nginx 请求限制配置# http 段配置请求限制,rate限制速率,限制1秒钟最多1个IP请求limit_req_zone $binary_remote_addr zone=req_zone:10m rate=1r/s;# 1r/s 只接收1个请求,其余请求拒绝处理并返回错误码给客户端limit_req zone=req_zone;# 请求超过1r/s,剩下的将被延迟处理,请求数超过burst定义的数量,1+3=4个请求 多余的请求返回503# 大量的并发攻击 可以控制# limit_req zone=req_zone burst=3 nodelay; 1234567891011121314[root@nginx-node1 ~]# vim /etc/nginx/conf.d/default.conf limit_req_zone $binary_remote_addr zone=req_zone:10m rate=1r/s; # http 段server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; limit_req zone=req_zone; 1234[root@nginx-node1 ~]# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful[root@nginx-node1 ~]# nginx -s reload 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 安装ab 工具yum install httpd-tools # 50个连接 一次20个请求[root@nginx-node1 down]# ab -n 50 -c 20 http://47.56.134.107/index.htmlThis is ApacheBench, Version 2.3 &lt;$Revision: 1430300 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking 47.56.134.107 (be patient).....doneServer Software: nginx/1.16.1Server Hostname: 47.56.134.107Server Port: 80Document Path: /index.htmlDocument Length: 612 bytesConcurrency Level: 20Time taken for tests: 0.026 secondsComplete requests: 50 # 请求次数Failed requests: 49 # 失败次数 (Connect: 0, Receive: 0, Length: 49, Exceptions: 0)Write errors: 0Non-2xx responses: 49 # 不是200的 49个Total transferred: 34557 bytesHTML transferred: 24818 bytesRequests per second: 1935.06 [#/sec] (mean) # rps 吞吐率 每秒1935Time per request: 10.336 [ms] (mean)Time per request: 0.517 [ms] (mean, across all concurrent requests)Transfer rate: 1306.05 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 1 0.1 1 1Processing: 0 9 10.5 1 22Waiting: 0 8 10.2 1 22Total: 1 10 10.5 1 23Percentage of the requests served within a certain time (ms) 50% 1 66% 22 75% 22 80% 23 90% 23 95% 23 98% 23 99% 23 100% 23 (longest request) Nginx 连接限制配置 http 段配置连接限制,同⼀时刻只允许1个客户端IP连接 1limit_conn_zone $binary_remote_addr zone=conn_zone:10m; 123456789101112131415# http段配置连接限制, 同1时刻只允许1个客户端IP连接limit_conn_zone $binary_remote_addr zone=conn_zone:10m;server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; # 同⼀时刻只允许1个客户端IP连接 limit_conn conn_zone 1; &#125; Nginx 访问控制12基于IP的访问控制 http_access_module基于用户登陆认证 http_auth_basic_module 基于IP的访问控制 12345678910111213141516171819202122232425# 允许所有人访问 拒绝我自己的IPlocation ~ ^/1.html &#123; root /soft/code; index index.html; deny 114.243.26.3; allow all; # allow 114.243.26.3; # 先允许 后拒绝 只允许一个IP 或者在配置一行其他IP:常用 # deny all; # 拒绝所有 自己本机也不行 &#125;[root@nginx-node1 ~]# echo &quot;&lt;h1&gt;Access allow&lt;/h1&gt;&quot; &gt;&gt; /soft/code/1.html[root@nginx-node1 ~]# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful[root@nginx-node1 ~]# nginx -s reload ; tailf /var/log/nginx/error.loghttp://47.56.134.107/1.html[root@nginx-node1 down]# curl 47.56.134.107/1.html&lt;h1&gt;Access allow&lt;/h1&gt; http_access_module 局限性 没有办法对真实IP 做任何限制 需要开启 http_x_forwarded_for 记录真实客户端IP地址以及代理服务器IP 再做限制 不开启的话 可能会看到自己的代理IP 访问 web服务器 基于用户登陆认证12345678910# 需要安装依赖组件[root@xuliangwei~]# yum install httpd-tools[root@xuliangwei~]# htpasswd -c /etc/nginx/auth_conf leo[root@nginx-node1 ~]# htpasswd -c /etc/nginx/auth_conf leoNew password: Re-type new password: Adding password for user leo[root@nginx-node1 ~]# cat /etc/nginx/auth_conf leo:$apr1$TYxX68qH$YV57jU1mR9JVYAIk9Msai1 1234567891011121314151617181920# 必须登录才能下载# 可在http,server,location 下添加如下信息location /down &#123; root /soft/package/src; autoindex on; autoindex_exact_size off; autoindex_localtime on; #charset utf-8,gbk; auth_basic &quot; Down need login in &quot;; auth_basic_user_file /etc/nginx/auth_conf;&#125;[root@nginx-node1 conf.d]# nginx -t[root@nginx-node1 conf.d]# nginx -s reload ; tailf /var/log/nginx/error.log[19/Oct/2019:23:54:08 +0800] 114.243.26.3 - leo &quot;GET /down/ HTTP/1.1&quot; 200 361 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&quot; &quot;-&quot; Nginx 虚拟主机所谓虚拟主机，在web服务器里是1个独立的网站站点，这个站点对应独立的域名(也可能是IP或端口)，具有独立的程序及资源目录，可以独立地对外提供服务供用户访问。 1个server就是1个虚拟主机 1234# 常用顺序1. 基于域名2. 基于端口3. 基于IP 不常用 基于域名12341. 创建web站点目录[root@nginx-node1 src]# mkdir -p /soft/package/src/&#123;www,blog&#125;[root@nginx-node1 src]# echo &quot;&lt;h1&gt;www&lt;/h1&gt;&quot; &gt;&gt; /soft/package/src/www/index.html[root@nginx-node1 src]# echo &quot;&lt;h1&gt;blog&lt;/h1&gt;&quot; &gt;&gt; /soft/package/src/blog/index.html 1234567891011121314151617181920212223242526272829302. 配置虚拟主机[root@nginx-node1 conf.d]# cp xiaoniao.conf www.conf[root@nginx-node1 conf.d]# cp xiaoniao.conf blog.confserver &#123; listen 80; server_name www.leo.com; root /soft/package/src/www/; index index.html index.htm; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125;server &#123; listen 80; server_name blog.leo.com; root /soft/package/src/blog/; index index.html index.htm; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125; 1233. 修改hosts文件47.56.134.107 www.leo.com47.56.134.107 blog.leo.com 基于端口 配置不同端口访问不同虚拟主机 仅修改listen监听端口即可,但不能和系统端口发⽣冲突 1234567891011121314151617181. 基于端口的虚拟主机server &#123; listen 8002; server_name localhost; root /soft/package/src/www/; index index.html index.htm;&#125;server &#123; listen 8001; server_name localhost; root /soft/package/src/blog/; index index.html index.htm;&#125; 12. aliyun打开安全组端口 基于IP地址 配置虚拟主机别名所谓虚拟主机别名，就是虚拟主机设置除了主域名以外的1个域名，实现用户访问的多个域名对应同1个虚拟主机网站的功能。以www.xiaoniao.com域名的虚拟主机为例:为其增加1个别名xiaoniao.com时,出现网站内容和访问www.xiaoniao.com是一样的,具体配置如下： 12345678server &#123; listen 80; server_name www.xiaoniao.com xiaoniao.com; root /data/www/xiaoniao; index index.html index.htm; &#125;&#125; 相同域名的优先级 相同的域名,相同的端口,谁的文件名在前,谁会被优先读取]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 Nginx 快速入门]]></title>
    <url>%2F2019%2F10%2F18%2Fnginx-base01%2F</url>
    <content type="text"><![CDATA[Nginx 简介什么是 Nginx Nginx是一个开源且高性能、可靠的HTTP中间件和代理服务软件。 Nginx 高性能 作为Web服务器，与Apache相比，Nginx能够支持更多的并发连接访问，但占用的资源却更少，效率更高。 为什么Nginx总体性能比Apache高? Nginx 与 Apache同样是基于IO多路复用, 需要了解IO模型 五种 IO Model12345# 1、blocking IO 阻塞IO# 2、nonblocking IO 非阻塞IO# 3、IO multiplexing IO多路复用# 4、signal driven IO 信号驱动IO# 5、asynchronous IO 异步IO 通常一次IO操作(Network IO read) ，一般会涉及到两个系统对象: 一个是调用这个IO的process (or thread) 另一个就是系统内核(kernel)。 当一个IO操作发生时，该操作会经历两个阶段：等待数据和拷贝数据两个阶段 阻塞IO: 在这两个阶段，线程都将被阻塞 非阻塞IP: 用户进程其实是需要不断的主动询问kernel数据准备好了没有，等待数据侧不阻塞但是拷贝数据依然阻塞。 IO多路复用: 一个进程可以同时对多个客户请求进行服务 复用一个进程(select和epoll)来对多个IO进行服务,在这两个阶段都不会阻塞 可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知应用程序进行相应的读写操作。 IO多路复⽤的实现⽅式有select、poll、Epool select 能够监视⽂件描述符的数量存在最⼤限制 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 select支持的文件描述符数量太小了，默认是1024,所以需要调整配置打开文件最大数等内核参数 Epool 无须遍历整个被侦听的fd集合 给每一个监听对象都绑定一个回调函数 数据准备就绪时，调用回调函数，把就绪fd放入就绪链表中 总结:select 线性轮询 随着连接数增加，性能减少, epell轮询就绪链表,只有准备好的才执行。 select 上洗手间 每个孩子都要过问一次 Epool 想上洗手间的孩子 排好队站在约定好的地方,epoll看有谁来了 就带谁去 CPU亲和(affinity)12将CPU核心和Nginx工作进程绑定方式，把每个worker进程固定在1个cpu上执行，减少切换cpu的切换(cache miss) ，获得更好的性能。 Sendfile 文件传输 nginx在支持了sendfile系统调用后，避免了内核层与用户层的上线文切换（content swith）工作，大大减少了系统性能的开销。 查看系统中上下文切换 vmstat 所以当 Nginx 是一个静态文件服务器的时候，开启 SENDFILE 配置项能大大提高 Nginx 的性能,高效传输。 但是当 Nginx 是作为一个反向代理来使用的时候，SENDFILE 则没什么用了，因为 Nginx 是反向代理的时候。in_fd 就不是文件句柄而是 socket，此时就不符合 sendfile 函数的参数要求了。 1sendfile on Nginx 的使用场景12345671. 静态处理2. 反向代理 - 代理一台3. 负载均衡 - 负载集群4. 资源缓存5. 安全防护 - nginx + lua 6. 访问限制7. 访问认 1234Nginx作为Web服务器的主要应用场景包括：1、使用Nginx运行HTML、JS、CSS、小图片等静态数据（此功能类似Lighttpd软件）2、Nginx结合FastCGI运行PHP等动态程序（例如使用fastcgi_pass方式）3、Nginx结合Tomcat/Resin等支持Java动态程序（常用proxy_pass）方式 Nginx 与 Apache 特点比较12345678apache: - 基于传统的select模型，高并发能力有限。 - 高并发时消耗系统资源相对多一些。 - 支持扩展库，通过DSO、apxs方法编译安装额外的插件功能，不需要重新编译Apachenginx: - 基于epoll模型,高并发能力很强。 - 高并发时消耗系统资源较低。 - 不支持类似Apache的DSO模式，扩展库必须编译进主程序（缺点） 常用的http软件123451. HTTPD -&gt; Apache基金会2. IIS -&gt; 微软3. GWS -&gt; Google4. openrestry -&gt; nginx+lua5. tengline -&gt; 淘宝基于Nginx开 Nginx YUM安装版本选取Mainline version 开发版Stable version 稳定版 # 选取长期维护的稳定版Legacy version 历史版本 启动aliyun主机1234567891011121314151617位置 香港 突发性能实例 t5 # 2U 4G操作系统 # CentOS 7.4公网带宽 # 100M端口开放 # 80、443创建密钥对 # home-leo实例名称 # Ecs_Nginx主机名 # Nginx-node1创建实例 yum 安装稳定版本123456789101112131415161718# 基本组件安装[root@Nginx-node1 ~]# yum install -y gcc gcc-c++ autoconf pcre pcre-devel make automake wget httpd-tools vim tree# 检查防火墙[root@Nginx-node1 ~]# systemctl status firewalld# 检查selinux[root@Nginx-node1 ~]# setenforce 0# 初始化基本目录 [root@Nginx-node1 ~]# mkdir -p /soft/&#123;code,logs,package/src,backup&#125;[root@Nginx-node1 ~]# tree /soft//soft/├── backup├── code├── logs└── package └── src 1234567891011# 配置Nginx官方yum源# 官方提供的yum # 阿里云epel的不是最新稳定版vim /etc/yum.repos.d/nginx.repo[nginx]name=nginx stable repobaseurl=http://nginx.org/packages/centos/7/$basearch/gpgcheck=0enabled=1 12345678# 安装[root@Nginx-node1 ~]# yum install nginx -y[root@Nginx-node1 ~]# nginx -vnginx version: nginx/1.16.1[root@Nginx-node1 ~]# rpm -qa|grep nginxnginx-1.16.1-1.el7.ngx.x86_64 1234567891011121314151617181920212223# 查看安装的依赖[root@Nginx-node1 ~]# nginx -Vnginx version: nginx/1.16.1built by gcc 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) built with OpenSSL 1.0.2k-fips 26 Jan 2017TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt=&apos;-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC&apos; --with-ld-opt=&apos;-Wl,-z,relro -Wl,-z,now -pie&apos; 1234567891011121314151617181920212223242526272829303132333435# 查看安装了那些东西[root@Nginx-node1 ~]# rpm -ql nginx/etc/logrotate.d/nginx/etc/nginx/etc/nginx/conf.d/etc/nginx/conf.d/default.conf/etc/nginx/fastcgi_params/etc/nginx/koi-utf/etc/nginx/koi-win/etc/nginx/mime.types/etc/nginx/modules/etc/nginx/nginx.conf/etc/nginx/scgi_params/etc/nginx/uwsgi_params/etc/nginx/win-utf/etc/sysconfig/nginx/etc/sysconfig/nginx-debug/usr/lib/systemd/system/nginx-debug.service/usr/lib/systemd/system/nginx.service/usr/lib64/nginx/usr/lib64/nginx/modules/usr/libexec/initscripts/legacy-actions/nginx/usr/libexec/initscripts/legacy-actions/nginx/check-reload/usr/libexec/initscripts/legacy-actions/nginx/upgrade/usr/sbin/nginx/usr/sbin/nginx-debug/usr/share/doc/nginx-1.16.1/usr/share/doc/nginx-1.16.1/COPYRIGHT/usr/share/man/man8/nginx.8.gz/usr/share/nginx/usr/share/nginx/html/usr/share/nginx/html/50x.html/usr/share/nginx/html/index.html/var/cache/nginx/var/log/nginx 123456789101112131415# 看配置文件[root@Nginx-node1 ~]# rpm -qc nginx/etc/logrotate.d/nginx/etc/nginx/conf.d/default.conf/etc/nginx/fastcgi_params/etc/nginx/koi-utf/etc/nginx/koi-win/etc/nginx/mime.types/etc/nginx/nginx.conf/etc/nginx/scgi_params/etc/nginx/uwsgi_params/etc/nginx/win-utf/etc/sysconfig/nginx/etc/sysconfig/nginx-debug Nginx 安装目录 Nginx 编译参数1[root@Nginx-node1 ~]# nginx -V Nginx 常用模块 Nginx 内置变量12常用:$remote_addr 记录客户端IP 123456789101112131415161718192021# 启动服务 查看日志和记录格式[root@Nginx-node1 ~]# nginx[root@Nginx-node1 ~]# ps -aux |grep nginxroot 20255 0.0 0.0 46384 976 ? Ss 17:52 0:00 nginx: master process nginxnginx 20256 0.0 0.0 46796 1932 ? S 17:52 0:00 nginx: worker processroot 20258 0.0 0.0 112660 968 pts/0 S+ 17:52 0:00 grep --color=auto nginx# 日志[root@Nginx-node1 ~]# tailf /var/log/nginx/access.log 访问阿里云IP = http://47.56.134.107/[root@Nginx-node1 nginx]# tailf /var/log/nginx/access.log 222.131.240.109 - - [18/Oct/2019:17:58:31 +0800] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&quot; &quot;-&quot;222.131.240.109 - - [18/Oct/2019:17:58:40 +0800] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&quot; &quot;-&quot;# 日志记录格式[root@Nginx-node1 ~]# vim /etc/nginx/nginx.conf log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; Nginx 状态码HTTP状态码（HTTP Status Code）是用以表示网页服务器HTTP响应状态的3位数字代码 1234567891011121314151617301—永久移动。被请求的资源已被永久移动位置；302—请求的资源现在临时从不同的 URI 响应请求；305—使用代理。被请求的资源必须通过指定的代理才能被访问；307—临时跳转。被请求的资源在临时从不同的URL响应请求；400—错误请求；402—需要付款。该状态码是为了将来可能的需求而预留的，用于一些数字货币或者是微支付；403—禁止访问。服务器已经理解请求，但是拒绝执行它；404—找不到对象。请求失败，资源不存在；406—不可接受的。请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体； 123456789101112131415161718192021301—永久移动。被请求的资源已被永久移动位置；302—请求的资源现在临时从不同的 URI 响应请求；305—使用代理。被请求的资源必须通过指定的代理才能被访问；307—临时跳转。被请求的资源在临时从不同的URL响应请求；400—错误请求；402—需要付款。该状态码是为了将来可能的需求而预留的，用于一些数字货币或者是微支付；403—禁止访问。服务器已经理解请求，但是拒绝执行它；404—找不到对象。请求失败，资源不存在；406—不可接受的。请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体502 后端服务没启动504 请求成功 但是响应超时 Nginx 主配置文件描述 Nginx主配置文件nginx.conf是一个纯文本类型的文件（其他配置文件大多也是如此），它位于nginx安装目录下的conf目录中，整个配置文件是以区块的形式组织的。一般，每个区块以一个大括号{}来表示，区块可以分为几个层次。 12341.Main位于 nginx.conf配置文件最高层2.Main层下可以有 Event、HTTP 层3.HTTP层下⾯有允许有多个 Server 层, 用于对不同的网站做不同的配置4.Server层也允许有多个 Location , Nginx整个配置文件nginx.conf的主体框架为： Nginx配置文件 基本配置1234567891011121314151617181920212223242526272829303132333435[root@Nginx-node1 conf.d]# vim /etc/nginx/nginx.confuser nginx;worker_processes 1; # 工作进程,配置和CPU个数一致error_log /var/log/nginx/error.log warn; # 错误日志 路径 级别 一般常用的是errorpid /var/run/nginx.pid; # Nginx服务启动时的pidevents &#123; worker_connections 1024; # 每个work进程的最大连接数 一般配置 10000个 use epool;&#125;http &#123; # 公共配置定义在http里面 , server的外面 include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@Nginx-node1 conf.d]# vim /etc/nginx/conf.d/default.conf# 每个虚拟主机使用1个server&#123;&#125;server &#123; listen 80; # 监听端口 默认80 server_name localhost; # 提供服务的域名或者主机名 #charset koi8-r; #access_log /var/log/nginx/host.access.log main; # location 控制网站访问路径 location / &#123; root /usr/share/nginx/html; # 存放的网站路径 index index.html index.htm; # 默认访问首页文件 &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # # 指定错误代码,统一定义错误页面,错误代码重定向到新的Locaiton error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125;&#125; 部署一个 静态小游戏 端口还是使用80 使用hosts文件解析一个域名访问 增加一个server虚拟主机 12345# 上传游戏目录 解压[root@Nginx-node1 conf.d]# mkdir -p /data/www/xiaoniao[root@Nginx-node1 xiaoniao]# unzip xiaoniaofeifei.zip [root@Nginx-node1 xiaoniao]# ls2000.png 21.js icon.png img index.html sound1.mp3 12345678910111213141516171819202122232425# 新增xiaoniao.conf配置文件[root@Nginx-node1 xiaoniao]# cd /etc/nginx/conf.d/[root@Nginx-node1 conf.d]# vim xiaoniao.conf server &#123; listen 80; server_name www.xiaoniao.com; root /data/www/xiaoniao; index index.html index.htm; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125;# 修改本机的hosts文件C:\Windows\System32\drivers\etc\hosts47.56.134.107 www.xiaoniao.com# 本地访问www.xiaoniao.com]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Saltstatk 部署 Nginx]]></title>
    <url>%2F2019%2F10%2F18%2Fsaltstack-base04%2F</url>
    <content type="text"><![CDATA[Saltstatk 部署 nginx创建nginx部署目录12# 1. 先完成基础软件安装 # 2. 增加优化配置文件]]></content>
      <categories>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>salt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 SaltStack CentOS7 初始化]]></title>
    <url>%2F2019%2F10%2F17%2Fsaltstack-base03%2F</url>
    <content type="text"><![CDATA[初始化实现目录1234567891011[root@salt prod]# mkdir -p /srv/salt/prod/init/files[root@salt prod]# tree /srv/salt/prod//srv/salt/prod/├── init│ └── files└── minions ├── files │ ├── minion │ └── salt-repo-latest.el7.noarch.rpm └── minions-install.sls 初始化服务配置 yum源1234567891011121314151617[root@salt init]# vim yum-repo.sls yum-CentOS: file.managed: - name: /etc/yum.repos.d/CentOS-Base.repo - source: salt://init/files/CentOS-Base.repo - user: root - group: root - mode: 644yum-epel: file.managed: - name: /etc/yum.repos.d/epel.repo - source: salt://init/files/epel.repo - user: root - group: root - mode: 644 关闭 SELinux1234567891011[root@salt init]# vi selinux.sls close-selinux: file.managed: - name: /etc/selinux/config - source: salt://init/files/selinux-config - user: root - group: root - mode: 644 cmd.run: - name: setenforce 0 || echo ok 关闭 防火墙123456[root@salt init]# vim stop-firewalld.sls firewalld-stop: service.dead: - name: firewalld.service - enable: False 时钟同步1234567891011[root@salt init]# vim ntp-client.sls install-ntpdate: pkg.installed: - name: ntpdatecron-ntpdate: cron.present: - name: ntpdate ntp1.aliyun.com - user: root - minute: &apos;*/5&apos; CentOS7的最大打开文件数1234567891011121314151617181920212223242526272829303132[root@salt init]# cat centos7-openfile.sls centos7-openfile: file.managed: - name: /etc/systemd/system.conf - source: salt://init/files/system.conf - user: root - group: root - mode: 644# 全局有效 需要重启DefaultLimitNOFILE=65535DefaultLimitNPROC=65535# 查看ulimit -Sn 查看的是软限制ulimit -Hn 查看的是硬限制# 针对单个Service，也可以设置，以nginx为例。编辑/usr/lib/systemd/system/nginx.service文件，或者/usr/lib/systemd/system/nginx.service.d/my-limit.conf文件，做如下配置：[Service]LimitCORE=infinityLimitNOFILE=100000LimitNPROC=100000# 然后运行如下命令，才能生效。sudo systemctl daemon-reloadsudo systemctl restart nginx.service# 查看一个进程的limit设置：cat/proc/YOUR-PID/limits 优化系统内核12345678910[root@salt init]# vim sysctl.sls # 优化随机端口个数net.ipv4.ip_local_port_range: sysctl.present: - value: 4000 65000# 优化TIME_WAITnet.ipv4.tcp_tw_reuse: sysctl.present: - value: 1 历史记录优化histroy（记录时间，用户）1234567[root@salt init]# vim history.sls histroy-init: file.append: - name: /etc/profile - text: - export HISTTIMEFORMAT=&quot;%F %T `whoami` &quot; 基础用户12345678910111213[root@salt init]# vim user-www.sls www-user-group: group.present: - name: www - gid: 1000 user.present: - name: www - fullname: www - shell: /sbin/bash - uid: 1000 - gid: 1000 常用基础命令1234567891011121314151617181920212223242526[root@salt init]# vim pkg-base.sls include: - init.yum-repobase-install: pkg.installed: - pkgs: - lrzsz - tree - openssl - telnet - iftop - iotop - sysstat - wget - dos2unix - lsof - net-tools - mtr - unzip - zip - vim - bind-utils - require: - file: yum-CentOS 1234567891011[root@salt init]# vim pkg-init.sls pkg-init: pkg.installed: - names: - gcc - gcc-c++ - glibc - openssl - openssl-devel - pcre-devel 入口 和 目录结构12345678910111213[root@salt init]# vim all-init.sls include: - init.yum-repo - init.stop-selinux - init.stop-firewalld - init.ntp-client - init.centos7-openfile - init.sysctl - init.history - init.user-www - init.pkg-base - init.pkg-init 12345678910111213141516171819202122232425[root@salt init]# tree /srv/salt/prod//srv/salt/prod/├── init│ ├── all-init.sls│ ├── centos7-openfile.sls│ ├── files│ │ ├── CentOS-Base.repo│ │ ├── epel.repo│ │ ├── limits.conf│ │ ├── selinux-config│ │ └── system.conf│ ├── history.sls│ ├── ntp-client.sls│ ├── pkg-base.sls│ ├── pkg-init.sls│ ├── stop-firewalld.sls│ ├── stop-selinux.sls│ ├── sysctl.sls│ ├── user-www.sls│ └── yum-repo.sls└── minions ├── files │ ├── minion │ └── salt-repo-latest.el7.noarch.rpm └── minions-install.sls 12345[root@salt minions]# salt-ssh &apos;liunx-node2&apos; -i state.sls minions.minions-install saltenv=prod[root@salt minions]# salt-key[root@salt minions]# salt-key -A[root@salt minions]# salt &apos;*&apos; cmd.run &apos;w&apos;[root@salt minions]# salt &apos;linux-node2&apos; state.sls init.all-init saltenv=prod]]></content>
      <categories>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>salt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 SaltStack-SSH 安装Salt-minion]]></title>
    <url>%2F2019%2F10%2F17%2Fsaltstack-base02%2F</url>
    <content type="text"><![CDATA[为什么这是第一步?当我们来到一个新环境,需要使用配置管理服务器,我们需要先让客户端安装Salt-minion 安装安装 salt-master salt-ssh1234* For RHEL/CentOS 7:yum install https://repo.saltstack.com/yum/redhat/salt-repo-latest.el7.noarch.rpmwget https://repo.saltstack.com/yum/redhat/salt-repo-latest.el7.noarch.rpmyum install salt-master salt-ssh 1234# 启动mastersystemctl enable salt-master systemctl start salt-mastersystemctl status salt-master 修改配置文件1234567891011121314# 修改/etc/salt/roster的配置文件# target的信息 host: # 远端主机的ip地址或者dns域名 user: # 登录的用户 passwd: # 用户密码,如果不使用此选项，则默认使用秘钥方式# 可选的部分 port: # ssh端口 sudo: # 可以通过sudo tty: # 如果设置了sudo，设置这个参数为true priv: # ssh秘钥的文件路径 timeout: # 当建立链接时等待响应时间的秒数 minion_opts: # minion的位置路径 thin_dir: # target系统的存储目录，默认是/tmp/salt-&lt;hash&gt; cmd_umask: # 使用salt-call命令的umask值 12345678910# 定义配置主机: salt-ssh是串行,没有C/S 快liunx-node2: host: 10.0.0.151 user: root password: 222222 port: 22# 测试远程执行命令salt-ssh &apos;*&apos; -r &apos;ip a&apos;salt-ssh &apos;liunx-node2&apos; -r &apos;free -m&apos; 创建状态配置文件目录 告诉 master 状态文件放在哪个位置 1234567891011121314# 编辑master配置文件# 搜索: /file_roots # base基础环境 dev开发环境 test测试环境 prod生产环境[root@linux-node1 ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base dev: - /srv/salt/dev test: - /srv/salt/test prod: - /srv/salt/prod 123456789101112# 创建目录mkdir -p /srv/salt/&#123;base,dev,test,prod&#125;tree /srv/salt//srv/salt/├── base # 必须有├── dev # 开发环境├── prod # 生产环境└── test # 测试环境# 重启master,改完配置就要重启systemctl restart salt-mastersystemctl status salt-master 创建 minions 目录1234567mkdir -p /srv/salt/prod/minions mkdir -p /srv/salt/prod/minions/files # minion 文件目录[root@salt yum.repos.d]# tree /srv/salt/prod//srv/salt/prod/└── minions # minion 状态目录 └── files # minion 文件目录 编写 minion-install.sls123456789101112131415161718192021222324252627282930313233343536373839404142434445cd /srv/salt/prod/minions/vim minions-install.slsinclude: - init.yum-repominions-init: file.managed: - name: /etc/yum.repos.d/salt-repo-latest.el7.noarch.rpm - source: salt://minions/files/salt-repo-latest.el7.noarch.rpm - user: root - group: root - mode: 644 cmd.run: - name: cd /etc/yum.repos.d/ &amp;&amp; rpm -ivh salt-repo-latest.el7.noarch.rpm - require: - file: minions-init - unless: rpm -qa|grep salt-repominions-install: pkg.installed: - name: salt-minion - require: - cmd: minions-init - unless: rpm -qa|grep salt-minion file.managed: - name: /etc/salt/minion - source: salt://minions/files/minion - user: root - group: root - mode: 644 - template: jinja - defaults: MASTER_IP: 10.0.0.150 - require: - pkg: minions-installminions_service: service.running: - name: salt-minion - enable: True - require: - file: minions-install 文件管理123456[root@salt files]# tree /srv/salt/prod/minions//srv/salt/prod/minions/├── files│ ├── minion│ └── salt-repo-latest.el7.noarch.rpm└── minions-install.sls 123# 模板文件[root@salt files]# vim minion master: &#123;&#123; MASTER_IP &#125;&#125; 执行12[root@salt minions]# salt-ssh &apos;liunx-node2&apos; -i state.sls minions.minions-install saltenv=prod test=True[root@salt minions]# salt-ssh &apos;liunx-node2&apos; -i state.sls minions.minions-install saltenv=prod 添加minion 和 测试12345678910111213141516171819202122232425262728293031# 查看[root@salt minions]# salt-keyAccepted Keys:Denied Keys:Unaccepted Keys:linux-node2Rejected Keys:# 添加[root@salt minions]# salt-key -AThe following keys are going to be accepted:Unaccepted Keys:linux-node2Proceed? [n/Y] yKey for minion linux-node2 accepted.[root@salt minions]# salt-keyAccepted Keys:linux-node2Denied Keys:Unaccepted Keys:Rejected Keys:# 远程执行命令测试[root@salt minions]# salt &apos;*&apos; cmd.run &apos;w&apos;linux-node2: 15:33:23 up 9 min, 3 users, load average: 0.13, 0.13, 0.06 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root tty1 11:04 4:28m 0.20s 0.20s -bash root pts/0 10.0.0.1 15:30 3:04 0.00s 0.00s -bash root pts/1 10.0.0.1 11:06 4:26m 0.01s 0.01s -bash 当前的目录结构: 安装salt-minion 下一步 初始化安装 - 包括yum源 和 初始化命令vim wget net-tools等… 1234567[root@salt prod]# tree /srv/salt/prod//srv/salt/prod/└── minions ├── files │ ├── minion │ └── salt-repo-latest.el7.noarch.rpm └── minions-install.sls]]></content>
      <categories>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>salt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix_base06]]></title>
    <url>%2F2019%2F10%2F16%2Fzabbix-base06%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[05 zabbix JMX监控]]></title>
    <url>%2F2019%2F10%2F16%2Fzabbix-base05%2F</url>
    <content type="text"><![CDATA[JMX监控数据 Zabbix中，JMX监控数据的获取由专门的代理程序来实现,即Zabbix-Java-Gateway来负责数据的采集，Zabbix-Java-Gateway和JMX的Java程序之间通信获取数据。 JMX在Zabbix中的运行流程123451.Zabbix-Server找Zabbix-Java-Gateway获取Java数据2.Zabbix-Java-Gateway找Java程序(zabbix-agent)获取数据3.Java程序返回数据给Zabbix-Java-Gateway4.Zabbix-Java-Gateway返回数据给Zabbix-Server5.Zabbix-Server进行数据展示 配置JMX监控的步骤123451.安装Zabbix-Java-Gateway。2.配置zabbix_java_gateway.conf参数。3.配置zabbix-server.conf参数。4.Tomcat应用开启JMX协议。5.ZabbixWeb配置JMX监控的Java应用。 部署JMX1[root@linux-node1 ~]# yum install zabbix-java-gateway java-1.8.0-openjdk -y 123### 启动[root@linux-node1 ~]# systemctl start zabbix-java-gateway[root@linux-node1 ~]# netstat -tnlp|grep 10052 修改zabbix-server配置12345[root@linux-node1 ~]# vim /etc/zabbix/zabbix_server.conf JavaGateway=10.0.0.160# java gateway端口,默认端口10052StartJavaPollers=5 # 启动进程轮询java gateway[root@linux-node1 ~]# systemctl restart zabbix-server 配置tomcat123456789101112131415161718192021222324[root@linux-node2 local]# tar -zxvf apache-tomcat-8.5.47.tar.gz [root@linux-node2 src]# mv apache-tomcat-8.5.47 /usr/local/tomcat# 开启tomcat的远程jvm配置文件vim /usr/local/tomcat/bin/catalina.shCATALINA_OPTS=&quot;$CATALINA_OPTS-Dcom.sun.management.jmxremote-Dcom.sun.management.jmxremote.port=12345-Dcom.sun.management.jmxremote.authenticate=false-Dcom.sun.management.jmxremote.ssl=false&quot;# 远程jvm配置文件解释: CATALINA_OPTS=&quot;$CATALINA_OPTS -Dcom.sun.management.jmxremote # # 启用远程监控JMX -Dcom.sun.management.jmxremote.port=12345 #jmx远程端口,Zabbix添加时必须一致 -Dcom.sun.management.jmxremote.authenticate=false #不开启用户密码认证 -Dcom.sun.management.jmxremote.ssl=false # -Djava.rmi.server.hostname=192.168.90.11&quot; #运行tomcat服务IP(不要填写错了) [root@linux-node2 bin]# sh shutdown.sh[root@linux-node2 bin]# sh startup.sh[root@linux-node2 bin]# netstat -tnlp|grep 12345tcp6 0 0 :::12345 :::* LISTEN 9392/java Web页面配置]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 zabbix proxy]]></title>
    <url>%2F2019%2F10%2F16%2Fzabbix-base04%2F</url>
    <content type="text"><![CDATA[zabbix-proxy 分布式监控 Zabbix Proxy是一个类似于代理的服务，可以代替Zabbix-server获取 zabbix-agent信息。 其中数据存到本地（Proxy有自己的数据库）然后在发送给Server，这样可以保证数据不丢失 使用场景 常用于多机房情况或者监控主机特别多，几千台左右。这时候使用Zabbix Proxy 可以减轻服务器server的压力，还可以减轻Zabbix的维护。 最常用的特点是适用于多机房、网络不稳定的时候，因为如果直接由Zabbix-server发送信息可能agent没有收到，但是直接使用Zabbix-Proxy就不会遇到这个问题。 特点 没有Web界面 本身不做任何告警通知（告警通知都是Server做） 小结： Zabbix Proxy 可以有多个，用来代理Zabbix server来运行。Proxy会将所有数据暂存于本地,然后同一转发到Zabbix Server上 Proxy只需要一条TCP链接，可以连接到Zabbix-server上即可。所以防火墙只需要添加一条Zabbix Proxy即可 我们可以参考上面的Zabbix Proxy图 Proxy是需要使用单独的数据库，所以不能将Server和Agent放在一起 安装部署yum安装12yum install -y zabbix-proxy zabbix-proxy-mysql mariadb-server mariadbrpm -qa|grep zabbix-proxy 修改配置文件123456789101112131415161718192021222324252627282930313233343536proxy是连接server的vim /etc/zabbix/zabbix_proxy.conf # ProxyMode=0 ### 默认是主动模式 proxy会主动连接serverServer=10.0.0.160Hostname=linux-node2.example.comDBHost=10.0.0.161DBUser=zabbix_proxyDBPassword=zabbix_prox# 配置文件中没有配置的内容如下：（有需要可以配置）# ProxyLocalBuffer=0# 数据保留的时间（小时为单位）# ProxyOfflineBuffer=1# 连不上Server，数据要保留多久（小时为单位，默认1小时）# DataSenderFrequency=1# 数据的发送时间间隔（默认是1秒）# StartPollers=5# 启动的线程数# StartIPMIPollers=0# 启动IPMI的线程数grep &apos;^[a-Z]&apos; /etc/zabbix/zabbix_proxy.conf Server=10.0.0.160Hostname=linux-node2.example.comLogFile=/var/log/zabbix/zabbix_proxy.logLogFileSize=0PidFile=/var/run/zabbix/zabbix_proxy.pidDBHost=10.0.0.161DBName=zabbix_proxyDBUser=zabbix_proxyDBPassword=zabbix_proxySNMPTrapperFile=/var/log/snmptrap/snmptrap.logTimeout=4ExternalScripts=/usr/lib/zabbix/externalscriptsLogSlowQueries=300 启动数据库123456789101112# 启动systemctl enable mariadbsystemctl start mariadbmysql_secure_installation mysql -uroot -p123456# 创建数据库create database zabbix_proxy character set utf8 collate utf8_bin;# 用户授权grant all on zabbix_proxy.* to zabbix_proxy@localhost identified by &apos;zabbix_proxy&apos;; grant all on zabbix_proxy.* to zabbix_proxy@&apos;linux-node2.example.com&apos; identified by &apos;zabbix_proxy&apos; 初始化数据123456789rpm -ql|grep zabbix-proxy-mysqlcd /usr/share/doc/zabbix-proxy-mysql-3.0.28/zcat schema.sql.gz | mysql -h 10.0.0.161 -uzabbix_proxy -pzabbix_proxy zabbix_proxy# 检查mysql -h 10.0.0.161 -uzabbix_proxy -pzabbix_proxyuse zabbix_proxyshow tables; 启动123# Zabbix-proxy 监控10051端口，因为是代理就必须跟Server的端口相同，对于Agent Proxy就是Serversystemctl start zabbix-proxynetstat -tnlp|grep 10051 Zabbix Web 配置proxy管理-agent代理程序-创建代理 修改node2上的agent配置123456789# 主动和被动都是 proxy的IPServer=10.0.0.161 ServerActive=10.0.0.161Hostname=linux-node2.example.com# 重启服务[root@linux-node2 zabbix-proxy-mysql-3.0.28]# systemctl restart zabbix-agent[root@linux-node2 ~]# tailf /var/log/zabbix/zabbix_proxy.log]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 zabbix 主动模式]]></title>
    <url>%2F2019%2F10%2F16%2Fzabbix-base03%2F</url>
    <content type="text"><![CDATA[zabbix 模式介绍 被动模式 主动模式 默认是被动模式,在web中的监控项类型中可以用来区分: 主动模式介绍主动模式: zabbix-agent会主动上报server端配置的监控项,无需server轮询,有效的降低server的压力 大规模的监控，建议使用主动模式，超过100+或者服务器配置很低。 需要根据队列中有没有延迟主机,如果延迟主机有点多的话，我们就需要将被动模式修改为主动模式. 主动模式设置修改zabbix-agent配置文件1234567891011vim /etc/zabbix/zabbix_agentd.conf# 只修改这两项 主动和被动会同时存在# server IPServerActive=10.0.0.160# 本机的主机名 和 监控项主机名一致，否则会去读取监控项名称Hostname=linux-node1.example.com# 主动无需监听10050端口，它是在被动模式下使用# RefreshActiveChecks=120 agent每120秒刷新一次检测列表,所以server增加监控项后不会很快生效 主动监控web配置 默认是没有主动监控模板,我们可以clone一个，选中想要的模板,点击全部克隆 取消掉链接的模板,保存 修改监控项为主动模式,进入监控项,全选,批量更新 将类型修改成 主动模式客户端,保存 将模板给对应的node,将原有的被动模式模板取消,添加最新的主动模式监控模板]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 zabbix 基础使用]]></title>
    <url>%2F2019%2F10%2F14%2Fzabbix-base02%2F</url>
    <content type="text"><![CDATA[安装 zabbix-agentsalt 状态管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@linux-node1 init]# vim zabbix-yum-repo.sls zabbix-repo: cmd.run: - name: rpm -Uvh https://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm[root@linux-node1 zabbix]# vim zabbix-agent.sls include: - init.zabbix-yum-repozabbix-agent: pkg.installed: - name: zabbix-agent - require: - cmd: zabbix-repo file.managed: - name: /etc/zabbix/zabbix_agentd.conf - source: salt://zabbix/files/zabbix_agentd.conf - user: root - group: root - mode: 644 - template: jinja - defaults: SERVER_IP: 10.0.0.160 AGENT_HOSTNAME: &#123;&#123; grains[&apos;fqdn&apos;] &#125;&#125; - require: - pkg: zabbix-agent service.running: - name: zabbix-agent - enable: True - watch: - pkg: zabbix-agent - file: zabbix-agentzabbix_agent.conf.d: file.directory: - name: /etc/zabbix_agent.conf.d - wacth: - service: zabbix-agent - require: - file: zabbix-agent - pkg: zabbix-agent yum 安装123456789101112131415rpm -Uvh https://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm[root@linux-node1 data]# yum install -y zabbix-agent# 修改配置文件[root@linux-node1 zabbix]# vim /etc/zabbix/zabbix_agentd.conf # zabbix serverServer=10.0.0.160 # 主机名Hostname=linux-node3.example.comInclude=/etc/zabbix/zabbix_agentd.d/UnsafeUserParameters=0 1234567891011121314# 启动agent服务[root@linux-node1 zabbix]# systemctl restart zabbix-agent[root@linux-node1 zabbix]# systemctl status zabbix-agent[root@linux-node1 zabbix]# netstat -tnlp|grep 10050[root@linux-node1 zabbix]# grep &apos;^[a-Z]&apos; /etc/zabbix/zabbix_agentd.conf PidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.logLogFileSize=0Server=10.0.0.160ServerActive=127.0.0.1Hostname=linux-node3.example.comInclude=/etc/zabbix/zabbix_agentd.d/UnsafeUserParameters=0 添加监控主机及设置web添加主机1234567web-配置-主机-创建主机1. 主机名2. 显示的名称3. 创建了新的群组4. agent服务器IP5. 描述6. 勾选已启动 查看最新数据1可以过滤条件查看服务器监控数据 zabbix 更换字符集12345678910新安装的zabbix-server会存在中文字符集显示问题,需要更换字符集1. 开始 --&gt; 控制面板 --&gt; 字体 --&gt; 微软雅黑 --&gt; 复制到桌面会有两个文件:msyh msyhbd(粗体)2. 更换字符集 先备份cd /usr/share/fonts/dejavucp DejaVuSans.ttf DejaVuSans.ttf_bak3. 上传 msyh.ttf 覆盖原字符集文件mv msyh.ttf DejaVuSans.ttf 配置邮件告警1234管理-报警媒介类型-Email1. QQ邮箱开启SNMP和一个授权码，填写发件人密码时需要设置授权码为密码 2. 选择用户的报警媒介,添加Email报警并设置好邮箱地址3. 开启邮件报警事件 自定义监控监控nginx 80页面是否存活12345678# 在node3 安装个 nginx并启动[root@linux-node1 zabbix]# yum install -y nginx[root@linux-node1 zabbix]# systemctl start nginx# 通过curl命令访问80查看是否返回200# 能获取到 说明配置成功[root@linux-node1 zabbix]# curl --head -s 10.0.0.162|grep &apos;200 OK&apos;|wc -l1 检查agent配置文件1234567891011[root@linux-node1 zabbix]# vim /etc/zabbix/zabbix_agentd.conf # 建议server 和 agent都打开1. 必须打开引入Include 自定义配置 Include=/etc/zabbix/zabbix_agentd.d/2. 不允许特殊字符UnsafeUserParameters=1# Format: UserParameter=&lt;key&gt;,&lt;shell command&gt;systemctl restart zabbix-agent 自定义一个 key 写到agent上12345678[root@linux-node1 zabbix_agentd.d]# vim zabbix-nginx.conf# Format: UserParameter=&lt;key&gt;,&lt;shell command&gt;# 执行的结果就是key值UserParameter=nginx-alive,curl --head -s 10.0.0.162 |grep &apos;200 OK&apos;| wc -l# 重启agentsystemctl restart zabbix-agent zabbix-server get 测试12345# 在server端 模拟发送数据 看有没有返回值[root@linux-node1 zabbix]# yum install zabbix-get -y[root@linux-node1 zabbix]# zabbix_get -s 10.0.0.162 -k nginx-alive1 页面配置监控项12345678910111213141516171819配置-主机-监控项-创建监控项# 可启用的状态为正常名称:建议和key一样健值:一定要和key一样Data type：数据类型，这里我们选择Decimal。其他的基本上用不上 Units：单位 超过1千就写成1k了。 可以在这里做一个单位的设置。默认就可以 Use custom multiplier：如果这里面设置了一个数，得出来的结果都需要乘以文本框设定的值 # 要看监控项是干嘛的 数据采集就不需要这么频繁 越频繁 压力越大Update interval（in sec） 监控项刷新时间间隔（一般不要低于60秒） 通常时间60秒 机器多的话 1-5分钟Custom intervals 创建时间间隔（例如：1点-7点每隔多少秒进行监控）格式大致为：周，时，分 History storage period 历史数据存储时间（根据业务来设置，默认就可以） Trend storage period 趋势图要保存多久 New application 监控项的组 application 选择一个监控项组 Populates host inventory field 资产，可以设定一个监控项。把获取的值设置在资产上面 添加触发器123配置-主机-触发器-创建触发器&#123;linux-node2.example.com:nginx-alive.last(0)&#125;=0 &#123;主机名:健值.last(0)&#125; =0 # (最后一次 = 0) 关闭nginx服务 查看邮件告警1[root@linux-node1 zabbix_agentd.d]# systemctl stop nginx 12# 恢复服务[root@linux-node1 zabbix_agentd.d]# systemctl start nginx 自定义监控项流程 小总结1234567891、添加用户自定义参数（在/etc/zabbix/zabbix.agent.d/定义了一个nginx.conf步骤如上） 2、重启zabbix-agent3、在Server端使用zabbix_get测试获取（命令如上） 4、在web界面创建item（监控项）5、在web界面创建触发器 自定义模板12上一章节增加了一个自定义监控和触发器,那么要是监控nginx多项数据难道要一个一个添加么?需要使用自定义模板 监控 nginx 状态123456789101112131415161718# 增加nginx_status server[root@linux-node1 zabbix_agentd.d]# vim /etc/nginx/nginx.confserver&#123; server_name 127.0.0.1; location /nginx_status &#123; stub_status on; access_log off; allow 127.0.0.1; deny all; &#125;&#125;[root@linux-node3 zabbix_agentd.d]# systemctl reload nginx[root@linux-node3 ~]# curl http://127.0.0.1/nginx_statusActive connections: 1 server accepts handled requests 10 10 10 Reading: 0 Writing: 1 Waiting: 0 Nginx状态解释1234567891011Active : # Nginx 正处理的 活动链接数 accepts : # Nginx启动到现在共 处理了多少个连接。handled : # Nginx启动到现在共 成功创建多少次握手。requests : # Nginx总共 处理了多少次请求。Reading : # 读取客户端的连接数Writing : # 响应数据到客户端的数量Waiting : # Nginx已经处理完正在等候下一次请求指令的驻留链接Keep-alive的情况下，这个值等于active-（reading + writing）请求丢失数=(握手数-连接数)可以看出,本次状态显示没有丢失请求。 编写nginx监控脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@linux-node3 zabbix]# vim nginx_status_plugin.sh #/bin/bash NGINX_PORT=$1NGINX_COMMAND=$2nginx_active()&#123; # $NF 表示的最后一个Field（列），即输出最后一个字段的内容; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | grep &apos;Active&apos; | awk &apos;&#123;print $NF&#125;&apos;&#125;nginx_reading()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | grep &apos;Reading&apos; | awk &apos;&#123;print $2&#125;&apos;&#125;nginx_writing()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | grep &apos;Reading&apos; | awk &apos;&#123;print $4&#125;&apos;&#125;nginx_waiting()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | grep &apos;Reading&apos; | awk &apos;&#123;print $6&#125;&apos;&#125;# accepts handled requestsnginx_accepts()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | awk &apos;NR==3 &#123;print $1&#125;&apos;&#125;nginx_handled()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | awk &apos;NR==3 &#123;print $2&#125;&apos;&#125;nginx_requests()&#123; /usr/bin/curl &quot;127.0.0.1:&quot;$NGINX_PORT&quot;/nginx_status/&quot; 2&gt;/dev/null | awk &apos;NR==3 &#123;print $3&#125;&apos;&#125;main()&#123; case $2 in active) nginx_active; ;; reading) nginx_reading; ;; writing) nginx_writing; ;; waiting) nginx_waiting; ;; accepts) nginx_accepts; ;; handled) nginx_handled; ;; requests) nginx_requests; ;; *) echo $&quot;USAGE:port &#123;active|reading|writing|waiting|accepts|handled|requests&#125;&quot; esac&#125;main $1 $2 123# 权限控制chmod +x nginx_status_plugin.sh chown -R zabbix:zabbix nginx_status_plugin.sh 添加到nginx监控文件中12345678[root@linux-node3 zabbix_agentd.d]# cd /etc/zabbix/zabbix_agentd.d[root@linux-node3 zabbix_agentd.d]# vim zabbix-nginx.conf # Format: UserParameter=&lt;key&gt;,&lt;shell command&gt;UserParameter=nginx-alive,curl --head -s 10.0.0.162 |grep &apos;200 OK&apos;| wc -lUserParameter=nginx-status[*],/etc/zabbix/nginx_status_plugin.sh &quot;$1&quot; &quot;$2&quot;# 重启zabbix-agent 测试123456789# 测试 [参数1,参数2][root@linux-node1 dejavu]# zabbix_get -s 10.0.0.162 -k nginx-status[80,accepts][root@linux-node1 zabbix]# zabbix_get -s 10.0.0.162 -k nginx-status[80]USAGE:port &#123;active|reading|writing|waiting|accepts|handled|requests&#125;[root@linux-node1 zabbix]# zabbix_get -s 10.0.0.162 -k nginx-status[80,active]1[root@linux-node1 zabbix]# zabbix_get -s 10.0.0.162 -k nginx-status[80,handled]27 创建 nginx 状态监控模板1231. 配置 模板 创建模板 Template Game-Nginx Status2. 模板 创建应用集 Game Nginx Status3. 监控项 创建监控项 复制多个 创建自定义图形 监控主机增加新模板 使用ab测试工具进行测试，设置10000并发进行访问1ab -c 100 -n 10000 http://10.0.0.162/nginx-status 自定义聚合图形1聚合图形-创建聚合图形 自定义幻灯片1聚合图形-创建幻灯片播放]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 zabbix yum 安装]]></title>
    <url>%2F2019%2F10%2F14%2Fzabbix-base01%2F</url>
    <content type="text"><![CDATA[1234参考:https://www.zabbix.com/documentation/3.4/zh/manual/installation/install_from_packageshttps://www.zabbix.com/download?zabbix=3.0&amp;os_distribution=centos&amp;os_version=7&amp;db=mysqlhttps://www.cnblogs.com/luoahong/articles/10557723.html 环境准备修改时区12timedatectl set-timezone Asia/Shanghaitimedatectl status 同步时间123456yum install ntpdate -yntpdate tiger.sina.com.cnping tiger.sina.com.cn crontab -e0 3 * * * /usr/sbin/ntpdate -s tiger.sina.com.cn 常用工具安装12345cd /etc/yum.repos.dmkdir bak;mv *.repo bak/curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoyum install -y net-tools vim wget lrzsz tree screen lsof tcpdump 关闭防火墙和selinux12systemctl disable firewalldsystemctl stop firewalld 关闭selinux 123sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/configsetenforce 0getenforce 搭建运行环境安装所需软件1yum install -y httpd mariadb-server mariadb php php-mysql php-gd libjpeg* php-ldap php-odbc php-pear php-xml php-xmlrpc php-mhash 1rpm -qa httpd php mariadb #安装完成后检查应用版本 编辑httpd1234vi /etc/httpd/conf/httpd.conf ServerName www.zabbix.com #修改为主机名DirectoryIndex index.html index.php # 添加首页支持格式 编辑配置php，配置中国时区12vim /etc/php.inidate.timezone = PRC # 配置时区 启动httpd，mysqld123456789#启动并加入开机自启动httpdsystemctl start httpd systemctl enable httpdsystemctl status httpd#启动并加入开机自启动mariadb systemctl start mariadb systemctl enable mariadbsystemctl status mariadb 创建一个测试页，测试LAMP是否搭建成功123456# 创建一个测试页，并编辑vim /var/www/html/index.php &lt;?phpphpinfo()?&gt; 初始化mysql数据库，并配置root用户密码1234567891011121314151617181920212223242526# 设置数据库root密码mysqladmin -u root password 123456 # root用户登陆数据库mysql -u root -p # 创建zabbix数据库（中文编码格式） CREATE DATABASE zabbix character set utf8 collate utf8_bin; # 有空用户名称占用导致本地无法登录远程可登录select user,host from mysql.user; # 删除空用户drop user &apos;&apos;@localhost; # 授予zabbix用户zabbix数据库的所有权限，密码123456GRANT all ON zabbix.* TO &apos;zabbix&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;; GRANT all ON zabbix.* TO &apos;zabbix&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;123456&apos;; GRANT all ON zabbix.* TO &apos;zabbix&apos;@&apos;linux-node2.example.com&apos; IDENTIFIED BY &apos;123456&apos;; # 刷新权限flush privileges; exit# 测试登录mysql -uzabbix -p123456 测试mysql与php连接1234567[root@ fonts]# vim /var/www/html/server.php&lt;?php$link=mysql_connect(&apos;10.0.0.161&apos;,&apos;zabbix&apos;,&apos;123456&apos;);if($link) echo &quot;&lt;h1&gt;Success!!&lt;/h1&gt;&quot;; #显示Success表示连接数据库成功 else echo &quot;Fail!!&quot;;mysql_close();?&gt; 安装zabbix安装依赖包 + 组件12# 依赖包yum -y install net-snmp net-snmp-devel curl curl-devel libxml2 libxml2-devel libevent-devel.x86_64 javacc.noarch javacc-javadoc.noarch javacc-maven-plugin.noarch javacc* 12345678# 组件# 安装php支持zabbix组件yum install php-bcmath php-mbstring -y # 选择了官方源# 安装zabbix组件rpm -Uvh https://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpmyum -y install zabbix-server-mysql zabbix-web-mysql zabbix-agent 导入数据 到数据库zabbix12zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbixEnter password: 配置数据库和用户名密码12vim /etc/zabbix/zabbix_server.conf DBPassword=password 修改时区12vim /etc/httpd/conf.d/zabbix.confphp_value date.timezone Asia/Shanghai 启动服务123456# 启动并加入开机自启动zabbix-serversystemctl restart zabbix-server zabbix-agent httpdsystemctl enable zabbix-server zabbix-agent httpd# 端口检查 10051netstat -anpt | grep zabbix web界面安装zabbixchrome浏览器 打开1http://10.0.0.161/zabbix/ 菜单栏中文 将zabbix server 加入到监控]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 kubernetes 简介]]></title>
    <url>%2F2019%2F10%2F13%2Fkubernetes-base01%2F</url>
    <content type="text"><![CDATA[12345678# 参考和学习资料http://k8s.unixhot.com/kubernetes/kubernetes-introduce.htmlhttps://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/https://jimmysong.io/kubernetes-handbook/concepts/https://github.com/opsnull/follow-me-install-kubernetes-clusterKubernetes官方文档：https://kubernetes.io/docs/home/Kuernetes Github：https://github.com/kubernetes/]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09 Dockerfile 私有仓库 harbor]]></title>
    <url>%2F2019%2F10%2F12%2Fdocker-harbor%2F</url>
    <content type="text"><![CDATA[Harbor 简介真正生产环境，我们无法有效的管理Docker Regisry。官方提供了收费版的Registry，社区有开源版本的Harbor。 Harbor简介:Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。 Harbor 特性 基于角色的访问控制 ：用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 镜像复制 ： 镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。 图形化用户界面 ： 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。 AD/LDAP 支持 ： Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。 审计管理 ： 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。 国际化 ： 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。 RESTful API ： RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。 部署简单 ： 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。 Harbor 组件 Proxy：Harbor的registry, UI, token等服务，通过一个前置的反向代理统一接收浏览器、Docker客户端的请求，并将请求转发给后端不同的服务。 Registry： 负责储存Docker镜像，并处理docker push/pull 命令。由于我们要对用户进行访问控制，即不同用户对Docker image有不同的读写权限，Registry会指向一个token服务，强制用户的每次docker pull/push请求都要携带一个合法的token, Registry会通过公钥对token 进行解密验证。 Core services： 这是Harbor的核心功能，主要提供以下服务： UI：提供图形化界面，帮助用户管理registry上的镜像（image）, 并对用户进行授权。 webhook：为了及时获取registry 上image状态变化的情况， 在Registry上配置webhook，把状态变化传递给UI模块。 token 服务：负责根据用户权限给每个docker push/pull命令签发token. Docker 客户端向Regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向Registry进行请求。 Database：为core services提供数据库服务，负责储存用户权限、审计日志、Docker image分组信息等数据。 Job Services：提供镜像远程复制功能，可以把本地镜像同步到其他Harbor实例中。 Log collector：为了帮助监控Harbor运行，负责收集其他组件的log，供日后进行分析。 Harbor安装1https://github.com/goharbor/harbor/releases 离线安装1234567891011121314151617# 解压安装包,修改配置文件[root@linux-node2 tools]# tar -xf harbor-offline-installer-v1.8.4-rc1.tgz [root@linux-node2 tools]# cd harbor# 修改主机名和管理员密码、数据库密码vim harbor.ymlhostname: 10.0.0.102harbor_admin_password: 123456database: password: 123456 #安装./install.sh[root@linux-node2 harbor]# vim harbor.yml [root@linux-node2 harbor]# ./install.sh 从 1.8.0 后，harbor配置文件由原先的 harbor.cfg 改为 harbor.yml hostname: 目标主机的主机名，用于访问Portal和注册表服务。它应该是目标计算机的IP地址或完全限定的域名（FQDN），例如，10.0.0.102或reg.yourdomain.com。不要使用localhost或127.0.0.1作为主机名 - 外部客户端需要访问注册表服务这里修改为我们的主机ip即可。 data_volume： 存储 harbor 数据的位置。这里可以修改为 /usr/local/workspace/harbor/data harbor_admin_password：管理员的初始密码。此密码仅在Harbor首次启动时生效。之后，将忽略此设置，并且应在Portal中设置管理员密码。请注意，默认用户名/密码为admin / Harbor12345。 12345678910111213关于端口配置：http：port：你的http的端口号https：用于访问Portal和令牌/通知服务的协议。如果启用了公证，则必须设置为https。请参阅使用HTTPS访问配置Harbor。port：https的端口号certificate：SSL证书的路径，仅在协议设置为https时应用。private_key：SSL密钥的路径，仅在协议设置为https时应用。 免https使用12345678# 配置免https# 修改 /etc/docker/daemon.json[root@linux-node2 harbor]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://a64eacm1.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;10.0.0.102&quot;]&#125; 1234567891011# 重启docker systemctl daemon-reloadsystemctl restart docker.service# 重启harbor仓库# cd 到 harbor的安装目录cd /data/tools/harbor# 执行命令docker-compose stopdocker-compose up -d 登录web12http://10.0.0.102 admin/123456 上传镜像12345678910# node1 默认docker访问register都是基于https的，需要添加信任[root@linux-node1 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://a64eacm1.mirror.aliyuncs.com&quot;], &quot;data-root&quot;: &quot;/data/docker&quot;, &quot;insecure-registries&quot;: [&quot;10.0.0.102&quot;]&#125;[root@linux-node1 ~]# systemctl daemon-reload[root@linux-node1 ~]# systemctl restart docker.service 123456789101112131415161718192021# 在镜像服务器node1,登录harbor仓库[root@linux-node1 ~]# docker login 10.0.0.102Username: adminPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded# 修改待上传镜像的tag[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/shop-api latest 950dfdaa7360 3 hours ago 365MB[root@linux-node1 ~]# docker tag 950dfdaa7360 10.0.0.102/library/shop-api:v[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/shop-api latest 950dfdaa7360 3 hours ago 365MB10.0.0.102/library/shop-api v1 950dfdaa7360 3 hours ago 365MB... 12345# 上传镜像[root@linux-node1 ~]# docker push 10.0.0.102/library/shop-api:v1# 出现 类似下述信息 表示上传成功v1: digest: sha256:298299afc2a1251dc8cef3e251a361475a9c8b59c2d7309a5f39f3820a6ba6e7 size: 3030 下载镜像 下载该镜像的host 都必须要先登录才行,而且必须有权限 harbor可以创建新用户,做权限管理，访客权限只能下载 12345678910111213141516# 先删除本地的,再登录[root@linux-node1 ~]# docker rmi 10.0.0.102/library/shop-api:v1[root@linux-node1 ~]# docker login 10.0.0.102# 下载镜像# harbor页面上有友好提示哦~[root@linux-node1 ~]# docker pull 10.0.0.102/library/shop-api:v1v1: Pulling from library/shop-apiDigest: sha256:298299afc2a1251dc8cef3e251a361475a9c8b59c2d7309a5f39f3820a6ba6e7Status: Downloaded newer image for 10.0.0.102/library/shop-api:v110.0.0.102/library/shop-api:v1[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE10.0.0.102/library/shop-api v1 950dfdaa7360 3 hours ago 365MB... 1234567891011# 启动个容器测试[root@linux-node1 ~]# docker run -d -p 80:5000 -p 8022:22 --name web1-ssh 10.0.0.102/library/shop-api:v1[root@linux-node1 ~]# ssh 10.0.0.100 -p 8022root@10.0.0.100&apos;s password: [root@a576c0ee04f0 ~]# netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:5000 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 8/sshd tcp6 0 0 :::22 :::* LISTEN 8/sshd Harbor 主从同步 Harbor 还提供双机的 镜像推送功能]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08 Dockerfile 构建镜像]]></title>
    <url>%2F2019%2F10%2F12%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[DockerfileDockerfile 是一个文本文件，记录了镜像构建的所有步骤。 构建镜像 由于官方提供的centos镜像有很多基础命令没有安装，所有我们自己来构建一个基础base 生产环境也可参照这种分层目录 123456789[root@linux-node1 opt]# mkdir -p /opt/Game/&#123;system,app,runtime&#125;[root@linux-node1 opt]# mkdir -p /opt/Game/system/centos[root@linux-node1 opt]# tree Game/Game/ # 项目目录├── app # 程序├── runtime # 运行环境└── system # 基础环境 └── centos # centos的基础环境 使用Dockerfile 创建自定义基础环境镜像 123456789101112131415161718[root@linux-node1 centos]# vim Dockerfile # BASE FROM centos:7# WHOMAINTAINER leo 365042337@qq.com# EPELADD epel.repo /etc/yum.repos.d/# PKGRUN yum install -y wget supervisor tree net-tools sudo vim &amp;&amp; yum clean all[root@linux-node1 centos]# ls -ltotal 8-rw-r--r--. 1 root root 183 Oct 12 10:30 Dockerfile-rw-r--r--. 1 root root 664 Oct 12 10:30 epel.repo docker build 构建镜像 1[root@linux-node1 centos]# docker build -t game/centos:7 . 镜像生成分析: 1234567891011121314151617181920212223[root@linux-node1 centos]# docker build -t game/centos:7 .Sending build context to Docker daemon 3.584kBStep 1/4 : FROM centos:7 ---&gt; 67fa590cfc1cStep 2/4 : MAINTAINER leo 365042337@qq.com ---&gt; Running in c7cd314eb180Removing intermediate container c7cd314eb180 ---&gt; 9023eddfd98cStep 3/4 : ADD epel.repo /etc/yum.repos.d/ ---&gt; fffbe0882202Step 4/4 : RUN yum install -y wget supervisor tree net-tools sudo vim &amp;&amp; yum clean all ---&gt; Running in 17e580d1346d... Complete!Loaded plugins: fastestmirror, ovlCleaning repos: base epel extras updatesCleaning up list of fastest mirrorsRemoving intermediate container 17e580d1346d ---&gt; ea065b022d5bSuccessfully built ea065b022d5bSuccessfully tagged game/centos:7 123456789101112131415161718192021222324251. 运行 docker build 命令，-t 将新镜像命名为 game/centos:7，命令末尾的 . 指明 build context 为当前目录。Docker 默认会从 build context 中查找 Dockerfile 文件，我们也可以通过 -f 参数指定 Dockerfile 的位置。2. 首先 Docker 将 build context 中的所有文件发送给 Docker daemon。build context 为镜像构建提供所需要的文件或目录Dockerfile 中的 ADD、COPY 等命令可以将 build context 中的文件添加到镜像。build context 为当前目录,该目录下的所有文件和子目录都会被发送给 Docker daemon不要将多余文件放到 build context，特别不要把 /、/usr 作为 build context，否则构建过程会相当缓慢甚至失败。3. Step 1：执行 FROM，将 centos:7 作为 base 镜像,镜像 ID 为 67fa590cfc1c。4. Step 2-4 一直执行各种配置, 包括配置文件传送,安装5. 安装成功后，将容器保存为镜像，其 ID 为 ea065b022d5b。这一步底层使用的是类似 docker commit 的命令6. 删除临时容器 17e580d1346d。7. 镜像构建成功。# 查看镜像ID 最后的镜像ID一致[root@linux-node1 centos]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/centos 7 ea065b022d5b 12 minutes ago 287MBcentos 7 67fa590cfc1c 7 weeks ago 202MB 查看镜像分层结构 docker history 会显示镜像的构建历史，也就是 Dockerfile 的执行过程,每一层由上至下排列 game/centos:7 镜像 是经过一层层形成的,他的最初层就是centos:7 注： 表示无法获取 IMAGE ID，通常从 Docker Hub 下载的镜像会有这个问题。 12345678[root@linux-node1 centos]# docker history game/centos:7IMAGE CREATED CREATED BY SIZE COMMENTea065b022d5b 15 minutes ago /bin/sh -c yum install -y wget supervisor tr… 84.7MB fffbe0882202 15 minutes ago /bin/sh -c #(nop) ADD file:4dab183e1e0e545b2… 664B 9023eddfd98c 15 minutes ago /bin/sh -c #(nop) MAINTAINER leo 365042337@… 0B 67fa590cfc1c 7 weeks ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 7 weeks ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 7 weeks ago /bin/sh -c #(nop) ADD file:4e7247c06de9ad117… 202MB Dockerfile 常用指令123456789101112FROM # 指定基础镜像FROM centosMAINTAINER # 指定维护者信息，可以没有RUN # 在命令前面加上RUN即可 RUN yum install httpd -yADD # 复制文件，与COPY不同的是,如果 src 是归档文件（tar, zip, tgz, xz 等），文件会被自动解压到 destWORKDIR # 设置当前工作目录VOLUME # 设置卷，挂载主机目录EXPOSE # 指定对外的端口CMD # 容器启动时执行的命令CMD [“/bin/bash”],只有最后一个生效。CMD 可以被 docker run 之后的参数替换。COPY # 将文件从 build context 复制到镜像。COPY src destENV # 环境变量ENTRYPOINT # 容器启动后执行的命令,只有最后一个生效（无法被替换，CMD 或 docker run 之后的参数会被当做参数传递给 ENTRYPOINT) 运行更多的指令1234567891011121314151617[root@linux-node1 test]# vim Dockerfile # Mybusybox imagesFROM busyboxMAINTAINER leo 365042337@qq.comWORKDIR /data/testRUN touch tmpfile1COPY [&quot;tmpfile2&quot;,&quot;/data/test/&quot;]ADD [&quot;test_add.tar.gz&quot;,&quot;/data/test/&quot;]ENV WELCOME &quot;hello,welcome!&quot; 1234567# 构建前确保 build context 中存在需要的文件。[root@linux-node1 test]# echo &quot;test2&quot; &gt;&gt;tmpfile2[root@linux-node1 test]# ls -ltotal 12-rw-r--r--. 1 root root 211 Oct 12 11:14 Dockerfile-rw-r--r--. 1 root root 113 Oct 12 11:09 test_add.tar.gz-rw-r--r--. 1 root root 6 Oct 12 11:15 tmpfile2 12345678910111213141516171819202122232425262728# 构建镜像[root@linux-node1 test]# docker build -t test/mybusybox:v1 .Sending build context to Docker daemon 4.096kBStep 1/7 : FROM busybox ---&gt; 19485c79a9bbStep 2/7 : MAINTAINER leo 365042337@qq.com ---&gt; Running in 59c22052d1fdRemoving intermediate container 59c22052d1fd ---&gt; 2346b2c44a91Step 3/7 : WORKDIR /data/test ---&gt; Running in c5c969afa0fbRemoving intermediate container c5c969afa0fb ---&gt; 594b22c7a7ecStep 4/7 : RUN touch tmpfile1 ---&gt; Running in a074aa6c9484Removing intermediate container a074aa6c9484 ---&gt; d127444b2188Step 5/7 : COPY [&quot;tmpfile2&quot;,&quot;/data/test/&quot;] ---&gt; 69e842587b9fStep 6/7 : ADD [&quot;test_add.tar.gz&quot;,&quot;/data/test/&quot;] ---&gt; 5d0318483d64Step 7/7 : ENV WELCOME &quot;hello,welcome!&quot; ---&gt; Running in 86c30e33c390Removing intermediate container 86c30e33c390 ---&gt; 9c531ea5e5d5Successfully built 9c531ea5e5d5Successfully tagged test/mybusybox:v1 1234567891011121314# 运行容器，验证镜像内容:[root@linux-node1 test]# docker run -it --rm test/mybusybox:v1/data/test # lstest_add tmpfile1 tmpfile2/data/test # echo $WELCOMEhello,welcome!1. 进入容器，当前目录即为 WORKDIR。如果 WORKDIR 不存在，Docker 会自动为我们创建。2. WORKDIR 中保存了我们希望的文件和目录: - 目录 test_add：由 ADD 指令从 build context 复制的归档文件 test_add.tar.gz，已经自动解压。 - 文件 tmpfile1：由 RUN 指令创建。 - 文件 tmpfile2：由 COPY 指令从 build context 复制。3. ENV 指令定义的环境变量已经生效。 镜像命名的最佳实践 如何在多个 Docker Host 上使用镜像? 123451. 用相同的 Dockerfile 在其他 host 构建镜像。2. 将镜像上传到公共 Registry（比如 Docker Hub），Host 直接下载使用。3. 搭建私有的 Registry 供本地 Host 使用。 为镜像命名 REPOSITORY TAG 实际上一个特定镜像的名字由两部分组成：repository 和 tag , [image name] = [repository]:[tag] 如果执行 docker build 时没有指定 tag，会使用默认值 latest tag 常用于描述镜像的版本信息，可以是任意字符串 latest 其实并没有什么特殊的含义。当没指明镜像 tag 时，Docker 会使用默认值 latest，仅此而已。 虽然 Docker Hub 上很多 repository 将 latest 作为最新稳定版本的别名，但这只是一种约定，而不是强制规定。 所以我们在使用镜像时最好还是避免使用 latest，明确指定某个 tag，比如 httpd:2.3，ubuntu:xenial。 12345[root@linux-node1 test]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest/mybusybox v1 9c531ea5e5d5 7 minutes ago 1.22MBgame/centos 7 ea065b022d5b 51 minutes ago 287MBnginx latest f949e7d76d63 2 weeks ago 126MB 多个 tag 可能对应的是同一个镜像 多个 tag 可能对应的是同一个镜像 假设我们现在发布了一个镜像 myimage，版本为 v1.9.1。那么我们可以给镜像打上四个 tag：1.9.1、1.9、1 和 latest。 12345678# 通过 docker tag 命令方便地给镜像打 tagdocker tag myimage-v1.9.1 myimage:1docker tag myimage-v1.9.1 myimage:1.9docker tag myimage-v1.9.1 myimage:1.9.1docker tag myimage-v1.9.1 myimage:latest 过了一段时间，我们发布了 v1.9.2。这时可以打上 1.9.2 的 tag，并将 1.9、1 和 latest 从 v1.9.1 移到 v1.9.2。 1234567docker tag myimage-v1.9.2 myimage:1docker tag myimage-v1.9.2 myimage:1.9docker tag myimage-v1.9.2 myimage:1.9.2docker tag myimage-v1.9.2 myimage:latest 这种 tag 方案使镜像的版本很直观，用户在选择非常灵活： 1234567myimage:1 始终指向 1 这个分支中最新的镜像。myimage:1.9 始终指向 1.9.x 中最新的镜像。myimage:latest 始终指向所有版本中最新的镜像。如果想使用特定版本，可以选择 myimage:1.9.1、myimage:1.9.2 或 myimage:2.0.0。 Dockerfile 的生产实践使用 game/centos:7 镜像为base镜像 搭建 python 运行环境镜像123456789101112131415[root@linux-node1 runtime]# mkdir -p /opt/Game/runtime/python-ssh[root@linux-node1 runtime]# tree /opt/Game//opt/Game/├── app├── runtime│ └── python-ssh└── system ├── centos │ ├── Dockerfile │ └── epel.repo └── test ├── Dockerfile ├── test_add.tar.gz └── tmpfile2 123456789101112131415161718# python + ssh# python运行环境镜像需要安装 python-devel python-pip openssh-clients openssl-devel openssh-server[root@linux-node1 python-ssh]# vim Dockerfile # BASEFROM game/centos:7# WHOMAINTAINER leo 365042337@qq.com# PKGRUN yum install -y python-devel python-pip openssh-clients openssl-devel openssh-server &amp;&amp; yum clean all# FOR SSHDRUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_keyRUN ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_keyRUN echo &quot;root:222222&quot; | chpasswd 12345678# 构建[root@linux-node1 python-ssh]# docker build -t game/centos7-python-ssh .[root@linux-node1 python-ssh]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/centos7-python-ssh latest 9b0a64f83274 14 seconds ago 357MBgame/centos 7 ea065b022d5b 2 hours ago 287MB... 在本地测试 python123456789101112131415161718192021# 测试程序 flask[root@linux-node1 python-ssh]# mkdir -p /opt/Game/app/shop-api[root@linux-node1 python-ssh]# cd /opt/Game/app/shop-api[root@linux-node1 shop-api]# vim app.pyfrom flask import Flaskapp = Flask(__name__)@app.route(&apos;/&apos;)def hello(): return &apos;Hello World!&apos; if __name__ == &quot;__main__&quot;: app.run(host=&quot;0.0.0.0&quot;, debug=True)# 安装python,运行测试程序yum install python-pip -ypip install flaskpython app.py访问:http://10.0.0.100:5000/ 添加python 服务的依赖文件1234# python pip 将会去安装包[root@linux-node1 shop-api]# vim requirements.txtflaskrequests supervisor 管理进程文件1234567891011121314151617181920212223242526272829303132333435363738注意:1. 照着案例文件写 案例文件:2. 在配置文件中的 必须配置为前台启动 ***** supervisorctl 必须要在前提启动nodaemon=true ; (start in foreground if true;default false)# 本地测试[root@linux-node1 shop-api]# yum install supervisor# 定义了两个服务 启动/opt/app.py 和 sshd[root@linux-node1 shop-api]# vim app-supervisor.ini [program:shop-api]command=/usr/bin/python2.7 /opt/app.pyprocess_name=%(program_name)sautostart=trueuser=wwwstdout_logfile=/tmp/app.logstderr_logfile=/tmp/app.error[program:sshd]command=/usr/sbin/sshd -Dprocess_name=%(program_name)sautostart=true# 将配置文件放到本地去测试执行# 添加本地www账户[root@linux-node1 shop-api]# cp app-supervisor.ini /etc/supervisord.d/[root@linux-node1 shop-api]# cp app.py /opt/[root@linux-node1 shop-api]# useradd -s /sbin/nologin -M www# 测试执行[root@linux-node1 shop-api]# supervisord -c /etc/supervisord.conf[root@linux-node1 ~]# supervisorctl statusshop-api RUNNING pid 3975, uptime 0:00:43sshd FATAL Exited too quickly (process log may have details)# 本地的sshd是存在的,shop-api正常即可,后续还需要继续学习supervisor的配置 shop-api 的Dockerfile12341. 添加启动用户2. pip安装依赖3. supervisord.conf 如果有变更也要传4. CMD 需要启动supervisord服务 12345678910111213141516171819202122232425[root@linux-node1 shop-api]# vim Dockerfile# BASE IMAGESFROM game/centos7-python-ssh# WHOMAINTAINER leo 365042337@qq.com# USER ADDRUN useradd -s /sbin/nologin -M www# ADD FILEADD app.py /opt/app.pyADD app-supervisor.ini /etc/supervisord.d/ADD requirements.txt /opt/ADD supervisord.conf /etc/supervisord.conf# PIPRUN /usr/bin/pip2.7 install -r /opt/requirements.txt# PORTEXPOSE 22 5000# CMDCMD [&quot;/usr/bin/supervisord&quot;,&quot;-c&quot;,&quot;/etc/supervisord.conf&quot;] 12345678[root@linux-node1 shop-api]# docker build -t game/shop-api .[root@linux-node1 shop-api]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgame/shop-api latest 950dfdaa7360 23 seconds ago 365MBgame/centos7-python-ssh latest 9b0a64f83274 3 hours ago 357MBgame/centos 7 ea065b022d5b 4 hours ago 287MB... 1234[root@linux-node1 shop-api]# docker run -d -p 80:5000 -p 8022:22 --name myshop-ssh game/shop-api [root@linux-node1 .ssh]# ssh 10.0.0.100 -p8022浏览器访问: http://10.0.0.100/ 需要理解123456789101112131415161718192021222324252627282930313233341. 如果没有起来的测试步骤- 在构建一个镜像 不要执行CMD[root@linux-node1 shop-api]# pwd/opt/docker/app/shop-api# CMD# CMD [&quot;/usr/bin/supervisord&quot;,&quot;-c&quot;,&quot;/etc/supervisord.conf&quot;] - 进到容器里查看服务[root@linux-node1 shop-api]# docker build -t oldboy/shop-api:v2 .[root@linux-node1 shop-api]# docker run -it --name shop-api-v2 -p 88:5000 -p 8022:22 oldboy/shop-api:v2 /bin/bash[root@3f1e55eb826d /]# supervisord -c /etc/supervisord.conf [root@3f1e55eb826d /]# supervisorctl statusshop-api RUNNING pid 19, uptime 0:00:03sshd RUNNING pid 18, uptime 0:00:032. 在配置文件中的 必须配置为前台启动 改好之后重新构建nodaemon=true ; (start in foreground if true;default false) 3. 分层的好处 1. 更快速的构建 1. 系统 2. 运行环境 3. app 2. 使用supervisor启动和管理进程 3. python 的依赖requirements.txt 4. 先尝试 把应用 用docker跑起来 再去写Dockerfile 5. 去看开源项目的 Dockerfile 6. 生产 之前一定先到虚拟机或者物理机跑一遍,然后再去 把配置文件copy过来 再去写 dockerfile]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07 Docker 手动构建镜像]]></title>
    <url>%2F2019%2F10%2F11%2Fwrite-images%2F</url>
    <content type="text"><![CDATA[123参考:https://www.cnblogs.com/wangxu01/articles/11325388.htmlhttps://www.cnblogs.com/luoahong/p/10273477.html 为什么要构建镜像 找不到现成的镜像,比如自己开发的应用程序,生产做自己的镜像 需要在镜像中加入特定的功能,比如官方镜像几乎都不提供 ssh。 两种构建镜像的方法: 手动构建镜像 docker commit 命令 Dockerfile 构建镜像文件 手动构建镜像步骤 启动基础容器安,装软件服务。 将安装好服务的容器commit提交为镜像。 启动新容器来测试新提交的镜像。 制作支持ssh远程登录的docker镜像启动基础容器,安装软件服务目的 : 以官方镜像为基础在这个基础之上 “做自己的镜像” 12345678# --privileged=true，该参数在docker容器运行时，让系统拥有真正的root权限# /usr/sbin/init：初始容器里的CENTOS，用于启动dbus-daemon。[root@linux-node1 ~]# docker run --privileged=true -d -p 1022:22 --name mycentos centos:7 /usr/sbin/init[root@linux-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES017da954e8d4 centos:7 &quot;/usr/sbin/init&quot; 11 seconds ago Up 10 seconds 0.0.0.0:1022-&gt;22/tcp mycentos 安装软件 openssh-server12345678910111213141516171819[root@linux-node1 ~]# ./docker_in.sh mycentos[root@017da954e8d4 /]# yum install openssh-server -y[root@017da954e8d4 /]# systemctl start sshd[root@017da954e8d4 /]# systemctl status sshd● sshd.service - OpenSSH server daemon Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2019-10-11 08:37:27 UTC; 6s ago Docs: man:sshd(8) man:sshd_config(5) Main PID: 200 (sshd) CGroup: /docker/017da954e8d47f86c0db7116fd90b0a7ee5da7e1150ea1a89442d1c59cb6d7ec/system.slice/sshd.service └─200 /usr/sbin/sshd -D ‣ 200 /usr/sbin/sshd -DOct 11 08:37:27 017da954e8d4 systemd[1]: Starting OpenSSH server daemon...Oct 11 08:37:27 017da954e8d4 sshd[200]: Server listening on 0.0.0.0 port 22.Oct 11 08:37:27 017da954e8d4 sshd[200]: Server listening on :: port 22.Oct 11 08:37:27 017da954e8d4 systemd[1]: Started OpenSSH server daemon. 1234567[root@017da954e8d4 /]# yum install net-tools -y[root@017da954e8d4 /]# netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 200/sshd tcp6 0 0 :::22 :::* LISTEN 200/sshd 设置远程登录密码123[root@017da954e8d4 /]# echo 123456|passwd --stdin rootChanging password for user root.passwd: all authentication tokens updated successfully. 测试远程访问1234# 在创建容器的时候 我们绑定了本地的 1022端口哦~[root@linux-node1 tools]# ssh 10.0.0.100 -p1022root@10.0.0.100&apos;s password: Last login: Fri Oct 11 08:43:05 2019 from 10.0.0.100 commit 提交为镜像12345678910# 提交本地镜像# commit -m &quot;centos7-ssh&quot; # 添加注释# mycentos # 容器名# test/mycentos-7-ssh # 仓库名称/镜像:版本 不加版本号就是latest 最后的版本[root@linux-node1 tools]# docker commit -m &quot;centos7-ssh&quot; mycentos test/mycentos-7-ssh[root@linux-node1 tools]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest/mycentos-7-ssh latest db3993e49556 About a minute ago 278MB... 使用新镜像 启动新容器 注意事项: 记得一定要在启动容器的后面加上 /usr/sbin/sshd -D 不加默认是 /bin/bash 1[root@linux-node1 tools]# docker run -d --name mycentos7 -p 2022:22 test/mycentos-7-ssh /usr/sbin/sshd -D 连接测试 20221234[root@linux-node1 ~]# ssh 10.0.0.100 -p 2022root@10.0.0.100&apos;s password: Last login: Fri Oct 11 09:01:24 2019 from 10.0.0.100[root@33b52a8d8adb ~]# 基于centos7的ssh+Nginx镜像启动基础容器1[root@linux-node1 ~]# docker run --privileged=true -d -p 80:80 --name centos-7-ssh-nginx centos:7 /usr/sbin/init 配置yum源安装 Nginx和sshd1234567891011121314[root@linux-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3d804a17f13b centos:7 &quot;/usr/sbin/init&quot; 34 seconds ago Up 33 seconds 0.0.0.0:80-&gt;80/tcp centos-7-ssh-nginx[root@linux-node1 ~]# ./docker_in.sh 3d804a17f13b[root@3d804a17f13b /]# cd /etc/yum.repos.d/[root@3d804a17f13b yum.repos.d]# mkdir bak ; mv *.repo bak/[root@3d804a17f13b yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo[root@3d804a17f13b yum.repos.d]# rpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm[root@3d804a17f13b yum.repos.d]# lsbak CentOS-Base.repo epel.repo epel-testing.repo 1234yum install net-tools -y yum install nginx -yyum install openssh-server -yecho 123456|passwd --stdin root 启动服务123456789[root@3d804a17f13b yum.repos.d]# systemctl start nginx[root@3d804a17f13b yum.repos.d]# systemctl start sshd[root@3d804a17f13b yum.repos.d]# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 292/nginx: master p tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 319/sshd tcp6 0 0 :::80 :::* LISTEN 292/nginx: master p tcp6 0 0 :::22 :::* LISTEN 319/sshd 1测试web访问 http://10.0.0.100/ 编写容器启动脚本12345678910[root@361b22a52b62 /]# vi init.sh#!/bin/bashsource /etc/profilesystemctl start nginxsystemctl status nginx &gt;&gt; /tmp/nginx.logsystemctl start sshd[root@361b22a52b62 /]# chmod 755 /init.sh commit提交为镜像123456[root@linux-node1 ~]# docker commit -m &quot;My CentOS7-ssh-nginx&quot; centos-7-ssh-nginx test/mycentos-7-ssh-nginx:v1[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtest/mycentos-7-ssh-nginx v1 60ed5f7aebf2 3 seconds ago 377MB... 启动新容器1[root@linux-node1 ~]# docker run --privileged=true -d -p 8080:80 -p 2222:22 --name we1 test/mycentos-7-ssh-nginx:v1 /usr/sbin/init init.sh 123# 如果服务无法启动 还是需要之前的超管权限步骤[root@05068b99ded6 tmp]# systemctl start nginxFailed to get D-Bus connection: Operation not permitted]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06 Docker 数据管理]]></title>
    <url>%2F2019%2F10%2F11%2Fdocker-base06%2F</url>
    <content type="text"><![CDATA[生产环境中使用Docker的过程中，往往需要对数据进行持久化，或者需要在多个容器之间进行数据共享，这必然涉及容器的数据管理操作。 容器中管理数据主要有两种方式: 123456数据卷（Data Volumes）： - 容器内数据直接映射到本地主机环境； - 如何在容器内创建数据卷，并且把本地的目录或文件挂载到容器内的数据卷中。数据卷容器（Data Volume Containers）： - 使用特定容器维护数据卷。 - 如何使用数据卷容器在容器和主机、容器和容器之间共享数据，并实现数据的备份和恢复。 数据卷的使用 数据卷是一个可供容器使用的特殊目录，它将主机操作系统目录直接映射进容器，类似Linux 的mount挂载 12345数据卷的特性: - 数据卷可以在容器之间共享和重用，容器间传递数据将变得高效方便； - 对数据卷内数据的修改会立马生效，无论是容器内操作还是本地操作； - 对数据卷的更新不会影响镜像，解耦了应用和数据； - 卷会一直存在，直到没有容器使用，可以安全地卸载它。 创建数据卷 在用docker run 命令的时候，使用 -v 标记可以在容器内创建一个数据卷。多次重复使用 -v 标记可以创建多个数据卷。 用户可以将一些程序或数据放到本地目录中，然后在容器内运行和使用。 另外，本地目录的路径必须是绝对路径，如果目录不存在,Docker会自动创建。 12-v /data -v src:dst # 挂载本地目录到容器上 123456789101112131415161718192021222324252627282930311. 准备好host挂载目录和里面的文件[root@linux-node1 tools]# mkdir -p /data/tools[root@linux-node1 tools]# ls -ltotal 232-rw-r--r--. 1 root root 15329 Nov 8 2018 2000.png-rw-r--r--. 1 root root 51562 Nov 8 2018 21.js-rw-r--r--. 1 root root 254 Nov 8 2018 icon.pngdrwxr-xr-x. 2 root root 96 Nov 8 2018 img-rw-r--r--. 1 root root 3049 Nov 8 2018 index.html-rw-r--r--. 1 root root 63008 Nov 8 2018 sound1.mp3-rw-r--r--. 1 root root 91725 Oct 11 10:22 xiaoniaofeifei.zip2. 开始挂载[root@linux-node1 tools]# docker run -it --name mycentos -v /data/tools/:/data/tools centos:7[root@ec7228193572 /]# cd /data/tools/[root@ec7228193572 tools]# ls2000.png 21.js icon.png img index.html sound1.mp3 xiaoniaofeifei.zip3. 测试host添加一个文件[root@linux-node1 tools]# echo &quot;hello leo&quot; &gt;&gt; leo.html[root@ec7228193572 tools]# ls -l...-rw-r--r--. 1 root root 10 Oct 11 02:28 leo.html...4. 测试在容器上删除该文件[root@ec7228193572 tools]# rm -f leo.html [root@linux-node1 tools]# ls -l leo.html # 已删除5. 如果我们删除了本地的tools 那么容器上的tools里面的数据将不存在[root@linux-node1 data]# rm -rf tools/ 很方便的在你进行数据修改的时候，直接修改本地文件就行了，容器直接就会同步了 提前都建好目录，再挂载上去，容器down没关系，数据都在指定的目录下 1234567891. docker 挂载数据卷的默认权限是读写(rw) ,我们也可以改权限[root@linux-node1 data]# docker run -it --name mycentos2 -v /data/tools/:/data/tools/:ro centos:7[root@d45deb896445 /]# cd /data/tools/[root@d45deb896445 tools]# touch hehetouch: cannot touch &apos;hehe&apos;: Read-only file system2. 也可以只挂载本地主机的单个文件到容器中作为数据卷(强烈不推荐，生产也很少用，可忽略)[root@linux-node1 ~]# docker run --rm -it -v /root/.bash_history:/.bash_history centos:7 /bin/bashroot@3ac18de44b62:/# cat .bash_history Docker 安装 Nginx 并使用数据卷管理拉取官方镜像1.16.11[root@linux-node1 data]# docker pull nginx:1.16.1 查看镜像123[root@linux-node1 data]# docker images nginx:1.16.1REPOSITORY TAG IMAGE ID CREATED SIZEnginx 1.16.1 0dac5b41d811 4 weeks ago 126MB 使用 NGINX 默认的配置来启动一个 Nginx 容器实例12345[root@linux-node1 data]# docker run -d -p 8000:80 --name web1 nginx:1.16.1- --name: 容器名web1- -p: 端口映射80- -d: 容器在后台运行- nginx: 123[root@linux-node1 data]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa41b970bfceb nginx1.16.1 &quot;nginx -g &apos;daemon of…&quot; 5 seconds ago Up 3 seconds 0.0.0.0:80-&gt;80/tcp web1 在浏览器中打开 http://10.0.0.100:8000/ 使用容器卷管理 nginx121. 创建目录[root@linux-node1 data]# mkdir -p /data/nginx/&#123;log,conf,www&#125; 123456782. 拷贝容器内 Nginx 默认配置文件到本地当前目录下的 conf 目录[root@linux-node1 data]# docker cp web1:/etc/nginx/nginx.conf /data/nginx/conf[root@linux-node1 data]# cd /data/nginx/-rw-r--r--. 1 root root 643 Sep 24 22:49 nginx.conf - www: 目录将映射为 nginx 容器配置的虚拟目录。 - logs: 目录将映射为 nginx 容器的日志目录。 - conf: 目录里的配置文件将映射为 nginx 容器的配置文件。 1234563. 启动挂载目录[root@linux-node1 nginx]# docker run -d -p 80:80 --name web2 -v /data/nginx/conf/nginx.conf:/etc/nginx/conf/nginx.conf -v /data/nginx/www:/usr/share/nginx/html -v /data/nginx/log/:/var/log/nginx nginx1.16.1- nginx文件 /data/nginx/conf/nginx.conf --&gt; /etc/nginx/conf/nginx.conf- nginx目录 /data/nginx/www --&gt; /usr/share/nginx/html- nginx日志 /data/nginx/log/ --&gt; /var/log/nginx 123456789104. 进入host的首页目录 上传测试文件[root@linux-node1 www]# ls -ltotal 232-rw-r--r--. 1 root root 15329 Nov 8 2018 2000.png-rw-r--r--. 1 root root 51562 Nov 8 2018 21.js-rw-r--r--. 1 root root 254 Nov 8 2018 icon.pngdrwxr-xr-x. 2 root root 96 Nov 8 2018 img-rw-r--r--. 1 root root 3049 Nov 8 2018 index.html-rw-r--r--. 1 root root 63008 Nov 8 2018 sound1.mp3-rw-r--r--. 1 root root 91725 Oct 11 11:16 xiaoniaofeifei.zip 15. 测试访问页面 123456786. 相关命令- 如果要重新载入 NGINX 可以使用以下命令发送 HUP 信号到容器：docker kill -s HUP container-name[root@linux-node1 www]# docker kill -s HUP web2- 重启 NGINX 容器命令：docker restart container-name[root@linux-node1 www]# docker restart web2 1234567. 查看日志 # 可以去看host的log目录下的日志[root@linux-node1 log]# ls -ltotal 8-rw-r--r--. 1 root root 3224 Oct 11 11:20 access.log-rw-r--r--. 1 root root 732 Oct 11 11:20 error.log 测试修改代码 修改host上首页的title标题 12[root@linux-node1 www]# vim index.html &lt;title&gt;docker-nginx-test&lt;/title&gt; 多端口映射 基于多端口多站点 80是主页，81端口是游戏，在host上写一个81的配置文件挂载上去 12345678910111213[root@linux-node1 conf]# pwd/data/nginx/conf[root@linux-node1 conf]# cat 81.game.confserver&#123; root /opt; listen 81;&#125;[root@linux-node1 conf]# ls -ltotal 8-rw-r--r--. 1 root root 40 Oct 11 11:39 81.game.conf-rw-r--r--. 1 root root 643 Sep 24 22:49 nginx.conf 123456789# 启动容器并挂载配置# 挂载81的配置文件到 /etc/nginx/conf.d/下让主配置文件iincludedocker run -d -p 80:80 -p 81:81 --name web3-v /data/nginx/conf/81.game.conf:/etc/nginx/conf.d/81.game.conf-v /data/nginx/www/:/opt -v /data/nginx/log/:/var/log/nginx nginx:1.16.1[root@linux-node1 conf]# docker run -d -p 80:80 -p 81:81 --name web3 -v /data/nginx/conf/81.game.conf:/etc/nginx/conf.d/81.game.conf -v /data/nginx/www/:/opt -v /data/nginx/log/:/var/log/nginx nginx:1.16.1 12345# 日志查看[root@linux-node1 log]# tailf access.log 10.0.0.1 - - [11/Oct/2019:04:00:40 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&quot; &quot;-&quot;10.0.0.1 - - [11/Oct/2019:04:00:41 +0000] &quot;GET /img/p2l.jpg HTTP/1.1&quot; 404 555 &quot;http://10.0.0.100:81/&quot; &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&quot; &quot;-&quot; 数据卷容器 如果数据在需要在多个容器之间共享，最简单的方式是使用数据卷容器。 数据卷容器也是一个容器，但是它的目的是专门用来提供数据卷供其他容器挂载。 不管数据卷容器是否运行 都可以访问到 –volumes-from volumes-from 启动一个容器挂载一个目录,其他的容器都不用挂载了,直接–volumes-from 这个容器 12345678910111213141516[root@linux-node1 ~]# docker run -it --name volume_test -v /data/tools/:/data/tools centos:7# 新建的容器 --volumes-from volume_test# 即使数据卷容器没有运行,数据也可以被访问[root@linux-node1 ~]# docker run -it --name game_test --volumes-from volume_test centos:7 /bin/bash[root@9b7b28b2f5a5 /]# cd /data/tools/[root@9b7b28b2f5a5 tools]# lsweb1.tar[root@9b7b28b2f5a5 tools]# touch game_test.log[root@linux-node1 tools]# lsgame_test.log web1.tar# 有些数据在多个容器之间共享,就可以使用数据卷容器# 单个容器挂载 然后其他的都用容器卷# 只要有容器卷使用这个目录就无法删除 总结 现在我们有数据层（镜像层和容器层）和 volume 都可以用来存放数据，具体使用的时候要怎样选择呢？ 考虑下面几个场景： 1234567- Database 软件 vs Database 数据- Web 应用 vs 应用产生的日志- 数据分析软件 vs input/output 数据- Apache Server vs 静态 HTML 文件 相信大家会做出这样的选择： 123前者放在数据层中。因为这部分内容是无状态的，应该作为镜像的一部分。后者放在 Data Volume 中。这是需要持久化的数据，并且应该与镜像分开存放。 如何使用 12345-v /data/nginx/www:/usr/share/nginx/html这与 linux mount 命令的行为是一致的-v 的格式为 &lt;host path&gt;:&lt;container path&gt;。/usr/share/nginx/html 就是 nginx 存放静态文件的地方。原有数据会被隐藏起来，取而代之的是 /data/nginx/www 中的数据 1234即使容器销毁,数据也还在，mount的是 host 文件系统中的数据，只是借给容器用用，哪能随便就删了啊mount 时还可以指定数据的读写权限，默认是可读可写，可指定为只读除了mount 目录，还可以单独指定一个文件使用mount 单个文件的场景是：只需要向容器添加文件，不希望覆盖整个目录。 1234使用场景:1. 可以将源代码目录 mount 到容器中，在 host 中修改代码就能看到应用的实时效果2. mysql 容器的数据放在 共享目录 里，这样 host 可以方便地备份和迁移数据3. 实现了容器与 host 的解耦]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05 Docker 容器的端口映射]]></title>
    <url>%2F2019%2F10%2F10%2Fdocker-base05%2F</url>
    <content type="text"><![CDATA[容器端口映射 docker的端口映射，能保证在外部网络 访问 同一设备上不同容器时候 不会因为端口问题产生冲突。 docker 可将容器对外提供服务的端口映射到 host 的某个端口，外网通过该端口访问容器。 容器启动时通过-p参数映射端口： 12345-p hostPort:containerPort # 0.0.0.0:80-p ip:hostPort:containerPort # 指定宿主机IP地址,例如192.168.0.136:80,多个容器都想使用80端口，添加多个IP-p ip::containerPort # 随机端口 ，32768端口，内核参数决定的net.ipv4.ip_local_port_range = 32768 60999-p hostPort:containerPort:udp # udp -p 81:80 –p 443:443 # 指定多个 指定端口1234[root@linux-node1 ~]# docker run -d -p 80:80 --name web1 nginx:1.16.18eee8d6ffc1580a2f80e4472ed51990af6c525a1d2734cac216f07b9ed816650[root@linux-node1 ~]# netstat -tnlp|grep 80tcp6 0 0 :::80 :::* LISTEN 1774/docker-proxy 多端口绑定123456[root@linux-node1 ~]# docker run -d -p 443:443 -p 82:80 --name web2 nginx:1.16.1[root@linux-node1 ~]# netstat -tnlp|grep dockertcp6 0 0 :::80 :::* LISTEN 1774/docker-proxy tcp6 0 0 :::82 :::* LISTEN 1969/docker-proxy tcp6 0 0 :::443 :::* LISTEN 1957/docker-proxy 随机[root@linux-node1 ~]# docker run -d -P –name web3 nginx:1.16.1 123456[root@linux-node1 ~]# netstat -tnlp|grep dockertcp6 0 0 :::5000 :::* LISTEN 2397/docker-proxy tcp6 0 0 :::80 :::* LISTEN 1774/docker-proxy tcp6 0 0 :::82 :::* LISTEN 1969/docker-proxy tcp6 0 0 :::443 :::* LISTEN 1957/docker-proxy tcp6 0 0 :::32768 :::* LISTEN 2517/docker-proxy 映射到指定的协议 tcp 或 udp1docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04 Docker 容器管理]]></title>
    <url>%2F2019%2F10%2F10%2Fdocker-base03%2F</url>
    <content type="text"><![CDATA[启动容器 docker run docker 的容器是镜像的一个运行实例。 docker 镜像是只读文件，而容器则带有运行时的可读写层，而且容器中的应用进程处于运行状态。 docker run ：创建一个新的容器并运行一个命令 123456789101112docker run [OPTIONS] IMAGE [COMMAND] [ARG...]-t ：打开一个终端，像使用交换机一样使用容器-i：交互式访问--name：容器名字--network：指定网络--rm：容器一停，自动删除-d：剥离与当前终端的关系；否则会一直占据着终端-p：端口映射，将容器内服务的端口映射在宿主机的指定端口-p &lt;container port&gt;-p &lt;hostport&gt;:&lt;container port&gt;-p &lt;hostip&gt;:&lt;hostport&gt;:&lt;container port&gt; 123456789101112# 如果不执行命令,那么容器使用的镜像名 在最后# 如果docker run 本地没有镜像，那么他会帮你pull下来 1. 下载 centos:7镜像[root@linux-node1 ~]# docker pull centos:72. 运行centos:7 镜像[root@linux-node1 ~]# docker run -it --rm --name mycentos centos:7 /bin/bash[root@593621d0b6cc /]# cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) [root@593621d0b6cc /]# uname -r3.10.0-327.el7.x86_64 查看运行的容器 docker ps123[root@linux-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd9ba7d5f9d22 centos:7 &quot;/bin/bash&quot; 8 seconds ago Up 7 seconds mycentos 显示所有 -a123456789# 现在执行exit 就退出bash 容器也跟着退出了,因为进程结束了,容器也结束了[root@linux-node1 ~]# docker run -it --name mycentos centos:7 /bin/bash[root@0dc9c214c2c9 /]# exitexit[root@linux-node1 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0dc9c214c2c9 centos:7 &quot;/bin/bash&quot; 5 seconds ago Exited (0) 2 seconds ago mycentos 显示ID -q12[root@linux-node1 ~]# docker ps -aq0dc9c214c2c9 启动容器 docker start12345678910[root@linux-node1 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0dc9c214c2c9 centos:7 &quot;/bin/bash&quot; 5 seconds ago Exited (0) 2 seconds ago mycentos[root@linux-node1 ~]# docker start mycentosmycentos[root@linux-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0dc9c214c2c9 centos:7 &quot;/bin/bash&quot; 11 minutes ago Up 2 seconds mycentos 查看容器的详细信息 docker inspect1234567# 查看所有[root@linux-node1 ~]# docker inspect mycentos[root@linux-node1 ~]# docker inspect mycentos|grep IPAddress# 查看容器的pid 可以用于nsenter进入容器[root@linux-node1 ~]# docker inspect -f &quot;&#123;&#123; .State.Pid &#125;&#125;&quot; 0dc9c214c2c92525 查看容器运行情况 docker top123[root@linux-node1 ~]# docker top 0dc9c214c2c9UID PID PPID C STIME TTY TIME CMDroot 2525 2508 0 10:30 pts/0 00:00:00 /bin/bas 停止容器 docker stop12345678910[root@linux-node1 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0dc9c214c2c9 centos:7 &quot;/bin/bash&quot; 27 minutes ago Up 16 minutes mycentos[root@linux-node1 ~]# docker stop mycentomycentos[root@linux-node1 ~]# docker ps -aCONTAINER ID IMGE COMMAND CREATED STATUS PORTS NAMES0dc9c214c2c9 centos:7 &quot;/bin/bash&quot; 27 minutes ago Exited (137) 4 seconds ago mycento 删除容器 docker rm先关闭容器，再删除容器 123[root@linux-node1 ~]# docker stop mycentos[root@linux-node1 ~]# docker rm mycentos[root@linux-node1 ~]# docker ps -a 12# 强制删除容器[root@linux-node1 ~]# docker rm -f mycentos 批量删除容器生产环境谨慎使用 12345# 只删除关闭状态的[root@linux-node1 ~]# docker rm $(docker ps -qa)# #强制删除全部，包括启动中的[root@linux-node1 ~]# docker rm -f $(docker ps -aq) 进入容器 进入容器的三种方法:attach(不推荐)、exec、 123* 和虚拟机不同 你不能进去 再执行新的东西,因为docker的理念和虚拟机不同* docker的理念是 再创建一个容器 也不能修改一个容器 * 不可变基础环境 --&gt; 环境必须一致 attach 进入到存活的容器中 如果再开一个窗口进来，操作是同步的，就像远程控制，单用户模式 exit 就会推出, 生产不会使用这个命令 1234[root@linux-node1 ~]# docker start mycentosmycentos[root@linux-node1 ~]# docker attach mycentos[root@45fce97e86cc /]# exec 对运行的容器执行指定命令 不想进入,让容器执行命令并返回结果 1234567 -d：在后台运行命令 -e：设置环境变量 -i：交互式 -t：打开一个终端 -u：用户名或UID暂时退出容器：ctrl+p，ctrl+q回到容器：docker attach id或name 1234[root@linux-node1 ~]# docker exec mycentos ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.1 0.0 11824 1700 pts/0 Ss+ 03:02 0:00 /bin/bashroot 14 0.0 0.0 51744 1724 ? Rs 03:02 0:00 ps -aux 退出并不停止 docker crtl 先p 再q 1234567# 交互式进入容器 -it[root@linux-node1 ~]# docker exec -it mycentos /bin/bash# 如果不想退出 就结束容器&gt;&gt;&gt; crtl 先p 再q[root@linux-node1 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES45fce97e86cc centos:7 &quot;/bin/bash&quot; 6 minutes ago Up 3 minutes mycentos nsenter nsenter 生产使用 即使exit 也不会关闭容器 12345678910111213141516171819202122232425262728293031323334353637381. 安装[root@linux-node1 ~]# yum install -y util-linux - nsenter : ns (namespace) 进入到命名空间 - docker 的实现 : Docker利用Linux核心中的资源分离机制，例如cgroups，以及Linux核心命名空间（namespaces） - 需要容器进程的pid2. 获取容器的PID,容器需要是启动状态,否则获取为0[root@linux-node1 ~]# docker inspect -f &quot;&#123;&#123; .State.Pid &#125;&#125;&quot; mycentos36653. nsenter 进入[root@linux-node1 ~]# nsenter -t 3665 -m -u -i -n -p /bin/bash[root@45fce97e86cc /]# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 11824 1700 pts/0 Ss+ 03:02 0:00 /bin/bashroot 24 0.0 0.0 11824 1696 pts/1 Ss+ 03:04 0:00 /bin/bashroot 118 0.0 0.0 15256 2020 pts/0 S 03:20 0:00 /bin/bashroot 127 0.0 0.0 55176 1872 pts/0 R+ 03:20 0:00 ps -aux4. 进入容器脚本[root@linux-node1 ~]# vim docker_in.sh #!/bin/bash# Use nsenter to access dockerdocker_in()&#123; NAME_ID=$1 PID=$(docker inspect -f &quot;&#123;&#123; .State.Pid &#125;&#125;&quot; $NAME_ID) nsenter -t $PID -m -u -i -n -p /bin/bash echo $PID&#125;docker_in $15. 脚本权限并执行[root@linux-node1 ~]# chmod +x docker_in.sh [root@linux-node1 ~]# ./docker_in.sh mycentos[root@45fce97e86cc /]# 查看日志 docker logs12345678910111213141516171. 启动一个nginx:1.4的容器# 根据镜像名称（tag指定版本）拉取镜像# alpine版本：构建容器小镜像的发型版本[root@linux-node1 ~]# docker pull nginx:1.14-alpine2. 启动nginx容器[root@linux-node1 ~]# docker run --name web1 -d -p 80:80 nginx:1.14-alpine16ced725c5f998f7a59b98731ba74305f2e3adc96449410ecfef20b8c51749443. 浏览器访问访问 http://10.0.0.1004. curl访问[root@linux-node1 ~]# docker logs web15. 查看日志[root@linux-node1 ~]# docker logs web1 copy容器文件 docker cp 该命令支持在容器和主机之间复制文件。 12-a， -archive： 打包模式， 复制⽂件会带有原始的uid/gid信息；-L， -follow-link： 跟随软连接。 当原路径为软连接时， 默认只复制链接信息， 使⽤该选项会复制链接的⽬标内容。 123456# 服务器向容器copy文件[root@linux-node1 ~]# docker cp /root/docker_in.sh mycentos:/tmp[root@linux-node1 ~]# ./docker_in.sh mycentos[root@45fce97e86cc /]# cd /tmp/[root@45fce97e86cc tmp]# lsdocker_in.sh ks-script-rnBCJB yum.log 123456789# copy容器的文件下来[root@linux-node1 ~]# docker cp web1:/etc/nginx/nginx.conf .[root@linux-node1 ~]# ls -ltotal 126796-rw-------. 1 root root 1239 Sep 30 17:27 anaconda-ks.cfg-rwxr-xr-x. 1 root root 196 Oct 9 14:01 docker_in.sh-rw-------. 1 root root 129824768 Oct 10 08:34 nginx:1.16.tar-rw-rw-r--. 1 root root 643 Apr 10 2019 nginx.co 查看端口映射 docker port12[root@linux-node1 ~]# docker port web180/tcp -&gt; 0.0.0.0:80 容器的导入和导出导出容器1234# 导出后的容器就可以直接复制到其他机器上导入运行了[root@linux-node1 ~]# docker export -o web1.tar web1[root@linux-node1 ~]# ls -lh web1.tar -rw-------. 1 root root 17M Oct 10 11:46 web1.tar 导入容器1234567[root@linux-node1 ~]# docker import web1.tar test/nginx:1.6.testsha256:482793b910f56b65c0dbb36c4424cca923765ecd3d052c30a0db70c185bd6571[root@linux-node1 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES16ced725c5f9 nginx:1.14-alpine &quot;nginx -g &apos;daemon of…&quot; 15 minutes ago Up 15 minutes 0.0.0.0:80-&gt;80/tcp web145fce97e86cc centos:7 &quot;/bin/bash&quot; 47 minutes ago Up 45 minutes mycentos 总结容器的使用操作 docker run 参数 镜像名 执行命令 进入容器的三种方 一图总结对容器的操作命令:]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 Docker 镜像管理]]></title>
    <url>%2F2019%2F10%2F09%2Fdocker-base02%2F</url>
    <content type="text"><![CDATA[搜索镜像 docker search docker search : 从Docker Hub查找镜像 OPTIONS说明： 12345--automated :只列出 automated build类型的镜像；--no-trunc :显示完整的镜像描述；-s :列出收藏数不小于指定值的镜像。 搜索所有nginx相关的镜像 12345[root@linux-node1 ~]# docker search nginxNAME DESCRIPTION STARS OFFICIAL AUTOMATEDnginx Official build of Nginx. 12036 [OK] ... 搜索结果解释 123456789NAME:镜像名称DESCRIPTION:镜像说明OFFICIAL:是否docker官方发布STARS:点赞数量AUTOMATED:是否是自动构建的 获取镜像 docker pull docker pull : 从镜像仓库中拉取或者更新指定镜像 1234567891011# 不指定版本默认为最新版，只写名字默认在官方拉取[root@linux-node1 docker]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginxb8f262c62ec6: Pull complete e9218e8f93b1: Pull complete 7acba7289aa3: Pull complete Digest: sha256:aeded0f2a861747f43a01cf1018cf9efe2bdd02afd57d2b11fcc7fcadc16ccd1Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest 12345678910# 拉取指定版本[root@linux-node1 docker]# docker pull nginx:1.16.11.16.1: Pulling from library/nginxb8f262c62ec6: Already exists 00b0a9251451: Pull complete 7cc4a8bdb72c: Pull complete Digest: sha256:0d0af9bc6ca2db780b532a522a885bef7fcaddd52d11817fc4cb6a3ead3eacc0Status: Downloaded newer image for nginx:1.16.1docker.io/library/nginx:1.16.1 查看镜像 docker images docker images : 列出本地镜像 OPTIONS说明： 1234567891011-a :列出本地所有的镜像；--digests :显示镜像的摘要信息；-f :显示满足条件的镜像；--format :指定返回值的模板文件；--no-trunc :显示完整的镜像信息；-q :只显示镜像ID。 1234[root@linux-node1 docker]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest f949e7d76d63 2 weeks ago 126MBnginx 1.16.1 0dac5b41d811 3 weeks ago 126MB 123456789REPOSITORY：表示镜像的仓库源TAG：镜像的标签IMAGE ID：镜像IDCREATED：镜像创建时间SIZE：镜像大小 导出镜像 docker image save123[root@linux-node1 ~]# docker save -o nginx:1.16.tar nginx:1.16.1[root@linux-node1 ~]# lsanaconda-ks.cfg docker_in.sh nginx:1.16.tar 删除镜像 docker rmi12345678910111213141516171819# docker rmi IMAGE[IMAGE...] IMAGE可以为标签或ID# 当该镜像存在容器时，不能删除镜像，但可以加上 -f 选项强制删除，同时也删除容器。[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest f949e7d76d63 2 weeks ago 126MBnginx 1.16.1 0dac5b41d811 3 weeks ago 126MB[root@linux-node1 ~]# docker rmi 0dac5b41d811Untagged: nginx:1.16.1Untagged: nginx@sha256:0d0af9bc6ca2db780b532a522a885bef7fcaddd52d11817fc4cb6a3ead3eacc0Deleted: sha256:0dac5b41d811ca6e1bfe68d31bec5fb1f5c37485b741674619a1a2a3dec5cc0eDeleted: sha256:e9aa30f728ac61d63293f77283681af6d7191aa9bd2326ce3f56839bbf4aa001Deleted: sha256:c32f482f1fd81bc84657ff0fb85a0a62ab3223f835875930e1e9658f57768c2e[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest f949e7d76d63 2 weeks ago 126MB 清理镜像 docker prune prune 命令用来删除不再使用的 docker 对象。 123456789101112131415161718# 生产环境要谨慎使用# 删除所有未被 tag 标记和未被容器使用的镜像:docker image prune# 删除所有未被容器使用的镜像:docker image prune -a# 删除所有停止运行的容器:docker container prune# 删除所有未被挂载的卷:docker volume prune# 删除所有网络:docker network prune# 删除 docker 所有资源:docker system prune 导入镜像 docker image load123456789[root@linux-node1 ~]# docker load --input nginx\:1.16.tar 2db44bce66cd: Loading layer [==================================================&gt;] 72.48MB/72.48MB2d62b975fbed: Loading layer [==================================================&gt;] 57.32MB/57.32MB520ed35c2642: Loading layer [==================================================&gt;] 3.584kB/3.584kBLoaded image: nginx:1.16.1[root@linux-node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx 1.16.1 0dac5b41d811 3 weeks ago 126MB 查看镜像详细信息 docker inspect inspect 命令来查看镜像的详细信息：docker [image] inspect 1234567# 查看镜像所有详细信息[root@linux-node1 docker]# docker inspect nginx# 查看其中一项[root@linux-node1 docker]# docker inspect -f &#123;&#123;&quot;.Created&quot;&#125;&#125; nginx2019-09-24T23:33:17.034191345Z]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02 Docker 安装]]></title>
    <url>%2F2019%2F10%2F07%2Fdocker-install%2F</url>
    <content type="text"><![CDATA[安装准备123https://www.cnblogs.com/along21/p/10215701.html#auto_id_12http://www.dockerinfo.net/documenthttps://www.runoob.com/docker/docker-tutorial.html 12345678910111213141516171819202122232425262728293031321. centos7 - 2C4G 三台 - 内核3.102. 关闭 防火墙 - systemctl stop firewalld.service - systemctl disable firewalld.service3. 关闭 selinux - vim /etc/selinux/config SELINUX=disable - setenforce 0 4. 做好主机名解析，即三台虚拟机能ping通彼此的主机名 - 推荐主机名: linux-node1.example.com - hostname linux-node1.example.com - vim /etc/hostname linux-node1.example.com - vim /etc/hosts 10.0.0.100 linux-node1.example.com linux-node1 10.0.0.101 linux-node2.example.com linux-node2 10.0.0.102 linux-node3.example.com linux-node3 - 重启ssh终端再进入 exit 5. 配置yum源 - cd /etc/yum.repos.d - mkdir bak;mv *.repo bak/ - curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo - curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 6. 安装基础软件 yum install -y net-tools wget vim lrzsz tree screen lsof tupdump nc mtr nmap gcc glibc gcc-c++ make yum 安装 docker添加docker-ce源信息1234561. 本次使用国内源wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.reposed -i 's@download.docker.com@mirrors.tuna.tsinghua.edu.cn/docker-ce@g' /etc/yum.repos.d/docker-ce.repo2. 官方源wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 安装 docker-CE1234yum makecache fast yum install docker-ce -y # 安装的是默认最新版本yum install python-pip -y # pippip install docker-compose # 单机编排工具 指定安装 docker版本12345yum list docker-ce.x86_64 --showduplicates | sort -ryum -y install docker-ce-17.03.2.ce# 学习版本[root@linux-node2 ~]# yum -y install docker-ce-17.12.0.ce 启动查看版本123systemctl enable docker.servicesystemctl start docker.servicesystemctl status docker.service 1docker version 生成绑定了docker的网卡 配置docker镜像加速 拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决 新版的 Docker 使用 /etc/docker/daemon.json（Linux） 来配置 Daemon。 阿里云加速1234567891011121314151. 注册阿里云账号,专用加速器地址获得路径：2. 配置镜像加速器3. 针对Docker客户端版本大于 1.10.0 的用户4. 您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://a64eacm1.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart dockersudo systemctl status docker docker cn 加速123456mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;EOF 网易加速12345678网易的镜像地址：http://hub-mirror.c.163.com。mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125;EOF]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01 Docker 简介]]></title>
    <url>%2F2019%2F10%2F07%2Fdocker-base01%2F</url>
    <content type="text"><![CDATA[Docker 简介1234# 官方文档https://www.runoob.com/docker/docker-tutorial.htmlDocker 官网：http://www.docker.comGithub Docker 源码：https://github.com/docker/docker Docker是Docker.lnc公司(前dotCloud,PaaS市场老牌提供商)开源的一个基于LXC技术之上构建的Container容器引擎,源代码托管在Github上,基于Go语言并遵从Apache2.0协议开源。 Docker是通过内核虚拟化技术（namespaces及cgroups等）来提供容器的资源隔离与安全保障等。由于Docker通过操作系统层的虚拟化实现隔离,所以Docker容器在运行时,不需要类似虚拟机（VM）额外的操作系统开销,提高资源利用率。 123451. kvm解决了硬件和操作系统之间的依赖。2. docker解决了软件和操作系统环境之间的依赖，能够让独立服务或应用程序在不同的环境中，得到相同的运行结果。3. docker容器是一种轻量级、可移植、自包含的软件打包技术，使应用程序可以在几乎任何地方以相同的方式运行。开发人员在自己笔记本上创建并测试好的容器，无需任何修改就能够在生产系统的虚拟机、物理服务器或公有云主机上运行。4. Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版），我们用社区版就可以了。 Docker 的三大理念 构建,运输,随处运行 Build,Ship and Run any App,Angwhere Docker 的应用场景12341. Web 应用的自动化打包和发布。2. 自动化测试和持续集成、发布。3. 在服务型环境中部署和调整数据库或其他的后台应用。4. 从头编译或者扩展现有的 OpenShift 或 Cloud Foundry 平台来搭建自己的 PaaS 环境。 Docker 的优点12345678910111、简化程序： - Docker 让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，便可以实现虚拟化。 Docker改变了虚拟化的方式，使开发者可以直接将自己的成果放入Docker中进行管理。 方便快捷已经是 Docker的最大优势，过去需要用数天乃至数周的 任务，在Docker容器的处理下，只需要数秒就能完成。2、避免选择恐惧症： - 如果你有选择恐惧症，还是资深患者。那么你可以使用 Docker 打包你的纠结！比如 Docker 镜像； Docker 镜像中包含了运行环境和配置，所以 Docker 可以简化部署多种应用实例工作。 比如 Web 应用、后台应用、数据库应用、大数据应用比如 Hadoop 集群、消息队列等等都可以打包成一个镜像部署。 3、节省开支： - 一方面，云计算时代到来，使开发者不必为了追求效果而配置高额的硬件，Docker 改变了高性能必然高价格的思维定势。 Docker 与云的结合，让云空间得到更充分的利用。不仅解决了硬件管理的问题，也改变了虚拟化的方式。 Docker 能做什么 Docker依赖“写时复制”(copy-on-write)模型,修改应用程序非常迅速,“随心所致,代码即改” 简化配置 123这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在 不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。 代码流水线（Code Pipeline）管理 12前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 提高开发效率 123456789这就带来了一些额外的好处：Docker能提升开发者的开发效率。如果你想看一个详细一点的例子，可以参考Aater在DevOpsDays Austin 2014 大会或者是DockerCon上的演讲。不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为 开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。 隔离应用 1有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等 整合服务器 123正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 调试能力 123Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。你可以在《Docker拯救世界》的文章中找到这一点的例证。 多租户环境 12345另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的 例子是为IoT（译者注：物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。 快速部署 12345在虚拟机之前，引入新的硬件资源需要消耗几天的时间。Docker的虚拟化技术将这个时间降到了几分钟，Docker只是创建一个容器进程而无需启动操作系统，这个过程只需要秒级的时间。这正是Google和Facebook都看重的特性。你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 小结:一句话说明docker的本质就是: 低开销（系统文件、内存 共用）的虚拟机 Docker 改变了什么123456789面向产品：产品交付面向开发：简化环境配置面向测试：多版本测试面向运维：环境一致面向架构：自动化扩容（微服务）]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器技术简介]]></title>
    <url>%2F2019%2F10%2F07%2Fdocker-base%2F</url>
    <content type="text"><![CDATA[容器简介 容器是在隔离的环境运行的一个进程，如果进程停止，容器就会销毁。隔离的环境拥有自己的文件系统，ip地址，主机名等。 容器是一种轻量级操作系统层面的虚拟机，它为应用软件及其依赖组件提供了一个资源独立的运行环境。应用软件所依赖的组件会被打包成一个可重用的镜像，镜像运行环境并不会与主操作系统共享内存、CPU和硬盘空间，由此也保证了容器内部的进程与容器外部进程的独立关系。 容器与虚拟化的区别 管理程序虚拟化(hypervisor virtualization,HV) 通过中间层将一台或多台独立懂得虚拟机运行于物理硬件之上。 容器(Containers) 运行在操作系统内核之上的用户空间。 12345简单解释，虚拟机运行在操作系统上，而docker是直接运行在应用上。所以docker无法提供一个像VMware那样完全的隔离，甚至到很多地方都没有进行隔离，比如说用户空间。这里可以解释一下,如果你用的是centos5版本那你就别想安装docker了，如果是centos6的你可以看一眼。因为内核版本比较低，但是如果使用乌班图就可以，因为乌班图的内核更新的比较快。如果公司服务器是centos5和centos6 用docker就需要升级内核，相对比较麻烦. Docker 和 KVM 的对比12KVM 和 Docker 的对比:https://www.qstack.com.cn/archives/148.html 1234567891011121314151617181920212223242526272829301. 虚拟化技术对比： - KVM全虚拟化,需要模拟各种硬件,跑在宿主机之上。 - Xen, VMware半虚拟化,直接跑在硬件上。 - docker严格来说不算是虚拟化技术(操作系统虚拟化),只是进程隔离和资源限制。 2. 占用内容资源对比: - KVM虚拟机一般会独占一段内存，即使闲置，其他虚拟机也无法使用。 - 容器可以只有一个内存上限，没有下限。如果它只使用1MB内存，那么它只占用宿主机1MB内存。宿主机可以将富余内存作为他用。3. 实例的内核对比: - kvm虚拟机内核 无需与宿主机一致 - docker实例内核和宿主机 内核一致 4. 操作系统支持度对比: - kvm支持多种操作系统，除了linux，还支持windows，uninx、solaris等 - docker只支持linux 5. 启动一个实例需要的时间对比: # lxc容器技术将操作系统抽象到了一个新的高度。 # 直接从init启动，省去了硬件自检、grub引导、加载内核、加载驱动等传统启动项目，因此启动飞速。 - docker:秒级 - kvm:分钟级别 6. 镜像模板占用空间对比: - kvm:占用空间大,G级别 - docker:占用空间小,M级别综合来说: - kvm资源隔离比docker更高,kvm支持的操作系统类型更多 - docker比kvm更省资源,可以提供非常接近宿主机的性能 容器更省资源1234561. 容器由于省去了操作系统,整个层级更简化,容器可以在单台服务器上运行更多的应用。2. 虚拟化技术针对每个虚拟机的资源分配是固定的。 - 如下图,三个虚拟机在一台计算机上运行，该计算机配备48 GB内存、12核处理器和3TB磁盘存储空间， - 每个虚拟机被分配了16 GB内存、4核和1TB存储空间，如果其中一个虚拟机使用的内存从不超过1GB，只存储100MB的文件系统， - 该虚拟机仍占用4GB内存和整整1TB的存储空间，仍是造成了大量的资源浪费。 - 而容器则是以共享方式使用主机上的内存、处理器和存储空间。 容器实现秒级启动1231. 虚拟机包含了完整的操作系统环境,同时还提供了对操作系统的控制支持。因此，虚拟机的规模较大，通常会达到数个G。2. 在运行应用前,虚拟机需要预先花费几分钟来引导操作系统,然后才能初始化和运行应用程序。3. 容器则规模较小，一般只有数个M，容器通常可以实现秒级启动。 Docker 与 OpenStack区别 百花齐放的容器技术 虽然 docker 把容器技术推向了巅峰，但容器技术却不是从 docker 诞生的。 实际上，容器技术连新技术都算不上，因为它的诞生和使用确实有些年头了。 下面的一串名称肯能有的你都没有听说过，但它们的确都是容器技术的应用： 1234567891011Chroot JailFreeBSD JailsLinux VServerSolaris ContainersOpenVZProcess ContainersLXCWardenLMCTFYDockerRKT 123456789101112131415161718192021222324252627282930313233341、Chroot Jail就是我们常见的 chroot 命令的用法。它在 1979 年的时候就出现了，被认为是最早的容器化技术之一。它可以把一个进程的文件系统隔离起来。2、The FreeBSD JailFreebsd Jail 实现了操作系统级别的虚拟化，它是操作系统级别虚拟化技术的先驱之一。3、Linux VServer使用添加到 Linux 内核的系统级别的虚拟化功能实现的专用虚拟服务器。4、Solaris Containers它也是操作系统级别的虚拟化技术，专为 X86 和 SPARC 系统设计。Solaris 容器是系统资源控制和通过 &quot;区域&quot; 提供边界隔离的组合。5、OpenVZOpenVZ 是一种 Linux 中操作系统级别的虚拟化技术。 它允许创建多个安全隔离的 Linux 容器，即 VPS。6、Process ContainersProcess 容器由 Google 的工程师开发，一般被称为 cgroups。7、LXCLXC 又叫 Linux 容器，这也是一种操作系统级别的虚拟化技术，允许使用单个 Linux 内核在宿主机上运行多个独立的系统。8、Warden在最初阶段，Warden 使用 LXC 作为容器运行时。 如今已被 CloudFoundy 取代。9、LMCTFYLMCTY 是 Let me contain that for you 的缩写。它是 Google 的容器技术栈的开源版本。Google 的工程师一直在与 docker 的 libertainer 团队合作，并将 libertainer 的核心概念进行抽象并移植到此项目中。该项目的进展不明，估计会被 libcontainer 取代。10、DockerDocker 是一个可以将应用程序及其依赖打包到几乎可以在任何服务器上运行的容器的工具。11、RKTRKT 是 Rocket 的缩写，它是一个专注于安全和开放标准的应用程序容器引擎。 在什么场景需要容器 如果你遇到以下场景，推荐使用容器： 1234567891011121314151. 部署无状态服务，同虚拟机互补使用，实现隔离性2. 如果要部署有状态服务，需要对里面的应用十分的了解3. 作为持续集成的重要工具，可以顺利在开发，测试，生产之间迁移4. 适合部署跨云，跨Region，跨数据中心，混合云场景下的应用部署和弹性伸缩5. 以容器作为应用的交付物，保持环境一致性，树立不可变更基础设施的理念6. 运行进程基本的任务类型的程序7. 用于管理变更，变更频繁的应用使用容器镜像和版本号，轻量级方便的多8. 使用容器一定要管理好应用，进行health check和容错的设计 容器有哪些优点12345678910111213141516171819202122231. 敏捷开发：容器技术最大的优势在于其快速的生成效率，轻量级的打包方式使其具有更好的性能和更小的规模。2. 快速开发：容器解决了应用程序的平台依赖和平台冲突问题，从而帮助开发人员更快地开发程序。每个容器可被看作为一个微服务，因而可以单独进行升级，而不必担心同步问题。3. 版本管理：容器中的镜像可被单独管理，由此可以追踪、记录、生成不同的容器版本，进而分析容器版本的差异。4. 计算环境可移植：容器封装了与应用相关的依赖组件及操作系统信息，由此减轻了应用在不同计算环境下的配置需求。例如，同一个镜像可应用于Windows或Linux环境，并适用于开发、测试、部署等阶段。5. 标准化：容器通常基于开放标准而设计，因而容器在主流Linux系统及Windows系统上都是适用的。6. 安全性：容器间的进程以及容器内外的进程是相互独立的。因此，每个容器的升级或修改对其它容器都是没有影响的。7. 弹性伸缩：由于容器单元间相互独立，由统一的编排工具管理，且编排工具具备发现容器节点的功能，所以容器的弹性扩容可以在短时间内自动完成；同时，由于每个容器均为独立的个体，容器调用的资源和容器的使用由编排工具管理，所以减少某一容器节点不影响整个容器系统的使用。8. 高可用：与弹性伸缩类似，在某一容器节点出现故障时，容器编排工具能够及时发现节点的变化，并根据外部请求情况及时作出调整，不影响整个容器系统的使用，实现系统的高可用。9. 管理便利：容器技术可通过简单的命令行，完成对单一容器的管理，完成对镜像的快速打包和迁移；同时也能通过Kubernetes、Swarm等工具，实现对大规模容器集群的管理。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SaltStack 基础]]></title>
    <url>%2F2019%2F09%2F30%2Fsaltstack-base%2F</url>
    <content type="text"><![CDATA[简介简单介绍 一个配置管理系统，能够维护预定义状态的远程节点。(比如，确保指定的软件被安装，指定的服务在运行) 一个分布式远程执行系统，用来在远程节点（可以是单个节点，也可以是任意规则挑选出来的节点）上执行命令和查询数据。 开发其的目的是为:远程执行提供最好的解决方案，并使远程执行变得更好，更快，更简单。 SaltStack是使用Python语言开发的，同时提供Rest API方便二次开发以及和其它平台进行集成。 参考学习123https://www.cnblogs.com/wangxu01/tag/saltstack/https://github.com/unixhothttps://github.com/unixhot/saltbook-code/tree/master/salt/prod 常用网址• 官方网站：http://www.saltstack.com• 官方文档：http://docs.saltstack.com• GitHub：https://github.com/saltstack• 中国SaltStack⽤户组：http://www.saltstack.cn 四大功能 配置管理 远程执行 云管理 事件驱动 基础架构• Saltstack基于C/S架构 – 服务端:Master – 客户端:Minion • 可以实现传统处理方式,即:客户端发送请求给服务器,服务器收到请求后处理请求,再将结果返回 • 也可以使用消息队列中的发布与订阅(pub/sub)服务模式 工作原理 SaltStack 采用 C/S模式，server端就是salt的master，client端就是minion。 minion与master之间通过ZeroMQ消息队列通信。 minion上线后先与master端联系，把自己的pub key发过去，这时master端通过salt-key -L命令就会看到minion的key，接受该minion-key后，也就是master与minion已经互信 master可以发送任何指令让minion执行了，salt有很多可执行模块，比如说cmd模块，在安装minion的时候已经自带了，它们通常位于你的python库中，locate salt | grep /usr/ 可以看到salt自带的所有东西。 这些模块是python写成的文件，里面会有好多函数，如cmd.run，当我们执行salt ‘*’ cmd.run ‘uptime’的时候，master下发任务匹配到的minion上去，minion执行模块函数，并返回结果。master监听4505和4506端口，4505对应的是ZMQ的PUB system，用来发送消息，4506对应的是REP system是来接受消息的。 运行流程 Salt stack的Master与Minion之间通过ZeroMq进行消息传递，使用了ZeroMq的发布-订阅模式，连接方式包括tcp，ipc salt命令，将cmd.run ls命令从salt.client.LocalClient.cmd_cli发布到master，获取一个Jobid，根据jobid获取命令执行结果。 master接收到命令后，将要执行的命令发送给客户端minion。 minion从消息总线上接收到要处理的命令，交给minion._handle_aes处理 minion._handle_aes发起一个本地线程调用cmdmod执行ls命令。线程执行完ls后，调用minion._return_pub方法，将执行结果通过消息总线返回给master master接收到客户端返回的结果，调用master._handle_aes方法，将结果写的文件中。 salt.client.LocalClient.cmd_cli通过轮询获取Job执行结果，将结果输出到终端。 通信端口master端：4505,4506minion端：4506 工作方式 Local Master/Minion Salt SSH 最传统的运行方式还是C/S模式，管理端安装Master，被管理节点上安装Minion客户端 安装和配置实验环境• CentOS6 10.0.0.200 linux-node1 master/minion• CentOS6 10.0.0.201 linux-node2 minion 关闭 Selinux12345678910111213141516171819202122查看selinux状态[root@linux-node2]# sestatusSELinux status: enabledSELinuxfs mount: /sys/fs/selinuxSELinux root directory: /etc/selinuxLoaded policy name: targetedCurrent mode: permissiveMode from config file: enforcingPolicy MLS status: enabledPolicy deny_unknown status: allowedMax kernel policy version: 28[root@linux-node2 minion]# getengetenforce getent [root@linux-node2]# getenforce Permissive# 临时关闭[root@linux-node2]# setenforce 0# 永久关闭可以修改配置文件/etc/selinux/config,将其中SELINUX设置为disabled。 安装123456789curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo* For RHEL/CentOS 6: 需要python2.7以上版本rpm -ivh http://repo.saltstack.com/yum/redhat/salt-repo-latest.el6.noarch.rpm* For RHEL/CentOS 7:yum install https://repo.saltstack.com/yum/redhat/salt-repo-latest.el7.noarch.rpm Salt Master安装1[root@linux-node1 yum.repos.d]# yum install -y salt-master salt-minion Salt Minion安装1[root@linux-node2 ~]# yum install -y salt-minion 启动 salt-master1234systemctl start salt-mastersystemctl enable salt-master ps -ef | grep salt-master|grep –v grepsystemctl status salt-master 查看目录12345678910111213[root@linux-node1 salt]# pwd/etc/salt[root@linux-node1 salt]# tree pki/pki/├── master│ ├── master.pem # 启动后的密钥│ ├── master.pub # 启动后的公钥 │ ├── minions│ ├── minions_autosign│ ├── minions_denied│ ├── minions_pre│ └── minions_rejected└── minion 客户端修改配置文件123456789# 修改minion配置文件:告诉master是谁# 两台客户端一起修改# 配置文件里不能敲tab键# 78 ID 该参数告诉master &quot;我是谁&quot;，如果不配，默认是主机名 [root@linux-node1 salt]# vim minion 16 master: 10.0.0.250 [root@linux-node2 ~]# sed -i &apos;s#\#master: salt#master: 10.0.0.250#g&apos; /etc/salt/minion 启动 salt-minion123systemctl start salt-minionsystemctl enable salt-minionsystemctl status salt-minion 查看客户端目录1234567# 客户端启动后创建自己的公钥和私钥[root@linux-node2 ~]# tree /etc/salt/pki/etc/salt/pki├── master└── minion ├── minion.pem # 启动后的密钥 └── minion.pub # 启动后的公钥 1234# 客户端ID文件# 如果之后要修改ID值,还需要再重新启动之前删除该文件[root@linux-node2 salt]# cat /etc/salt/minion_idlinux-node2 配置认证 salt-key 在启动minion后,会将自己的公钥发送给master 1234567891011[root@linux-node1 pki]# tree /etc/salt/pki/master/etc/salt/pki/master├── master.pem├── master.pub├── minions├── minions_autosign├── minions_denied├── minions_pre│ ├── linux-node1 # 通过ID命名│ └── linux-node2 # 通过ID命名└── minions_rejected 通过命令同意公钥,让 master 管理 minions 123456789# 查看minion salt-key -L[root@linux-node1 pki]# salt-key -LAccepted Keys:Denied Keys:Unaccepted Keys:linux-node1linux-node2Rejected Keys: 12345salt-key -A # 同意所有minionsalt-key -a linux-node1 -y # 同意指定的nodesalt-key -D # 拒绝,删除salt-key -d linux-node1 123456789101112131415161718192021222324[root@linux-node1 pki]# salt-key -LAccepted Keys:Denied Keys:Unaccepted Keys:linux-node1linux-node2Rejected Keys:[root@linux-node1 pki]# salt-key -AThe following keys are going to be accepted:Unaccepted Keys:linux-node1linux-node2Proceed? [n/Y] y Key for minion linux-node1 accepted.Key for minion linux-node2 accepted.[root@linux-node1 pki]# salt-keyAccepted Keys:linux-node1linux-node2Denied Keys:Unaccepted Keys:Rejected Keys: 查看 minion 目录,master 会把公钥发过来 12345678910111213141516[root@linux-node1 pki]# tree /etc/salt/pki//etc/salt/pki/├── master│ ├── master.pem│ ├── master.pub│ ├── minions│ │ ├── linux-node1│ │ └── linux-node2│ ├── minions_autosign│ ├── minions_denied│ ├── minions_pre│ └── minions_rejected└── minion ├── minion_master.pub # master 把公钥发过来了，双向的交换了密钥 ├── minion.pem └── minion.pub 查看 master目录,原来是在 minions_pre 现在到了minions，说明master可以管理 12345678910111213141516[root@linux-node1 pki]# tree /etc/salt/pki//etc/salt/pki/├── master│ ├── master.pem│ ├── master.pub│ ├── minions # 原来是在 minions_pre│ │ ├── linux-node1│ │ └── linux-node2│ ├── minions_autosign│ ├── minions_denied│ ├── minions_pre│ └── minions_rejected└── minion ├── minion_master.pub ├── minion.pem └── minion.pub 验证通信12345678# 返回true为正常 # * 代表所有，单引号为转义 [root@linux-node1 pki]# salt &apos;*&apos; test.pinglinux-node2: Truelinux-node1: True 12345678# 如果出现salt &apos;*&apos; test.ping出错Minion did not return. [Not connected]# 解决这种错误，需要删除minion端的key，重新认证。rm -rf /etc/salt/pki/minion/minion_master.pubrm -rf /etc/salt/pki/minion/# 重启minion端systemctl restart salt-minion# 一定要记得初始化环境要关闭SEXLINUX 12345# 单独验证# 实际上所有的minion 都收到了 但是只有 node2返回了结果[root@linux-node1 pki]# salt &apos;linux-node2&apos; test.pinglinux-node2: True 查看端口 服务端与客户端发送消息 通过 消息队列 zeromq 所有的minion 都连接到 slat的 4505端口上 1234567# master [root@linux-node1 pki]# netstat -tnlpProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:4505 0.0.0.0:* LISTEN 1470/python tcp 0 0 0.0.0.0:4506 0.0.0.0:* LISTEN 1476/python ... 123456789# -n 不做域名解析# -i 检查所有和4505所有的连接[root@linux-node1 pki]# lsof -n -i:4505COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsalt-mast 1470 root 16u IPv4 20545 0t0 TCP *:4505 (LISTEN)salt-mast 1470 root 18u IPv4 39421 0t0 TCP 10.0.0.250:4505-&gt;10.0.0.250:58670 (ESTABLISHED)salt-mast 1470 root 19u IPv4 39477 0t0 TCP 10.0.0.250:4505-&gt;10.0.0.251:45200 (ESTABLISHED)salt-mini 7502 root 27u IPv4 39420 0t0 TCP 10.0.0.250:58670-&gt;10.0.0.250:4505 (ESTABLISHED) 1234# 返回消息 发送给4506[root@linux-node1 pki]# lsof -n -i:4506COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsalt-mast 1476 root 24u IPv4 20569 0t0 TCP *:4506 (LISTEN) 端口开启123# master使用两个端口和minion通信，要确保这两个端口通信正常。# 4505 publish_port 消息发布系统端口# 4506 ret_port 客户端与服务端通信端口 1234# 可以关闭掉防火墙，或者将端口添加至INPUT链表。# iptables链表规则是自上而下，这里插入在REJECT ALL之前。 iptables -I INPUT 5 -p tcp --dport 4505 -j ACCEPT -m comment --comment &quot;salt_publish_port&quot;iptables -I INPUT 5 -p tcp --dport 4506 -j ACCEPT -m comment --comment &quot;salt_ret_port&quot; 普通用户使用saltstack修改minion_ID123456停止minion服务salt-key –d minion_id 删除minion IDminion端rm –f /etc/salt/minion_idminion端 rm –rf /etc/salt/pki/修改配置文件id启动minion Salt 远程执行命令基础语法 执行salt的基本语法： 1salt &apos;&lt;target&gt;&apos; &lt;function&gt; [arguments] 1234567# 在所有客户端上执行uptime命令# cmd.run[root@linux-node1 ~]# salt &apos;*&apos; cmd.run &apos;uptime&apos;linux-node1: 18:02:09 up 17 min, 1 user, load average: 0.01, 0.05, 0.12linux-node2: 18:02:09 up 17 min, 1 user, load average: 0.00, 0.02, 0.06 选择目标1https://docs.saltstack.com/en/latest/topics/targeting/index.html 和 minion_ID 相关的 通配符 12salt &apos;*&apos; test.versionsalt &apos;linux-node*&apos; test.ping 正则 1[root@linux-node1 base]# salt -E &apos;linux-node(1|2)&apos; test.ping 列表 1234567salt -L &apos;linux-node1,linux-node2&apos; test.version# top file: 所有匹配目标的方式，都可以用到top file里面来指定目标。base: &apos;web1-(prod|devel)&apos;: - match: pcre - webserver minion ID设置方案：IP地址、根据业务来进行设置 12345redis-node1 # redis第一个节点redis04 # 集群game01 # 业务线 和 minion_ID 无关的 GRAINS匹配 1salt -G &apos;os:CentOS&apos; test.version IP地址 1salt -S 10.0.0.0/24 test.version 批处理 123slat并发改成串行 每次自行10个salt &apos;*&apos; -b 10 test.versiosalt -G &apos;os:RedHat&apos; --batch-size 25% apache.signal restart #一次处理25% 执行模块 远程执行三大组件 1231. 选择目标:通配符、正则、list2. 返回:job cache3. 执行模块:network、service、state 1234虚拟模块:针对不同的操作系统 调用不同的方法通过yum默认安装salt所有模块存放路径 /usr/lib/python2.7/site-packages/salt/modules（centos 7）https://www.unixhot.com/docs/saltstack/ref/modules/all/index.html#all-salt-moduleshttps://www.unixhot.com/docs/saltstack/ref/modules/all/salt.modules.service.html#module-salt.modules.service network 123salt &apos;*&apos; network.active_tcpsalt &apos;*&apos; network.arpsalt &apos;*&apos; network.connect archlinux.org 8 service 123# 如果所有的服务器都是统一的操作系统 比如CentOS7 那么可以使用cmd.run 执行命令# 如果 既有7也有6 那么就使用service模块，他底层封装好了帮我们,针对不同的系统,调用不同的命salt &apos;*&apos; service.get_all state 1234567https://www.unixhot.com/docs/saltstack/ref/modules/all/salt.modules.state.html#module-salt.modules.statesalt &apos;*&apos; state.show_top1. 执行模块和状态模块本质区别: 1. 执行模块上来就执行 2. 状态模块先看你有没有,没有就帮你执行salt &apos;*&apos; state.single pkg.installed name=vi 123456789# 常用salt &apos;*&apos; network.active_tcp # 列出所有主机运行的tcp连接salt &apos;*&apos; network.arp # 列出所有主机arpsalt &apos;*&apos; service.available sshd # 列出所有主机sshdsalt &apos;*&apos; service.get_all # 列出所有主机的所有运行服务salt &apos;*&apos; service.status sshd # 列出所有主机sshd运行状态salt-cp &apos;*&apos; /etc/hosts /tmp/test # 将master上/etc/hosts文件拷贝到所有主机的/tmp/testsalt &apos;*&apos; state.show_top # 查看topsalt &apos;*&apos; state.single pkg.installed name=lsof # 所有主机安装lsof Salt 配置管理YAMLYAML 是一个可读性高，用来表达数据序列化的格式。在使用YANL编辑配置文件时，需要注意: 格式: 两个空格缩进 冒号后面要后空格 - 代表列表, - 后面有空格 数据结构可以用类似大纲的缩排方式呈现，结构通过缩进来表示，连续的项目通过减号“-”来表示，map结构里面的key/value对用冒号“:”来分隔。 样例如下：123456789101112131415house: family: name: Doe parents: - John - Jane children: - Paul - Mark - Simone address: number: 34 street: Main Street city: Nowheretown zipcode: 1234 Salt 状态管理 什么叫状态管理状态是对minion的一种描述和定义，管理人员可以不关心具体部署任务时如何完成的，只需要描述minion要达到什么状态，底层由salt的状态模块来完成功能 创建状态配置文件目录 告诉 master 状态文件放在哪个位置 1234567891011121314# 编辑master配置文件# 搜索: /file_roots # base基础环境 dev开发环境 test测试环境 prod生产环境[root@linux-node1 ~]# vim /etc/salt/masterfile_roots: base: - /srv/salt/base dev: - /srv/salt/dev test: - /srv/salt/test prod: - /srv/salt/prod 创建目录 1234567[root@linux-node1 ~]# mkdir -p /srv/salt/&#123;base,dev,test,prod&#125;[root@linux-node1 ~]# tree /srv/salt//srv/salt/├── base # 必须有├── dev # 开发环境├── prod # 生产环境└── test # 测试环境 重启master,改完配置就要重启 1234567[root@linux-node1 ~]# systemctl restart salt-master[root@linux-node1 ~]# systemctl status salt-master[root@linux-node1 ~]# salt &apos;*&apos; test.pinglinux-node2: Truelinux-node1: True 查看日志 123vim /etc/salt/masterlog_level: debug [root@linux-node1 salt]# tail /var/log/salt/master 简单状态管理 自动化安装apache并启动 编写状态文件需要注意 123所有的状态文件 都需要以.sls结尾 --&gt; Salt State所有文件名小写所有主机名都用-,不许用_下划线,因为DNS解析不支持_,主机名也都小写 CentOS7 服务管理 12345678910111213141.启动、终止、重启systemctl start httpd.service #启动systemctl stop httpd.service #停止systemctl restart httpd.service #重启2.设置开机启动/关闭systemctl enable httpd.service #开机启动systemctl disable httpd.service #开机不启动3.检查httpd状态systemctl status httpd.service4.查看服务开机启动列表systemctl list-unit-files; apache.sls1234567891011[root@linux-node1 base]# cd /srv/salt/base/[root@linux-node1 base]# vim apache.slsapache-install: # 唯一表示ID pkg.installed: # pkg状态模块,installed方法 - name: httpd # installed方法参数,这台机器安装http,如果有什么也不做,没有就安装apache-service: # 唯一表示ID service.running: # service状态模块.running方法 - name: httpd # 保证http处于运行状态,运行啥也不做,没有就起来 - enable: True # 开机自动启动,等于当前状态 yum install httpd | systemctl start|enable httpd 12345678910[root@linux-node1 base]# vim apache.slsapache-install: pkg.installed: - name: httpdapache-service: service.running: - name: httpd - enable: True 单独执行1234# salt '*' state.sls apache# state(状态执行模块).sls(方法)# 去base下找 apache.sls 文件 并执行这个状态 # .sls(省略) 1234# 去node2看看是否在yum安装httpd[root@linux-node2 ~]# ps aux | grep yumroot 1829 0.0 2.3 322532 23896 ? R 18:57 0:00 /usr/bin/python /usr/bin/yum --quiet --assumeyes check-update --setopt=autocheck_running_kernel=falseroot 1833 0.0 0.0 112712 956 pts/0 R+ 18:57 0:00 grep --color=auto yum 查看node1的执行状态 12345678# 需要关注的执行状态ID: apache-service # 状态IDResult: True # 返回结果Changes: # 变化Succeeded: 2 (changed=2) # 成功2个,改变2个Failed: 0Total states run: 2 # 总共运行了2 查看node2上的服务和端口状态 可以将node2的httpd关闭 再执行测试 1[root@linux-node2 ~]# systemctl stop httpd 123使用彩色输出时，颜色代码如下：green表示成功，red表示失败，blue表示更改和成功以及yellow表示预期的将来配置更改。状态管理,就算你卸载了httpd,再执行都会帮你安装并启动 分类 分类所有的sls文件都存放在base目录下,在生产项目中会相当难找到,所以需要根据软件的功能进行目录分类 123456789101112# 创建分类目录[root@linux-node1 base]# mkdir -p &#123;sql,web,nosql,monitor&#125;[root@linux-node1 base]# mv apache.sls web/[root@linux-node1 base]# tree.├── monitor├── nosql├── sql└── web └── apache.sls 多级目录执行 12# base是基准目录[root@linux-node1 base]# salt '*' state.sls web.apache 高级状态管理 top.sls 编辑top.sls如果我们有很多的sls文件,但是这些文件只能单独执行,如果批量执行呢?这个时候我们可以使用top.sls,他相当于任务编排 1234[root@linux-node1 base]# vim /etc/salt/master/top.sls# 打开注释state_top: top.sls 1234567# top.sls默认放在base环境下[root@linux-node1 base]# vim /srv/salt/base/top.slsbase: # 执行环境 'linux-node1': # minion_id - web.apache # 要执行的状态 'linux-node2': - web.apache 1234567[root@linux-node1 base]# vim /srv/salt/base/top.slsbase: 'linux-node1': - web.apache 'linux-node2': - web.apache 执行 12345[root@linux-node1 base]# salt '*' state.highstate# 高级状态根据ID单独执行[root@linux-node1 base]# salt 'linux-node1' state.highstate# 高级状态,salt默认会去base下找top.sls,读取编排,哪个minion执行什么状态# 如果80端口被nginx服务占用,记得要先关闭,salt无法帮你关闭不是要求的工作状态 常用的状态模块 服务的基本要求: 安装、配置、启动 pkg 功能:管理软件包状态 常用方法: 1234• pkg.installed # 确保软件包已安装. 如果没有安装就安装• pkg.latest # 确保软件包是最新版本, 如果不是, 升级• pkg.remove # 确保软件包已卸载, 如果之前已安装, 卸载• pkg.purge # 除remove外, 也会删除其配置文件 官方文档： https://docs.saltstack.cn/ref/states/all/index.html https://docs.saltstack.cn/ref/states/all/salt.states.pkg.html#module-salt.states.pkg 1234# pkg是虚拟模块,在CentOS上调用 yum安装# 正确的写法是 先找一台机器,yum安装看看需要哪些包,然后再写# 一个ID下面只允许一个状态执行一次# pkgs:要从软件存储库安装的软件包列表 12345678910lamp-install: pkg.installed: - pkgs: - httpd - php - php-pdo - php-mysql - php-cli - php-common - mysql file 功能:管理文件状态 常用方法: 123• file.managed # 保证文件存在并且为对应的状态• file.recurse # 保证目录存在并且为对应状态• file.absent # 确保文件不存在, 如果存在就删除 官方文档： https://docs.saltstack.cn/ref/states/all/salt.states.file.html#module-salt.states.file 1234name # 在客户端的位置source # 源目录base/web/ 会把源目录下的文件,替换放过去,也支持ftp和http下载# 以后都以 source 的文件为准了,所有的位置都在一个地方改# 不一样的配置如何设置呢? 1234567891011121314151617181920212223apache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://web/files/httpd.conf - user: root - group: root - mode: 644php-config: file.managed: - name: /etc/php.ini - source: salt://web/files/php.ini - user: root - group: root - mode: 644mysql-config: file.managed: - name: /etc/my.cnf - source: salt://web/files/my.cnf - user: root - group: root - mode: 644 12345678910111213141516# 其他模块方法file.copy /etc/nginx/nginx.conf /home/zhanqi/nginx.conf # 文件复制file.get_sum /home/zhanqi/nginx.conf md5 # 文件md5校验file.chown /home/zhanqi/nginx.conf zhanqi zhanqi # 修改文件所属用户以及组file.set_mode /home/zhanqi/nginx.conf 755 # 修改文件权限file.mkdir /home/zhanqi/testdir # 创建目录file.remove /home/zhanqi/testdir # 移除目录file.append /home/zhanqi/nginx.conf 'add_header "Access-Control-Allow-Origin" "*";' # 追加内容到文件file.recurse # 下发目录 service 功能:管理服务状态 常用方法: 1234• service.running # 确保服务处于运行状态,如果没有行就启动• service.enabled # 确保服务开机启动• service.disabled # 确保服务开机不启动启动• service.dead # 确保服务当前没有运行,如果运行就停止 官方文档： https://docs.saltstack.cn/ref/states/all/salt.states.service.html#module-salt.states.service 123456789httpd-service: service.running: - name: httpd - enable: Truemariadb-service: service.running: - name: mariadb - enable: True 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 执行成功日志[root@linux-node1 web]# salt 'linux-node2' state.sls web.lamplinux-node2:---------- ID: lamp-install Function: pkg.installed Result: True Comment: All specified packages are already installed Started: 09:09:40.109789 Duration: 664.41 ms Changes: ---------- ID: apache-config Function: file.managed Name: /etc/httpd/conf/httpd.conf Result: True Comment: File /etc/httpd/conf/httpd.conf is in the correct state Started: 09:09:40.776230 Duration: 18.621 ms Changes: ---------- ID: php-config Function: file.managed Name: /etc/php.ini Result: True Comment: File /etc/php.ini is in the correct state Started: 09:09:40.795022 Duration: 9.828 ms Changes: ---------- ID: mysql-config Function: file.managed Name: /etc/my.cnf Result: True Comment: File /etc/my.cnf is in the correct state Started: 09:09:40.805020 Duration: 8.337 ms Changes: ---------- ID: httpd-service Function: service.running Name: httpd Result: True Comment: The service httpd is already running Started: 09:09:40.814034 Duration: 32.034 ms Changes: ---------- ID: mariadb-service Function: service.running Name: mariadb Result: True Comment: Service mariadb has been enabled, and is running Started: 09:09:40.846290 Duration: 4801.984 ms Changes: ---------- mariadb: TrueSummary for linux-node2 salt缓存 123456789101112131415[root@linux-node1 salt]# cd /var/cache/salt/minion/[root@linux-node1 minion]# tree.├── accumulator├── extmods├── files│ └── base│ ├── lamp.sls│ ├── top.sls│ └── web│ └── apache.sls├── highstate.cache.p├── module_refresh├── proc└── sls.p 状态间关系file.recurse 管理多个配置文件 功能: 通过主目录上的子目录递归，并将所述子目录复制到指定的路径 12345httpd管理多个配置文件:1. 管理conf.d/*.conf2. 先看Include目录[root@linux-node2 etc]# vim /etc/httpd/conf/httpd.confInclude conf.modules.d/*.conf 12345678910111213141516171819[root@linux-node1 files]# pwd/srv/salt/base/web/files[root@linux-node1 files]# mkdir -p apache-conf.d[root@linux-node1 files]# cd apache-conf.d/[root@linux-node1 apache-conf.d]# cp /etc/httpd/conf.d/* ./[root@linux-node1 apache-conf.d]# touch leo.conf[root@linux-node1 apache-conf.d]# tree.├── autoindex.conf├── leo.conf├── php.conf├── README├── userdir.conf└── welcome.conf0 directories, 6 files 添加管理语句 12345678910[root@linux-node1 web]# vim lamp.sls apache-conf: file.recurse: - name: /etc/httpd/conf.d - source: salt://web/files/apache-conf.d /etc/profile: file.append: - text: - "#Redis_PATH=''" watch 和 watch_in 处理状态之间的关系 功能 如果我的配置文件变更,服务就自动reload 12• watch # 我关注某个状态• watch_in # 我被某个状态关注 1234# file 是要监控的状态模块 apache-conf 是状态的ID# apache-config 文件 更新就重启# 增加了 reload: True watch会执行reload# 服务不支持reload 比如mysql 那就不要加,mysql动态改参数 set 1234567httpd-service: service.running: - name: httpd - enable: True - reload: True - watch: - file: apache-config 文件的状态被关注 wacth # 服务关注什么 watch_in # 文件被什么所关注 目录有更新 就执行重启 123456apache-conf: file.recurse: - name: /etc/httpd/conf.d - source: salt://web/files/apache-conf.d - watch_in: - service: httpd-service require和 require_in依赖某个状态 功能: 比如安装某个软件没有成功,就不往下走了 12• require # 我依赖某个状态• require_in # 我被某个状态依赖 lamp-install 如果安装不成功,就不会向下执行 123456789apache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://web/files/httpd.conf - user: root - group: root - mode: 644 - require: - pkg: lamp-install unless 状态判断添加info.php页面12345678910[root@linux-node2 conf]# cd /var/www/html/[root@linux-node2 html]# mkdir admin[root@linux-node2 html]# cd admin/[root@linux-node2 admin]# vim info.php&lt;?phpphpinfo();?&gt;# 访问http://10.0.0.251/admin/info.php 访问admin的时候输入用户名和密码1234567891011121314151617181920212223[root@linux-node2 admin]# vim /etc/httpd/conf/httpd.conf &lt;Directory "/var/www"&gt; AllowOverride None # Allow open access: Require all granted&lt;/Directory&gt;&lt;Directory "/var/www/html/admin"&gt; Options Indexes FollowSymLinks AllowOverride All Order allow,deny Allow from all AuthName "sys" AuthType Basic AuthUserFile /etc/httpd/conf/httppwd_file require user admin&lt;/Directory&gt;[root@linux-node2 admin]# systemctl restart httpd# 访问http://10.0.0.251/admin/info.php 修改状态文件123456789101112[root@linux-node1 web]# vim /srv/salt/base/web/files/httpd.conf&lt;Directory "/var/www/html/admin"&gt; Options Indexes FollowSymLinks AllowOverride All Order allow,deny Allow from all AuthName "sys" AuthType Basic AuthUserFile /etc/httpd/conf/httppwd_file require user admin&lt;/Directory 12345# 保证要有用到的命令[root@linux-node2 admin]# whereis htpasswdhtpasswd: /usr/bin/htpasswd /usr/share/man/man1/htpasswd.1.gz[root@linux-node2 admin]# rpm -qf /usr/bin/htpasswdhttpd-tools-2.4.6-90.el7.centos.x86_64 状态判断 unless123# 很多命令只需要执行一次,unless 通过判断 0 和 1 # 包httpd-tools 被下面的cmd.run使用,如果包不安装成功,下面的命令也不执行,因为apache-auth只有一个 cmd# unless 如果条件为真0,那么cmd.run就不执行 12345678apache-auth: pkg.installed: - name: httpd-tools - require_in: - cmd: apache-auth cmd.run: - name: /usr/bin/htpasswd -bc /etc/httpd/conf/httppwd_file admin admin - unless: test -f /etc/httpd/conf/httppwd_file 1234567# 执行测试[root@linux-node1 web]# salt 'linux-node2' state.sls web.lamp test=True[root@linux-node1 web]# salt 'linux-node2' state.sls web.lamp#访问:http://10.0.0.251/admin/info.phpadmin/admin lamp.sls 最终版12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@linux-node1 web]# vim lamp.sls lamp-install: pkg.installed: - pkgs: - httpd - php - php-pdo - php-mysql - php-cli - php-common - mariadb - mariadb-serverapache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://web/files/httpd.conf - user: root - group: root - mode: 644 - require: - pkg: lamp-installapache-conf: file.recurse: - name: /etc/httpd/conf.d - source: salt://web/files/apache-conf.d - watch_in: - service: httpd-serviceapache-auth: pkg.installed: - name: httpd-tools - require_in: - cmd: apache-auth cmd.run: - name: /usr/bin/htpasswd -bc /etc/httpd/conf/httppwd_file admin admin - unless: test -f /etc/httpd/conf/httppwd_file/etc/profile: file.append: - text: - "#Redis_PATH=''"php-config: file.managed: - name: /etc/php.ini - source: salt://web/files/php.ini - user: root - group: root - mode: 644mysql-config: file.managed: - name: /etc/my.cnf - source: salt://web/files/my.cnf - user: root - group: root - mode: 644httpd-service: service.running: - name: httpd - enable: True - reload: True - watch: - file: apache-configmariadb-service: service.running: - name: mariadb - enable: True 源码安装 Tomcat和 JDK 需求:判断minion有没有安装jdk和tomcat，没有就分别源码安装，并以普通用户启动。 上传软件包12345678[root@linux-node1 tools]# pwd/srv/salt/base/web/tools[root@linux-node1 tools]# tree.├── apache-tomcat-8.0.27.tar.gz└── jdk-8u60-linux-x64.tar.gz0 directories, 2 files 12345# 安装执行步骤将安装包传递到指定的目录解压添加环境变量启动服务 编写 tomcat.sls1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@linux-node1 web]# mkdir tools[root@linux-node1 web]# vim tomcat.sls jdk-install: file.managed: - name: /data/tools/jdk-8u60-linux-x64.tar.gz - source: salt://web/tools/jdk-8u60-linux-x64.tar.gz - user: root - group: root - made: 755 cmd.run: - name: cd /data/tools/ &amp;&amp; tar -zxf jdk-8u60-linux-x64.tar.gz &amp;&amp; chown -R root:root jdk1.8.0_60 &amp;&amp; mv jdk1.8.0_60 /data/ &amp;&amp; ln -s /data/jdk1.8.0_60 /data/jdk - require: - file: jdk-install - unless: test -L /data/jdk &amp;&amp; test -d /data/jdk1.8.0_60jdk-config: file.append: - name: /etc/profile - text: - '#JDK-ENV:' - JAVA_HOME=/data/jdk - PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH - CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar - export PATH JAVA_HOME CLASSPATH cmd.run: - name: source /etc/profile - require: - file: jdk-config - unless: source /etc/profile &amp;&amp; java -versiontomcat-install: file.managed: - name: /data/tools/apache-tomcat-8.0.27.tar.gz - source: salt://web/tools/apache-tomcat-8.0.27.tar.gz - user: root - group: root - made: 755 cmd.run: - name: cd /data/tools/ &amp;&amp; tar -zxf apache-tomcat-8.0.27.tar.gz &amp;&amp; chown -R root:root apache-tomcat-8.0.27 &amp;&amp; mv apache-tomcat-8.0.27 /data/ &amp;&amp; ln -s /data/apache-tomcat-8.0.27 /data/tomcat - require: - file: tomcat-install - unless: test -L /data/tomcat &amp;&amp; test -d /data/apache-tomcat-8.0.27tomcat-start: cmd.run: - name: source /etc/profile &amp;&amp; /bin/bash /data/tomcat/bin/startup.sh - require: - file: tomcat-install - unless: ps -ef | grep tomcat|grep -v 'grep' grains 和 pillar 数据收集 grains 收集静态数据Grains是saltstack的组件，用于收集salt-minion在启动时候的信息，又称为静态信息。Grains是服务器的一系列粒子信息，也就是服务器的一系列物理，软件环境信息。在执行salt的sls时候可以根据Grains信息的不同对服务器进行匹配分组，例如可以根据系统是centos服务器跟系统是redhat环境的安装不同的软件包。 功能: 负责minion第一次启动的时候采集的静态数据，可以用在salt的模块和其他组件中。每次的minion启动（重启）的时候都会采集。 收集资产信息,信息查询 静态数据，当Minion启动的时候收集的MInion本地的相关信息。（包含操作系统版本、内核版本、CPU、内存、硬盘、设备型号等）备注：不重启minion，这些信息数据是不会改变的。 123Grains可以在state系统中使用,用于配置管理模块Grains可以target中使用,在用来匹配Minion,比如匹配操作系统使用-G选项Grains可以用于信息查询,Grains保存着收集到的客户端的详细信息 目标选择123456789# grains 匹配所有CentOS的机器 执行test.ping [root@linux-node1 ~]# salt -G 'os:CentOS' test.ping # grains[root@linux-node1 ~]# salt -G 'os:CentOS' cmd.run "date"linux-node1: Wed Oct 2 21:31:14 CST 2019linux-node2: Wed Oct 2 20:59:05 CST 2019 信息采集1234salt 'linux-node2' grains.ls # 列出ID为linux-node2的主机,grains的所有keysalt 'linux-node2' grains.items # 列出主机的详细信息,可用于资产管理salt '*' grains.item os # 列出所有主机的系统版本salt '*' grains.item fqdn_ip4 # 列出所有主机的IP地址 12345678910111213# 需要配置好 vim /etc/hosts 主机名与IP解析cat /etc/salt/minion_idcat /etc/hosts[root@linux-node1 ~]# salt &apos;*&apos; grains.item fqdn_ip4linux-node1: ---------- fqdn_ip4: - 10.0.0.250linux-node2: ---------- fqdn_ip4: - 10.0.0.251 jinja2 模板12345文档：http://docs.jinkan.org/docs/jinja2/Jinja2的应用场景：针对不同的操作系统安装软件，针对不同的cpu数量、内存等动态生成软件的配置文件，都需要Jinja2以及Grains和pillar的辅助Jinja2是一个强大的python模板引擎，他的设计思想来源于Django的模板引擎，并扩展了其语法和一系列强大的功能。其中最显著的一个是增加了沙箱执行功能和可选的自动转义功能，这对大多应用的安全性来说是非常重要的。jinja模板包含 变量 或 表达式，两种分隔符: &#123;% ... %&#125; 和 &#123;&#123; ... &#125;&#125; 。前者用于执行诸如 for 循环 或赋值的语句，后者把表达式的结果打印到模板上。salt中如何使用jinja2：https://docs.saltstack.com/en/latest/topics/jinja/index.html 使用 jinja2 和 grains 修改配置文件httpd 本机IP:PORT 1234567891011121314[root@linux-node1 web]# vim lamp.sls apache-config: file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://web/files/httpd.conf - user: root - group: root - mode: 644 - template: jinja - defaults: PORT: 9090 IPADDR: &#123;&#123; grains[&apos;fqdn_ip4&apos;][0] &#125;&#125; - require: - pkg: lamp-install 1234[root@linux-node1 web]# vim files/httpd.conf/80#Listen 12.34.56.78:80Listen &#123;&#123; IPADDR &#125;&#125;:&#123;&#123; PORT &#125;&#125; 123[root@linux-node1 web]# salt &apos;linux-node2&apos; state.sls web.lamp# 验证访问:http://10.0.0.251:9090/ 日常管理生产建议12345678910111213141516171819202122232425261. 不建议使用salt的FILE模块进行代码部署 1. 命令编排的状态管理: 压缩包, file.managed cmd.run 执行部署2. 配置管理,不建议使用salt管理项目的配置文件 1. 分层管理,salt只管理应用服务,例如Nginx Tomcat Apache 2. 开发的配置文件 不管 3. 如果你有固定的文件服务器,可以使用 source: salt:// http:// ftp://4. SLS版本化 1. 创建一个git项目 2. 找一个测试环境,编写SLS, 测试git commit &amp;&amp; git push到版本仓库 3. 生产环境git pull 测试,全部执行 4. 保留操作记录 5. 完整可控:谁 什么时间 干了什么 输出是什么 5. 使用Master Job Cache 保存job的输出到SQL 1. 保存在 [root@linux-node1 web]# cd /var/cache/salt/master/jobs/ 2. 保存时长: ***** 默认24小时 vim /etc/salt/master Set the number of hours to keep old job information in the job cache: #keep_jobs: 24 include 功能:有时候我们写了很多的sls文件,这些文件可以被复用 备份1[root@linux-node1 salt]# zip -r base.zip base/ 复制lamp中安装部分,写到httpd.sls里面12345678910111213[root@linux-node1 web]# vim httpd.slslamp-install: pkg.installed: - pkgs: - httpd - php - php-pdo - php-mysql - php-cli - php-common - mariadb - mariadb-server 123456# 修改lamp.sls,引入[root@linux-node1 web]# vim lamp.sls include: - web.httpd... 12# 执行[root@linux-node1 web]# salt &apos;linux-node2&apos; state.sls web.lamp test=True 常用命令12345678910111213141516# 查看状态salt-run manage.status# 查看版本salt-run manage.versions# 测试salt &apos;*&apos; state.sls web.tomcat test=true# 修改minion_id1. 停止minion服务2. salt-key -d minion_id 删除 minion 3. rm -f /etc/salt/minion_id4. rm -rf /etc/salt/pki5. 修改配置文件ID6. 启动minio 无 master 架构123456789101112131415161718192021222324252627282930313233343536371. salt本地管理2. masterless架构 1. 没有master,相当于单机使用 2. salt 本地来执行 3. 在node1测试 1. 修改配置文件 [root@CentOS7 ~]# vim /etc/salt/minion /file_client #file_client: remote 修改成 file_client: local 2.把file_root 加进来到node1的minion配置 file_roots: base: - /srv/salt/base dev: - /srv/salt/dev test: - /srv/salt/test prod: - /srv/salt/prod 3. 关闭minion服务 [root@linux-node1 web]# systemctl stop salt-minion4. 执行状态 salt-call [root@linux-node1 web]# salt-call --local state.sls web.tomcat # --local 在本地 5. 用于单台服务器安装软件执行 1. 网络不允许上外网,只能本地安装 2. 安装saltstack 3. 把状态文件解压 4. 执行命令 5. 查看安装状态 日志入库 job cache12job cache:https://docs.saltstack.com/en/latest/ref/returners/all/salt.returners.mysql.html#module-salt.returners.mysql 安装mysql，建库建表12345678910[root@linux-node1 base]# yum -y install mariadb mariadb-server mariadb-client[root@linux-node1 base]# systemctl start mariadb[root@linux-node1 base]# systemctl enable mariadb# 初始化[root@linux-node1 base]# mysql_secure_installation# 创建用户MariaDB [(none)]&gt; grant all on salt.* to salt@10.0.0.250 identified by 'salt';MariaDB [(none)]&gt; flush privileges; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 建库建表语句CREATE DATABASE `salt` DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;USE `salt`;---- Table structure for table `jids`--DROP TABLE IF EXISTS `jids`;CREATE TABLE `jids` ( `jid` varchar(255) NOT NULL, `load` mediumtext NOT NULL, UNIQUE KEY `jid` (`jid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE INDEX jid ON jids(jid) USING BTREE;---- Table structure for table `salt_returns`--DROP TABLE IF EXISTS `salt_returns`;CREATE TABLE `salt_returns` ( `fun` varchar(50) NOT NULL, `jid` varchar(255) NOT NULL, `return` mediumtext NOT NULL, `id` varchar(255) NOT NULL, `success` varchar(10) NOT NULL, `full_ret` mediumtext NOT NULL, `alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP, KEY `id` (`id`), KEY `jid` (`jid`), KEY `fun` (`fun`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;---- Table structure for table `salt_events`--DROP TABLE IF EXISTS `salt_events`;CREATE TABLE `salt_events` (`id` BIGINT NOT NULL AUTO_INCREMENT,`tag` varchar(255) NOT NULL,`data` mediumtext NOT NULL,`alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,`master_id` varchar(255) NOT NULL,PRIMARY KEY (`id`),KEY `tag` (`tag`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 123456789101112[root@linux-node1 base]# mysql -h 10.0.0.250 -usalt -psaltMariaDB [(none)]&gt; use saltMariaDB [salt]&gt; show tables;+----------------+| Tables_in_salt |+----------------+| jids || salt_events || salt_returns |+----------------+3 rows in set (0.00 sec) 安装 MySQL-python1[root@linux-node1 base]# yum install -y MySQL-python 配置 master123456789[root@linux-node1 base]# vim /etc/salt/master# 在文档最底下添加master_job_cache: mysqlmysql.host: &apos;10.0.0.250&apos;mysql.user: &apos;salt&apos;mysql.pass: &apos;salt&apos;mysql.db: &apos;salt&apos;mysql.port: 3306 12# 重启服务[root@linux-node1 base]# systemctl restart salt-master 执行 test.ping 查看cache 是否入库123456789[root@linux-node1 base]# salt &apos;*&apos; test.pinglinux-node1: Truelinux-node2: True[root@linux-node1 base]# mysql -h 10.0.0.250 -usalt -psaltMariaDB [(none)]&gt; use saltMariaDB [salt]&gt; select * from salt_returns\G; 12345# 显示 jid 执行[root@linux-node1 base]# salt &apos;*&apos; cmd.run &apos;w&apos; -vExecuting job with jid 20191006000128231437MariaDB [salt]&gt; select * from salt_returns where jid = &apos;20191006000128231437&apos;\G; kill salt 正在执行的任务12https://www.cnblogs.com/shhnwangjian/p/6048891.htmlhttps://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.saltutil.html#module-salt.modules.saltutil 12345678salt &apos;*&apos; saltutil.running # 查看正在运行的任务，找到jidsalt &apos;*&apos; saltutil.kill_job jid # 根据jid杀掉任务salt &apos;*&apos; saltutil.clear_cache # 清除minion缓存备注：1）正在执行的salt任务，job id会存在minion端的/var/cache/salt/minion/proc目录下2）正在执行的salt任务，根据上面master cache的配置，Job的路径/var/cache/salt/master/jobs目录下 二次开发建议1231. master job cache 将所有的job输出保存到mysql2. 如果做管理平台，可以将user_id 和 jid 关联3. 使用List做目标选择 案例实战 zabbix-agent定义安装目录1[root@linux-node1 base]# mkdir -p init zabbix logstash init epel源创建目录获取yum文件12[root@linux-node1 base]# mkdir -p /srv/salt/base/init/files[root@linux-node1 files]# wget http://mirrors.aliyun.com/repo/epel-7.repo 编写 yum-repo.sls123456789[root@linux-node1 init]# vim yum-repo.slsepel-7.repo: file.managed: - name: /etc/yum.repos.d/epel-7.repo: - source: salt://init/files/epel-7.repo - user: root - group: root - mode: 644 zabbix-agent模拟安装123[root@linux-node1 files]# yum list|grep zabbix[root@linux-node1 files]# yum list|grep zabbix30-agent[root@linux-node1 files]# yum install -y zabbix30-agent 模板文件123456[root@linux-node1 file]# mkdir -p /srv/salt/base/zabbix/file[root@linux-node1 file]# cp /etc/zabbix/zabbix_agentd.conf ./[root@linux-node1 file]# vim zabbix_agentd.conf Server=&#123;&#123; SERVER_IP &#125;&#125;Hostname=&#123;&#123; AGENT_HOSTNAME &#125;&#125; 测试获取主机名123456789[root@linux-node1 file]# salt &apos;*&apos; grains.item fqdnlinux-node1: ---------- fqdn: linux-node1linux-node2: ---------- fqdn: linux-node2 编写 zabbix-agent.sls123456789101112131415161718192021222324252627282930313233343536373839404142[root@linux-node1 zabbix]# cd /srv/salt/base/zabbix/[root@linux-node1 zabbix]# vim zabbix-agent.slsinclude: - init.yum-repo # 引入epel源zabbix-agent: pkg.installed: - name: zabbix30-agent - require: - file: epel-7.repo # 依赖 file.managed: - name: /etc/zabbix_agentd.conf - source: salt://zabbix/files/zabbix_agentd.conf - user: root - group: root - mode: 644 - require: - pkg: zabbix-agent - template: jinja - defaults: SERVER_IP: 10.0.0.250 AGENT_HOSTNAME: &#123;&#123; grains[&apos;fqdn&apos;] &#125;&#125; - require: - pkg: zabbix-agent # 依赖,pkg安装后才能执行 service.running: - name: zabbix-agent - enable: True - watch: - pkg: zabbix-agent - file: zabbix-agent # 文件有变化就重启zabbix_agent.conf.d: file.directory: - name: /etc/zabbix_agent.conf.d - watch_in: - service: zabbix-agent # 目录有变化了就重启 - require: - file: zabbix-agent # 依赖,需要安装和文件都执行才能执行 - pkg: zabbix-agent 123[root@linux-node1 zabbix]# salt &apos;linux-node2&apos; state.sls zabbix.zabbix-agent test=True[root@linux-node1 zabbix]# salt &apos;linux-node2&apos; state.sls zabbix.zabbix-agent[root@linux-node1 zabbix]# salt &apos;linux-node2&apos; cmd.run &apos;netstat -tnlp&apos; 编译安装 redis创建目录123456789101112[root@linux-node1 redis]# mkdir -p /srv/salt/prod/modules/&#123;apache,haproxy,keepalived,mysql,redis&#125;[root@linux-node1 prod]# tree.└── modules ├── apache ├── haproxy ├── keepalived ├── mysql └── redis ├── files │ └── redis-4.0.14.tar.gz └── redis-install.sls 编辑基础安装文件1234567891011121314[root@linux-node1 redis]# vim redis-install.sls redis-source.install: file.managed: - name: /data/tools/redis-4.0.14.tar.gz - source: salt://modules/redis/files/redis-4.0.14.tar.gz - user: root - group: root - mode: 644 cmd.run: - name: cd /data/tools/ &amp;&amp; tar xzf redis-4.0.14.tar.gz &amp;&amp; cd redis-4.0.14 &amp;&amp; make &gt;&gt;/dev/null &amp;&amp; make install &gt;&gt;/dev/null &amp;&amp; mv /data/tools/redis-4.0.14 /data/redis - require: - file: redis-source.install - unless: test -d /data/redis || test -d /data/redis_7006 redis安装 根据需求进行安装,可以使单机、主从也可以是集群 只需要导入基础的安装,再进行相应的配置启动就可以了 实现创建好目录和基础配置文件,做好模板 123[root@linux-node1 redis]# mkdir -p /srv/salt/prod/redis/files# 清理注释并改名[root@linux-node1 files]# egrep -v &apos;#|^$&apos; /root/redis.conf &gt;/srv/salt/prod/redis/files/redis-7006.conf 1234567891011# 模板 比如单机安装的时候我们选择7006为端口[root@linux-node1 files]# vim redis-7006.conf bind 0.0.0.0port &#123;&#123; PORT &#125;&#125;daemonize yespidfile /data/redis_&#123;&#123; PORT &#125;&#125;/redis_&#123;&#123; PORT &#125;&#125;.pidlogfile /data/redis_&#123;&#123; PORT &#125;&#125;/logs/redis_&#123;&#123; PORT &#125;&#125;.logdbfilename dump_&#123;&#123; PORT &#125;&#125;.rdbdir /data/redis_&#123;&#123; PORT &#125;&#125;/datarequirepass 123456 123456789101112131415161718192021222324252627[root@linux-node1 redis]# vim redis-server.sls include: - modules.redis.redis-installredis-init: cmd.run: - name: cd /data &amp;&amp; mv redis redis_7006 &amp;&amp; mkdir -p /data/redis_7006/&#123;conf,logs,data&#125; - unless: test -d /data/redis_7006 - require: - cmd: redis-source.installredis-server: file.managed: - name: /data/redis_7006/conf/redis-7006.conf - source: salt://redis/files/redis-7006.conf - user: root - group: root - mode: 755 - template: jinja - defaults: PORT: 7006 cmd.run: - name: redis-server /data/redis_7006/conf/redis-7006.conf - require: - file: redis-server - unless: ps -ef | grep redis|grep -v &apos;grep&apos; 123# 测试执行[root@linux-node1 prod]# salt &apos;linux-node2&apos; state.sls redis.redis-server saltenv=prod test=True[root@linux-node1 prod]# salt &apos;linux-node2&apos; state.sls redis.redis-server saltenv=prod 123456789101112131415[root@linux-node1 prod]# tree.├── modules # 基础软件安装模块│ ├── apache│ ├── haproxy│ ├── keepalived│ ├── mysql│ └── redis│ ├── files│ │ └── redis-4.0.14.tar.gz│ └── redis-install.sls # 只有安装软件，也可以加上源码安装└── redis # 需求模块 ├── files │ └── redis-7006.conf └── redis-server.sls # inclued 安装即可 配置文件 启动 参考学习12https://github.com/unixhothttps://github.com/unixhot/saltbook-code/tree/master/salt/pro salt-ssh12341.salt-ssh 是 0.17.0 新引入的一个功能，不需要minion对客户端进行管理，也不需要master。2.salt-ssh 支持salt大部分的功能：如 grains、modules、state 等3.salt-ssh 没有使用ZeroMQ的通信架构，执行是串行模式类似 paramiko、pssh、ansible 这类的工具 123没有minion如何使用salt,通过salt-ssh实际上就是把salt基础的 放在客户端的/tmp 然后在本地执行，执行完成后删除写个脚本 发到客户端 执行 将结果返回 12345678910111213141516171819202122232425262728salt-ssh需要一个名单系统来确定哪些执行目标，Salt的0.17.0版本中salt-ssh引入roster系统roster系统编译成了一个数据结构，包含了targets，这些targets是一个目标系统主机列表和或如连接到这些targets# target的信息 host: # 远端主机的ip地址或者dns域名 user: # 登录的用户 passwd: # 用户密码,如果不使用此选项，则默认使用秘钥方式# 可选的部分 port: # ssh端口 sudo: # 可以通过sudo tty: # 如果设置了sudo，设置这个参数为true priv: # ssh秘钥的文件路径 timeout: # 当建立链接时等待响应时间的秒数 minion_opts: # minion的位置路径 thin_dir: # target系统的存储目录，默认是/tmp/salt-&lt;hash&gt; cmd_umask: # 使用salt-call命令的umask值 安装1234# 没有minion如何使用salt,通过salt-ssh,客户端关闭minionsystemctl stop salt-minion# 安装salt-ssh,master也是要有的,客户端需要支持ssh,python也要统一版本,生产 = python虚拟环境yum install -y salt-ssh 123456789101112# 定义配置主机: salt-ssh是串行,没有C/S 快liunx-node1: host: 10.0.0.251 user: root password: 222222 port: 22CentOS7: host: 10.0.0.252 user: root password: 222222 port: 2 执行123[root@linux-node1 salt]# salt-ssh &apos;*&apos; test.ping -i [root@linux-node1 salt]# salt-ssh &apos;*&apos; -r &apos;w&apos;[root@linux-node1 salt]# man salt-ss 总结12345678910111.salt-ssh 是在salt基础上打了一个python包上传到客户端的默认tmp目录下 在客户端上面解压并执行返回结果,最后删除tmp上传的临时文件2.salt-minion方法是salt-mater先执行语法验证，验证通过后发送到minion minion收到Msater的状态文件默认保存在/var/cache/salt/minion 注意：也有时候salt-master语法验证通过，在minion上可能因为环境问题会执行失败3.salt-ssh和salt-minion可以共存，salt-minion不依赖于ssh服务 salt-api 使用条件：1）https调用，或者需要生成证书 2）配置文件 3）使用PAM验证 4）启动salt-api12https://www.unixhot.com/docs/saltstack/topics/api.htmlhttps://www.unixhot.com/docs/saltstack/ref/netapi/all/salt.netapi.rest_cherrypy.html#a-rest-api-for-salt 安装123[root@linux-node1 salt]# yum install -y salt-api[root@linux-node1 salt]# rpm -qa|grep cherrypython-cherrypy-3.2.2-4.el7.noarch 生成证书1234[root@linux-node1 salt]# yum install pyOpenSSL -y[root@linux-node1 salt]# salt-call --local tls.create_self_signed_cert[ERROR ] You should upgrade pyOpenSSL to at least 0.14.1 to enable the use of X509 extensionslocal: Created Private Key: &quot;/etc/pki/tls/certs/localhost.key.&quot; Created Certificate: &quot;/etc/pki/tls/certs/localhost.crt.&quot; 编辑配置文件:12345678910111213141516171819202122[root@linux-node1 salt]# vim master # Include a config file from some other path:include: master.d/*.conf[root@linux-node1 salt]# mkdir master.d[root@linux-node1 master.d]# vim api.confrest_cherrypy: host: 10.0.0.251 port: 8000 ssl_crt: /etc/pki/tls/certs/localhost.crt ssl_key: /etc/pki/tls/certs/localhost.key[root@linux-node1 redis-cluster]# useradd -M -s /sbin/nologin saltapi[root@linux-node1 redis-cluster]# echo &quot;saltapi&quot; | passwd saltapi --stdin[root@linux-node1 master.d]# vim auth.conexternal_auth: pam: saltapi: - .* - &apos;@wheel&apos; - &apos;@runner&apos; - &apos;@jobs 12345[root@linux-node1 master.d]# systemctl restart salt-master[root@linux-node1 master.d]# systemctl start salt-api[root@linux-node1 master.d]# netstat -tnlp|grep 8000tcp 0 0 10.0.0.251:8000 0.0.0.0:* LISTEN 6866/python 12345curl -sSk https://10.0.0.251:8000/login \-H &apos;Accept: application/x-yaml&apos; \-d username=&apos;saltapi&apos; \-d password=&apos;saltapi&apos; \-d eauth=&apos;pam&apos; 12345678910111213141516[root@linux-node1 master.d]# curl -sSk https://10.0.0.251:8000/login \&gt; -H &apos;Accept: application/x-yaml&apos; \&gt; -d username=&apos;saltapi&apos; \&gt; -d password=&apos;saltapi&apos; \&gt; -d eauth=&apos;pam&apos;return:- eauth: pam expire: 1570315348.575254 perms: - .* - &apos;@wheel&apos; - &apos;@runner&apos; - &apos;@jobs&apos; start: 1570272148.575253 token: 596b2306cc374d7c2f228bee381cfa80031d2fd6 user: saltapi 123456789101112131415161718[root@linux-node1 master.d]# systemctl restart salt-minioncurl -sSk https://10.0.0.251:8000 \ -H &apos;Accept: application/x-yaml&apos; \ -H &apos;X-Auth-Token: 596b2306cc374d7c2f228bee381cfa80031d2fd6&apos;\ -d client=local \ -d tgt=&apos;*&apos; \ -d fun=test.ping [root@linux-node1 master.d]# curl -sSk https://10.0.0.251:8000 \&gt; -H &apos;Accept: application/x-yaml&apos; \&gt; -H &apos;X-Auth-Token: 596b2306cc374d7c2f228bee381cfa80031d2fd6&apos;\&gt; -d client=local \&gt; -d tgt=&apos;*&apos; \&gt; -d fun=test.pingreturn:- CentOS7: true linux-node1.example.com: true Master高可用多master1https://www.unixhot.com/docs/saltstack/topics/tutorials/multimaster.html 12345678master 配置文件相同master file_roots 一样同步master的公钥和私钥minion 配置文件都是双机systemctl restart salt-minionnode2 做认证 salt-key -A别忘记安装MySQL-python 还有 mysql的登录授权给node2grant all on salt.* to salt@10.0.0.252 identified by &apos;salt&apos;]]></content>
      <categories>
        <category>SaltStack</category>
      </categories>
      <tags>
        <tag>salt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django ORM]]></title>
    <url>%2F2019%2F09%2F27%2Fdjango-orm%2F</url>
    <content type="text"><![CDATA[Django ORM系统ORM介绍对象关系映射（Object Relational Mapping，简称ORM）模式是一种为了解决面向对象与关系数据库存在的互不匹配的现象的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系数据库中 ORM的优势ORM解决的主要问题是对象和关系的映射。它通常把一个类和一个表一一对应，类的每个实例对应表中的一条记录，类的每个属性对应表中的每个字段。ORM提供了对数据库的映射，不用直接编写SQL代码，只需像操作对象一样从数据库操作数据。让软件开发人员专注于业务逻辑的处理，提高了开发效率。 ORM的劣势ORM的缺点是会在一定程度上牺牲程序的执行效率。ORM用多了SQL语句就不会写了，关系数据库相关技能退化… ORM的总结ORM只是一种工具，工具确实能解决一些重复，简单的劳动。这是不可否认的。但我们不能指望某个工具能一劳永逸地解决所有问题，一些特殊问题还是需要特殊处理的。但是在整个软件开发过程中需要特殊处理的情况应该都是很少的，否则所谓的工具也就失去了它存在的意义。 Django中的ORMDjango项目如何使用ORM连接MySQL 手动创建数据库 1CREATE DATABASE mybook CHARSET utf8 在settings.py里面配置数据库信息(告诉Django连接哪一个数据库) 1234567891011121314151617DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mybook', 'HOST': '10.0.0.200', 'PORT': 3306, 'USER': 'root', 'PASSWORD': '123456', &#125;, 'test':&#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'test', 'HOST': '10.0.0.200', 'PORT': 3306, 'USER': 'root', 'PASSWORD': '123456', &#125;&#125; 在项目下的init.py文件中,告诉Django用pymysql代替MySQLdb连接数据库 12import pymysqlpymysql.install_as_MySQLdb( 在app/models.py 中定义类，类一定要继承models.Model 123class Book(models.Model): id = models.AutoField(primary_key=True) title = models.CharField(max_length=32 执行两条命令: 123451. 在哪执行: 在项目的根目录(有manage.py文件的目录):2. 命令: python manage.py makemigrations --&gt; 将models.py文件中的改动记录在app/migrations 目录下 python manage.py migrate --&gt; 将改动翻译成SQL语句，去数据库中执行 Model在Django中model是你数据的单一、明确的信息来源。它包含了你存储的数据的重要字段和行为。通常，一个模型（model）映射到一个数据库表， 基本情况： 每个模型都是一个Python类，它是django.db.models.Model的子类。 模型的每个属性都代表一个数据库字段。综上所述，Django为您提供了一个自动生成的数据库访问API，详询官方文档链接。 快速入门下面这个例子定义了一个 Person 模型，包含 first_name 和 last_name。 12345from django.db import modelsclass Person(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) first_name 和 last_name 是模型的字段。每个字段被指定为一个类属性，每个属性映射到一个数据库列。12345CREATE TABLE myapp_person ( "id" serial NOT NULL PRIMARY KEY, "first_name" varchar(30) NOT NULL, "last_name" varchar(30) NOT NULL); 一些说明： 表myapp_person的名称是自动生成的，如果你要自定义表名，需要在model的Meta类中指定 db_table 参数，强烈建议使用小写表名，特别是使用MySQL作为后端数据库时。 id字段是自动添加的，如果你想要指定自定义主键，只需在其中一个字段中指定 primary_key=True 即可。如果Django发现你已经明确地设置了Field.primary_key，它将不会添加自动ID列。 本示例中的CREATE TABLE SQL使用PostgreSQL语法进行格式化，但值得注意的是，Django会根据配置文件中指定的数据库后端类型来生成相应的SQL语句。 Django支持MySQL5.5及更高版本。 表与表之间的关系 一对多(出版社和书) 外键:publisher = models.ForeignKey(to=’Publisher’) 在数据库里有没有publisher这个字段？数据库实际生成的是 publisher_id ,Django默认加上_i 多对多(作者和书) 多对多关联:from Django.db import modelsbooks = models.ManyToManyField(to=’Book’)多对多在数据库中，是通过第三章表建立的关 增删改查操作 单表增删改查: 1234567891011121314增: from app01 import models models.Publisher.objects.create(name=&quot;新街口出版社&quot;)查: models.Publisher.objects.get(id=1) models.Publisher.objects.get((name=&quot;新街口出版社&quot;))删: models.Publisher.objects.get(id=1).delete() 改: obj = models.Publisher.objects.get(id=1) obj.name = &quot;西单出版社&quot; obj.save() 外键的增删改查: 12345678增、删、查同上book_obj = models.Book.objects.get(id=1)# book_obj.Publisher 是什么? 和这本书关联的出版社对象 ***** book_obj.Publisher.id # id book_obj.Publisher.name # 名称# book_obj.Publisher_id 是什么? 和这本书关联的出版社ID 多对多操作: 1234561. 查询id为1的作者都写过哪些书: author_obj = models.Author.objects.get(id=1) author_obj.books.all() --&gt; 好我这个作者关联的所有的书籍对象 2. 想给作者绑定多本书 author_obj = models.Author.objects.get(id=1) author_obj.books.set([1,2,3]) --&gt; 把ID是1,2,3的书和我这个作者关联上 Django ORM 常用字段和参数1234567891011121314AutoFieldint自增列，必须填入参数 primary_key=True。当model中如果没有自增列，则自动会创建一个列名为id的列。IntegerField一个整数类型,范围在 -2147483648 to 2147483647。CharField字符类型，必须提供max_length参数， max_length表示字符长度。DateField日期字段，日期格式 YYYY-MM-DD，相当于Python中的datetime.date()实例。DateTimeField日期时间字段，格式 YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]，相当于Python中的datetime.datetime()实例。 基础使用1234567891011121314151617181920212223242526272829303132333435# models.pyclass FixedCharField(models.Field): """ 自定义的char类型的字段类 """ def __init__(self, max_length, *args, **kwargs): super().__init__(max_length=max_length, *args, **kwargs) self.length = max_length def db_type(self, connection): """ 限定生成数据库表的字段类型为char，长度为length指定的值 """ return 'char(%s)' % self.lengthclass Game_assets(models.Model): id = models.AutoField(primary_key=True) address = models.CharField(max_length=64,unique=True) port = models.IntegerField() user = models.CharField(max_length=64) password = models.CharField(max_length=64) name = FixedCharField(max_length=64,default='game_admin') # char(64) # 时间字段独有: # DatetimeField、DateField、TimeField这个三个时间字段，都可以设置如下属性。 # auto_now_add # 配置auto_now_add = True，创建数据记录的时候会把当前时间添加到数据库。 # auto_now # 配置上auto_now = True，每次更新数据记录的时候会更新该字段。 create_time = models.DateField(auto_now_add=True) update_time = models.DateTimeField(auto_now = True) 1234567891011121314# Create TableCREATE TABLE `app02_game_assets` ( `id` int(11) NOT NULL AUTO_INCREMENT, `address` varchar(64) NOT NULL, `port` int(11) NOT NULL, `user` varchar(64) NOT NULL, `password` varchar(64) NOT NULL, `create_time` date NOT NULL, `update_time` datetime(6) NOT NULL, `name` char(64) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `app02_game_assets_address_276ba0ac_uniq` (`address`)) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8 12&gt;&gt;&gt; from app02 import models&gt;&gt;&gt; models.Game_assets.objects.create(address='10.0.0.204',port='3306',user='root',password='123456',name='leo') 字段参数123456null：用于表示某个字段可以为空。unique：如果设置为unique=True 则该字段在此表中必须是唯一的 。db_index：如果db_index=True 则代表着为此字段设置数据库索引。default：为该字段设置默认值。auto_now_add：配置auto_now_add=True，创建数据记录的时候会把当前时间添加到数据库。auto_now：配置上auto_now=True，每次更新数据记录的时候会更新该字段。 ORM 操作基础操作,必知必会13条:all 查询所有结果12345678910111213import osif __name__ == '__main__': os.environ.setdefault("DJANGO_SETTINGS_MODULE", "ORM_test.settings") import django django.setup() from app01 import models ret = models.Person.objects.all() print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:leo&gt;&gt;, &lt;Person: &lt;Penson object:lex&gt;&gt;, # &lt;Person: &lt;Penson object:rubin&gt;&gt;, &lt;Person: &lt;Penson object:夜雨&gt;&gt;]&gt; filter 筛选123456789# 它包含了与所给筛选条件相匹配的对象# filterret = models.Person.objects.filter(id=1)print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:leo&gt;&gt;]&gt; 返回查询的结果集，将结果放在 QuerySet对象(列表)print(ret[0]) # &lt;Penson object:leo&gt; 根据索引取得对象# filter id &gt; 1的数据ret = models.Person.objects.filter(id=100)print(ret) # 如果查询不到结果,将返回一个&lt;QuerySet [] get 筛选1234567# get查询# 返回与所给筛选条件相匹配的对象，返回结果有且只有一个，如果符合筛选条件的对象超过一个或者没有都会抛出错误。# 如果查询条件不存在,报错ret = models.Person.objects.get(id=1)# ret = models.Person.objects.get(name='夜雨')print(ret) # &lt;Penson object:leo&gt; 具体的对象 exclude 筛选不匹配12345# exclude 它包含了与所给筛选条件不匹配的对象ret = models.Person.objects.exclude(id=1)print(ret) # 返回不匹配的对象ret = models.Person.objectsexclude(id=100)print(ret) # 不存在就返回所有 values 返回可迭代的字典序列123# values 返回一个QuerySet对象,里面都是字典,不写字段名默认查询所有字段ret = models.Person.objects.values("name","birthday")print(ret values_list 返回可迭代的元组序列123# values_list 返回一个QuerySet对象,里面都是元祖ret = models.Person.objects.values_list("name","birthday")print(ret order_by 按照指定的字段排序123# ret = models.Person.objects.all() # ordering = "birthday"ret = models.Person.objects.all().order_by("id")print(ret reverse 对一个有序的QuerySet 反向排序12345# 通常都使用order_by,不使用meta# 通常只能在具有已定义顺序的QuerySet上调用(在model类的Meta中指定ordering或调用order_by()方法)# ret = models.Person.objects.all().reverse() # ordering = "birthday"ret = models.Person.objects.all().order_by("birthday").reverse()print(ret) distinct 从返回结果中剔除重复纪录count 返回QuerySet对象中对象的数量12ret = models.Person.objects.all().count()print(ret) first 返回QuerySet第一个对象12ret = models.Person.objects.all().first()print(ret) last 返回QuerySet最后一个对象12ret = models.Person.objects.all().last()print(ret) exists 判断表里是否有数据12ret = models.Person.objects.all().exists()print(ret) # True 单表的双下划线查询123456789101112131415161718192021222324252627282930313233343536373839404142434445# 单表查询的 双下划线方法# 大于和小于# 查询ID &gt; 1 and ID &lt; 4ret = models.Person.objects.filter(id__gt=1,id__lt=4)print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:lex&gt;&gt;, &lt;Person: &lt;Penson object:rubin&gt;&gt;]&gt;# in 在...范围# 查询id在[1,3,5]中的结果ret = models.Person.objects.filter(id__in=[1,3,5])print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:leo&gt;&gt;, &lt;Person: &lt;Penson object:rubin&gt;&gt;]&gt;# not in 不在...范围# 查询id不在[1,3,5]中的结果ret = models.Person.objects.exclude(id__in=[1, 3, 5])print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:lex&gt;&gt;, &lt;Person: &lt;Penson object:夜雨&gt;&gt;]&gt;# contains 检索,模糊查询# 获取name字段包含"l"的结果ret = models.Person.objects.filter(name__contains='l')print(ret)ret = models.Person.objects.filter(name__contains='夜')print(ret) # &lt;QuerySet [&lt;Person: &lt;Penson object:leo&gt;&gt;, &lt;Person: &lt;Penson object:lex&gt;&gt;]&gt;# icontains 英文检索print('icontains'.center(80,'*'))# 英文检索ret = models.Person.objects.filter(name__icontains='l')print(ret)ret = models.Person.objects.filter(name__icontains='夜')print(ret)# range 范围查询# 判断ID值在哪个范围之内 SQL语句中的between and 1&lt;= &lt;=3ret = models.Person.objects.filter(id__range=[1,3])print(ret)# 类似的还有：startswith，istartswith, endswith, iendswith # 日期和时间字段可以有以下写法ret = models.Person.objects.filter(birthday__year=2018)print(ret)ret = models.Person.objects.filter(birthday__month=9)print(ret) ForeignKey操作外键的 正向查询 和 反向查询 先看外键在哪张表里 从有外键字段的表 查询 正向查询,反之 叫做反向查询 正向查询12345678# 基于对象 跨表查询book_obj = models.Book.objects.first()ret = book_obj.publisher # 和这本书关联的出版社对象print(ret, type(ret)) # 西单出版社 &lt;class 'app01.models.Publisher'&gt;# 对象.属性ret = book_obj.publisher.nameprint(ret,type(ret)) # 西单出版社 &lt;class 'str'&gt; 123456# 双下划线 跨表查询# 查询ID是1的书的出版社名称# __双下划线就表示 跨了一张表ret = models.Book.objects.filter(id=1).values("publisher__name")# ret = models.Book.objects.filter(id=1).values_list("publisher__name")print(ret) # &lt;QuerySet [&#123;'publisher__name': '西单出版社'&#125;]&gt; 反向查询1234567891011121314# 基于对象 跨表查询# 第一个出版社出版了什么书publisher_obj = models.Publisher.objects.first() # 得到一个具体的对象# 关联表名小写 + _set# ret = publisher_obj.book_set.all()# models里面的 related_name 参数可以代替关联表名小写 + _set# related_name="books" 反向查询是用来代替 book_set# publisher = models.ForeignKey(to='Publisher',related_name="books")ret = publisher_obj.books.all()# models里面的 related_query_name= 是直接代替表名,用的不多print(ret) # &lt;QuerySet [&lt;Book: Python&gt;, &lt;Book: CSS&gt;]&gt; 1234# 双下划线 跨表查询# 使用filter得到的QuerySet对象 才可以 values_list() 和 values()ret = models.Publisher.objects.filter(id=1).values_list("books__title")print(ret) # &lt;QuerySet [('Python',), ('CSS',)]&gt; 12345678910111213141516# models.py 外键表配置class Book(models.Model): id = models.AutoField(primary_key=True) title = models.CharField(max_length=32) # 外键: # related_name="books" 反向查询是用来代替 book_set publisher = models.ForeignKey( to='Publisher', on_delete=models.CASCADE, # 删除关联数据时,应该怎么处理，默认级联操作,Django2.0要写上 related_name="books", # 反向查询的时候 用来代替 表名_set related_query_name="books" # 反向双下划线跨表查询用来代替表名 ) def __str__(self): return self.title Django终端打印SQL语句12345678910111213141516171819# ORM操作 查看具体的SQL语句# 在Django项目的settings.py文件中配置LOGGING = &#123; 'version': 1, 'disable_existing_loggers': False, 'handlers': &#123; 'console':&#123; 'level':'DEBUG', 'class':'logging.StreamHandler', &#125;, &#125;, 'loggers': &#123; 'django.db.backends': &#123; 'handlers': ['console'], 'propagate': True, 'level':'DEBUG', &#125;, &#125;&#125; 多对多查询“关联管理器”是在一对多或者多对多的关联上下文中使用的管理器。 它存在于下面两种情况： 外键关系的反向查询 多对多关联关系 简单来说就是当 点后面的对象 可能存在多个的时候就可以使用以下的方法。 123456789# 多对多# 获取作者对象author_obj = models.Author.objects.first()pint(author_obj.name) # Leo# 查询leo都出版了哪些书ret = author_obj.books.all()print(type(author_obj.books)) # ManyRelatedManager 关联管理类对象print(ret) # &lt;QuerySet [&lt;Book: Python&gt;]&gt; 123456# 1. create# 通过作者创建一本书,会自动保存不用提交# 两部操作:# 在关联表里添加关系#在书籍表里添加了一本新书author_obj.books.create(title="番茄物语",publisher_id=2) 123456789101112# 2. add# 给leo添加一本 id=5 的书book_obj = models.Book.objects.get(id=5)author_obj.books.add(book_obj)# 添加多个book_objs = models.Book.objects.filter(id__gt=5)author_obj.books.add(*book_objs) # 要把列表打散再传进去print(*book_objs)# 直接添加IDauthor_obj.books.add(8) 1234567# revome# 从leo关联的书 把 做饭大全删除掉book_obj = models.Book.objects.get(title='做饭大全')author_obj.books.remove(book_obj)# 直接删除IDauthor_obj.books.remove(5 1234567891011# clear# 清空# 把作者ID是2的 所有图书一起删除author_obj = models.Author.objects.get(id=2)author_obj.books.clear()# 额外补充,外键的反向操作# 找到ID是1的 出版社publisher_obj = models.Publisher.objects.get(id=1)# object has no attribute 'clear' 当这个外键可以为空时,才能够clearpublisher_obj.books.clear( 12对于所有类型的关联字段，add()、create()、remove()和clear(),set()都会马上更新数据库。换句话说，在关联的任何一端，都不需要再调用save()方法。 聚合查询和分组查询123456789101112131415# 聚合 aggregatefrom django.db.models import Avg, Sum, Max, Min, Countret = models.Book.objects.all().aggregate(Avg("price"))print(ret)ret = models.Book.objects.all().aggregate(price_avg = Avg("price"))print(ret) # &#123;'price_avg': 50.0&#125;# 多个参数ret = models.Book.objects.all().aggregate( price_avg = Avg("price"), price_max = Max("price"), price_min = Min("price"),)print(ret) # &#123;'price_avg': 50.0, 'price_max': Decimal('80.00'), 'price_min': Decimal('20.00')&#125;print(ret.get("price_max"),type(ret.get("price_max"))) # 80.00 &lt;class 'decimal.Decimal'&gt; 123456789101112131415161718# 分组 annotate# group by# 统计每一本书的作者个数book_list = models.Book.objects.all().annotate(author_num=Count("author"))for book in book_list : print("书名:",book.title,"作者数量:",book.author_num)# 查询作者数量大于1的书ret = models.Book.objects.all().annotate(author_num=Count("author")).filter(author_num__gt=1)print(ret)# 查询各个作者出的书的总价格ret = models.Author.objects.all().annotate(price_sum=Sum("books__price")).values_list("name","price_sum")print(ret)ret = models.Author.objects.all().annotate(price_sum=Sum("books__price"))for i in ret: print(i,i.name,i.price_sum) F查询和Q查询1234567891011121314151617181920212223242526# 价格大于9.9的书ret = models.Book.objects.all().filter(price__gt=9.9)print(ret)# 两个字段的值做比较 使用F查询# 字段之间做加减# 增加两个字段 : 卖书的数量 和 库存数# 库存数 大于 卖出数的 所有数from django.db.models import Fret = models.Book.objects.filter(kucun__gt=F("maichu"))print(ret) # &lt;QuerySet [&lt;Book: Python&gt;, &lt;Book: 做饭大全&gt;, &lt;Book: 宇宙大全&gt;]&gt;# 刷单,把每一本书的卖出数都乘以3# obj = models.Book.objects.first()# obj.maichu = 1000 * 3# obj.save()# 具体的对象没有update方法,QureySet对象才有# models.Book.objects.update(maichu=F("maichu") / 100)# 把所有书名后面加上(第一版)# 修改char字段咋办？from django.db.models.functions import Concatfrom django.db.models import Value# models.Book.objects.all().update(title=Concat(F("title"), Value("("), Value("第一版"), Value(")"))) 1234567891011121314# Q查询# 卖出数大于1000，并且 价格小于100的书# ret = models.Book.objects.filter(maichu__gt=1000,price__lt=100)# print(ret)# 卖出数大于5000，或者 价格小于100的书from django.db.models import Qret = models.Book.objects.filter(Q(maichu__gt=1000) | Q(price__lt=100))print(ret)# 书名里 包含 "大全"的书# Q查询和字段查询同时存在时,字典查询需要放在Q查询的后面ret2 = models.Book.objects.filter(Q(maichu__gt=1000) | Q(price__lt=100),title__contains="大全")print(ret2 在Python脚本中调用Django环境1234567891011import osif __name__ == '__main__': os.environ.setdefault("DJANGO_SETTINGS_MODULE", "BMS.settings") import django django.setup() from app01 import models books = models.Book.objects.all() print(books) 返回对象和QuerySet1234567# 返回QuerySet对象的方法有all()filter()exclude()order_by()reverse()distinct() 123# 特殊的QuerySet values() # 返回一个可迭代的字典序列values_list() # 返回一个可迭代的元祖序列 1234# 返回具体对象的get()first()last() 12# 返回布尔值的方法有：exists() 12# 返回数字的方法有count() 练习单标双下划线1234567891011121314151617181920212223# 获取id大于1 且 小于5的书籍book_list = models.Book.objects.filter(id__gt=1,id__lt=5)print(book_list)# 获取id等于1、2、3的数据book_list = models.Book.objects.filter(id__in=[1,2,3])print(book_list)# 获取id不等于1、2、3的数据book_list = models.Book.objects.exclude(id__in=[1,2,3])print(book_list)# 获取title字段包含"大全"的书籍book_list = models.Book.objects.filter(title__contains="大全")print(book_list)# id范围是1到3的书book_list = models.Book.objects.filter(id__range=[1,3])print(book_list)# 查询人员生日是09月的author_list= models.Person.objects.filter(birthday__month=9)print(author_list) 正向查询123456789101112131415161718192021222324# 正向查询# 获取书籍ID为5的出版社名称# 对象查找（跨表）# 对象.关联字段.属性book_obj = models.Book.objects.get(id=5)print(book_obj.publisher.name)# 字段查找（跨表）# 关联字段__字段ret = models.Book.objects.filter(id=5).values_list("publisher__name")print(ret) # &lt;QuerySet [('新街口出版社',)]&gt;# 反向查询# 获取id=2的出版社 出版的所有书籍# 对象查找 obj.表名_setpub_obj = models.Publisher.objects.get(id=2)ret = pub_obj.books.all().values_list("title") # 找到id=2这个出版社所有的书print(ret) # &lt;QuerySet [('Linux',), ('番茄物语',), ('做饭大全',), ('新西兰攻略',)]&gt;# 字段查找# 表名__字段ret = models.Publisher.objects.filter(id=2).values_list("books__title")print(ret) # &lt;QuerySet [('Linux',), ('番茄物语',), ('做饭大全',), ('新西兰攻略',)]]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 进阶]]></title>
    <url>%2F2019%2F09%2F08%2Fdjango-plus%2F</url>
    <content type="text"><![CDATA[MVC框架 和 MTV框架MVC,全名是Model View Controller，是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型(Model)、视图(View) 和 控制器(Controller)，具有耦合性低、重用性高、生命周期成本低等优点。 Django的 MTV模式Django框架的设计模式借鉴了MVC框架的思想，也是分成三部分，来降低各个部分之间的耦合性。Django框架的不同之处在于它拆分的三部分为：Model（模型）、Template（模板）和 View（视图），也就是MTV框架。 123Model(模型)：负责业务对象与数据库的对象(ORM)Template(模版)：负责如何把页面展示给用户View(视图)：负责业务逻辑，并在适当的时候调用Model和Template 介绍Django框架时,可以说: Django框架类似MCV模式，不同的是他的模式成为MTV模式。此外，Django还有一个urls分发器，它的作用是将一个个URL的页面请求分发给不同的view处理，view再调用相应的Model和Template Django 模板系统常用语法1231. 只需要记两种特殊符号：&#123;&#123; &#125;&#125;和 &#123;% %&#125;2. 变量相关的用&#123;&#123; &#125;&#125;，逻辑相关的用&#123;% %&#125;。 变量123456789101112在Django的模板语言中按此语法使用：&#123;&#123; 变量名 &#125;&#125;当模版引擎遇到一个变量，它将计算这个变量，然后用结果替换掉它本身。 变量的命名包括任何字母数字以及下划线 (&quot;_&quot;)的组合。 变量名称中不能有空格或标点符号。# 点（.）在模板语言中有特殊的含义。当模版系统遇到点(&quot;.&quot;)，它将以这样的顺序查询：字典查询（Dictionary lookup）属性或方法查询（Attribute or method lookup）数字索引查询（Numeric index lookup）# 注意事项：如果计算结果的值是可调用的，它将被无参数的调用。调用的结果将成为模版的值。如果使用的变量不存在， 模版系统将插入 string_if_invalid 选项的值， 它被默认设置为&apos;&apos; (空字符串) 。 12345678910111213141516171819202122232425262728293031323334353637383940414243# views中的变量的例子:# 模板语言测试例子class Person: def __init__(self,name,age): self.name = name self.age = age def __str__(self): return "&lt;Object &#123;&#125; name&gt;" .format(self.name) def dream(self): return "梦想" def __unicode__(self): return "打印的对象 Python2 调用这个" def t_test(request): name = "leo" age = 28 name_list = ["leo", "lex", "rubin"] name_dict = &#123;"first_name": "leo", "last_name": "Lex"&#125; # 实例化类 p1 = Person("雷尼",18) p2 = Person("加纳",18) # 列表类 p_list = [p1,p2] return render( request, 't_test.html', &#123; "name": name, "age": age, "name_list": name_list, "name_dict": name_dict, "person":p1, "p_list":p_list, &#125; ) 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!--模板中支持的写法：--&gt;&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;模板语言测试&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;传值:字符、数字、列表、字典&lt;/p&gt;&lt;p&gt;&#123;&#123; name &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; age &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name_list &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name_dict &#125;&#125;&lt;/p&gt;&#123;#不传值不显示#&#125;&lt;p&gt;传送不存在的变量: &#123;&#123; null &#125;&#125;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;传值:类对象,类列表&lt;/p&gt;&lt;p&gt;&#123;&#123; person &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; person.name &#125;&#125;&lt;/p&gt;&#123;#模板语言里,方法不要加括号(),只能调用不带参数的方法#&#125;&lt;p&gt;&#123;&#123; person.dream &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; p_list.0 &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; p_list.1.name &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; p_list.1.dream &#125;&#125;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;字典&lt;/p&gt;&lt;!--如果字典中有一个key叫做items,字典key的优先级,要大于方法items的优先级--&gt;&lt;p&gt;&#123;&#123; name_dict.first_name &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name_dict.last_name &#125;&#125;&lt;/p&gt;&#123;% for k,v in name_dict.items %&#125; &#123;&#123; k &#125;&#125; : &#123;&#123; v &#125;&#125;&#123;% endfor %&#125;&lt;/body&gt;&lt;/html&gt; Filters（过滤器）1234在Django的模板语言中，通过使用 过滤器 来改变变量的显示。过滤器的语法： &#123;&#123; value|filter_name:参数 &#125;&#125;使用管道符&quot;|&quot;来应用过滤器。例如：&#123;&#123; name|lower &#125;&#125;会将name变量应用lower过滤器之后再显示它的值。lower在这里的作用是将文本全都变成小写。 123456注意事项：过滤器支持“链式”操作。即一个过滤器的输出作为另一个过滤器的输入。过滤器可以接受参数，例如：&#123;&#123; sss|truncatewords:30 &#125;&#125;，这将显示sss的前30个词。过滤器参数包含空格的话，必须用引号包裹起来。比如使用逗号和空格去连接一个列表中的元素，如：&#123;&#123; list|join:&apos;, &apos; &#125;&#125;&apos;|&apos;左右没有空格没有空格没有空格 Django的模板语言中提供了大约六十个内置过滤器。 default如果一个变量是false或者为空，使用给定的默认值。 否则，使用变量的值。 12&lt;!--如果value没有传值或者值为空的话就显示nothing--&gt;&lt;p&gt;传送不存在的变量: &#123;&#123; null|default:"没有传值" &#125;&#125;&lt;/p&gt; length返回值的长度，作用于字符串和列表。 1234&lt;!--返回value的长度，如 value=['a', 'b', 'c', 'd']的话，就显示4--&gt;&lt;p&gt;&#123;&#123; name|length &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name_list|length &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name_dict|length &#125;&#125;&lt;/p&gt; filesizeformat将值格式化为一个 “人类可读的” 文件尺寸 （例如 ‘13 KB’, ‘4.1 MB’, ‘102 bytes’, 等等）。例如： 12&lt;!-- 如果 value 是 123456789，输出将会是 117.7 MB。--&gt;&lt;p&gt;&#123;&#123; file_size|filesizeformat &#125;&#125;&lt;/p&gt; slice切片 1&#123;&#123;value|slice:"2:-1"&#125;&#125; date格式化 12&#123;&#123; value|date:"Y-m-d H:i:s"&#125;&#125;&lt;p&gt;&#123;&#123; now_time|date:"Y-m-d H:i:s" &#125;&#125;&lt;/p&gt; safe12345Django的模板中会对HTML标签和JS等语法标签进行自动转义，原因显而易见，这样是为了安全。但是有的时候我们可能不希望这些HTML元素被转义，比如我们做一个内容管理系统，后台添加的文章中是经过修饰的，这些修饰可能是通过一个类似于FCKeditor编辑加注了HTML修饰符的文本，如果自动转义的话显示的就是保护HTML标签的源文件。为了在Django中关闭HTML的自动转义有两种方式，如果是一个单独的变量我们可以通过过滤器“|safe”的方式告诉Django这段代码是安全的不必转义。 1234&#123;#多用于文章评论时,放入的html连接或者其他恶意如死循环代码XSS攻击,跨站脚本攻击,做安全效验转义#&#125;&#123;#评论一定不要加save,就让其成为文本即可#&#125;&lt;p&gt;a标签 &#123;&#123; a_html|safe &#125;&#125;&lt;/p&gt;&lt;p&gt;script签: &#123;&#123; script_html &#125;&#125;&lt;/p&gt; truncatecharstruncatechars 如果字符串字符多于指定的字符数量，那么会被截断。截断的字符串将以可翻译的省略号序列（“…”）结尾。参数：截断的字符数 1&lt;p&gt;&#123;&#123; p_str|truncatechars:5 &#125;&#125;&lt;/p&gt; 自定义filter12345自定义过滤器只是带有一个或两个参数的Python函数:变量（输入）的值 - -不一定是一个字符串参数的值 - 这可以有一个默认值，或完全省略例如，在过滤器&#123;&#123;var | foo:&apos;bar&apos;&#125;&#125;中，过滤器foo将传递变量var和参数“bar”。 自定义filter代码文件摆放位置： 1234567app01/ __init__.py models.py templatetags/ # 在app01下面新建一个package package:templatetags __init__.py app01_filters.py # 建一个存放自定义filter的文件 views.py 编写自定义filter 1234567891011121314151617from django import template# 注册器register = template.Library()@register.filter(name="come_on")def come_on(arg): return "&#123;&#125; you got it!" .format(arg)@register.filter(name="addstr")def addstr(arg,arg2): """ :param arg: 第一个参数永远是管道符|，前面的那个变量 :param arg2: 后面的参数，冒号后面引号里面的变量 :return: """ return "&#123;&#125; &#123;&#125; you got it!" .format(arg,arg2) 使用自定义filter需要重启项目加载 123456789&lt;hr&gt;&#123;# 自定义filter #&#125;&#123;# 先导入我们自定义filter那个文件 #&#125;&#123;# 需要重启项目加载 #&#125;&#123;% load app01_filters %&#125;&#123;# 使用我们自定义的filter #&#125;&lt;p&gt;&#123;&#123; name|come_on &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; name|addstr:"加油兄弟" &#125;&#125;&lt;/p&gt; Tagsfor循环普通for循环 12345&lt;ul&gt;&#123;% for user in user_list %&#125; &lt;li&gt;&#123;&#123; user.name &#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; for循环可用的一些参数： 12345&lt;p&gt;for 循环 &#123;% for name in name_list %&#125; &lt;li&gt;&#123;&#123; forloop.counter &#125;&#125; &#123;&#123; name &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/p&gt; for … empty 1234&#123;% for book in book_list %&#125;&#123;% empty %&#125; &lt;p&gt;暂时没有数据&lt;/p&gt;&#123;% endfor %&#125; 双层for循环 12345678910&lt;p&gt;双层for循环&lt;/p&gt;&#123;% for li in name2_list %&#125; &#123;&#123; li &#125;&#125; &#123;&#123; forloop.counter &#125;&#125; &#123;% for name in li %&#125; &#123;&#123; name &#125;&#125; &#123;&#123; forloop.counter &#125;&#125; &#123;&#123; forloop.parentloop.counter &#125;&#125; &#123;% endfor %&#125;&#123;% endfor %&#125; if判断if,elif和elseif语句支持 and 、or、==、&gt;、&lt;、!=、&lt;=、&gt;=、in、not in、is、is not判断。 1234567&#123;% if user_list %&#125; 用户人数：&#123;&#123; user_list|length &#125;&#125;&#123;% elif black_list %&#125; 黑名单数：&#123;&#123; black_list|length &#125;&#125;&#123;% else %&#125; 没有用户&#123;% endif %&#125; 12345678&lt;p&gt;if ... elif ... else&lt;/p&gt; &#123;% if p3 %&#125; &lt;p&gt;p3:&#123;&#123; p3 &#125;&#125;&lt;/p&gt; &#123;% elif p2 %&#125; &lt;p&gt;p2:&#123;&#123; p2 &#125;&#125;&lt;/p&gt; &#123;% else %&#125; &lt;p&gt;什么人都没有&lt;/p&gt; &#123;% endif %&#125; 12345&#123;% if name_list|length &gt;= 3 %&#125; &lt;p&gt;需要两辆车&lt;/p&gt;&#123;% else %&#125; &lt;p&gt;一台车&lt;/p&gt;&#123;% endif %&#125; with定义一个中间变量，多用于给一个复杂的变量起别名,注意等号左右不要加空格 123456&lt;p&gt;with&lt;/p&gt;&#123;&#123; name_list.1.1 &#125;&#125;&#123;% with k=name_list.1.1 %&#125; &#123;&#123; k &#125;&#125;&#123;% endwith %&#125; 注释 和 注意事项1&#123;# ...注释... #&#125; 1234567891011121314151. Django的模板语言不支持连续判断，即不支持以下写法：&#123;% if a &gt; b &gt; c %&#125;...&#123;% endif %&#125; 2. Django的模板语言中属性的优先级大于方法def xx(request): d = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3, &quot;items&quot;: &quot;100&quot;&#125; return render(request, &quot;xx.html&quot;, &#123;&quot;data&quot;: d&#125;)如上，我们在使用render方法渲染一个页面的时候，传的字典d有一个key是items并且还有默认的 d.items() 方法，此时在模板语言中:&#123;&#123; data.items &#125;&#125;默认会取d的items key的值。 母版和继承为什么要有母版和继承html页面有重复的时候,把他们提取出来放到一个单独的html文件(比如:导航条和左侧菜单),把多个页面公用的部分提取出来,放在一个母版页面里面,其他的页面只需要 继承 母版就可以了 具体使用的步骤 把公用的HTML部分提取出来，在项目目录templates中新建一个 base.html 基础页面，将内容放入。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;!DOCTYPE html&gt;&lt;!-- saved from url=(0042)https://v3.bootcss.com/examples/dashboard/ --&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;meta name="description" content=""&gt; &lt;meta name="author" content=""&gt; &lt;link rel="icon" href="https://v3.bootcss.com/favicon.ico"&gt; &lt;title&gt;图书管理系统&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href="/static/dashboard.css" rel="stylesheet"&gt; &lt;link rel="stylesheet" href="/static/fontawesome/css/font-awesome.min.css"&gt; &#123;#专门替换css文件的块,别的页面没有用到#&#125; &#123;% block page_css %&#125; &#123;% endblock %&#125;&lt;/head&gt;&lt;body&gt;&#123;#&#123;% include "nav.html" %&#125;#&#125;&lt;div class="container-fluid"&gt; &lt;div class="row"&gt; &lt;div class="col-sm-3 col-md-2 sidebar"&gt; &lt;ul class="nav nav-sidebar"&gt; &#123;#只要传来的值有all_book或者author_list、publisher_list 我就给这个li加成选中状态#&#125; &lt;li class= "&#123;% block publisher_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/publisher_list/"&gt;出版社列表页&lt;/a&gt;&lt;/li&gt; &lt;li class= "&#123;% block book_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/book_list/"&gt;书籍列表&lt;/a&gt;&lt;/li&gt; &lt;li class= "&#123;% block author_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/author_list/"&gt;作者列表&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &#123;# 需要替换的部分,每个页面内容不同 #&#125; &lt;div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main"&gt; &#123;# 这里是每个页面内容不同 #&#125; &#123;% block page-main %&#125; &#123;% endblock %&#125; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="modal fade" tabindex="-1" role="dialog" id="myModal"&gt; &lt;div class="modal-dialog" role="document"&gt; &lt;div class="modal-content"&gt; &lt;div class="modal-header"&gt; &lt;button type="button" class="close" data-dismiss="modal" aria-label="Close"&gt;&lt;span aria-hidden="true"&gt;&amp;times;&lt;/span&gt;&lt;/button&gt; &lt;h4 class="modal-title"&gt;用户信息&lt;/h4&gt; &lt;/div&gt; &lt;div class="modal-body"&gt; &lt;form class="form-horizontal"&gt; &lt;div class="form-group"&gt; &lt;label for="inputEmail3" class="col-sm-2 control-label"&gt;邮箱&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;input type="email" class="form-control" id="inputEmail3" placeholder="Email"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="inputPassword3" class="col-sm-2 control-label"&gt;密码&lt;/label&gt; &lt;div class="col-sm-10"&gt; &lt;input type="password" class="form-control" id="inputPassword3" placeholder="Password"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;div class="modal-footer"&gt; &lt;button type="button" class="btn btn-default" data-dismiss="modal"&gt;取消&lt;/button&gt; &lt;button type="button" class="btn btn-primary"&gt;保存&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;!-- /.modal-content --&gt; &lt;/div&gt;&lt;!-- /.modal-dialog --&gt;&lt;/div&gt;&lt;!-- /.modal --&gt;&lt;!-- Bootstrap core JavaScript================================================== --&gt;&lt;!-- Placed at the end of the document so the pages load faster --&gt;&lt;script src="/static/jquery-3.3.1.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&#123;% block page_js %&#125;&#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt; 在base.html 中,通过定义block,把每个页面不同的部分区分出来: 12345678910111213141516&#123;#专门替换css文件的块,别的页面没有用到#&#125;&#123;% block page_css %&#125;&#123;% endblock %&#125;&#123;# 需要替换的部分,每个页面内容不同 #&#125;&lt;div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main"&gt;&#123;# 这里是每个页面内容不同 #&#125;&#123;% block page-main %&#125;&#123;#专门替换js文件的块,别的页面没有用到#&#125;&#123;% endblock %&#125;&#123;% block page_js %&#125;&#123;% endblock %&#125; 在具体的页面中,先继承母版 1&#123;% extends 'base.html' %&#125; 然后通过block名字去指定替换母版中相应的位置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&#123;% extends 'base.html' %&#125;&#123;% block page-main %&#125; &lt;h1 class="page-header"&gt;出版社管理页面&lt;/h1&gt; &lt;div class="panel panel-primary"&gt; &lt;!-- Default panel contents --&gt; &lt;div class="panel-heading"&gt;出版社列表 &lt;i class="fa fa-thumb-tack pull-right"&gt;&lt;/i&gt;&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;div class="row" style="margin-bottom: 15px"&gt; &lt;div class="col-md-4"&gt; &lt;div class="input-group"&gt; &lt;input type="text" class="form-control" placeholder="Search for..."&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="button"&gt;搜索&lt;/button&gt; &lt;/span&gt; &lt;/div&gt;&lt;!-- /input-group --&gt; &lt;/div&gt;&lt;!-- /.col-md-4 --&gt; &lt;div class="col-md-1 pull-right"&gt; &lt;button class="btn btn-success" data-toggle="modal" data-target="#myModal"&gt;新增&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;!-- /.row --&gt; &lt;table class="table table-bordered"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;出版社名称&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for publisher in publisher_list %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;a class="btn btn-danger" href="/del_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;删除&lt;/a&gt; &lt;a class="btn btn-danger" href="/del_publisher/&#123;&#123; publisher.id &#125;&#125;/"&gt;删除2&lt;/a&gt; &lt;a class="btn btn-info" href="/edit_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;编辑&lt;/a&gt; &lt;a class="btn btn-info" href="&#123;% url 'edit_publisher' publisher_id %&#125;"&gt;编辑&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt; &lt;/table&gt; &lt;nav aria-label="Page navigation" class="text-right"&gt; &lt;ul class="pagination"&gt; &lt;li&gt; &lt;a href="#" aria-label="Previous"&gt; &lt;span aria-hidden="true"&gt;&amp;laquo;&lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;5&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="#" aria-label="Next"&gt; &lt;span aria-hidden="true"&gt;&amp;raquo;&lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/nav&gt; &lt;/div&gt; &lt;/div&gt;&#123;% endblock %&#125; 导航栏选中的判断方法123456789&lt;!--方法1:判断传进来的变量是否有值--&gt; &lt;div class="col-sm-3 col-md-2 sidebar"&gt; &lt;ul class="nav nav-sidebar"&gt; &#123;#只要传来的值有all_book或者author_list、publisher_list 我就给这个li加成选中状态#&#125; &lt;li &#123;% if publisher_list %&#125; class = "active" &#123;% endif %&#125;&gt;&lt;a href="/publisher_list/"&gt;出版社列表页&lt;/a&gt;&lt;/li&gt; &lt;li &#123;% if all_book %&#125; class = "active" &#123;% endif %&#125;&gt;&lt;a href="/book_list/"&gt;书籍列表&lt;/a&gt;&lt;/li&gt; &lt;li &#123;% if author_list %&#125; class = "active" &#123;% endif %&#125;&gt;&lt;a href="/author_list/"&gt;作者列表&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 1234567891011121314&lt;!--方法2:使用block--&gt; &lt;div class="col-sm-3 col-md-2 sidebar"&gt; &lt;ul class="nav nav-sidebar"&gt; &#123;#只要传来的值有all_book或者author_list、publisher_list 我就给这个li加成选中状态#&#125; &lt;li class= "&#123;% block publisher_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/publisher_list/"&gt;出版社列表页&lt;/a&gt;&lt;/li&gt; &lt;li class= "&#123;% block book_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/book_list/"&gt;书籍列表&lt;/a&gt;&lt;/li&gt; &lt;li class= "&#123;% block author_class %&#125;&#123;% endblock %&#125;"&gt; &lt;a href="/author_list/"&gt;作者列表&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;!--在相应的页面 添加块--&gt;&#123;% block book_class %&#125; active&#123;% endblock %&#125; 专门替换CSS样式的块123456789101112131415161718# 定义一个book_list_only.css文件body&#123; background-color: lawngreen; &#125;# 只有在指定的页面 用到这个样式&#123;#专门替换css文件的块,别的页面没有用到#&#125;&lt;head&gt;...&#123;% block page_css %&#125;&#123;% endblock %&#125;&lt;/head&gt;# book_list2.html替换&#123;% block page_css %&#125; &lt;link rel="stylesheet" href="/static/book_list_only.css"&gt;&#123;% endblock %&#125; 专门替换JS文件的块123456789101112131415161718# 定义一个author_list_only.js 文件alert('这是作者页面')# 只有在指定的页面 用到这个JS&lt;/body&gt;...&lt;script src="/static/jquery-3.3.1.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&#123;% block page_js %&#125;&#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt;# 引用&#123;% block page_js %&#125; &lt;script src="/static/author_list_only.js"&gt;&lt;/script&gt;&#123;% endblock %&#125; 使用母版和继承的注意事项12341. &#123;% extends &apos;base.html&apos; %&#125; --&gt; 母版文件:base.html要加引号,2. &#123;% extends &apos;base.html&apos; %&#125; --&gt; 必须放在子页面的第一行3. 可以在base.html中定义很多block,通常我们会额外定义page_cs和page_js,这两个块4. views.py相应的函数中，返回的是对应的子页面文件不是不是不是base.html 组件可以将常用的页面内容如导航条，页尾信息等组件保存在单独的文件中，然后在需要使用的地方按如下语法导入即可。什么时候用组件:重复的代码,包装成一个独立的小html文件。 单独提取导航条 成为小组件,把导航条单独写成一个nav.html把base.html中的nav剪切分割到nav.html,里面的内容只放导航条的内容 12345678910111213141516171819202122232425&lt;nav class="navbar navbar-inverse navbar-fixed-top"&gt; &lt;div class="container-fluid"&gt; &lt;div class="navbar-header"&gt; &lt;button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"&gt; &lt;span class="sr-only"&gt;Toggle navigation&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class="navbar-brand" href="https://v3.bootcss.com/examples/dashboard/#"&gt;BMS&lt;/a&gt; &lt;/div&gt; &lt;div id="navbar" class="navbar-collapse collapse"&gt; &lt;ul class="nav navbar-nav navbar-right"&gt; &lt;li&gt;&lt;a href="https://v3.bootcss.com/examples/dashboard/#"&gt;Dashboard&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://v3.bootcss.com/examples/dashboard/#"&gt;Settings&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://v3.bootcss.com/examples/dashboard/#"&gt;Profile&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://v3.bootcss.com/examples/dashboard/#"&gt;Help&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;form class="navbar-form navbar-right"&gt; &lt;input type="text" class="form-control" placeholder="Search..."&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt;&lt;/nav&gt; 如何使用在base.html中导入 12&lt;body&gt;&#123;% include "nav.html" %&#125; 在add_book.html中添加导航条 12345&#123;#加导航条#&#125;&#123;% include "nav.html" %&#125;&lt;div class="container" style="margin-top: 100px"&gt;.. 静态文件路径的灵活写法 利用static方法帮我拼接静文件的路径多用于base.html 母版中的css和js文件引入 123&#123;% load static %&#125;&lt;link href = "&#123;% static 'bootstrap/css/bootstrap.min.css' %&#125;" rel="stylesheet"&lt;script src="&#123;% static 'bootstrap/js/bootstrap.min.js' %&#125;"&gt;&lt;/script&gt; 利用内置的get_static_prefix获取静态文件路径的别名，我们自行拼接路径 12&#123;% load static %&#125;&lt;link href="&#123;% get_static_prefix %&#125;bootstrap/css/bootstrap.min.css" rel=stylesheet&gt; as 语法(一个路径多次用到，可以使用as保存到一个变量,后面直接使用变量代替具体路径 自定义的simple_tag比filter高级一点点，它可以接收的参数大于2 1234567from django import templateregister = template.Library()@register.simple_tag(name="my_sum")def my_sum(arg,arg2,arg3): return "&#123;&#125; &#123;&#125; &#123;&#125;" .format(arg,arg2,arg3) 自定义的inclusion_tag用来返回一段html代码(示例:返回ul标签) 定义阶段:在app下新建:templatetags 目录(注意是Python包)新建python文件 my_inclusion.py 12345678from django import template# 生成一个注册的实例,必须写成是registerregister = template.Library()@register.inclusion_tag("ul.html")def show_ul(num): num = 1 if num &lt; 1 else int(num) data = ["第&#123;:0&gt;3&#125;" .format(i) for i in range(1,num+1)] return &#123;"data":data&#125; 编辑要用到的ul.html 12345&lt;ul&gt; &#123;% for ret in data %&#125; &lt;li&gt; &#123;&#123; ret &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; 调用阶段: 123&lt;!--# 要记得重启--&gt;&#123;% load my_inclusion %&#125;&#123;% show_ul 5 %&#125; Django 视图系统什么是视图系统 一个视图函数（类），简称视图，是一个简单的Python 函数（类），它接受Web请求并且返回Web响应。 视图view(接收请求，返回响应这一部分就叫视图,也可以叫处理函数) CBV 和 FBV CBV( class base view 基于类的视图 )和FBV( function base view 基于函数的视图 ) 例如将出版社的添加方法,修改为CBV 基于类的视图:views.py添加: 123456789101112# CBV 出版社添加from django.views import Viewclass AddPublisher(View): def get(self,request): # 用户第一次来，我给他返回一个用来填写的HTML页面 return render(request, 'add_publisher.html') def post(self,request): if request.method == "POST": publisher_name = request.POST.get("name") models.Publisher.objects.create(name=publisher_name) return redirect('/publisher_list/') 123456789# FBV 出版社添加# 出版社添加def add_publisher(request): if request.method == "POST": publisher_name = request.POST.get("name") models.Publisher.objects.create(name=publisher_name) return redirect('/publisher_list/') return render(request, 'add_publisher.html') 使用CBV时，urls.py中也做对应的修改： 123# urls.py中# url(r'^add_publisher/', views.add_publisher),url(r'^add_publisher/', views.AddPublisher.as_view()), Request 对象 和 Response 对象Request 对象当一个页面被请求时，Django就会创建一个包含本次请求原信息的HttpRequest对象。Django会将这个对象自动传递给响应的视图函数，一般视图函数约定俗成地使用 request 参数承接这个对象。 请求相关的常用值:12345path_info 返回用户访问url，不包括域名method 请求中使用的HTTP方法的字符串表示，全大写表示。GET 包含所有HTTP GET参数的类字典对象POST 包含所有HTTP POST参数的类字典对象body 请求体，byte类型 request.POST的数据就是从body里面提取到的 常用属性: 12345678910111213request.method # 获取请求的方法(GET,POST等)request.GET # 获取URL里面的参数 127.0.0.1:8000/edit_book/?id=1&amp;name=python request.GET --&gt; &#123;&quot;id&quot;:1,&quot;name&quot;:&quot;python&quot;&#125; request.GET.get(&quot;id&quot;)request.POST # 用来获取POST提交过来的数据 request.POST.get(&quot;book_title&quot;)request.path_info # 获取用户请求的路径(不包含IP端口和URL参数)request.body # post提交中会有数据 注意：键值对的值是多个的时候,比如checkbox类型的input标签，select标签，需要用： 1request.POST.getlist("hobby") 上传文件示例 urls.py 12# 上传文件url(r'^upload_files/', views.upload_files), views.py 12345678910111213141516171819202122232425# 处理上传文件的函数def upload_files(request): """ 1. 保存上传文件前，数据需要存放在某个位置。 默认当上传文件小于2.5M时，django会将上传文件的全部内容读进内存。 2. 从内存读取一次，写磁盘一次。 在f.chunks()上循环而不是用read()保证大文件不会大量使用你的系统内存。 3. 但当上传文件很大时，django会把上传文件写到临时文件中，然后存放到系统临时文件夹中。 :param request: :return: """ if request.method == "POST": # Request.FILES # 从请求的FILES中获取上传文件的文件名，file为页面上type=files类型input的name属性值 filename = request.FILES["file1"].name # 在项目目录下新建一个文件 with open(filename,"wb") as f: # 从上传的文件对象中一点一点读 for chunk in request.FILES["file1"].chunks(): # 写入本地文件 f.write(chunk) return HttpResponse('&#123;&#125; 上传完成' .format(filename)) return render(request,'upload_files.html') upload_files.html 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action="/upload_files/" method="post" enctype="multipart/form-data"&gt;&#123;# FILES 中的每个键为&lt;input type="file" name="" /&gt; 中的name，值则为对应的数据。#&#125;&#123;# 注意：FILES 只有在请求的方法为POST 且提交的&lt;form&gt; 带有enctype="multipart/form-data" 的情况下才会#&#125;&#123;# 包含数据。否则，FILES 将为一个空的类似于字典的对象。#&#125; &lt;p&gt;&lt;input type="file" name="file1"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="file" name="file2"&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="提交"&gt;&lt;/p&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; Response 对象与由Django自动创建的HttpRequest对象相比，HttpResponse对象是我们的职责范围了。我们写的每个视图都需要实例化，填充和返回一个HttpResponse。HttpResponse类位于django.http模块中。 基础必备三件套 123HttpResponse --&gt; 返回字符串内容render --&gt; 返回一个页面redirect --&gt; 返回一个重定向告诉浏览器去访问另外的网址 JsonResponse 对象JsonResponse是HttpResponse的子类，专门用来生成JSON编码的响应。 urls.py 12# JsonResponse对象url(r'^json_test/', views.json_test), views.py 123456789101112131415# json_testdef json_test(request): import json data = &#123;"name": "夜雨", "age": 18&#125; data2 = [1,2,3,4,5,"夜雨"] # 默认只能传递字典类型 # json_data = json.dumps(data,ensure_ascii=False) # 把data序列化成json格式的字符串 # return HttpResponse(json_data) # django帮我们封装的 专门用来返回JSON格式字符串响应方法 # return JsonResponse(data,json_dumps_params=&#123;'ensure_ascii':False&#125;) # django通常只能接收字典格式的,单独返回列表类型数据,需要加上safe=False return JsonResponse(data2,safe=False,json_dumps_params=&#123;'ensure_ascii':False&#125;) Django 路由系统URL配置(URLconf)就像Django 所支撑网站的目录。它的本质是URL与要为该URL调用的视图函数之间的映射表。你就是以这种方式告诉Django，对于这个URL调用这段代码，对于那个URL调用那段代码。 URLconf配置 基本格式: 12345from django.conf.urls import urlurlpatterns = [ url(正则表达式, views视图函数，参数，别名),] 参数说明: 1234正则表达式：一个正则表达式字符串views视图函数：一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串参数：可选的要传递给视图函数的默认参数（字典形式）别名：一个可选的name参数 正则表达式详解 基本配置 123456789101112131415161718192021222324# urls.py:# JsonResponse对象url(r'^json_test/$', views.json_test),# http://127.0.0.1:8000/json_test/1/2/3 无法访问# http://127.0.0.1:8000/json_test/?id=1 可以访问,因为?id是参数# 路由系统# book/2-4位数字# http://127.0.0.1:8000/book/12/# url(r'^book/[0-9]&#123;2,4&#125;/$', views.book),# 分组匹配()url(r'^book/([0-9]&#123;2,4&#125;)/([a-zA-Z]&#123;2&#125;)/$', views.book),# http://127.0.0.1:8000/book/12/ab/# book() takes 1 positional argument but 3 were given# 如果我们使用分组匹配,会将()分组匹配里面的值,当做参数发送给视图函数,函数中除了request,还需要接收()里面的参数# 这种方式可以代替?id=1，地址栏传值# views.py:# 分组匹配,位置参数def book(request,arg1,arg2): print('arg1:',arg1) # arg1: 12 print('arg2:',arg2) # arg2: ab return HttpResponse("分组匹配") 注意事项: 1234urlpatterns中的元素按照书写顺序从上往下逐一匹配正则表达式，一旦匹配成功则不再继续。若要从URL中捕获一个值，只需要在它周围放置一对圆括号（分组匹配）。不需要添加一个前导的反斜杠，因为每个URL 都有。例如，应该是^articles 而不是 ^/articles。每个正则表达式前面的&apos;r&apos; 是可选的但是建议加上。 分组匹配分组匹配:给视图函数传递位置参数分组匹配和分组命名匹配不能混用,看需求使用哪一种1234567# urls.py# 分组匹配()url(r'^book/([0-9]&#123;2,4&#125;)/([a-zA-Z]&#123;2&#125;)/$', views.book),# http://127.0.0.1:8000/book/12/ab/# book() takes 1 positional argument but 3 were given# 如果我们使用分组匹配,会将()分组匹配里面的值,当做参数发送给视图函数,函数中除了request,还需要接收()里面的参数# 这种方式可以代替?id=1，地址栏传值 12345678910111213# views.py# 分组匹配,位置参数def book(request,arg1,arg2): print('arg1:',arg1) # arg1: 1000 print('arg2:',arg2) # arg2: py return HttpResponse("分组匹配")# 分组匹配,args接收参数def book(request,*args): print(args) # args接收返回元祖('1000', 'py') print(args[0]) print(args[1]) return HttpResponse("分组匹配") 分组命名匹配分组命名:给视图函数传关键字参数 123456# urls.py# 分组命名匹配url(r'^book/(?P&lt;year&gt;[0-9]&#123;2,4&#125;)/(?P&lt;title&gt;[a-zA-Z]&#123;2&#125;)/$', views.book),# 会将year和title当做关键字参数# 分组匹配相当于 位置传参# 分组命名相当于 关键字传参 1234567891011121314views.py# 分组命名匹配,关键字参数def book(request,year,title): print('year:',year) print('title:',title) return HttpResponse("分组命名匹配")# 分组命名匹配,**kwargs接收参数def book(request,**kwargs): print(kwargs) # &#123;'year': '1000', 'title': 'py'&#125; print(kwargs['year']) # 1000 print(type(kwargs['year'])) # &lt;class 'str'&gt; 捕获的参数永远是字符串类型 print(kwargs['title']) # py return HttpResponse("分组命名匹配") 设置默认值 如果用户访问blog/ 那么我们默认返回num1,也就是blog的第一页 如果用户到这page100访问,那么我们直接返回对应的页面 返回的是blog/page具体第几页的blog，num=捕获的值,而不是num=1 12345678910111213# urls.py中from django.conf.urls import urlfrom . import viewsurlpatterns = [ url(r'^blog/$', views.page), url(r'^blog/page(?P&lt;num&gt;[0-9]+)/$', views.page),]# views.py中，可以为num指定默认值def page(request, num="1"): pass 分组匹配的使用出版社删除 改为分组匹配1234# urls.pyurl(r'^del_publisher/$', views.del_publisher),#url分组匹配,括号里的值会传给函数url(r'^del_publisher2/([0-9]+)/$', views.del_publisher2), 12345678# views.py# url分组匹配传参形式-删除出版社def del_publisher2(request,del_id): # url(r'^del_publisher/([0-9]+)', views.del_publisher2), # 至少接收一个数字作为del_id # 不用再从request.GET取值 models.Publisher.objects.get(id=del_id).delete() return redirect("/publisher_list/") 1&lt;a href="/del_publisher2/&#123;&#123; publisher.id &#125;&#125;/"&gt;分组匹配删除&lt;/a&gt; 出版社修改 改为分组匹配1234# urls.pyurl(r'^edit_publisher/$', views.edit_publisher),# url分组匹配,括号里的值会传给函数url(r'^edit_publisher2/([0-9]+)/$', views.edit_publisher2), 12345678910111213# views.py# url分组匹配传参形式-编辑出版社def edit_publisher2(request,edit_id): if request.method == "POST": edit_id = request.POST.get("id") edit_name = request.POST.get("name") edit_obj = models.Publisher.objects.get(id=edit_id) edit_obj.name = edit_name edit_obj.save() return redirect("/publisher_list/") # 不用再从request.GET取值edit_id edit_obj = models.Publisher.objects.get(id=edit_id) return render(request, "edit_publisher.html", &#123;"edit_obj": edit_obj&#125;) 1&lt;a href="/edit_publisher2/&#123;&#123; publisher.id &#125;&#125;/"&gt;分组匹配编辑&lt;/a&gt; include 其他的URLconfs 我们现在所有的路由指向都是写在一个配置文件 project/urls.py 当我们的app有很多的时候,就需要分组管理 include 分流urls 执行命令 1D:\MyProject\test_0922&gt;python manage.py startapp app02 在settings中添加配置,告诉Django我们添加了新的app 12345678910INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'app01.apps.App01Config', 'app02.apps.App02Config',] 在app01中新建ursl.py 编辑2级路由app01/urls.py 12345678910from django.conf.urls import urlfrom app01 import viewsurlpatterns = [ # 分组匹配() url(r'^book/([0-9]&#123;2,4&#125;)/([a-zA-Z]&#123;2&#125;)/$', views.book), # index url(r'^$', views.publisher_list),] 编辑1级路由project/urls.py 1234567891011121314from django.conf.urls import url,includefrom django.contrib import adminfrom app01 import views,urlsurlpatterns = [ url(r'^admin/', admin.site.urls), # http://127.0.0.1:8000/app01/publisher_list/ # 要记得导入include # 不要忘记修改对应的路径如:base下的 # &lt;a href="/app01/publisher_list/"&gt; # 所有以app01开头的url都交给app01/urls.py处理 url(r'^app01/',include(urls)),] include app02.urls1234567# app02.urlsfrom django.conf.urls import urlfrom app02 import viewsurlpatterns = [ url(r'^home/$', views.home),] 123# app02.viewsdef home(request): return HttpResponse('app02/home') 12345678910111213141516# project.urls# 要注意urls会重名,导入app02的urls后as别名from django.conf.urls import url,includefrom django.contrib import adminfrom app01 import views,urls as app01_urlsfrom app02 import urls as app02_urlsurlpatterns = [ url(r'^admin/', admin.site.urls), # http://127.0.0.1:8000/app01/publisher_list/ url(r'^app01/',include(urls)), # http://127.0.0.1:8000/app02/home/ url(r'^app02/',include(app02_urls)),] 命名URL和URL反向解析 Templates里面创建多级目录 templates car home.html house home.html 编写urls.py 对应各自的路由: 123# project/urls.pyurl(r'^car/',include(app01_urls)),url(r'^house/',include(app02_urls)) 1234# app01/urls.pyurlpatterns = [ url(r'^home/$', views.home),] 1234# app02/urls.pyurlpatterns = [ url(r'^home/$', views.home),] 编写views.py 12def home(request): return render(request,'car/home.html') 12def home(request): return render(request,'house/home.html') 编写html 123&lt;h1&gt;app01/home 这是卖车的首页&lt;/h1&gt;&lt;p&gt;友情链接&lt;/p&gt;&lt;a href="/house/home"&gt;想买房子点我&lt;/a&gt; 123&lt;h1&gt;app02/home 这是卖房子的首页&lt;/h1&gt;&lt;p&gt;友情链接&lt;/p&gt;&lt;a href="/car/home"&gt;想买车点我&lt;/a&gt; 现在我们有了两个app并且都各自用有自己的路由和视图函数,返回各自的首页,并且在页面中都由a标签跳转 这个时候如果我们的urls的路径被修改了,相应的跳转也就无法访问,这是由于url被固定了 1url(r'^carcar/',include(app01_urls)), 使用url反向解析来获取页面 一级路由解析 路由侧添加了别名:name=’别名’,html就可以动态调用url=’别名’此功能可以防止url被修改,导致页面无法找到url路径 12345# project.urls# JsonResponse对象# url(r'^json_test/$', views.json_test,name='json_test'),# 如果我们修改页面,跳转将失败，这是由于url被固定了 url(r'^json_data111/$', views.json_test,name='json_test'), 1234567# house/home.html&lt;p&gt;返回JsonResponse页面# &lt;a href="/json_data/"&gt;Json_test的反向解析&lt;/a&gt;# /json_data/不能被固定写死,通过别名去动态查询url# url会去整个项目的urls去找别名是json_test的url &lt;a href="&#123;% url 'json_test' %&#125;"&gt;Json_test的反向解析&lt;/a&gt;&lt;/p&gt; 二级路由,通过别名反向解析找到url 123# project/urls.py# url(r'^car/',include(app01_urls)),url(r'^car999/',include(app01_urls)), 1234# app01/urls.pyurlpatterns = [ url(r'^home/$', views.home,name='car_home'),] 1&lt;a href="&#123;% url 'car_home' %&#125;"&gt;想买车点我&lt;/a&gt; 使用url反向解析,在视图views中跳转 一般views中的跳转使用redirect 12# project/urlsurl(r'^json_data3/$', views.json_test,name='json_test'), 12def home(request): return redirect('/json_data3/') 根据别名找到跳转的url1234567def home(request): from django.urls import reverse # reverse 反向解析,根据别名找到跳转的url # 会去urls里面找到对应的别名 redirect_url = reverse("json_test") return redirect(redirect_url) 反向解析URL 本质上就是给url匹配模式起别名,然后通过别名拿到具体的url地址 如何使用 在url匹配模式中,定义name=’别名’ 在模板语言里使用 url ‘别名’ 在视图函数里使用 12from django.urls import reverseredirect_url = reverse("json_test") # 得到 URL 使用url反向解析,加上参数跳转 当url里面含有参数的时候,别名如何解析呢? 123app01/urls# 分组匹配()url(r'^book/([0-9]&#123;2,4&#125;)/([a-zA-Z]&#123;2&#125;)/$', views.book,name='book'), 123456789101112# 在反转的时候带参数def home(request): from django.urls import reverse # reverse 反向解析,根据别名找到跳转的url # 会去urls里面找到对应的别名 # redirect_url = reverse("book",kwargs=&#123;"year":2018,"title":"py"&#125;) redirect_url = reverse("book",args=[2018,"py"]) print(redirect_url) # /car999/book/2018/py/ # return redirect(redirect_url) return render(request,'car/home.html') 2.在模板语言中,测试带参数的url反向解析 12345&lt;p&gt;测试带参数的url反向解析: &lt;a href="&#123;% url 'book' 2018 "py" %&#125;"&gt;book点我&lt;/a&gt;&lt;/p&gt;# http://127.0.0.1:8000/car999/book/2018/py/ 把编辑按钮的连接改成反向解析URL形式 适用于分组匹配12# url分组匹配,括号里的值会传给函数url(r'^edit_publisher2/([0-9]+)/$', views.edit_publisher2,name="edit_pub"), 1&lt;a href="&#123;% url 'edit_pub' publisher.id %&#125;"&gt;分组匹配编辑&lt;/a&gt; 命名空间模式 当app多了,urls中的别名也可能会出现重复 123# project/urls.pyurl(r'^car999/',include(app01_urls,namespace="car")),url(r'^house/',include(app02_urls,namespace="house")), 1234# app02/urls.pyurlpatterns = [ url(r'^home/$', views.home,name='home'),] 12&lt;a href="&#123;% url 'car:car_home' %&#125;"&gt;想买车点我&lt;/a&gt;&lt;a href="&#123;% url 'house:home' %&#125;"&gt;想买房子点我&lt;/a&gt; 通过URL分组和反射 将方法三合一123456789101112131415161718192021222324252627# modelsfrom django.db import models# Create your models here.class Publisher(models.Model): id = models.AutoField(primary_key=True) name = models.CharField(max_length=64,null=False,unique=True) def __str__(self): return '&lt;&#123;&#125;出版社是:&gt;' .format(self.name)class Book(models.Model): id = models.AutoField(primary_key=True) title = models.CharField(max_length=64,null=False) publisher = models.ForeignKey(to='Publisher') def __str__(self): return '&lt;&#123;&#125;书名是:&gt;' .format(self.title)class Author(models.Model): id = models.AutoField(primary_key=True) name = models.CharField(max_length=64,null=False) book = models.ManyToManyField(to='Book') def __str__(self): return '&lt;&#123;&#125;作家姓名:&gt;' .format(self.name) 1234# urls.py# url(r'^delete/表名/id值',views.delete),# http://127.0.0.1:8000/delete/book/10/url(r'^delete/([a-zA-Z]+)/(\d+)/$',views.delete), 12345678import rer = re.compile(r'^delete/([a-zA-Z]+)/(\d+)/$')ret = r.match("delete/author/10/")print(ret.groups()) # ('author', '10')print(ret.group(1)) # authorprint(ret.group(2)) # 10 123456789101112131415161718192021222324# 正则表达式的匹配参数:# table_name = ([a-zA-Z]+)# del_id = (\d+)def delete(request,table_name,del_id): print(table_name,del_id) # 额外需要判断下表名和ID值,是否都是正经的数据,表名有并且ID值存在 # 反射,通过字符串去找一个函数,变量或者是类 # 从另外一个文件 根据字符串 反射具体的变量 # models 里面有没有table_name table_name = table_name.capitalize() # 首字母大写 if hasattr(models,table_name): # 如果能找到 table_class = getattr(models,table_name) try: table_class.objects.get(id=del_id).delete() except Exception as e: print(str(e)) print('id值不存在!') return HttpResponse("表名是&#123;&#125;,ID是&#123;&#125;" .format(table_name,del_id)) else: return HttpResponse("表不存在!")]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django 基础]]></title>
    <url>%2F2019%2F09%2F03%2Fdjango-base%2F</url>
    <content type="text"><![CDATA[Django 基础使用Django 官网下载页面https://www.djangoproject.com/download/ 安装方法 命令行安装 121. pip3 install django==1.1.232. pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple/ django==1.1.23 # -i 指定哪个库 Pycharm安装 1安装不要勾选将包安装到指定的地址目录,否则需要将该地址加到环境变量才能使用django-admin命令 创建 Django 项目 命令行: 121. cd 要保存Django的目录2. python django-admin startproject 项目名 Pycharm 创建 1231. File --&gt; new project --&gt; Django2. 可以配置app名称3. 不要勾选虚拟环境 配置 Django 项目Django的配置文件为 project/settings.py文件,作为初学者要先记住4个需要配置的地方: templates (存放HTML文件的配置) 12345678TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', # 告诉Django 我用到的html文件都去这个目录找 'DIRS': [os.path.join(BASE_DIR, 'templates'),] # ... 列表中多个目录会依次查找 &#125;] 静态文件: 123456789# 静态文件夹的别名(在HTML文件中使用)# 静态文件保存的文件别名,会依次的查找下面的static1和static2,一直到找不到STATIC_URL = '/static/'# 静态文件夹的真正路径# 所有静态文件(css/js/图片) 都放在下面配置文件目录中STATICFILES_DIRS = [ os.path.join(BASE_DIR,'static')] 注释掉 csrf 相关的中间件 12# CSRF verification failed. Request aborted. 问题需要注释:大概是:46行# 'django.middleware.csrf.CsrfViewMiddleware' Django项目连接数据库 12345678910DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', # 连接的数据库类型 'NAME': 'mybook', 'HOST': '10.0.0.200', 'PORT': 3306, 'USER': 'root', 'PASSWORD': '123456', &#125;&#125; Django Web 请求流程 启动Django项目，等待用户连接 浏览器在地址栏输入URL,来连接我的Django项目 在urls.py中找到路径和函数的对应关系 执行对应的函数 返回响应 urls.py 的配置 project/urls.py 负责保存路径和函数的对应关系 12from .views import index,sport,login,baobao --&gt; 项目中的views 默认从项目使用相对路径找到from app01 import views --&gt; app中的views,从项目使用绝对路径导 12345from app01 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^publisher_list/', views.publisher_list),] views.py 的配置 app/views.py 专门用来定义处理请求的函数 123456# 必备三件套 # shortcuts(快捷键)from django.shortcuts import HttpResponse,render,redirect1. HttpResponse(&quot;要返回的字符串内容&quot;) --&gt; 通常用于直接返回数据2. render(request,&quot;html文件&quot;,&#123;&quot;k1&quot;:v1&#125;) --&gt; 返回一个HTML页面或者打开文件进行字符串替换3. redirect(&quot;URL&quot;) --&gt; 告诉用户的浏览器去访问指定的UR 123# request相关:request.method --&gt; 查看请求的方法request.POST --&gt; 获取POST请求的数 123def publisher_list(request): # 使用ORM获取所有出版社信息 return render(request,"publisher_list.html") ORM 的使用 什么是ORM 121. 是一种编程的方法论(模型)，和语言无关，其他的语言也有类似的实现2. 按照规定的语法写，自动翻译成对应的SQL语 ORM的本质 123类 --&gt; 数据表对象 --&gt; 数据行属性 --&gt; 字 ORM的功能 12ORM 操作数据表ORM 操作数据行 Django里的ORM使用 121. 手动创建数据库CREATE DATABASE mybook CHARACTER SET utf8; 123456789102. 在settings.py 里面配置数据库的链接信息DATABASES = &#123; &apos;default&apos;: &#123; &apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;, &apos;NAME&apos;: &apos;mybook&apos;, &apos;HOST&apos;: &apos;10.0.0.200&apos;, &apos;PORT&apos;: 3306, &apos;USER&apos;: &apos;root&apos;, &apos;PASSWORD&apos;: &apos;123456&apos;, &#125; 123# 3. 在project/__init__.py 文件中 告诉Django用pymysql模块代替MySQLDB来连接import pymysqlpymysql.install_as_MySQLdb() 1234# 4. 在app下的models.py 里面定义类class Publisher(models.Model): id = models.AutoField(primary_key=True) # 自增主键 name = models.CharField(max_length=64,null=False,unique=True) # 出版社名称,不能为空,唯一 1235. 执行两个命令:1. python3 manage.py makemigrations # 记录我们对文件的改动,放到migrations目录下2. python3 manage.py migrate # 把改动翻译成sql语句，再去数据库执行 1234567891011121314151617181920212223246. 创建成功的输出结果D:\MyProject\day03&gt;python manage.py makemigrationsMigrations for &apos;app01&apos;: app01\migrations\0001_initial.py - Create model PublisherD:\MyProject\day03&gt;python manage.py migrateOperations to perform: Apply all migrations: admin, app01, auth, contenttypes, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying app01.0001_initial... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying sessions.0001_initial... OK 1234567891011121314151617181920212223247. 删除表1. 注释掉在models.py下的类# class Userinfo(models.Model):# id = models.AutoField(primary_key=True) # 创建一个自增的主键字段# name = models.CharField(null=False,max_length=20) # 创建varchar类型并且不能为空的字段2. 执行命令: 1. python3 manage.py makemigrations # 记录我们对文件的改动,放到migrations目录下 2. python3 manage.py makemigrate # 把改动翻译成sql语句，再去数据库执行 D:\MyProject\app&gt;python manage.py makemigrations Migrations for &apos;app01&apos;: app01\migrations\0002_delete_userinfo.py - Delete model Userinfo D:\MyProject\app&gt;python manage.py migrate Operations to perform: Apply all migrations: admin, app01, auth, contenttypes, sessions Running migrations: Applying app01.0002_delete_userinfo... OK3. migrations目录下的文件 会记录的改 123456789101112131415161718198. 修改表# 修改了name字段的长度 从20修改到32class Userinfo(models.Model): id = models.AutoField(primary_key=True) # 创建一个自增的主键字段 name = models.CharField(null=False,max_length=32) # 创建varchar类型并且不能为空的字段1. python3 manage.py makemigrations # 记录我们对文件的改动,放到migrations目录下2. python3 manage.py makemigrate # 把改动翻译成sql语句，再去数据库执行 D:\MyProject\app&gt;python manage.py makemigrationsMigrations for &apos;app01&apos;: app01\migrations\0004_auto_20190903_1054.py - Alter field name on userinfoD:\MyProject\app&gt;python manage.py migrateOperations to perform: Apply all migrations: admin, app01, auth, contenttypes, sessionsRunning migrations: Applying app01.0004_auto_20190903_1054... O 刷新数据到展示页面 完善views中的publisher_list函数,将数据库表中刚建好的数据展示出来 123456from app01 import modelsdef publisher_list(request): # 使用ORM去数据库查出所有的出版社，填充到HTML页面中，给用户返回 ret = models.Publisher.objects.all().order_by("id") # 排序 return render(request,"publisher_list.html",&#123;"publisher_list":ret&#125;) # http://127.0.0.1:8000/publisher_list/ 在publisher_list.html中使用模板语言刷新出数据 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;出版社首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;序号&lt;/th&gt; &lt;th&gt;ID&lt;/th&gt; &lt;th&gt;出版社名称&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for publisher in publisher_list %&#125; &lt;tr&gt; &#123;#自动序号计数#&#125; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 启动Django项 命令行启动: D:\MyProject\项目路径&gt;python manage.py runserver 8099 Pycharm启动 点绿色的小三角，直接启动Django项目,选择项目名编辑可以修改端口 出版社信息 添加、删除、修改操作添加出版社 首先在urls.py文件中增加函数与访问路径的对应关系 12345urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^publisher_list/', views.publisher_list), url(r'^add_publisher/', views.add_publisher),] 在templates路径下添加add_publisher.html页面 12345678910111213141516&lt;!--post提交指向当前页面--&gt;&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;添加出版社&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;添加出版社&lt;/h1&gt;&lt;form action="/add_publisher/" method="POST"&gt; &lt;input type="text" name="publisher_name"&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;p style="color: red"&gt;&#123;&#123; error_msg &#125;&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 123&lt;!-- 在原有的展示页面上添加一个a标签作为入口指向add_publisher.html--&gt;&lt;a href="/add_publisher/"&gt;添加出版社&lt;/a&gt;&lt;hr&gt; 在views.py文件写 编写add_publisher函数方法 1234567891011121314151617181920# 添加出版社def add_publisher(request): error_msg = "" # 如果是POST请求 就取到用户提交的数据 if request.method == "POST": # 拿到用户提交的数据 new_name = request.POST.get("publisher_name",None) print(new_name ) if new_name: # 通过ORM 将数据入库 models.Publisher.objects.create(name=new_name) # 添加完成后 跳转回展示页面 查看结果 return redirect("/publisher_list/") else: # 如果添加的字符为空,那么久直接返回添加页面,并给提示 error_msg = "您填写的数据为空" return render(request, "add_publisher.html",&#123;"error_msg":error_msg&#125;) # GET请求放回添加页面 return render(request,"add_publisher.html") 删除出版社 在urls.py文件中添加对应关系 123456urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^publisher_list/', views.publisher_list), url(r'^add_publisher/', views.add_publisher), url(r'^del_publisher/', views.del_publisher),] 定义删除按钮 123456789101112&lt;tbody&gt; &#123;% for publisher in publisher_list %&#125; &lt;tr&gt; &#123;#自动序号计数#&#125; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&lt;a href="/del_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;删除&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125;&lt;/tbody&gt; 编写del_publisher函数方法 1234567891011121314# 删除出版社def del_publisher(request): # 当在页面上点击删除的时候,首先应该通过ID获取用户想删除那条数据 # 页面上会通过/del_publisher/?id=&#123;&#123; publisher.id &#125;&#125; 在get的时候传过来id # 从GET请求的参数里面拿到将要删除的数据的ID值 del_id = request.GET.get("id",None) if del_id: # 通过ORM删除指定数据 del_obj = models.Publisher.objects.get(id=del_id).delete() # 删除后返回出版社页面，查看是否成功 return redirect("/publisher_list/") else: HttpResponse("您要删除的数据不存在") 编辑出版社 在urls.py文件中添加对应关系 1234567urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^publisher_list/', views.publisher_list), url(r'^add_publisher/', views.add_publisher), url(r'^del_publisher/', views.del_publisher), url(r'^edit_publisher/', views.edit_publisher),] 定义编辑按钮 123456789101112131415&lt;tbody&gt; &#123;% for publisher in publisher_list %&#125; &lt;tr&gt; &#123;#自动序号计数#&#125; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;a href="/del_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;删除&lt;/a&gt; &lt;a href="/edit_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;编辑&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125;&lt;/tbody&gt; 创建edit_publisher.html页面 123456789101112131415161718&lt;!--1. id 将要编辑的数据id放在页面中，但是不显示--&gt;&lt;!--2. 将查询出来的要编辑的数据对象的名字刷新到编辑页面中--&gt;&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;编辑出版社&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;编辑出版社&lt;/h1&gt;&lt;form action="/edit_publisher/" method="POST"&gt; &lt;input type="text" name="id" value="&#123;&#123; publisher_obj.id &#125;&#125;" style="display: none"&gt; &lt;input type="text" name="publisher_name" value="&#123;&#123; publisher_obj.name &#125;&#125;"&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;p style="color: red"&gt;&#123;&#123; error_msg &#125;&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 编写edit_publisher函数方法1234567891011121314151617181920212223242526# 编辑出版社def edit_publisher(request): # 如果是POST提交,将用户从编辑页面修改的数据刷新到数据库中 if request.method == "POST": edit_id = request.POST.get("id",None) new_name = request.POST.get("publisher_name",None) # 更新出版社 # 根据ID取到编辑的是哪个出版社 edit_publisher_obj = models.Publisher.objects.get(id=edit_id) edit_publisher_obj.name = new_name # 把修改后的数据提交到数据 edit_publisher_obj.save() # 去出版社列表页面展示，查看是否更新成功 return redirect("/publisher_list/") # 返回编辑的是哪个出版社对象 # 从GET请求中取到要编辑的ID # 当用户从展示页点击编辑按钮,需要获取用户点击的数据ID,然后得到数据对象传给编辑页面 edit_id = request.GET.get("id",None) if edit_id: publisher_obj = models.Publisher.objects.get(id=edit_id) return render(request,"edit_publisher.html",&#123;"publisher_obj":publisher_obj&#125;) else: return HttpResponse("您要编辑的数据不存在") 图书信息 添加、删除、修改一对多关系表的建立 ORM添加外键关联 models.ForeignKey12345class Book(models.Model): id = models.AutoField(primary_key=True) # 自增主键 title = models.CharField(max_length=64, null=False, unique=True) # 书籍名称,不能为空,唯一 # ForeignKey会自动加_id , 在数据库就叫做publisher_id_id, 所以只要写publisher就可以 publisher = models.ForeignKey(to=Publisher) # ForeignKey外键 展示所有书籍 在urls.py文件中添加对应关系 12# 书籍所有相关url(r'^book_list/', views.book_list), 在book_list.html中使用模板语言刷新出数据 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;书籍列表&lt;/title&gt;&lt;/head&gt;&lt;h1&gt;书籍列表&lt;/h1&gt;&lt;a href="/add_book/"&gt;添加书籍&lt;/a&gt;&lt;body&gt;&lt;table border="1"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;序号&lt;/th&gt; &lt;th&gt;ID&lt;/th&gt; &lt;th&gt;书名&lt;/th&gt; &lt;th&gt;出版社&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for book in book_list %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.title &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.publisher.name &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &#123;#book.publisher 得到的是出版社对象,继续.出属性#&#125; &#123;#book --&gt; 书对象#&#125; &#123;#book.publisher --&gt; book对应的出版社对象#&#125; &#123;#book.publisher_id --&gt; 数据库表中实际保存的外键值#&#125; &#123;#book.publisher.name --&gt; book对应的出版社对象的属性#&#125; &lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; views.py文件中添加展示书籍函数 book_list 12345def book_list(request): # ORM去数据库中查询所有的数据 all_book = models.Book.objects.all().order_by("id") # 去HTMK页面渲染数据 return render(request,"book_list.html",&#123;"book_list":all_book&#125;) 添加书籍 在urls.py文件中添加对应关系 123# 书籍所有相关url(r'^book_list/', views.book_list),url(r'^add_book/', views.add_book), 别忘记添加入口 1&lt;a href="/add_book/"&gt;添加书籍&lt;/a&gt; 编写add_book函数 1234567891011121314151617181920# 添加书籍def add_book(request): if request.method == "POST": # &#123;"book_title":"css","publisher":1&#125; new_book_title = request.POST.get("book_title",None) new_publisher = request.POST.get("publusher",None) print(new_book_title) print(new_publisher) # 创建新书,添加新数据 models.Book.objects.create( title=new_book_title, publisher_id = new_publisher ) # 返回书籍列表页面 return redirect("/book_list/") # 取到所有的出版社，给添加页面的select all_publisher = models.Publisher.objects.all() return render(request,"add_book.html",&#123;"all_publisher":all_publisher&#125;) 添加add_book.html页面 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;添加书籍&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;添加书籍&lt;/h1&gt;&lt;hr&gt;&lt;form action="/add_book/" method="post"&gt; &lt;input type="text" name="book_title"&gt; &#123;#&#123;"publisher":pid&#125;#&#125; &lt;select name="publusher" id=""&gt; &#123;% for publisher in all_publisher %&#125; &lt;option value="&#123;&#123; publisher.id &#125;&#125;"&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/option&gt; &#123;% endfor %&#125; &lt;/select&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 删除书籍 在urls.py文件中添加对应关系 1234# 书籍所有相关url(r'^book_list/', views.book_list),url(r'^add_book/', views.add_book),url(r'^del_book/', views.del_book), 定义删除按钮 123456789101112131415&lt;tbody&gt; &#123;% for book in book_list %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.title &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; book.publisher.name &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;a href="/del_book/?id=&#123;&#123; book.id &#125;&#125;"&gt;删除&lt;/a&gt; &lt;a href="/edit_book/?id=&#123;&#123; book.id &#125;&#125;"&gt;编辑&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125;&lt;/tbody&gt; 编写del_book方法 1234567def del_book(request): # 获取要删除的书籍的ID del_id = request.GET.get("id") # 去数据库根据del_id查找要删除的书籍对象,然后直接删除 models.Book.objects.get(id=del_id).delete() # 返回书籍展示页面 return redirect("/book_list/") 编辑书籍 在urls.py文件中添加对应关系 12345# 书籍所有相关url(r'^book_list/', views.book_list),url(r'^add_book/', views.add_book),url(r'^del_book/', views.del_book),url(r'^edit_book/', views.edit_book) 编写edit_book方法 12345678910111213141516171819202122def edit_book(request): # 从POST提交的数据中取出id、书名和书关联的出版社 if request.method == "POST": edit_id = request.POST.get("id") edit_book_title = request.POST.get("book_title") edit_publusher_id = request.POST.get("publusher") # 根据edit_id得到要修改的书籍对象,然后更新数据并提交 edit_obj = models.Book.objects.get(id=edit_id) edit_obj.title = edit_book_title edit_obj.publisher_id = edit_publusher_id # 将修改提交到数据 edit_obj.save() # 返回书籍列表页面 return redirect("/book_list/") # 通过Get获取ID 将要修改的书籍对象传给页面 edit_id = request.GET.get("id",None) edit_obj = models.Book.objects.get(id=edit_id) # 获取所有出版社对象,传给页面的select all_publisher = models.Publisher.objects.all() return render(request,"edit_book.html",&#123;"all_publisher":all_publisher,"edit_obj":edit_obj&#125;) 编写edit_book.html页面 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;编辑书籍&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;编辑书籍&lt;/h1&gt;&lt;hr&gt;&lt;form action="/edit_book/" method="post"&gt; &#123;#提交数据的时候拿到书籍的ID#&#125; &lt;input type="text" name="id" value="&#123;&#123; edit_obj.id &#125;&#125;" style="display: none"&gt; &lt;input type="text" name="book_title" value="&#123;&#123; edit_obj.title &#125;&#125;"&gt; &#123;#select默认选中编辑书籍的出版社#&#125; &lt;select name="publusher"&gt; &#123;# 当前书籍关联的出版社才默认选中，其他的出版社不选中 #&#125; &#123;# 如果书的出版社的id值 &#123;&#123; book_obj.publisher.id &#125;&#125; 等于 publisher.id#&#125; &#123;% for publisher in all_publisher %&#125; &#123;% if edit_obj.publisher.id == publisher.id %&#125; &lt;option selected value="&#123;&#123; publisher.id &#125;&#125;"&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/option&gt; &#123;#否则不选中#&#125; &#123;% else %&#125; &lt;option value="&#123;&#123; publisher.id &#125;&#125;"&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/option&gt; &#123;% endif %&#125; &#123;% endfor %&#125; &lt;/select&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 作者信息 添加、删除、修改多对多关系表的建立 ORM添加多对多的关联关系 models.ManyToManyField12345678class Author(models.Model): id = models.AutoField(primary_key=True) # 自增主键 name = models.CharField(max_length=64,null=False,unique=True) # 作者名字,不能为空,唯一 # 告诉ORM 这张表与book表是多对多的关联关系，ORM自动帮我们生成第三张关联表 名字是author_book book = models.ManyToManyField(to='Book') # ManyToManyField 关联Book表 会创建第三张关联表 def __str__(self): return '&lt;Author 名字&#123;&#125;&gt;'.format(self.name) urls.py预先定义所有方法函数和路径的对应关系12345# 作者所有相关url(r'^author_list/', views.author_list),url(r'^add_author/', views.add_author),url(r'^del_author/', views.del_author),url(r'^edit_author/', views.edit_author), 展示所有作者 author_list函数 12345678910111213def author_list(request): # 查询出所有作家信息 all_author = models.Author.objects.all().order_by("id") # 单取一个作者对象,查看他的book属性 # 作者对象里的book # author_obj = all_author[0] author_obj = models.Author.objects.get(id=1) print(author_obj.book.all()) # &lt;QuerySet [&lt;Book: Java&gt;, &lt;Book: Python&gt;]&gt; 当前作者.所有的书籍对象 print("=" * 120) return render(request,"author_list.html",&#123;"author_list":all_author&#125;) 展示作者页面 author_list.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;作家信息&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;作家信息&lt;/h1&gt;&lt;hr&gt;&lt;a href="/add_author/"&gt;添加新作者&lt;/a&gt;&lt;table border="1"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;序号&lt;/th&gt; &lt;th&gt;ID&lt;/th&gt; &lt;th&gt;作家姓名&lt;/th&gt; &lt;th&gt;书籍&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for author in author_list %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; author.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; author.name &#125;&#125;&lt;/td&gt; &lt;td&gt; &#123;#循环作者所有书籍对象 判断最后一个加上|分割#&#125; &#123;% for book in author.book.all %&#125; &#123;% if forloop.last %&#125; &#123;&#123; book.title &#125;&#125; &amp;nbsp; &#123;% else %&#125; &#123;&#123; book.title &#125;&#125; | &#123;% endif %&#125; &#123;% endfor %&#125; &lt;/td&gt; &lt;td&gt; &lt;a href="/del_author/?id=&#123;&#123; author.id &#125;&#125;"&gt;删除&lt;/a&gt; &lt;a href="/edit_author/?id=&#123;&#123; author.id &#125;&#125;"&gt;编辑&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 添加作者 add_author函数 12* 要将所有书籍传给添加页面的select* 先创建新作者,使用set添加新作者与书的对应关系 12345678910111213141516171819# 添加作者def add_author(request): # 取到提交数据 if request.method == "POST": new_name = request.POST.get("author_name") # POST 提交的数据是多个值的时候,要用getlist,如多选的checkbox和多选的select new_author_book = request.POST.getlist("author_book") print(new_name,new_author_book) print("*" * 150) # 创建新作者 new_author_obj = models.Author.objects.create(name=new_name) # 使用set添加新作者与书的对应关系 new_author_obj.book.set(new_author_book) # 返回作者列表页面 return redirect("/author_list/") # 要将所有书籍传给添加页面的select all_book = models.Book.objects.all().order_by("id") return render(request,"add_author.html",&#123;"book_list":all_book&#125;) 添加add_author.html页面 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;添加作者&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;添加作者&lt;/h1&gt;&lt;hr&gt;&lt;form action="/add_author/" method="post"&gt; &lt;p&gt;作者姓名: &lt;input type="text" name="author_name"&gt; &lt;/p&gt; &lt;p&gt;选择作品: &#123;#多选,将所有图书放入多选框中#&#125; &lt;select multiple name="author_book"&gt; &#123;% for book in book_list %&#125; &lt;option value="&#123;&#123; book.id &#125;&#125;"&gt;&#123;&#123; book.title &#125;&#125;&lt;/option&gt; &#123;% endfor %&#125; &lt;/select&gt; &lt;/p&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 删除作者 del_author函数: 1234567# 删除作者def del_author(request): # 从GET里获取要删除的作者ID，然后直接删除 del_id = request.GET.get("id",None) models.Author.objects.get(id=del_id).delete() # 返回作者列表页面 return redirect("/author_list/") 编辑作者 edit_author函数 123451. 注意获取页面上的select或者checkbox多个值需要使用getlist接收2. 页面条件判断 当书在作者对象.book.all里时，才默认选中该书,否则不选中3. edit_obj.book.all()在函数中得到对象所有的书籍对象,但是在页面中无法使用()方法4. 更新数据需要使用对象.save()方法5. 更新作者对象的书籍,edit_obj.book.set(edit_books) 123456789101112131415161718192021222324252627282930def edit_author(request): if request.method == "POST": # 从POST里取得编辑作者的id、姓名、作品 edit_id = request.POST.get("id") edit_name = request.POST.get("author_name") edit_books = request.POST.getlist("author_books") print(edit_books) print(edit_name) print("*" * 100) # 根据ID找到编辑的是哪个作者对象, 最后提交更新数据 edit_obj = models.Author.objects.get(id=edit_id) # 更新数据 edit_obj.name = edit_name # 更新作者关联书籍的对应关系 edit_obj.book.set(edit_books) # 将所有的修改提交 edit_obj.save() # 返回作者列表页面 return redirect("/author_list/") # 通过GET获取用户编辑的作者ID，找到要编辑的作者对象,返回给edit_author.html edit_id = request.GET.get("id",None) edit_obj = models.Author.objects.get(id=edit_id) print(edit_obj.book.all()) # &lt;QuerySet [&lt;Book: &lt;object book_title=Python&gt;&gt;, &lt;Book: &lt;object book_title=GO&gt;&gt;]&gt; # 返回所有书籍返回给页面的select all_book = models.Book.objects.all() return render(request,'edit_author.html',&#123;"book_list":all_book,"edit_obj":edit_obj&#125;) 添加edit_author.html页面 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;编辑作者&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;编辑作者&lt;/h1&gt;&lt;hr&gt;&lt;form action="/edit_author/" method="post"&gt; &lt;input type="text" name="id" value="&#123;&#123; edit_obj.id &#125;&#125;" style="display: none"&gt; &lt;p&gt;作者姓名: &lt;input type="text" name="author_name" value="&#123;&#123; edit_obj.name &#125;&#125;"&gt; &lt;/p&gt; &lt;p&gt;作者作品: &lt;select name="author_books" multiple&gt; &#123;% for book in book_list %&#125; &#123;#判断,如果当前这本书 在 当前作者关联的书列表里 edit_id_obj.book.all #&#125; &#123;% if book in edit_obj.book.all %&#125; &lt;option selected value="&#123;&#123; book.id &#125;&#125;"&gt;&#123;&#123; book.title &#125;&#125;&lt;/option&gt; &#123;#否则，不选中#&#125; &#123;% else %&#125; &lt;option value="&#123;&#123; book.id &#125;&#125;"&gt;&#123;&#123; book.title &#125;&#125;&lt;/option&gt; &#123;% endif %&#125; &#123;% endfor %&#125; &lt;/select&gt; &lt;/p&gt; &lt;input type="submit" value="提交"&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 使用 Bootstrap模板 和 Font Awesome图标字体库去Bootstrap下载想要的模板 选择控制台模板https://v3.bootcss.com/examples/dashboard/ 右键另存为桌面,会下载两个文件: Dashboard Template for Bootstrap_files 目录:只保留dashboard.css即可 Dashboard Template for Bootstrap.html html文件 去Font Awesome官网下载图标字体库和CSS框架 http://fontawesome.dashgame.com 将下载好的文件放入static目录 套用首页模板 修改publisher_list 为old_publisher_list 修改Dashboard Template for Bootstrap.html 为publisher_list 替换首页的css引入: 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;!-- saved from url=(0042)https://v3.bootcss.com/examples/dashboard/ --&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;meta name="description" content=""&gt; &lt;meta name="author" content=""&gt; &lt;link rel="icon" href="https://v3.bootcss.com/favicon.ico"&gt; &lt;title&gt;添加作者&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href="/static/dashboard.css" rel="stylesheet"&gt; &lt;link href="/static/fontawesome/css/font-awesome.min.css" rel="stylesheet"&gt;&lt;/head&gt; 替换首页的jQuery引入: 12&lt;script src="/static/jquery-3.3.1.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt; Bootstrap和fontawesome的使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;div class="container-fluid"&gt; &lt;div class="row"&gt; &lt;div class="col-sm-3 col-md-2 sidebar"&gt; &lt;ul class="nav nav-sidebar"&gt; &lt;li class="active"&gt;&lt;a href="/publisher_list/"&gt;出版社列表&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/book_list/"&gt;书籍列表&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/author_list/"&gt;作者列表&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main"&gt; &lt;h1 class="page-header"&gt;出版社管理页面&lt;/h1&gt; &#123;#面板#&#125; &lt;div class="panel panel-primary"&gt; &lt;div class="panel-heading"&gt;书籍列表&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;div class="row" style="margin-bottom: 20px"&gt; &lt;div class="col-md-4"&gt; &lt;div class="input-group"&gt; &lt;input type="text" class="form-control" placeholder="Search for..."&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="button"&gt;搜索&lt;/button&gt; &lt;/span&gt; &lt;/div&gt;&lt;!-- /input-group --&gt; &lt;/div&gt;&lt;!-- /.col-md-4 --&gt; &lt;div class="col-md-2 pull-right"&gt; &lt;a href="/add_publisher/" class="btn btn-success"&gt;新页面添加&lt;/a&gt; &lt;button class="btn btn-success" data-toggle="modal" data-target="#myModal"&gt;新增&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;!-- /.row --&gt; &lt;table class="table table-bordered " &gt; &lt;thead &gt; &lt;tr&gt; &lt;th class="text-center"&gt;序号&lt;/th&gt; &lt;th class="text-center"&gt;ID&lt;/th&gt; &lt;th class="text-center"&gt;出版社&lt;/th&gt; &lt;th class="text-center"&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody class="text-center"&gt; &#123;% for publisher in all_publisher %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; forloop.counter &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; publisher.name &#125;&#125;&lt;/td&gt; &lt;td&gt; &lt;a class="btn btn-danger" href="/del_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;&lt;i class="fa fa-trash-o fa-fw"&gt;&lt;/i&gt;删除&lt;/a&gt; &lt;a class="btn btn-primary" href="/edit_publisher/?id=&#123;&#123; publisher.id &#125;&#125;"&gt;&lt;i class="fa fa-pencil fa-fw"&gt;&lt;/i&gt;编辑&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 简单的写一个add_book.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;!-- saved from url=(0042)https://v3.bootcss.com/examples/dashboard/ --&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;meta name="description" content=""&gt; &lt;meta name="author" content=""&gt; &lt;link rel="icon" href="https://v3.bootcss.com/favicon.ico"&gt; &lt;title&gt;添加作者&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href="/static/dashboard.css" rel="stylesheet"&gt; &lt;link href="/static/fontawesome/css/font-awesome.min.css" rel="stylesheet"&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div class="col-md-6 col-md-offset-3"&gt; &#123;#页头#&#125; &lt;div class="page-header"&gt; &lt;h1&gt;添加页面&lt;/h1&gt; &lt;/div&gt; &lt;div class="panel panel-primary"&gt; &lt;div class="panel-heading"&gt;添加出版社&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;form class="form-horizontal" action="/add_publisher/" method="post"&gt; &lt;div class="form-group"&gt; &lt;label for="publisher_name" class="col-sm-3 control-label"&gt;出版社名称&lt;/label&gt; &lt;div class="col-sm-9"&gt; &lt;input type="text" name="publisher_name" class="form-control" id="publisher_name" placeholder="出版社名称"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-3 col-sm-9"&gt; &lt;button type="submit" class="btn btn-default"&gt;提交&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script src="/static/jquery-3.3.1.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/html&gt; 简单的写一个edit_book.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;!DOCTYPE html&gt;&lt;!-- saved from url=(0042)https://v3.bootcss.com/examples/dashboard/ --&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;meta name="description" content=""&gt; &lt;meta name="author" content=""&gt; &lt;link rel="icon" href="https://v3.bootcss.com/favicon.ico"&gt; &lt;title&gt;编辑出版社&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href="/static/bootstrap/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href="/static/dashboard.css" rel="stylesheet"&gt; &lt;link href="/static/fontawesome/css/font-awesome.min.css" rel="stylesheet"&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div class="col-md-6 col-md-offset-3"&gt; &#123;#页头#&#125; &lt;div class="page-header"&gt; &lt;h1&gt;编辑页面&lt;/h1&gt; &lt;/div&gt; &lt;div class="panel panel-primary"&gt; &lt;div class="panel-heading"&gt;编辑出版社&lt;/div&gt; &lt;div class="panel-body"&gt; &lt;form class="form-horizontal" action="/edit_publisher/" method="post"&gt; &lt;input type="text" name="edit_id" value="&#123;&#123; edit_obj.id &#125;&#125;" style="display: none" &gt; &lt;div class="form-group"&gt; &lt;label for="publisher_name" class="col-sm-3 control-label"&gt;出版社名称&lt;/label&gt; &lt;div class="col-sm-9"&gt; &lt;input type="text" name="publisher_name" class="form-control" id="publisher_name" value="&#123;&#123; edit_obj.name &#125;&#125;"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-3 col-sm-9"&gt; &lt;button type="submit" class="btn btn-default"&gt;提交&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script src="/static/jquery-3.3.1.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyMySQL]]></title>
    <url>%2F2019%2F08%2F20%2Fpymysql%2F</url>
    <content type="text"><![CDATA[PyMySQL介绍 PyMySQL 是在 Python3.x 版本中用于连接 MySQL 服务器的一个库，Python2中则使用 mysqldb。 Django 中也可以使用 PyMySQL 连接 MySQL 数据库。 PyMySQL安装pip3 install pymysql 连接数据库 前提需要: 你有一个MySQL数据库，并且已经启动。 你有可以连接该数据库的用户名和密码 你有一个有权限操作的database 基本使用连接数据库并查询数据123456789101112131415161718192021222324252627282930313233import pymysql# http://www.cnblogs.com/liwenzhou/p/8032238.htmlif __name__ == '__main__': # 拿到用户输入用户名和密码 username = input('请输入用户名: ') password = input('请输入密码: ') # 去数据库里面判断用户名和密码是否正确 # 1. 连接数据库 conn = pymysql.connect( host="localhost", port=3306, database="userinfo", user="root", password="123", charset="utf8" # 千万记得么有 - ) # 光标 获取输入sql语句的光标对象 cursor = conn.cursor() # SQL语句 sql = "select * from info where username = 'leo' and password = '123';" # 执行语句 ret = cursor.execute(sql) print(ret) # 1 返回结果的行数 # 关闭连接 cursor.close() conn.close() 登录效验,规避SQL注入123456789101112131415161718192021222324252627282930import pymysqlif __name__ == '__main__': username = input("请输入用户名; ") passowrd = input("请输入密码: ") conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() sql = "select * from info where username = %s and password = %s ;" print(sql) print('*' * 100) ret = cursor.execute(sql,[username,passowrd]) # 让pymysql帮我们拼接sql语句 if ret: print('登录成功') else: print('登录失败!') cursor.close() conn.close() 增删改查操作增1234567891011121314151617181920212223242526272829303132333435import pymysqlif __name__ == '__main__': username = input("请输入要添加的用户名: ") password = input("请输入用户的密码: ") conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() sql = "insert into info (username,password) VALUES(%s,%s)" # 执行 try: cursor.execute(sql,[usernam,password]) # 提交事务: 读操作不用提交，写操作一定要提交 conn.commit() # 提交之后，获取刚插入的数据ID last_id = cursor.lastrowid print(last_id) except Exception as e: print("报错: ",str(e)) conn.rollback() # 有异常，回滚事务 # 插入数据失败回滚 # 在执行增删改操作时，如果不想提交前面的操作，可以使用 rollback() 回滚取消操作。 # 关闭 cursor.close() conn.close() 获取插入数据的ID(关联操作时会用到)12345678910111213141516171819202122232425262728293031323334353637383940414243import pymysqlif __name__ == '__main__': # username = input("请输入要添加的用户名: ") # password = input("请输入用户的密码: ") conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() # 创建班级 sql1 = "insert into class (name) VALUES(%s)" # 创建学生 sql2 = "insert into student (name,cid) VALUES(%s,%s)" # 执行 try: cursor.execute(sql1,['全棧10期']) new_id = cursor.lastrowid # 拿到前一条语句执行后的id值 print(new_id) cursor.execute(sql2, ['leo',new_id]) # 提交事务: 读操作不用提交，写操作一定要提交 conn.commit() # 提交之后，获取刚插入的数据ID last_id = cursor.lastrowid print(last_id) except Exception as e: print("报错: ",str(e)) conn.rollback() # 有异常，回滚事务 # 关闭 cursor.close() conn.close() 批量执行 executemany12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import pymysqlif __name__ == '__main__': conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() data = [ ("k", "123"), ("w", "123"), ("m", "123"), ] # 创建用户 sql = "insert into info (username,password) VALUES(%s,%s)" # 批量执行 try: # 一次执行多个语句 传入可迭代的数据类型 cursor.executemany(sql,data) new_id = cursor.lastrowid # 拿到前一条语句执行后的id值 print(new_id) # for 循环执行语句 一次只执行一条语句 # for i in data: # cursor.execute(sql,i) # new_id = cursor.lastrowid # 拿到前一条语句执行后的id值 # print(new_id) # 提交事务: 读操作不用提交，写操作一定要提交 conn.commit() # 提交之后，获最新的数据ID last_id = cursor.lastrowid print(last_id) except Exception as e: print("报错: ",str(e)) conn.rollback() # 有异常，回滚事务 # 关闭 cursor.close() conn.close() 删1234567891011121314151617181920212223if __name__ == '__main__': conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() # sql = "delete from info WHERE username = %s ;" # cursor.execute(sql,'k') sql = "delete from info WHERE id = %s ;" cursor.execute(sql,10) # 提交 conn.commit() # 关闭 cursor.close() conn.close() 改12345678910111213141516171819202122import pymysqlif __name__ == '__main__': conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) cursor = conn.cursor() sql = "update info set password = %s WHERE username = %s ;" cursor.execute(sql,["000","rubin"]) # 提交 conn.commit() # 关闭 cursor.close() conn.close() 查 指定返回的数据格式为 字典格式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import pymysqlif __name__ == '__main__': conn = pymysql.connect( host = "localhost", port = 3306, user = "root", password = "123", database = "userinfo", charset = "utf8" ) # cursor = conn.cursor() # 指定返回的数据格式为 字典格式 cursor = conn.cursor(cursor=pymysql.cursors.DictCursor) # sql语句 sql = "select * from info;" # 执行 # 返回的是受影响的行数，不是具体数据 cursor.execute(sql) # print("&#123;&#125; rows in set" .format(ret)) # 返回所有数据 元组 ((1, 'leo', '123')... ret = cursor.fetchall() print(ret) # 获取一条数据,查一条打印一条，没有就是None # ret = cursor.fetchone() # print(ret) # # ret = cursor.fetchone() # print(ret) # 获取指定数量数据 # ret = cursor.fetchmany(3) # print(ret) # ret = cursor.fetchone() # print(ret) # ret = cursor.fetchall() # print(ret) # [] 按照光标移动查询 之前已经都查过了 # 按照光标位置查询 absolute 绝对定位 到第一条 # cursor.scroll(1,mode="absolute") # ret = cursor.fetchall() # print(ret) # 按照光标位置查询 相对定位 从当前位置向下移动 # ret = cursor.fetchmany(2) # print(ret) # # cursor.scroll(1,mode="relative") # ret = cursor.fetchall() # print(ret) # 提交 查询无需提交 # conn.commit() # 关闭 cursor.close() conn.close() 查询多条数据12345678910111213141516# 导入pymysql模块import pymysql# 连接databaseconn = pymysql.connect(host="你的数据库地址", user="用户名",password="密码",database="数据库名",charset="utf8")# 得到一个可以执行SQL语句的光标对象cursor = conn.cursor()# 查询数据的SQL语句sql = "SELECT id,name,age from USER1;"# 执行SQL语句cursor.execute(sql)# 获取多条查询数据ret = cursor.fetchall()cursor.close()conn.close()# 打印下查询结果print(ret) 剥皮函数1234567891011121314151617181920212223if __name__ == '__main__': list1 = [11, [22, 3], [4, ], [55, 66], 8, [9, [7, [12, [34, [26]]]]]] # 去除多余嵌套的列表,得到[11, 22, 3, 4, 55, 66, 8]# 小剥皮def func(x): return [a for b in x for a in b]def func2(x): ret = [] for b in x: if isinstance(b,list): for a in func2(b): ret.append(a) else: ret.append(b) return retlist2 = [11,22]list2 = [11,[22,33]]list2 = [11,[22,33,[44,55]]]ret = func2(list1)print(ret)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>pymysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bootstrap]]></title>
    <url>%2F2019%2F07%2F29%2Fbootstrap%2F</url>
    <content type="text"><![CDATA[Bootstrap 介绍Bootstrap是Twitter开源的基于HTML、CSS、JavaScript的前端框架。它是为实现快速开发Web应用程序而设计的一套前端工具包。它支持响应式布局，并且在V3版本之后坚持移动设备优先。 为什么要使用 Bootstrap？在Bootstrap出现之前：命名：重复、复杂、无意义（想个名字费劲）样式：重复、冗余、不规范、不和谐页面：错乱、不规范、不和谐在使用Bootstrap之后： 各种命名都统一并且规范化。 页面风格统一，画面和谐。重点是记住人家定义好的样式类 Bootstrap 下载官方地址：https://getbootstrap.com中文地址：http://www.bootcss.com https://v3.bootcss.com下载：用于生产环境的 Bootstrap ， https://v3.bootcss.com/getting-started/#download Bootstrap环境搭建目录结构:1234567891011121314151617181920212223bootstrap-3.3.7-dist/├── css // CSS文件│ ├── bootstrap-theme.css // Bootstrap主题样式文件│ ├── bootstrap-theme.css.map│ ├── bootstrap-theme.min.css // 主题相关样式压缩文件│ ├── bootstrap-theme.min.css.map│ ├── bootstrap.css│ ├── bootstrap.css.map│ ├── bootstrap.min.css // 核心CSS样式压缩文件│ └── bootstrap.min.css.map├── fonts // 字体文件│ ├── glyphicons-halflings-regular.eot│ ├── glyphicons-halflings-regular.svg│ ├── glyphicons-halflings-regular.ttf│ ├── glyphicons-halflings-regular.woff│ └── glyphicons-halflings-regular.woff2└── js // JS文件 ├── bootstrap.js ├── bootstrap.min.js // 核心JS压缩文件 └── npm.js处理依赖由于Bootstrap的某些组件依赖于jQuery，所以请确保下载对应版本的jQuery文件，来保证Bootstrap相关组件运行正常。 Bootstrap引入基本上使用min压缩后的文件，并且需要引入jQuery1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel="stylesheet" href="bootstrap/css/bootstrap.min.css"&gt;&lt;/head&gt;&lt;body&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script src="bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; Bootstrap全局样式排版、按钮、表格、表单、图片等我们常用的HTML元素，Bootstrap中都提供了全局样式。我们只要在基本的HTML元素上通过设置class就能够应用上Bootstrap的样式，从而使我们的页面更美观和谐。 引入文件示例12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt; &lt;!--IE浏览器用edge渲染--&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;!--手机适配--&gt; &lt;!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt; &lt;title&gt;title&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href="bootstrap/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;你好，世界！&lt;/h1&gt; &lt;!-- jQuery (Bootstrap 的所有 JavaScript 插件都依赖 jQuery，所以必须放在前边) --&gt; &lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt; &lt;!-- 加载 Bootstrap 的所有 JavaScript 插件。你也可以根据需要只加载单个插件。 --&gt; &lt;script src="bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 登录注册示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;link rel="stylesheet" href="bootstrap/css/bootstrap.min.css"&gt; &lt;style&gt; body&#123; background-color: #eeeeee; &#125; #login-box&#123; margin-top: 100px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div id="login-box" class="col-md-4 col-md-offset-4"&gt; &lt;h3 class="text-center"&gt;请登录&lt;/h3&gt; &lt;form class="form-horizontal"&gt; &lt;div class="form-group"&gt; &lt;label for="inputEmail3" class="col-sm-3 control-label"&gt;Email&lt;/label&gt; &lt;div class="col-sm-9 has-error"&gt; &lt;input type="email" class="form-control" id="inputEmail3" placeholder="Email"&gt; &lt;span id="helpBlock" class="help-block hide"&gt;邮箱不能为空&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="inputPassword3" class="col-sm-3 control-label"&gt;Password&lt;/label&gt; &lt;div class="col-sm-9"&gt; &lt;input type="password" class="form-control" id="inputPassword3" placeholder="Password"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-3 col-sm-9"&gt; &lt;div class="checkbox"&gt; &lt;label&gt; &lt;input type="checkbox"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;div class="col-sm-offset-3 col-sm-9"&gt; &lt;button type="submit" class="btn btn-primary btn-block"&gt;Sign in&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script src="bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; font-awesome 图标12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt; &lt;link rel="stylesheet" href="bootstrap/css/bootstrap.min.css"&gt; &lt;link rel="stylesheet" href="fontAwesome/css/font-awesome.min.css"&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="container"&gt; &lt;button class="btn btn-danger"&gt; &lt;i class="fa fa-trash-o"&gt;&lt;/i&gt; 删除 &lt;/button&gt; &lt;/div&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script src="bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫面试题 170 道]]></title>
    <url>%2F2019%2F07%2F22%2Fcase-170%2F</url>
    <content type="text"><![CDATA[语言特性谈谈对 Python 和其他语言的区别12345678Python的定位是“优雅”、“明确”、“简单”，容易入门，开发效率高，拥有非常强大的第三方库。Python属于解释型编程语言,属于动态语言,并且是强类型语言。同时具备可移植、可扩展，可嵌入等优点。拿 C 语言和 Python 比： Python 的第三方类库比较齐全并且使用简洁,很少代码就能实现一些功能，如果用 C 去实现相同的功能可能就比较复杂。但是对于速度来说 Python 的运行速度相较于 C 就比较慢了。所以有利的同时也有弊端，毕竟我们的学习成本降低了。线程不能利用多CPU问题，GIL即全局解释器锁。 简述解释型和编译型编程语言12编译型: 一次性将所有代码编译成二进制文件,运行效率高。解释型: 当程序执行时,将代码从上至下一行一行的解释成二进制，运行效率慢。 Python 的解释器种类以及相关特点？12345CPython C语言开发的，使用最广的解释器。IPython是基于CPython之上的一个交互式解释器,交互方式增强功能和 cPython 一样。PyPy,采用 JIT 技术。对 Python 代码进行动态编译，提高执行效率JPython 运行在 Java 上的解释器，直接把 Python 代码编译成 Java 字节码执行IronPython 运行在微软 .NET 平台上的解释器，把 Python 编译成 .NET 的字节码。 Python3 和 Python2 的区别？12345print 在 Python3 中是函数,必须加括号 print(&apos;hello&apos;)，Python2 中 print 为 class,print &apos;Hello, World!&apos;Python3中 input得到的为str;Python2的input的到的为int型，Python2的raw_input得到的为str类型Python2中 使用xrange，Python3 使用range。Python2 中存在老式类和新式类的区别，Python3 统一采用新式类。Python2 中默认的字符串类型默认是 ASCII，Python3 中默认的字符串类型是 Unicode。 Python3 和 Python2 中 int 和 long 区别？12python2中有long类型python3中没有long类型，只有int类型 xrange 和 range 的区别？12xrange 是在 Python2 中的用法，Python3 中只有 range。range 生成的不是一个 list 对象，而是一个生成器。 编码规范什么是 PEP8?1《Python Enhancement Proposal #8》（8 号 Python 增强提案）又叫 PEP8，他针对的 Python 代码格式而编订的风格指南。 了解 Python 之禅么？1234567891011121314151617181920212223242526272829303132333435363738通过 import this 语句可以获取其具体的内容。它告诉大家如何写出高效整洁的代码。&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly.# 优美胜于丑陋（Python以编写优美的代码为目标）Explicit is better than implicit.# 明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似）Simple is better than complex.# 简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现）Complex is better than complicated.# 复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁）Flat is better than nested.# 扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套）Sparse is better than dense.# 间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题）Readability counts.# 可读性很重要（优美的代码是可读的）Special cases aren&apos;t special enough to break the rules.Although practicality beats purity.# 即便假借特例的实用性之名，也不可违背这些规则（这些规则至高无上）Errors should never pass silently.Unless explicitly silenced.# 不要包容所有错误，除非你确定需要这样做（精准地捕获异常，不写except:pass风格的代码）In the face of ambiguity, refuse the temptation to guess.# 当存在多种可能，不要尝试去猜测There should be one-- and preferably only one --obvious way to do it# 而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法）Although that way may not be obvious at first unless you&apos;re Dutch.# 虽然这并不容易，因为你不是 Python 之父（这里的Dutch是指Guido）Now is better than never.Although never is often better than *right* now.# 做也许好过不做，但不假思索就动手还不如不做（动手之前要细思量）If the implementation is hard to explain, it&apos;s a bad idea.If the implementation is easy to explain, it may be a good idea.# 如果你无法向人描述你的方案，那肯定不是一个好方案；反之亦然（方案测评标准）Namespaces are one honking great idea -- let&apos;s do more of those!# 命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召）]]></content>
      <categories>
        <category>Python 面试题</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 在CentOS6下由 2.7 升级到 3.6]]></title>
    <url>%2F2019%2F07%2F11%2Fpython-versions%2F</url>
    <content type="text"><![CDATA[升级原因由于公司内部的服务器使用CentOS6.6，默认的python版本是Python 2.7.14,自己所学习的事Python3,所以想要升级到学习版本。我在网上看了一些文章,由于很多基本的命令、软件包都依赖旧版本，比如：yum。所以，在更新 Python 时，建议不要删除旧版本（新旧版本可以共存）。 安装配置 在服务器下载新版本 1wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz 解压缩 1tar -zxvf Python-3.6.9.tgz 安装配置 1234cd Python-3.6.9 ./configure# 如果执行报错:configure: error: no acceptable C compiler found in $PATH # 说明没有安装合适的编译器,这时，需要安装/升级 gcc 及其它依赖包: gcc yum install make gcc gcc-c++ 编译 &amp; 安装 12makemake install 验证 12345678910111213141516171819[root@localhost Python-3.6.9]# pythonPython 2.6.6 (r266:84292, Jul 23 2015, 15:22:56) [GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux2Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; [root@localhost Python-3.6.9]# python3Python 3.6.9 (default, Jul 11 2019, 09:22:29) [GCC 4.4.7 20120313 (Red Hat 4.4.7-23)] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; [root@localhost bin]# ls -l /usr/local/bin/python3*lrwxrwxrwx. 1 root root 9 Jul 11 09:24 /usr/local/bin/python3 -&gt; python3.6-rwxr-xr-x. 2 root root 10030878 Jul 11 09:23 /usr/local/bin/python3.6lrwxrwxrwx. 1 root root 17 Jul 11 09:24 /usr/local/bin/python3.6-config -&gt; python3.6m-config-rwxr-xr-x. 2 root root 10030878 Jul 11 09:23 /usr/local/bin/python3.6m-rwxr-xr-x. 1 root root 3083 Jul 11 09:24 /usr/local/bin/python3.6m-configlrwxrwxrwx. 1 root root 16 Jul 11 09:24 /usr/local/bin/python3-config -&gt; python3.6-config 设置 3.x 为默认版本 查看 Python 的路径，在 /usr/bin 下面。 可以看到 python 链接的是 python 2.7，所以，执行 python 就相当于执行 python 2.7。 1234[root@localhost bin]# ls -l /usr/bin/python*-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/pythonlrwxrwxrwx. 1 root root 6 Aug 20 2018 /usr/bin/python2 -&gt; python-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/python2.7 将原来 python 的软链接重命名： 12345678910111213141516[root@localhost bin]# mv /usr/bin/python /usr/bin/python.bak[root@localhost bin]# ls -l /usr/bin/python*lrwxrwxrwx. 1 root root 6 Aug 20 2018 /usr/bin/python2 -&gt; python-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/python2.6-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/python.bak[root@localhost bin]# ln -s /usr/local/bin/python3 /usr/bin/python[root@localhost bin]# ls -l /usr/bin/python*lrwxrwxrwx. 1 root root 22 Jul 11 09:43 /usr/bin/python -&gt; /usr/local/bin/python3lrwxrwxrwx. 1 root root 6 Aug 20 2018 /usr/bin/python2 -&gt; python-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/python2.6-rwxr-xr-x. 2 root root 9032 Jul 24 2015 /usr/bin/python.bak[root@localhost bin]# python -VPython 3.6.9 配置 yum 升级 Python 之后，由于将默认的 python 指向了 python3，yum 不能正常使用，需要编辑 yum 的配置文件：123vim /usr/bin/yumvim /usr/libexec/urlgrabber-ext-down # CentOS6.8 没找到这个文件将 #!/usr/bin/python 改为 #!/usr/bin/python2.7，保存退出即可。]]></content>
      <categories>
        <category>Linux运维</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery]]></title>
    <url>%2F2019%2F06%2F20%2Fweb-jquery%2F</url>
    <content type="text"><![CDATA[jQuery 基础jQuery 介绍 jQuery是一个轻量级的、兼容多浏览器的JavaScript库。 jQuery使用户能够更方便地处理HTML Document、Events、实现动画效果、方便地进行Ajax交互，能够极大地简化JavaScript编程。它的宗旨就是：“Write less, do more.“ jQuery 的优势 jQuery 写起来极其简练，比JS使用起来方便顺手。 jQuery 是轻量级的JS框架，jQuery 核心js文件才几十kb，不会影响页面加载速度。、 jQuery 相当于Python 的第三方模块,第三方模块就是别人写好(封装)的一些代码，我们拿过来（按照别人定好的规则），原生的JS DOM操作是基础。 链式表达式，jQuery的链式操作可以把多个操作写在一行代码里，更加简洁。 事件、样式、动画支持。jQuery还简化了js操作css的代码，并且代码的可读性也比js要强。 Ajax操作支持。jQuery简化了AJAX操作，后端只需返回一个JSON格式的字符串就能完成与前端的通信。 跨浏览器兼容。jQuery基本兼容了现在主流的浏览器，不用再为浏览器的兼容问题而伤透脑筋。 插件扩展开发。jQuery有着丰富的第三方的插件，例如：树形菜单、日期控件、图片切换插件、弹出窗口等等基本前端页面上的组件都有对应插件，并且用jQuery插件做出来的效果很炫，并且可以根据自己需要去改写和封装插件，简单实用。 jQuery 内容123456789101112131. 选择器2. 筛选器3. 样式操作4. 文本操作5. 属性操作6. 文档处理7. 事件8. 动画效果9. 插件10. each、data、Ajax官网:https://jquery.com/中文文档:http://jquery.cuishifeng.cn/ jQuery 对象 jQuery对象就是通过jQuery包装DOM对象后产生的对象。 jQuery对象是 jQuery独有的。 如果一个对象是 jQuery对象，那么它就可以使用jQuery里的方法。 声明一个jQuery对象变量的时候在变量名前面加上$ 虽然 jQuery对象是包装 DOM对象后产生的，但是 jQuery对象无法使用 DOM对象的任何方法，同理 DOM对象也没不能使用 jQuery里的方法。 引入 12// 先导入,再使用&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt; 123// jQuery 和 DOM 找到d1的方法document.getElementById("d1") // DOM对象，找到一个具体的标签$("#d1") // JQuery对象,找到一个对象 123456789101112131415161718//JQuery对象转换成DOM对象,用索引去除具体的标签$("#i1").html() // 的意思是：获取id值为 i1的元素的html代码。其中 html()是jQuery里的方法。相当于document.getElementById("i1").innerHTML;// 当已经是一个 DOM对象的标签的时候 就可以调用DOM的方法了$("#p3")[0].innerHTMLvar $variable = jQuery对像var variable = DOM对象$variable[0] //jQuery对象转成DOM对象$("#i1").html();//jQuery对象可以使用jQuery的方法$("#i1")[0].innerHTML;// DOM对象使用DOM的方法// 把DOM转换成JQuery对象var pEle = document.getElementById("p1")pEle = $(pEle)$(pEle).html() jQuery 基础语法查找标签基本选择器 id选择器 1$("#id") class选择器 1$(".className") 标签选择器 1$("tagName") 配合使用 1$("div.c1") // 找到有c1 class类的div标签 所有元素选择器 1$("*") 组合选择器 123// 逗号,分割条件$("#id, .className, tagName")$("a,.c2,.c1") // ,代表或,找出a标签或class = c2,或class = c1的所有标签 层级选择器1234$("x y"); // x的所有后代y（子子孙孙）$("x &gt; y"); // x的所有儿子y（儿子）$("x + y"); // 找到所有紧挨在x后面的y$("x ~ y"); // x之后所有的兄弟y 1234567891011// 找到本页面中form标签中的所有input标签$("form input")// 找到本页面中被包裹在label标签内的input标签$("label&gt;input")// 找到本页面中紧挨在label标签后面的input标签$("label + input"); // 找到本页面中id为p2的标签后面所有和它同级的li标签$("#p2~li") 基本筛选器：123456789:first // 第一个:last // 最后一个:eq(index) // 索引等于index的那个元素:even // 匹配所有索引值为偶数的元素，从 0 开始计数:odd // 匹配所有索引值为奇数的元素，从 0 开始计数:gt(index) // 匹配所有大于给定索引值的元素:lt(index) // 匹配所有小于给定索引值的元素:not(元素选择器) // 移除所有满足not条件的标签:has(元素选择器) // 选取所有包含一个或多个标签在其内的标签(指的是从后代元素找) 1234567891011// 第一个li标签$("li:first")// 最后一个li标签$("li:last")// not移除不满足条件的: 有c2的样式不要$("p:not(.c2)")// 找到含有a标签的div$("#d3 div:has(a)") 1234$("div:has(h1)"); // 找到所有后代中有h1标签的div标签$("div:has(.c1)"); // 找到所有后代中有c1样式类的div标签$("li:not(.c1)"); // 找到所有不包含c1样式类的li标签$("li:not(:has(a))"); // 找到所有后代中不含a标签的li标签 属性选择器123[attribute][attribute=value] // 属性等于[attribute!=value] // 属性不等于 1234567891011121314151617181920&lt;!--属性选择器--&gt;&lt;form&gt; &lt;label&gt;用户名: &lt;input name="username" type="text" disabled="disabled"&gt;&lt;/label&gt; &lt;label&gt;密码: &lt;input name="pwd" type="password"&gt;&lt;/label&gt; &lt;label&gt;篮球 &lt;input name="hobby" value="basketball" type="checkbox"&gt;&lt;/label&gt; &lt;label&gt;足球 &lt;input name="hobby" value=football type="checkbox"&gt;&lt;/label&gt; &lt;label&gt;男 &lt;input name="gender" value="1" type="radio"&gt;&lt;/label&gt; &lt;label&gt;女 &lt;input name="gender" value="0" type="radio"&gt;&lt;/label&gt;&lt;/form&gt; 12$("input[name='hobby']")$("input[name='gender']") 表单筛选器123456789:text:password:file:radio:checkbox:submit:reset:button 123// 找到所有的text$("input[type='text']")$(":text") 表单对象属性 1234:enabled:disabled:checked:selected 12$(":disabled")$(":checked") 筛选器 下一个元素 123456789$("#id").next()$("#id").nextAll()$("#id").nextUntil("#i2") // ``` ** 上一个元素 **```javascript$("#id").prev()$("#id").prevAll()$("#id").prevUntil("#i2") 父亲元素 123$("#id").parent()$("#id").parents() // 查找当前元素的所有的父辈元素$("#id").parentsUntil() // 查找当前元素的所有的父辈元素，直到遇到匹配的那个元素为止。 儿子和兄弟元素 12$("#id").children();// 儿子们$("#id").siblings();// 兄弟们 1234567$("#l3")$("#l3").prev()$("#l3").next()$("#l3").prevAll()$("#l3").nextAll()$("#l0").nextUntil("#l3")$("#l3").prevUntil("#l0") 12345678910链式操作:$("a")$("a").parent()$("a").parent().parent()$("a").parents()$("a").parents("body")$("#dd").children()$("#p2").siblings() 查找标签 123// 搜索所有与指定表达式匹配的元素。这个函数是找出正在处理的元素的后代元素的好方法。$("div").find("p")// 等价于$("div p") 12var $c1Ele = $(".c1") // 有现成变量$c1Ele.find("div") 1234// 筛选// 筛选出与指定表达式匹配的元素集合。这个方法用于缩小匹配的范围。用逗号分隔多个表达式。$("div").filter(".c1") // 从结果集中过滤出有c1样式类的// 等价于 $("div.c1") 12345.first() // 获取匹配的第一个元素.last() // 获取匹配的最后一个元素.not() // 从匹配元素的集合中删除与指定表达式匹配的元素.has() // 保留包含特定后代的元素，去掉那些不含有指定后代的元素。.eq() // 索引值等于指定值的元素 12$("div").first()$("div").last() 筛选器方法总结1234567891011121314151 .next() # 找到挨着的下一个同级标签2 .nextAll(&quot;.c1&quot;) # 下边同级的所有 有c1这个class的3 .nextUntil() # 往下找，直到找到终止条件为止4 .prev()5 .prevAll()6 .prevUntil()7 .siblings() # 前后都能找到8 .children()9 .parent()10 .parents() # 一级一级的父标签，最后html11 .parentsUntil()12 .find(各种条件都可以写) 操作标签样式操作1234addClass(); // 添加指定的CSS类名。removeClass(); // 移除指定的CSS类名。hasClass(); // 判断样式存不存在toggleClass(); // 切换CSS类名，如果有就移除，如果没有就添加。 1234567891011121314151617181920212223242526272829303132&lt;!-- 开关灯--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; .c1&#123; height:200px; width: 200px; border-radius: 50%; background-color: red; &#125; .c2&#123; background-color: green; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="c1"&gt;&lt;/div&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 找标签 $("div.c1").click(function () &#123; console.log(this); // this 是DOM 对象 $(this).toggleClass("c2") // 转换成jQuery对象,使用jQuery方法 &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;!--自定义模态框--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; .cover&#123; position: absolute; top: 0; right: 0; left: 0; bottom: 0; background-color: rgba(0,0,0,0.4); z-index: 998; &#125; .modal&#123; height: 400px; width: 600px; background-color: white; position: absolute; top: 50%; left: 50%; /*宽度的一半*/ margin-left: -300px; /*高度的一半*/ margin-top: -200px; z-index: 1000; &#125; .hide&#123; display: none; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button id="b1"&gt;点我点我&lt;/button&gt;&lt;div class="cover hide"&gt;&lt;/div&gt;&lt;div class="modal hide"&gt; &lt;form action=""&gt; &lt;p&gt; &lt;label for=""&gt;用户名: &lt;input type="text"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;密码: &lt;input type="password"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;登录: &lt;input type="submit"&gt; &lt;input id="cancel" type="button" value="取消"&gt; &lt;/label&gt; &lt;/p&gt; &lt;/form&gt;&lt;/div&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 找到点击弹出模态框按钮 $("#b1").click(function () &#123; $(".cover").removeClass("hide"); // 显示背景 $(".modal").removeClass("hide"); // 显示模态框 &#125;) // 找到取消按钮,绑定事件 $("#cancel").click(function () &#123; // 给背景和模态框 都加上hide $(".cover").addClass("hide"); // 显示背景 $(".modal").addClass("hide"); // 显示模态框 &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 直接修改样式12css("color","red") // DOM操作：tag.style.color="red"原生DOM .style.color = 'green' 1$("div").css("color","greeen"); 12345// 修改多个样式时，传入键值对$(this).css(&#123; "color":"pink", "font-size":"24px"&#125;) 1234567891011121314151617181920212223242526&lt;!--修改样式--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;AAA&lt;/p&gt;&lt;p&gt;BBB&lt;/p&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 把当前点击的标签 变绿 // 在处理事件的函数中 用this 表示 当前触发事件的标签 $("p").click( function () &#123;// $(this).css("color","red")// $(this).css("font-size","24px") $(this).css(&#123; "color":"pink", "font-size":"24px" &#125;) &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 位置操作1234offset() // 获取匹配元素在当前窗口的相对偏移或设置元素位置position() // 获取匹配元素相对父元素的偏移scrollTop() // 获取匹配元素相对滚动条顶部的偏移scrollLeft() // 获取匹配元素相对滚动条左侧的偏移 123456789101112131415161718192021222324# 获取位置$(".c1").offset()&#123;top: 0, left: 0&#125;# 设置位置$(".c1").offset(&#123;top:"100",left:100&#125;)$(".c3").offset() # 对左上角定位&#123;top: 400, left: 300&#125;$(".c3").position()&#123;top: 100, left: 100&#125;# 尺寸# 内容区 高和宽height()width()# 内容区 + paddinginnerHeight()innerWidth()# 内容区 + padding + borderouterHeight()outerWidth() 1234567891011121314151617181920&lt;!-- 返回顶部 --&gt;&lt;button id="b2" class="btn btn-default c2 hide"&gt;返回顶部&lt;/button&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; $("#b1").on("click", function () &#123; $(".c1").offset(&#123;left: 200, top:200&#125;); &#125;); $(window).scroll(function () &#123; if ($(window).scrollTop() &gt; 100) &#123; $("#b2").removeClass("hide"); &#125;else &#123; $("#b2").addClass("hide"); &#125; &#125;); $("#b2").on("click", function () &#123; $(window).scrollTop(0); &#125;)&lt;/script&gt; 文本操作123456789101112&lt;!--HTML代码：--&gt; html()// 取得第一个匹配元素的html内容 html(val)// 设置所有匹配元素的html内容&lt;!--文本值：--&gt; text()// 取得所有匹配元素的内容 text(val)// 设置所有匹配元素的内容&lt;!--值：--&gt; val()// 取得第一个匹配元素的当前值 val(val)// 设置所有匹配元素的值 val([val1, val2])// 设置多选的checkbox、多选select的值 12345678910111213141516171819202122232425262728293031323334$("#d1")[0].innerHTML$("#d1").html()$("#d1")[0].innerText$("#d1").text()# 都会修改整个标签,主要使用html修饰标签,可以支持html格式的字符串文本$("#d1").text("呵呵")$("#d1").html("&lt;a href='www.baidu.com'&gt;go&lt;/a&gt;")$(":text").val()$(":password").val()val() 永远只取第一个值，设置值的话就是全部设置// 获取多个值 需要使用循环var $checkedEles = $(":checkbox:checked");for (var i=0;i&lt;$checkedEles.length;i++)&#123; console.log($checkedEles[i]) console.log($($checkedEles[i]).val())&#125;// radio$(":radio")$(":radio:checked").val()// select$("#s1").val()// 多选$("#s2").val()(2) ["021", "020"]$("#s2").val(["021","020"]) //传值可以设置值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&lt;!--文本操作--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; .error&#123; color: red; &#125; /*.error:before&#123;*/ /*content: "*";*/ /*&#125;*/ &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="d1"&gt; &lt;p&gt; &lt;span&gt;span&lt;/span&gt; &lt;div&gt;div&lt;/div&gt; &lt;/p&gt;&lt;/div&gt; &lt;form action="" id="f1"&gt; &lt;p&gt; &lt;label for=""&gt;用户名: &lt;input class="need" name="username" type="text"&gt; &lt;span class="error"&gt;&lt;/span&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;密码: &lt;input class="need" name="password" type="password"&gt; &lt;span class="error"&gt;&lt;/span&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt;爱好: &lt;label for=""&gt;篮球: &lt;input name="hobby" type="checkbox" value="basketball"&gt; &lt;/label&gt; &lt;label for=""&gt;足球: &lt;input name="hobby" type="checkbox" value="football"&gt; &lt;/label&gt; &lt;label for=""&gt;双色球: &lt;input name="hobby" type="checkbox" value="doubleball"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt;性别: &lt;label for=""&gt;男: &lt;input name="gender" type="radio" value="1"&gt; &lt;/label&gt; &lt;label for=""&gt;女: &lt;input name="gender" type="radio" value="0"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="s1"&gt;从哪来&lt;/label&gt; &lt;select name="from" id="s1"&gt; &lt;option value="010"&gt;北京&lt;/option&gt; &lt;option value="021"&gt;上海&lt;/option&gt; &lt;option value="020"&gt;广州&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="s2"&gt;到哪去&lt;/label&gt; &lt;select name="from" id="s2" multiple&gt; &lt;option value="010"&gt;北京&lt;/option&gt; &lt;option value="021"&gt;上海&lt;/option&gt; &lt;option value="020"&gt;广州&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="t1"&gt;个人简介&lt;/label&gt; &lt;textarea name="memo" id="t1" cols="30" rows="10"&gt;&lt;/textarea&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;登录: &lt;input type="submit" id="b1"&gt; &lt;input id="cancel" type="button" value="取消"&gt; &lt;/label&gt; &lt;/p&gt; &lt;/form&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 点击登录按钮 用户名和密码 是否为空 // 为空就在对应input标签下面显示一个 错误信息 // 1. 给登录按钮绑定点击事件 // 2. 点击事件要做的事 // 1. 找到input标签取值,判断是否为空（长度是否为0,length是否=0） // 2. 如果不为空，则什么都不做 // 3. 如果为空，就要： // 1. 当前这个input标签的下面，添加一个新的标签，内容为 XX 不能为空 // 2. class="need" 可以为空 $("#b1").click(function () &#123; var $needEles = $(".need"); console.log($needEles); for (var i = 0;i &lt; $needEles.length;i++)&#123; if ($($needEles[i]).val().trim().length === 0)&#123; var labelName = $($needEles[i]).parent().text().trim().slice(0,-1) $($needEles[i]).next().text(labelName + "不能为空") &#125; &#125; return false &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 属性操作123456用于ID等或自定义属性：attr(attrName)// 返回第一个匹配元素的属性值attr(attrName, attrValue)// 为所有匹配元素设置一个属性值attr(&#123;k1: v1, k2:v2&#125;)// 为所有匹配元素设置多个属性值removeAttr()// 从每一个匹配的元素中删除一个属性 12345.attr() // $("a").attr("href") // $("a").attr("href","设置值") // $("a").attr(&#123;"href":"设置值"."title":"设置值"&#125;).prop() // 适用于checkbox 和 radio(返回true或false属性) 123456789101112$("img")r.fn.init [img, prevObject: r.fn.init(1)]$("img").attr("src")"https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=7b36b75d25f5e0feee188e07645b5395/a8014c086e061d9581528f7c73f40ad162d9ca5a.jpg"$("img").attr("src","https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562835484&amp;di=8a585e9fe3abf1cf8a85aa910c74f975&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180907%2F6efd963b18e5404dadb7b314576dc530.jpeg")r.fn.init [img, prevObject: r.fn.init(1)]$("img").attr(&#123;"src":"https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562835484&amp;di=8a585e9fe3abf1cf8a85aa910c74f975&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180907%2F6efd963b18e5404dadb7b314576dc530.jpeg","title":"jojo"&#125;)r.fn.init [img, prevObject: r.fn.init(1)]$("img").removeAttr("title") 1234567891011121314151617181920212223242526&lt;!--属性操作--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=7b36b75d25f5e0feee188e07645b5395/a8014c086e061d9581528f7c73f40ad162d9ca5a.jpg" alt=""&gt;&lt;input type="button" id="b1" value="下一个"&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; var oldUrl; var newUrl = "https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562835484&amp;di=8a585e9fe3abf1cf8a85aa910c74f975&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180907%2F6efd963b18e5404dadb7b314576dc530.jpeg" $("#b1").click(function () &#123; var $imgEles = $("img") // 修改img标签的src属性 oldUrl = $imgEles.attr("src") $imgEles.attr("src",newUrl) newUrl = oldUrl &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择按钮操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border="1"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;职位&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="checkbox"&gt;&lt;/td&gt; &lt;td&gt;jojo&lt;/td&gt; &lt;td&gt;黄金之风&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="checkbox"&gt;&lt;/td&gt; &lt;td&gt;bobo&lt;/td&gt; &lt;td&gt;钢链手指&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="checkbox"&gt;&lt;/td&gt; &lt;td&gt;东方仗助&lt;/td&gt; &lt;td&gt;疯狂钻石&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;input type="button" id="b1" value="全选"&gt;&lt;input type="button" id="b2" value="反选"&gt;&lt;input type="button" id="b3" value="取消"&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 点击全选 表格中所有的checkbox都选中 // 1. 找checkbox // 2. 全部选中 checked : ture $("#b1").click(function () &#123; $(":checkbox").prop("checked",true) &#125;) // 点击取消 // 点击全选 表格中所有的checkbox都选中 // 1. 找checkbox // 2. 取消选中 checked : false $("#b3").click(function () &#123; $(":checkbox").prop("checked",false) &#125;) // 反选 // 1. 找到所有checkbox // 2. 判断 ： // 2.1 没有选中的要选中 // 2.2 原来选中的要取消选中 $("#b2").click(function () &#123; // 把所有的checkbox 都存在$checkboxEles中 var $checkboxEles = $(":checkbox"); // 遍历所有的checkbox,根据每一个checkbox的选种状态做不通的操作 for (var i=0;i&lt;$checkboxEles.length;i++)&#123; // 把每一个checkbox包装成jQuery对象 var $tmp = $($checkboxEles[i]) // 如果 checkbox是选中的，我们就取消选中 if ($tmp.prop("checked"))&#123; $tmp.prop("checked",false) &#125;else &#123; // 否则不选中 $tmp.prop("checked",true) &#125; &#125; &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 文档处理12345678910111213141516171819202122232425262728293031323334var liEle = document.createElement("li")var liEle = document.createElement("li")liEle.innerText=0liEle&lt;li&gt;​0​&lt;/li&gt;​$("#u1").append(liEle) // 在最后添加r.fn.init [ul#u1]$("#u1").prepend(liEle) // 同一个dom对象 liElevar liEle = document.createElement("li")liEle.innerText=44$(liEle).appendTo("#u1")$("#l1")r.fn.init [li#l1]var liEle = document.createElement("li")liEle.innerText=1.5$("#l1").after(liEle)r.fn.init [li#l1]移除和清空元素remove()// 从DOM中删除所有匹配的元素。empty()// 删除匹配的元素集合中所有的子节点。$("#u1")$("#u1").remove() // 直接把ul标签急内部子标签都删除$("#u1").empty() // 删除ul内部标签 增加表格记录12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border="1" id="t1"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;替身&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Leo&lt;/td&gt; &lt;td&gt;黄金之风&lt;/td&gt; &lt;td&gt;&lt;button class="delete"&gt;删除&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;Lex&lt;/td&gt; &lt;td&gt;疯狂钻石&lt;/td&gt; &lt;td&gt;&lt;button class="delete"&gt;删除&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;button id="b1"&gt;添加一行数据&lt;/button&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 绑定事件 // 生成添加的tr标签和数据 // 把生成的tr插入到表格 $("#b1").click(function () &#123; var trEle = document.createElement("tr"); $(trEle).html("&lt;td&gt;3&lt;/td&gt;&lt;td&gt;rubin&lt;/td&gt;" + "&lt;td&gt;钢链手指&lt;/td&gt;&lt;td&gt;&lt;button class=\"delete\"&gt;删除&lt;/button&gt;&lt;/td&gt;"); $("#t1").find("tbody").append(trEle); &#125;) // 每一行的删除按钮绑定事件 $(".delete").click(function () &#123; $(this).parent().parent().remove() // 找到他的tr .remove &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 替换1234567891011121314151617181920212223242526&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;&lt;a href="www.baidu.com"&gt;aaa&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=""&gt;bbb&lt;/a&gt;&lt;/p&gt;&lt;button id="b1"&gt;替换&lt;/button&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; $("#b1").click(function () &#123; var imgEle = document.createElement("img") $(imgEle).attr("src","https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1562666099791&amp;di=f40cd71f3f9b66125fb72217ac77e1b5&amp;imgtype=0&amp;src=http%3A%2F%2Fimg2.178.com%2Facg1%2F201204%2F128126936227%2F128127453027.jpg") $("a").replaceWith(imgEle) $(imgEle).replaceAll("a") // 替换所有的a标签 &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 克隆12345678910111213141516171819&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button id="b1"&gt;克隆&lt;/button&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; $("#b1").click(function () &#123; $(this).clone(true).insertAfter(this) // true 代表连标签的事件也一起clone &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 事件常用事件123456click(function()&#123;...&#125;)hover(function()&#123;...&#125;)blur(function()&#123;...&#125;)focus(function()&#123;...&#125;)change(function()&#123;...&#125;)keyup(function()&#123;...&#125;) 事件绑定1234.on( events [, selector ],function()&#123;&#125;)events： 事件selector: 选择器（可选的）function: 事件处理函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; .error&#123; color: red; &#125; /*.error:before&#123;*/ /*content: "*";*/ /*&#125;*/ &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="" id="f1"&gt; &lt;p&gt; &lt;label for=""&gt;用户名: &lt;input class="need" name="username" type="text"&gt; &lt;span class="error"&gt;&lt;/span&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;密码: &lt;input class="need" name="password" type="password"&gt; &lt;span class="error"&gt;&lt;/span&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt;爱好: &lt;label for=""&gt;篮球: &lt;input name="hobby" type="checkbox" value="basketball"&gt; &lt;/label&gt; &lt;label for=""&gt;足球: &lt;input name="hobby" type="checkbox" value="football"&gt; &lt;/label&gt; &lt;label for=""&gt;双色球: &lt;input name="hobby" type="checkbox" value="doubleball"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt;性别: &lt;label for=""&gt;男: &lt;input name="gender" type="radio" value="1"&gt; &lt;/label&gt; &lt;label for=""&gt;女: &lt;input name="gender" type="radio" value="0"&gt; &lt;/label&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="s1"&gt;从哪来&lt;/label&gt; &lt;select name="from" id="s1"&gt; &lt;option value="010"&gt;北京&lt;/option&gt; &lt;option value="021"&gt;上海&lt;/option&gt; &lt;option value="020"&gt;广州&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="s2"&gt;到哪去&lt;/label&gt; &lt;select name="from" id="s2" multiple&gt; &lt;option value="010"&gt;北京&lt;/option&gt; &lt;option value="021"&gt;上海&lt;/option&gt; &lt;option value="020"&gt;广州&lt;/option&gt; &lt;/select&gt; &lt;/p&gt; &lt;p&gt; &lt;label for="t1"&gt;个人简介&lt;/label&gt; &lt;textarea name="memo" id="t1" cols="30" rows="10"&gt;&lt;/textarea&gt; &lt;/p&gt; &lt;p&gt; &lt;label for=""&gt;登录: &lt;input type="submit" id="b1"&gt; &lt;input id="cancel" type="button" value="取消"&gt; &lt;/label&gt; &lt;/p&gt; &lt;/form&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 点击登录按钮 用户名和密码 是否为空 // 为空就在对应input标签下面显示一个 错误信息 // 1. 给登录按钮绑定点击事件 // 2. 点击事件要做的事 // 1. 找到input标签取值,判断是否为空（长度是否为0,length是否=0） // 2. 如果不为空，则什么都不做 // 3. 如果为空，就要： // 1. 当前这个input标签的下面，添加一个新的标签，内容为 XX 不能为空 // 2. class="need" 可以为空 $("#b1").click(function () &#123; var $needEles = $(".need"); // 定义一个标志位: var flag = true; console.log($needEles); for (var i = 0;i &lt; $needEles.length;i++)&#123; // 如果有错误 if ($($needEles[i]).val().trim().length === 0)&#123; var labelName = $($needEles[i]).parent().text().trim().slice(0,-1) $($needEles[i]).next().text(labelName + "不能为空") // 将标志位 变成false flag = false break; &#125; &#125; return false &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!--使用on绑定事件--&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border="1" id="t1"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;替身&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;Leo&lt;/td&gt; &lt;td&gt;黄金之风&lt;/td&gt; &lt;td&gt;&lt;button class="delete"&gt;删除&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;Lex&lt;/td&gt; &lt;td&gt;疯狂钻石&lt;/td&gt; &lt;td&gt;&lt;button class="delete"&gt;删除&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;button id="b1"&gt;添加一行数据&lt;/button&gt;&lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 绑定事件 // 生成添加的tr标签和数据 // 把生成的tr插入到表格 $("#b1").on("click",function () &#123; var trEle = document.createElement("tr"); $(trEle).html("&lt;td&gt;3&lt;/td&gt;&lt;td&gt;rubin&lt;/td&gt;" + "&lt;td&gt;钢链手指&lt;/td&gt;&lt;td&gt;&lt;button class=\"delete\"&gt;删除&lt;/button&gt;&lt;/td&gt;"); $("#t1").find("tbody").append(trEle); &#125;) // 每一行的删除按钮绑定事件 $("tbody").on("click",".delete",function () &#123;// console.log(this) &lt;button class="delete"&gt;删除&lt;/button&gt; $(this).parent().parent().remove() // 找到他的tr .remove &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 页面载入12345678910111213141516171819202122&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src="jquery-3.2.1.min.js"&gt;&lt;/script&gt; &lt;script&gt; // 确保绑定事件的时候DOM树是生成好 // 等DOM树生成之后，我再执行 $(document).ready(function () &#123; console.log($("#d1").text()) // 执行绑定事件的操作 &#125;) &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="d1"&gt;div&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BOM 和 DOM 对象]]></title>
    <url>%2F2019%2F06%2F12%2Fweb-object%2F</url>
    <content type="text"><![CDATA[基础知识 JavaScript分为 ECMAScript，DOM，BOM。 BOM （Browser Object Model）是指浏览器对象模型，它使 JavaScript 有能力与浏览器进行“对话”。 DOM （Document Object Model）是指文档对象模型，通过它，可以访问HTML文档的所有元素(标签)。 BOM 对象BOM（Browser Object Mode）浏览器对象模型，是Javascript的重要组成部分。它提供了一系列对象用于与浏览器窗口进行交互，这些对象通常统称为BOM。 window 对象 – BOM核心1、它表示整个浏览器窗口，主要用来操作浏览器窗口2、所有 JavaScript 全局对象、函数以及变量均自动成为 window 对象的成员。3、全局变量是 window 对象的属性。全局函数是 window 对象的方法。4、Window对象是客户端JavaScript最高层对象之一，由于window对象是其它大部分对象的共同祖先，在调用window对象的方法和属性时，可以省略window对象的引用。例如：window.document.write()可以简写成：document.write()。 所有浏览器都支持 window 对象。 概念上讲.一个html文档对应一个window对象。 功能上讲: 控制浏览器窗口的。 使用上讲: window对象不需要创建对象,直接使用即可。 12345// 一些常用的Window方法：window.innerHeight - 浏览器窗口的内部高度window.innerWidth - 浏览器窗口的内部宽度window.open() - 打开新窗口window.close() - 关闭当前窗口 window 的子对象navigator对象（了解即可）12345// 浏览器对象，通过这个对象可以判定用户所使用的浏览器，包含了浏览器相关信息。navigator.appName // Web浏览器全称navigator.appVersion // Web浏览器厂商和版本的详细字符串navigator.userAgent // 客户端绝大部分信息navigator.platform // 浏览器运行所在的操作系统 screen对象（了解即可）123// 屏幕对象，不常用。screen.availWidth - 可用的屏幕宽度screen.availHeight - 可用的屏幕高度 history对象（了解即可）123// 浏览历史对象，包含了用户对当前页面的浏览历史，但我们无法查看具体的地址，可以简单的用来前进或后退一个页面。history.forward() history.back() location对象12345678910// 用于获得当前页面的地址 (URL)，并把浏览器重定向到新的页面。location.herf = 'url地址'location.hostname // 返回 web 主机的域名location.pathname // 返回当前页面的路径和文件名location.port // 返回 web 主机的端口 （80 或 443）location.portocol // 返回页面使用的web协议。 http:或https:location.href // 获取当前的URLlocation.href = 'http://www.baidu.com' // 跳转到指定的URLlocation.reload() // 重新加载当前页面 弹出框1234// JavaScript 中创建三种消息框：警告框、确认框、提示框。alert('提示信息'); // 警告框confirm("确认信息"); // 确认框prompt("请在下方输入","你的答案"); // 提示框 定时器 setTimeout() 和 clearTimeout() 在指定时间之后执行一次相应函数。 setInterval() 和 clearInterval() 在指定的周期（以毫秒计）来调用函数或计算表达式。 123456789101112// 一定时间间隔之后执行// 语法: var t=setTimeout("JS语句",毫秒)var t = setTimeout("alert('发送成功')",3000)// 函数定时执行function func()&#123; alert('发送成功')&#125;var t = setTimeout(func,3000)// 取消setTimeout设置clearTimeout(t) 123456789// 每隔多少秒执行一次// 语法：var t = setInterval("JS语句",时间间隔)function foo()&#123; console.log(1)&#125;;var t = setInterval(foo,1000)// 取消setInterval设置clearInterval(t); DOM 对象 DOM（Document Object Model）是一套对文档的内容进行抽象和概念化的方法。 当网页被加载时，浏览器会创建页面的文档对象模型（Document Object Model）。 HTML DOM 模型被构造为对象的树。 HTML DOM 树 dom树是为了展示文档中各个对象之间的关系，用于对象的导航。 DOM都有哪一些内容 HTML 文档中的每个成分都是一个节点。 DOM标准规定HTML文档中的每个成分都是一个节点(node)： 文档节点(document对象)：代表整个文档 元素节点(element 对象)：代表一个元素（标签） 文本节点(text对象)：代表元素（标签）中的文本 属性节点(attribute对象)：代表一个属性，元素（标签）才有属性 注释是注释节点(comment对象) JS 操作DOM JavaScript 可以通过DOM创建动态的 HTML JavaScript 能够改变页面中的所有 HTML 元素 JavaScript 能够改变页面中的所有 HTML 属性 JavaScript 能够改变页面中的所有 CSS 样式 JavaScript 能够对页面中的所有事件做出反应 查找标签直接查找12// 通过id查找标签document.getElementById("d1") 12// 根据class属性获取所有标签document.getElementsByClassName("p1") 12// 根据标签名获取标签合集document.getElementsByTagName("div") 间接查找123456parentElement // 父节点标签元素children // 所有子标签firstElementChild // 第一个子标签元素lastElementChild // 最后一个子标签元素nextElementSibling // 下一个兄弟标签元素previousElementSibling // 上一个兄弟标签元素 1234567// 查找父节点// 子节点.parentElementvar d3Ele = document.getElementById("d3")d3Ele.parentElement// &lt;div id=​"d2"&gt;​…​&lt;/div&gt;​d3Ele.parentElement.parentElement// &lt;body&gt;​…​&lt;/body&gt;​ 12345// 查找所有子标签// 父节点.childrenvar d2Ele = document.getElementById("d2")d2Ele.childrend2Ele.childNodes 123456// 第一个子标签元素 和 最后一个var d2Ele = document.getElementById("d2")d2Ele.firstElementChild // &lt;div id=​"d3"&gt;​d2里面的d3​&lt;/div&gt;​d2Ele.lastElementChild// &lt;div id=​"d5"&gt;​d2里面的d5​&lt;/div&gt;​ 123456// 下一个兄弟标签元素 和 上一个var d4Ele = document.getElementById("d4")d4Ele.previousElementSibling// &lt;div id=​"d3"&gt;​d2里面的d3​&lt;/div&gt;​d4Ele.nextElementSibling// &lt;div id=​"d5"&gt;​d2里面的d5​&lt;/div&gt;​ 节点操作创建节点12// 语法：createElement(标签名)var divEle = document.createElement("div"); 添加节点1234// 追加一个子节点（作为最后的子节点）// somenode.appendChild(newnode)；// 把增加的节点放到某个节点的前边。// somenode.insertBefore(newnode,某个节点); 1234567var imgEle = document.createElement("img") // 创建img节点// 给igm节点添加一个src属性imgEle.src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1560341306312&amp;di=67d00a7580c18d13000ced0c9df87e35&amp;imgtype=0&amp;src=http%3A%2F%2Fpic.rmb.bdstatic.com%2Fd487c4c496b952e7463a9ac2a041643a.jpeg"// 添加img节点到d1标签中var d1Ele = document.getElementById("d1")d1Ele.appendChild(imgEle) // 把创建好的imgEle添加到id=d1的标签内部 123456789// 在某个标签前面添加 insertBefore(要插入的新标签，哪个标签之前)var divEle = document.createElement("div") // 创建新的div标签divEle.innerText = "大家好" // 添加文本"大家好"var d4Ele = document.getElementById("d4") // 找到要加入的父标签var d2Ele = document.getElementById("d2") // 找到要加入到前面的子标签d2.insertBefore(divEle,d4Ele) // 在d4标签之前加入新div标签&lt;div&gt;​大家好​&lt;/div&gt;​ 删除节点1234567// 语法:somenode.removeChild(要删除的节点)// 获得要删除的元素，通过父元素调用该方法删除。var d2Ele = document.getElementById("d2")d2Ele.firstElementChild// &lt;div id=​"d3"&gt;​d2里面的d3​&lt;/div&gt;​var sonEle = d2Ele.firstElementChild;d2Ele.removeChild(sonEle) 替换节点123456789// 语法:somenode.replaceChild(newnode, 某个节点);var d2Ele = document.getElementById("d2")var sonEle = d2Ele.firstElementChild;var aEle = document.createElement("a")aEle.innerText="点我""点我"aEle.href="http://www.baidu.com""http://www.baidu.com"d2Ele.replaceChild(aEle,sonEle) 属性操作获取属性123// 获取元素节点中指定属性的属性值var d2Ele = document.getElementById("d2")d2Ele.getAttribute("id") 设置属性123456789// 默认属性都可以修改 d2Ele.setAttribute("age",18)d2.getAttribute("age")"18"// 自带的属性还可以直接.属性名来获取和设置imgEle.src="..."aEle.innerText="点我"aEle.href="http://www.baidu.com" 删除属性123d2.removeAttribute("age")d2.getAttribute("age")null 文本操作 innerHTML 和 innerText的区别: * innerHTML:子标签和子标签的内容都取出来,主要记这个:快速添加简单的标签 * innerText:只取标签之间的内容 获取标签文本12345678910// 获取标签内的文本信息var d2Ele = document.getElementById("d2")d2Ele.innerText// "d2里面的d3// d2里面的d4// d2里面的d5"var d4Ele = document.getElementById("d4")d4Ele.innerText// "d2里面的d4" 替换标签文本1d2Ele.innerText = "全部修改" // 修改文本内容 连子标签都没了,只剩下了文本 获取标签和文本12345678// 获取d2下面所有html内容,包括标签和文本var d2Ele = document.getElementById("d2")d2Ele.innerHTML//"// &lt;div id="d3"&gt;d2里面的d3&lt;/div&gt;// &lt;div id="d4"&gt;d2里面的d4&lt;/div&gt;// &lt;div id="d5"&gt;d2里面的d5&lt;/div&gt;//" 快速添加简单的标签12// 原先的标签会被替换d2Ele.innerHTML = "&lt;p&gt;我是新的p标签&lt;/p&gt;" 获取值操作123// 语法：elementNode.value// 适用于以下标签：// input、select、textarea 1234567891011121314151617181920var textEle = document.getElementById("i1")textEle.value"leo"var selectEle = document.getElementById("s1")selectEle.value"010"var tEle = document.getElementById("t1")tEle.value"哈哈哈"``` ### class 操作```javascriptclassName // 获取所有样式类名(字符串)classList.remove(cls) // 删除指定类classList.add(cls) // 添加类classList.contains(cls) // 存在返回true，否则返回falseclassList.toggle(cls) // 存在就删除，否则添加 1&lt;div class="c1 c2 c3"&gt;div&lt;/div&gt; 1234567891011121314151617181920212223242526272829var divEles = document.getElementsByTagName("div")divEles[0].className"c1 c2 c3"// 删除其中一个class样式 c3divEles[0].classList.remove("c3")divEles[0].className"c1 c2"// 添加一个class样式 c99divEles[0].classList.add("c99")divEles[0].className"c1 c2 c99"// 测试是否存在c3 和 c99 divEles[0].classList.contains("c3")falsedivEles[0].classList.contains("c99")true// 测试存在删除,不存在则添加divEles[0].classList.toggle("c3")true divEles[0].className"c1 c2 c99 c3" divEles[0].classList.toggle("c3")false divEles[0].className"c1 c2 c99" 123456789101112131415161718192021// 切换颜色实例&lt;style&gt; .c1&#123; height: 200px; width: 200px; border-radius: 50%; background-color: gray; &#125; .c2 &#123; background-color: yellow; &#125;&lt;/style&gt;&lt;div class="c1 c2 c3" onclick="change(this);"&gt;div&lt;/div&gt;&lt;script&gt; function change(ths) &#123; ths.classList.toggle("c2") // 点一下 有c2就去掉,没c2就加上，相当于来回切换 &#125;&lt;/script&gt; 指定CSS操作123// dom 直接修改样式var divEles = document.getElementsByTagName("div")divEles[0].style.backgroundColor="blue" // 原先带_改成了驼峰 JS操作CSS属性的规律： 123456// 1.对于没有中横线的CSS属性一般直接使用style.属性名即可。如：obj.style.marginobj.style.widthobj.style.leftobj.style.position 123456// 2.对含有中横线的CSS属性，将中横线后面的第一个字母换成大写即可。如：obj.style.marginTopobj.style.borderLeftWidthobj.style.zIndexobj.style.fontFamily 事件常用事件123456789101112131415161718onclick //当用户点击某个对象时调用的事件句柄。ondblclick //当用户双击某个对象时调用的事件句柄。onfocus //元素获得焦点。 练习：输入框onblur //元素失去焦点。 应用场景：用于表单验证,用户离开某个输入框时,代表已经输入完了,我们可以对它进行验证.onchange //域的内容被改变。 应用场景：通常用于表单元素,当元素内容被改变时触发.（select联动）onkeydown //某个键盘按键被按下。 应用场景: 当用户在最后一个输入框按下回车按键时,表单提交.onkeypress //某个键盘按键被按下并松开。onkeyup //某个键盘按键被松开。onload //一张页面或一幅图像完成加载。onmousedown //鼠标按钮被按下。onmousemove //鼠标被移动。onmouseout //鼠标从某元素移开。onmouseover //鼠标移到某元素之上。onselect //在文本框中的文本被选中时发生。onsubmit //确认按钮被点击，使用的对象是form。 绑定方式123456789101112131415161718192021&lt;div class="c1 c2 c3" onclick="change(this);"&gt;div&lt;/div&gt;&lt;div class="c1 c2 c3"&gt;div&lt;/div&gt;&lt;div class="c1 c2 c3"&gt;div&lt;/div&gt;&lt;div class="c1 c2 c3"&gt;div&lt;/div&gt;&lt;script&gt; // 第一种方式 在标签里调用onclick，然后写函数方法 function change(ths) &#123; ths.classList.toggle("c2") // 点一下 有c2就去掉,没c2就加上，相当于来回切换 &#125; // 第二种方式 js代码绑定事件 找所有的div，循环每一个去绑定一个事件 var divEles = document.getElementsByTagName("div") // 获得div的数组 for (var i = 0;i&lt;divEles.length;i++)&#123; divEles[i].onclick = function ()&#123; this.classList.toggle("c2") // 哪个标签被点击,this就是谁 &#125; &#125;&lt;/script&gt; 事件示例定时器练习1234567891011121314151617181920212223242526272829303132333435363738394041&lt;input id="i1" type="text"&gt;&lt;input type="button" id = "start" value="开始"&gt;&lt;input type="button" id = "stop" value="停止"&gt;&lt;script&gt; // 声明全局的t，保存定时器的ID var t; // undefind // 在input框 显示当前时间 // 1.获取当前时间 function get_time() &#123; var now = new Date(); var nowStr = now.toLocaleString(); // 2.时间字符串填到input框 var i1Ele = document.getElementById("i1"); i1Ele.value = nowStr &#125; // 3. 点开始让时间动起来 每隔一秒钟就执行一次获取时间函数 get_time() // 找到开始按钮，并绑定事件 var startbutton = document.getElementById("start"); startbutton.onclick = function () &#123; // 先执行一次get_time，就不会有空隙 get_time() // 每隔一秒执行 定时器 // 如果不等于undefined 说明已经存在定时器 if (t === undefined)&#123; t = setInterval(get_time,1000); // 把定时器的ID 赋值给之前声明的全局变量t &#125; &#125; var stopbutton = document.getElementById("stop"); stopbutton.onclick = function () &#123; // 停止定时器 clearInterval(t) // 清除T对应的定时器,t的值还在 console.log(t); t = undefined // 初始化值 &#125;&lt;/script&gt; 搜索框1234567891011121314151617181920&lt;!--test框的默认值，在点击test框时消失--&gt;&lt;input type="text" id="i1" value="对子哈特" &gt;&lt;input type="button" value="搜索"&gt;&lt;script&gt; // 找到input框 var i1Ele = document.getElementById("i1") i1Ele.onfocus = function () &#123; // 把value青龙 this.value = ""; &#125; i1Ele.onblur = function () &#123; // 失去焦点之后，如果值是空叫填写回去 if (!this.value.trim())&#123; this.value = "对子哈特" &#125; &#125; &lt;/script&gt; select 联动123456789101112131415161718192021222324252627282930313233343536&lt;select name="" id="s1"&gt; &lt;option value="0"&gt;--请选择--&lt;/option&gt; &lt;option value="1"&gt;北京&lt;/option&gt; &lt;option value="2"&gt;上海&lt;/option&gt;&lt;/select&gt;&lt;select name="" id="s2"&gt; &lt;option value=""&gt;&lt;/option&gt;&lt;/select&gt;&lt;script&gt; var data = &#123;1:["西城区","朝阳区","东城区"],2:["静安区","闵行区","浦东区"]&#125;; // 给第一个select绑定事件,onchange var s1Ele = document.getElementById("s1"); s1Ele.onchange = function () &#123; // 取到选择的是哪一个 市 // 把对应市的区 填写到第二个select框里 console.log(this.value); var areas = data[this.value]; // 获得对应的区 var s2Ele = document.getElementById("s2") // 清空之前的标签 s2Ele.innerHTML = ""; // 生成option标签 for (var i = 0;i&lt;areas.length;i++)&#123; var opEle = document.createElement("option"); opEle.innerText= areas[i]; s2Ele.appendChild(opEle) &#125; // 添加到select内部 &#125;&lt;/script&gt;]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript]]></title>
    <url>%2F2019%2F05%2F29%2Fjavascript%2F</url>
    <content type="text"><![CDATA[JavaScript 介绍 完整的 JavaScript 实现是由以下 3 个不同部分组成的: 核心（ECMAScript） 文档对象模型（DOM） Document object model (整合js，css，html) 浏览器对象模型（BOM） Broswer object model（整合js和浏览器） Javascript 在开发中绝大多数情况是基于对象的.也是面向对象的. ECMAScriptECMAScript 是一个重要的标准，但它并不是 JavaScript 唯一的部分，ECMAScript和JavaScript的关系是，前者是后者的规格，后者是前者的一种实现。 ES6就是指ECMAScript 6。 简单地说，ECMAScript 描述了以下内容： 语法 类型 语句 关键字 保留字 运算符 对象 (封装 继承 多态) 基于对象的语言.使用对象. JavaScript 概述 JavaScript是脚本语言, 将JS插入 HTML 页面后，可由所有的现代浏览器执行。 JS跑在浏览器上，他的解释器就是浏览器，有浏览器就能跑 JS。 JavaScript 基础JS 引入方式 JS文件是在后端写的，浏览器通过网络从后端拿过来，然后在执行，如果找不到JS文件浏览器并不会报错 直接编写 123&lt;script&gt; alert('hello')&lt;/script&gt; 文件引入 1&lt;script src="jojo.js"&gt;&lt;/script&gt; JS 语言规范注释1// 单行注释 1/* 我是多行注释 */ 结束符 js语句之间有分号(;)为结束符,python通过缩进 JS 语言基础变量声明 js属于动态类型。(同一个变量可以存不同数据类型的值) 声明变量使用 var 变量名; 的格式来进行声明。 一行可以声明多个变量.并且可以是不同类型。 JavaScript的变量名可以使用_，数字，字母，$组成，不能以数字开头。 js中$也可以做变量名。12var userName = 'leo';var userName = 'leo',age = 28,city="beijing"; 注意 变量名是区分大小写的。 推荐使用驼峰式命名规则。Camel 标记法首字母是小写的，接下来的字母都以大写字符开头。例如：var myTestValue = 0, mySecondValue = “hi”; 保留字不能用做变量名。 保留字列表12345678910111213141516171819202122232425262728293031abstractbooleanbytecharclassconstdebuggerdoubleenumexportextendsfinalfloatgotoimplementsimportintinterfacelongnativepackageprivateprotectedpublicshortstaticsupersynchronizedthrowstransientvolatile JS 数据类型 JS 动态类型 JS属于动态类型语言 动态类型：声明变量即可以是字符串也可以是数字 静态类型，在声明变量的时候，声明类型，严谨，执行效率快，不用判断变量，范围缩小 1234567891011var x typeof(x)"undefined"var name = "leo"typeof(name)"string"var age = 28typeof(age)"number" 数值(Number) JavaScript不区分整型和浮点型，就只有一种数字类型。 12345678910var n1 = 28var n2 = 28.00var n3 = NaN ### NaN，表示不是一个数字（Not a Number）。typeof(n1)"number"typeof(n2)"number"typeof(n3)"number" 123var name = 'leo'parseInt(name) ### 表示强转后，结果并不是一个数字NaN 字符串(String) 字符串拼接用 + 号 12345var s1 = "jojo"var s2 = "bobo"var s3 = s1+s2console.log(s3)### jojobobo 字符串的常用方法 布尔值(Boolean) 区别于Python，true和false都是小写。 “”(空字符串)、0、null、undefined、NaN都是false 12var a = true;var b = false; null和undefined null表示值是空，一般在需要指定或清空一个变量时才会使用，如 name=null; undefined表示当声明一个变量但未初始化时，该变量的默认值是undefined。 函数无明确的返回值时，返回的也是undefined。 null表示变量的值是空，undefined则表示只声明了变量，但还没有赋值。 123null == undefined弱等于 10 == "10"强等于 10 === "10" 1234567var ageageundefinedtypeof age"undefined" 对象(Object) JavaScript 中的所有事物都是对象：字符串、数值、数组、函数…此外，JavaScript 允许自定义对象。 JavaScript 提供多个内建对象，比如 String、Date、Array 等等。 对象只是带有属性和方法的特殊数据类型。 数组: 数组对象的作用是：使用单独的变量名来存储一系列的值。类似于Python中的列表。 12var l1 = ['a','b','c','d','e']console.log(l1[1]) 常用方法: 遍历数组中的元素: 12345678for (var i=0;i&lt;l1.length;i++)&#123; console.log(i)&#125;01234 字典对象: 1234567891011var person = &#123; name : 'leo', age : 18&#125;;undefinedperson.name"leo"person.age18person&#123;name: "leo", age: 18&#125; 运算符算数运算符+ - * / % ++ -- 比较运算符&gt; &gt;= &lt; &lt;= != == === !== 12345678910111213141510 == '10'true ### 弱等于10 === '10'false ### 强等于,既判断类型又判断值 ,一般使用强等于null == undefinedtruenull === undefinedfalsetypeof null"object"typeof undefined"undefined" 逻辑运算符&amp;&amp; || !1与and 或or 非! 赋值运算符= += -= *= /=1234567var n = 1n = 22n += 24n -= 13 流程控制if-else if-else12345678var a = 5;if (a &gt; 5)&#123; console.log(" a &gt; 5");&#125;else if (a &lt; 5)&#123; console.log(" a &lt; 5");&#125;else&#123; console.log(" a = 5");&#125; switch1234567891011121314// 获取今天周几var day = new Date().getDay();switch (day)&#123; case 0: console.log("Sunday"); break; case 1: console.log("Monday"); break;default: console.log("...")&#125;Monday switch中的case子句通常都会加break语句，否则程序会继续执行后续case中的语句。 for1234567891011var l1 = [1,2,'leo']for (var i = 0;i &lt; l1.length;i++)&#123; console.log(i) // 索引 console.log(l1[i]) // 值&#125;01122leo while123456// 循环要有一个准确的终止条件var age = 10;while (age &lt; 18)&#123; console.log("可以进入"); age += 1&#125; 三元运算123456### pythona = 10b = 5c = a if a &gt; b else bprint(c)10 123456// jsvar a = 10;var b = 20;var c = a &gt; b ? a:b// 问号 ？ 前面是条件，后面是取值,成立取a，不成立取b(冒号后面的值) 函数函数定义123456789// 普通函数 带参函数function func(a,b) &#123; console.log(a); console.log(b); return a + b // 返回值&#125;ret = func(10,20) // 函数执行 接收返回值console.log('a + b = ' + ret) 123456789// 匿名函数 没有函数名,存在变量里var func2 = function (a,b) &#123; console.log(a); console.log(b); return a + b&#125;ret = func2(10,20)console.log('a + b = ' + ret) 123456789101112131415// 参数个数function func(a,b) &#123; console.log(a); console.log(b); return a + b&#125;// ret = func(10,20,30)// 多一个参数传进来 并不会报错 继续执行ret = func(10)// 少一个参数传进来 b = undefined , a + b = NaN// 也就是说b被声明了，但是没有赋值,相加的结果是NANconsole.log('a + b = ' + ret) 12345// 立即执行函数(function (a,b) &#123; console.log('立即执行函数') console.log(a+b)&#125;)(10,20); arguments 参数12345678910111213141516171819// arguments参数 有点像 python中的args 他可以用来接收多个参数// arguments 找到所有传进的参数 == *argsfunction func(a,b) &#123; console.log(arguments); console.log('参数个数:' + arguments.length); var ret = 0; for (var i = 0;i&lt;arguments.length;i++)&#123; ret += arguments[i] &#125; return ret // 3个参数相加之和&#125;ret = func(10,20,30)console.log(ret)// Arguments(3) [10, 20, 30, callee: ƒ, Symbol(Symbol.iterator): ƒ]// 参数个数:3// 60 函数的全局变量和局部变量 局部变量：121. 在JavaScript函数内部声明的变量（使用 var）是局部变量，所以只能在函数内部访问它（该变量的作用域是函数内部）。2. 只要函数运行完毕，本地变量就会被删除。 全局变量：11. 在函数外声明的变量是全局变量，网页上的所有脚本和函数都能访问它。 变量生存周期：1231. JavaScript变量的生命期从它们被声明的时间开始。2. 局部变量会在函数运行以后被删除。3. 全局变量会在页面关闭后被删除。 作用域 变量的查找顺序: 121. 函数的调用，要往回找，找到函数的定义阶段2. 变量首先在 函数的内部找 -&gt; 内部找不到就往外找 -&gt; 直到找到全局为止，全局都没有就是undefined。 12345678910var city = "BeiJing";function f() &#123; var city = "ShangHai"; function inner()&#123; var city = "ShenZhen"; console.log(city); &#125; inner();&#125;f(); //输出结果是？ ShenZhen 12345678910var city = "BeiJing";function Bar() &#123; console.log(city);&#125;function f() &#123; var city = "ShangHai"; return Bar;&#125;var ret = f();ret(); // 打印结果是？ BeiJing 1234567891011// 闭包var city = "BeiJing";function f()&#123; var city = "ShangHai"; function inner()&#123; console.log(city); &#125; return inner;&#125;var ret = f();ret(); // ShangHai 词法分析（尝试理解）123451.分析(函数内部变量的定义过程) 1.先看参数 2.看有没有局部变量 3.看有没有函数定义2.执行(实际执行代码) JavaScript中在调用函数的那一瞬间，会先进行词法分析。 当函数 调用 的前一瞬间，会先形成一个激活对象：Avtive Object（AO），并会分析以下3个方面： 1231:函数参数，如果有，则将此参数赋值给AO，且值为undefined。如果没有，则不做任何操作。2:函数局部变量，如果AO上有同名的值，则不做任何操作。如果没有，则将此变量赋值给AO，并且值为undefined。3:函数声明，如果AO上有，则会将AO上的对象覆盖。如果没有，则不做任何操作。 函数内部无论是使用参数还是使用局部变量都到AO上找。1234567891011121314var age = 18;function foo()&#123; console.log(age); var age = 22; console.log(age);&#125;foo(); // 问：执行foo()之后的结果是？// 分析:// 1.函数没有参数// 2.有局部变量 age 那么 ao.age = undefined// 3.有变量赋值 age = 22// 第一个age = undefined // 第二个age = 22 123456789101112131415161718192021var age = 18;function foo()&#123; console.log(age); var age = 22; console.log(age); function age()&#123; console.log("呵呵"); &#125; console.log(age);&#125;foo(); // 执行后的结果是？// 分析:// 1 没有参数// 2.age = undefined// 3.age = 22// 4.function age = ao.age = function age()...// 1. function age()...// 2. 22 // 3 .22 同理第三个输出的还是22, 因为中间再没有改变age值的语句了 内置对象和方法对象的属性 JavaScript中的所有事物都是对象：字符串、数字、数组、日期，等等。 123456789// 字符串对象创建// var 变量 = “字符串”// var 字串对象名称 = new String (字符串)var name1 = "leo"var name2 = new String("Leo")typeof name1"string"typeof name2"object" 1234567// 数组对象创建var l1 = [1,2,3]var l2 = new Array(1,2,3)l1(3) [1, 2, 3]l2(3) [1, 2, 3] 123456789101112// 数字类型对象创建var n1 = 28n128var n2 = new Number(30)n2Number &#123;30&#125;typeof n1"number"typeof n2"object" 自定义对象123456789101112131415161718192021// 第一种方法 直接创建// 自定义对象var person = &#123; "name":"leo", "age":28&#125;console.log(person)console.log(typeof person)console.log(person.name)console.log(person.age)// 循环遍历对象属性for (var i in person)&#123; console.log(i) // 键 console.log(person[i]) // 值&#125;// 注意:// 1. 键不用加引号,加上也不出错 &#123;name:"leo",age:28&#125;// 2. 值如果是字符串，必须写双引号 12345678// 第二种方法 new 关键字创建var person = new Object()person.name = "leo"person.age = 28person.city = "北京"console.log(person.name + person.age)var index3 = 'city' // 刚巧我有一个变量的值是对象里面的键console.log(person[index3]) // 变量取值 内置对象Date对象 内置对象相当于python当中的内置模块 1234567// 创建Date对象// Date对象var d1 = new Date();console.log(d1); // Tue Jun 11 2019 18:04:34 GMT+0800 (中国标准时间)console.log(typeof d1); // objectconsole.log(d1.toLocaleString()); // 2019/6/11 下午6:04:34 字符串时间console.log(typeof d1.toLocaleString()); // string 1234567// 生成指定时间的date对象var d2 = new Date("2019/06/11 18:10");var d3 = new Date("2019-06-11 18:10");var d4 = new Date("2019/06/11 18:10");console.log(d2.toLocaleString()); // 转成字符串格式的 本地时间 2019/6/11 下午6:10:00console.log(d3.toLocaleString()); // 2019/6/11 下午6:10:00console.log(d4.toUTCString()); // Tue, 11 Jun 2019 10:10:00 GMT UTC时间 1234567891011// Date对象的方法var d = new Date();console.log(d.getDate()); //获取日console.log(d.getDay ()); //获取星期console.log(d.getMonth ()); //获取月（0-11）console.log(d.getFullYear ()); //获取完整年份console.log(d.getHours ()); //获取小时console.log(d.getMinutes ()); //获取分钟console.log(d.getSeconds ()); //获取秒console.log(d.getMilliseconds ()); //获取毫秒console.log(d.getTime ()); //返回累计毫秒数(从1970/1/1午夜) 123456789101112131415161718192021222324// 编写代码，将当前日期按“2017-12-27 11:11 星期三”格式输出。var weekday = &#123; "0":"星期日", "1":"星期一", "2":"星期二", "3":"星期三", "4":"星期四", "5":"星期五", "6":"星期六"&#125;function get_day() &#123; var d = new Date() var year = d.getFullYear() var month = d.getMonth()+1; // 月份是0-11 所以加1 var day = d.getDay(); var h = d.getHours(); var m = d.getMinutes() &lt; 10 ? "0" + d.getMinutes():d.getMinutes(); // 三元运算 如果分钟&lt;10 在前面加个0 var week_day = weekday[d.getDay()] var strTime = `$&#123;year&#125;-$&#123;month&#125;-$&#123;day&#125; $&#123;h&#125;:$&#123;m&#125; $&#123;week_day&#125;`; console.log(strTime)&#125;get_day() JSON对象12345678910// 字符串转成对象var person = '&#123;"name":"leo","age":28&#125;' // 字符串var json_s = JSON.parse(person)console.log(json_s)console.log(typeof json_s) // object// 对象转换成字符串var ob_s = JSON.stringify(json_s)console.log(ob_s)console.log(typeof ob_s) // string Math对象 同样Math用于数据科学计算 123456789101112abs(x); //返回数的绝对值。exp(x); //返回 e 的指数。floor(x); //对数进行下舍入。log(x); //返回数的自然对数（底为e）。max(x,y); //返回 x 和 y 中的最高值。min(x,y); //返回 x 和 y 中的最低值。pow(x,y); //返回 x 的 y 次幂。random(); //返回 0 ~ 1 之间的随机数。round(x); //把数四舍五入为最接近的整数。sin(x); //返回数的正弦。sqrt(x); //返回数的平方根。tan(x); //返回角的正切。 12var n = -100console.log(Math.abs(n)) RegExp对象12345678910111213141516// 创建正则对象方式1// 参数1 正则表达式(不能有空格)// 参数2 匹配模式：常用g(全局匹配;找到所有匹配，而不是在第一个匹配后停止)和i(忽略大小写)// 例子:用户名,只能是英文字母、数字和_，并且首字母必须是英文字母。长度最短不能少于6位 最长不能超过12位。// ^[a-zA-Z][a-zA-Z0-9_]&#123;5,12&#125;// 创建RegExp对象方式（逗号后面不要加空格）var reg1 = new RegExp("^[a-zA-Z][a-zA-Z0-9_]&#123;5,12&#125;$");var str1 = "abcde" // 待匹配字符串: 5位字符组成var str2 = "abcdef" // 待匹配字符串: 6位字符组成var str3 = "1abcdef" // 待匹配字符串: 7位字符组成,数字开头//RegExp对象的test方法，测试一个字符串是否符合对应的正则规则，返回值是true或false。console.log(reg1.test(str1)) // false 开头占1位，后面如果是4个字符，满足不了 &#123;5，12&#125;console.log(reg1.test(str2)) // true 1+5console.log(reg1.test(str3)) // false 不能数字开头 1234567// 第二种写法 /正则/// 创建方式2// /填写正则表达式/匹配模式（逗号后面不要加空格）=var reg2 = /^[a-zA-Z][a-zA-Z0-9_]&#123;5,12&#125;$/;var str2 = 'abcdef'console.log(reg2.test(str2)) 1234567// String对象与正则结合的4个方法var s2 = "hello world";s2.match(/o/g); // ["o", "o"] 查找字符串中 符合正则 的内容s2.search(/h/g); // 0 查找字符串中符合正则表达式的内容位置s2.split(/o/g); // ["hell", " w", "rld"] 按照正则表达式对字符串进行切割s2.replace(/o/g, "s"); // "hells wsrld" 对字符串按照正则进行替换 1234// 坑1console.log("==============================================================")// 正则表达式之间 不能有空格 &#123;5, 11&#125; 要不然就被崩//console.log(/^[a-zA-Z][a-zA-Z0-9]&#123;5, 11&#125;$/.test('xiaoqiang')) 12345678// 坑2// test 不传值 相当于传了一个 undefined// test() 就把undefined 当成是 "undefined" 来判断console.log("==============================================================")console.log(/^[a-zA-Z][a-zA-Z0-9]&#123;5,11&#125;$/.test())console.log(/^[0-9a-zA-Z][a-zA-Z0-9]&#123;5,11&#125;$/.test())console.log(/^[0-9a-zA-Z][a-zA-Z0-9]&#123;5,11&#125;$/.test(undefined))console.log(/^[0-9][a-zA-Z0-9]&#123;5,11&#125;$/.test()) 123456789101112131415161718192021222324252627// 字符串替换 不是改变原来的字符串,需要生成新的字符串var ss = 'Leo'var s3 = ss.replace("o","x")console.log(s3)// 正则 全部替换g 忽略大小写i// JS正则的两种模式:// 1. g 表示全局// 2. i 忽略大小写var ss = 'AbAcad'var s4 = ss.replace(/A/gi,"哈哈哈")console.log(s4)// 坑3// 当正则表达式使用了全局模式(g)时,并且你还让他去检测一个字符串// 此时会引出 lastIndex 记录上次匹配成功位置,并把下一次要开始效验的位置记住// 所以一般不要设置全局// a开头后面2-4位字符// console.log(/^a[a-zA-Z]&#123;2,4&#125;$/g.test("alex"))// console.log(/^a[a-zA-Z]&#123;2,4&#125;$/g.test("alex"))var r = /alex/g;console.log(r.test("alex")) // tureconsole.log(r.lastIndex) // 4console.log(r.test("alex")) // false 从头开始console.log(r.lastIndex) // 0]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS]]></title>
    <url>%2F2019%2F05%2F20%2Fweb-css%2F</url>
    <content type="text"><![CDATA[CSS 介绍 CSS（Cascading Style Sheet，层叠样式表)定义如何显示HTML元素。 当浏览器读到一个样式表，它就会按照这个样式表来对文档进行格式化（渲染）。 HTML定义了页面的框架，CSS样式表用来装饰页面。 思维导图: https://www.processon.com/mindmap/5a2f4ec4e4b0abb143af4a08 CSS 语法 CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明。 声明又包括属性和属性值。每个声明之后用分号结束。 选择器 {样式1;样式2} h1 {color:red; font-size:14px;} CSS引入方式行内样式 行内式是在标记的style属性中设定CSS样式,不推荐大规模使用。 1&lt;p style="color: blue"&gt;杰尼龟&lt;/p&gt; 内部样式 嵌入式是将CSS样式集中写在网页的&lt;head&gt;&lt;/head&gt;标签对的&lt;style&gt;&lt;/style&gt;标签对中。格式如下： 12345678910&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel="stylesheet" href="index.css"&gt; &lt;style&gt; p &#123; color: green; &#125; &lt;/style&gt;&lt;/head&gt; 外部样式(推荐) 外部样式就是将css写在一个单独的文件中，然后在页面进行引入即可。推荐使用此方式。 123456789### link语句在&lt;head&gt;&lt;/head&gt;&gt;下使用&lt;link rel="stylesheet" href="index.css"&gt;### index.css/* 标签选择器 */p &#123; color: red; font-size: 48px;&#125; CSS 选择器 CSS选择器的作用: 查找标签 基本选择器 标签选择器123/* 标签选择器 *//*适用于：批量、统一、默认的样式*/p &#123; color: red;&#125; ID选择器123/* ID选择器 # *//*适用于：给特定标签设置特定样式*/#p2 &#123;color: blue;&#125; 类选择器123/* 类选择器 . *//*适用于：给某一些标签摄者相同的样式*/.c1 &#123;color: yellow;&#125; 123# 注意:# 样式类名不要用数字开头（有的浏览器不认）。# 标签中的class属性如果有多个，要用空格分隔。 通用选择器 通配选择器 * 会找到body里面所有的标签，包括body标签，所以页面整体都会被找到 12/* 类选择器 */* &#123;color: pink;&#125; 组合选择器后代选择器 空格分割 1234/* d1下的所有p标签被找到 */#d1 p &#123; color: red;&#125; 子代选择器 &gt; 只找儿子 1234/* id是d1标签 所有儿子是p标签 */#d1&gt;p&#123; color: green;&#125; 毗邻选择器 + 找紧挨着的兄弟标签 只往下找 1234/* 选择所有紧接着div元素之后的p标签 */div+p&#123; color: yellow;&#125; 弟弟选择器 只要是兄弟标签1234/* div后面的所有的p标签 */div~p&#123; color: blue;&#125; 分组和嵌套分组 当多个元素的样式相同的时候，我们没有必要重复地为每个元素都设置样式，可以通过在多个选择器之间使用逗号分隔的分组选择器来统一设置元素样式。 , 代表 或 123div, p&#123; color: red;&#125; 上面的代码为div标签和p标签统一设置字体为红色。通常，我们会分两行来写，更清晰,但是别忘记加上逗号: 1234div, p&#123; color: red;&#125; 嵌套 多种选择器可以混合起来使用，比如：.d1类内部所有p标签设置字体颜色为红色。 . 代表 且 123.c1 p &#123; color: red;&#125; 找一个class = d1 的span标签 123span.d1&#123; color: pink;&#125; 属性选择器123&lt;p cid="leo"&gt;我是一个P标签&lt;/p&gt;&lt;p cid="lex"&gt;我也是一个P标签&lt;/p&gt;&lt;p&gt;我还是一个P标签&lt;/p&gt; 用于选取带有指定属性的元素 123[cid]&#123; color: pink;&#125; 用于选取带有指定属性和值的元素 123[cid='lex']&#123; color: green;&#125; 伪类选择器 a：link（从未访问过的链接的崭新状态），用于定义了链接的常规状态。 a:hover（鼠标移动到链接上时未点击的悬浮状态），用于产生视觉效果。 a:visited（已访问过的链接的状态）用于阅读文章，能清楚的判断已经访问过的链接。 a:active（在链接上按下鼠标时的状态），用于表现鼠标按下时的链接激活状态。 123456789101112131415161718192021222324252627282930/* 所有没有访问过的a标签 */a:link &#123; color: deeppink;&#125;/* 所有访问过的a标签 */a:visited &#123; color: green;&#125;/* 鼠标移上去 变色 */a:hover &#123; color: blueviolet;&#125;#d1:hover &#123; color: darksalmon;&#125;/* 被选定 鼠标按下去变色 */a:active&#123; color: deeppink;&#125;/*input获取光标时*/input:focus&#123; outline: 0; background-color: deeppink;&#125; 伪元素选择器1234567891011121314151617/*常用的给首字母设置特殊样式：*/p:first-letter&#123; font-size: 48px; color: deeppink;&#125;/*在标签内容开头操作*/.p1:before&#123; content: "*"; color: red;&#125;/*在标签内容最后操作*/.p1:after&#123; content: "[?]"; color: blue;&#125; 选择器优先级 所谓CSS优先级，即是指CSS样式在浏览器中被解析的先后顺序。 1.内联样式表的权值最高 嵌套引入 style=”” －－－ 1000 2.统计选择符中的ID属性个数。 #id －－－ 100 3.统计选择符中的CLASS属性个数。 .class －－－ 10 4.统计选择符中的HTML标签名个数。 p －－－ 1 5.选择器都一样的情况下，谁靠近标签，谁的优先级高 6.万不得已可以使用!important CSS 常用属性宽和高 width 属性可以为元素设置宽度。 height 属性可以为元素设置高度。 只有块级标签才能设置宽和高，内联标签的宽度由内容的长度来决定 1234.d1 &#123; width: 300px; height: 200px;&#125; 字体属性文字字体 font-family 可以把多个字体名称作为一个“回退”系统来保存。 如果浏览器不支持第一个字体，则会尝试下一个。 浏览器会使用它可识别的第一个值。 1234/*全局样式*/*&#123; font-family: "幼圆","Microsoft YaHei" ,"Arial";&#125; 字体大小123.d1 &#123; font-size: 32px;&#125; 字重（粗细）123.d1 &#123; font-weight: bold;&#125; 文本颜色 颜色属性被用来设置文字的颜色。 颜色是通过CSS最经常的指定： 十六进制值 - 如: ＃FF0000 一个RGB值 - 如: RGB(255,0,0) 颜色的名称 - 如: red 还有rgba(255,0,0,0.3)，第四个值为alpha, 指定了色彩的透明度/不透明度，它的范围为0.0到1.0之间。 可以通过浏览器工具获取颜色的十六进制值,然后用到自己的样式中 12345.d1 &#123; color: deeppink; /*color: #FF1493;*/ /*color: rgba(255,0,0,0.3);*/&#125; 文字对齐 text-align 属性规定元素中的文本的水平对齐方式。 123.p1&#123; text-align: justify;&#125; 文字装饰 text-decoration 属性用来给文字添加特殊效果。 常用方法: 1234/* 去掉a标签默认的自划线：*/a&#123; text-decoration: none;&#125; 首行缩进 首行缩进 字体像素的两倍就相当于是首行缩进两个字: 12345.p1&#123; font-size: 32px; text-indent: 32px;&#125; 文本其它属性 font-style: oblique; 斜体 letter-spacing: 10px; 字母间距 word-spacing: 20px; 单词间距 text-transform: capitalize/uppercase/lowercase ; 文本转换，用于所有字句变成大写或小写字母，或每个单词的首字母大写 line-height: 200px; 文本行高 通俗的讲，文字高度加上文字上下的空白区域的高度 50%:基于字体大小的百分比 vertical-align:－4px 设置元素内容的垂直对齐方式 ,只对行内元素有效，对块级元素无效 背景属性背景颜色123.d1&#123; background-color: deeppink;&#125; 背景图片 设置背景图片如:200 X 200像素 如果设置的div层宽和高 与图片的像素一致，则直接铺满 如果设置的div层宽和高 大于图片的像素 如 600 X 600 ，如下图，就要进行背景调整12345.d1&#123; width: 600px; height: 600px; background-image: url("xiaoxin.jpg");&#125; 背景重复 repeat(默认):背景图片平铺排满整个网页 repeat-x：背景图片只在水平方向上平铺 repeat-y：背景图片只在垂直方向上平铺 no-repeat：背景图片不平铺 1234567.d1&#123; width: 600px; height: 600px; background-image: url("xiaoxin.jpg"); /*图片超出后不重复(平铺)*/ background-repeat: no-repeat;&#125; 调整背景位置 1234567891011.d1&#123; width: 600px; height: 600px; background-image: url("xiaoxin.jpg"); /*图片超出后不重复(平铺)*/ background-repeat: no-repeat; /*background-position: center;*/ background-position: center; /*x轴 -&gt; 200，y轴100*/ /*background-position: 200px 100px;*/&#125; 支持简写：background:#ffffff url(&#39;1.png&#39;) no-repeat right top; 使用背景图片的一个常见案例就是很多网站会把很多小图标放在一张图片上，然后根据位置去显示图片。减少频繁的图片请求。http://www.w3school.com.cn/css/css_background.asp 鼠标滚动背景不动 123&lt;div class="c1"&gt;&lt;/div&gt;&lt;div class="c2"&gt;&lt;/div&gt;&lt;div class="c3"&gt;&lt;/div&gt; 123456789101112131415.c1&#123; height: 500px; background-color: red;&#125;.c2&#123; height: 500px; background: url("CSS 属性/img23.jpg") no-repeat center; background-attachment: fixed;&#125;.c3&#123; height: 500px; background-color: green;&#125; 边框属性设置边框 border-width 粗细 border-style 样式 border-color 颜色 12345678.d1&#123; height: 200px; width: 300px; background-color: deeppink; border-width: 5px; border-style: solid; background-color: green;&#125; 12345/*通常使用简写方式：*/.d1&#123; border: 5px solid green;&#125; 除了可以统一设置边框外还可以单独为某一个边框设置样式，如下所示： 1234567.d1&#123; border-top-style:dotted; border-top-color: red; border-right-style:solid; border-bottom-style:dotted; border-left-style:none;&#125; 圆角边框123456.d1&#123; height: 200px; width: 200px; border-radius: 100px 100px; background-color: deeppink;&#125; 列表属性12341、list-style-type 设置列表项标志的类型。2、list-style-image 将图象设置为列表项标志。3、list-style-position 设置列表中列表项标志的位置。4、list-style 简写属性。用于把所有用于列表的属性设置于一个声明中 1234/*使用图像来替换列表项的标记:*/ul &#123; list-style-image: url('图片');&#125; display 属性 用于控制HTML元素的显示效果。 none(隐藏某标签) display: none 与 visibility: hidden的区别 visibility:hidden: 可以隐藏某个元素，但隐藏的元素仍需占用与未隐藏之前一样的空间。也就是说，该元素虽然被隐藏了，但仍然会影响布局。 display:none: 可以隐藏某个元素，且隐藏的元素不会占用任何空间。也就是说，该元素不但被隐藏了，而且该元素原本占用的空间也会从页面布局中消失。 display: none不占位，visibility: hidden占位 12345.d1&#123; height: 100px; color: red; display: none; /*visibility: hidden;*/ block (内联标签设置为块级标签)1234.s1&#123; display: block; background-color: green;&#125; inline(块级标签设置为内联标签)123456&lt;ul class="u1"&gt; &lt;li&gt;&lt;a href=""&gt;新闻&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;体育&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;娱乐&lt;/a&gt;&lt;/li&gt; &lt;li class="last"&gt;&lt;a href=""&gt;天气&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt; 12345678910111213li&#123; display: inline; border-right: 1px solid black; padding: 1px 10px;&#125;li a&#123; text-decoration: none;&#125;li.last&#123; border-right: none;&#125; inline-block display:inline-block可做列表布局，其中的类似于图片间的间隙小bug可以通过如下设置解决： 1234#outer&#123; border: 3px dashed; word-spacing: -5px;&#125; CSS 盒子模型 margin: 用于控制元素与元素之间的距离；margin的最基本用途就是控制元素周围空间的间隔，从视觉角度上达到相互隔开的目的。 padding: 用于控制内容与边框之间的距离； Border(边框): 围绕在内边距和内容外的边框。 Content(内容): 盒子的内容，显示文本和图像。 margin 外边距12345678910111213141516.d1&#123; height: 300px; width: 300px; background-color: deeppink; /*margin-top:10px;*/ /*margin-right:20px;*/ /*margin-bottom:10px;*/ /*margin-left:20px;*/ /*顺序：上右下左*/ margin: 10px 20px 10px 20px; /*常见居中*/ margin: 0 auto;&#125; 123456789101112131415161718### 简写属性margin:10px 20px 20px 10px； 上边距为10px 右边距为20px 下边距为20px 左边距为10pxmargin:10px 20px 10px; 上边距为10px 左右边距为20px 下边距为10pxmargin:10px 20px; 上下边距为10px 左右边距为20pxmargin:25px; 所有的4个边距都是25px padding内填充123456.padding-test &#123; padding-top: 5px; padding-right: 10px; padding-bottom: 15px; padding-left: 20px;&#125; 123.padding-test &#123; padding: 5px 10px 15px 20px;&#125; 123456顺序：上右下左补充padding的常用简写方式：1、提供一个，用于四边；2、提供两个，第一个用于上－下，第二个用于左－右；3、如果提供三个，第一个用于上，第二个用于左－右，第三个用于下；4、提供四个参数值，将按上－右－下－左的顺序作用于四边； 12345678.d1&#123; height: 200px; width: 300px; background-color: deeppink; margin: 20px 30px; padding: 20px 30px; border: 10px solid black;&#125; body标签的外边距 有时候div的宽度我们设置为100%,依然发现不能占满整个页面的宽度，这是由于body存在8px的外边距(不同的浏览器分辨率有所区别可能), 通用初始化,浏览器不设置外边距,解决屏幕铺满问题 1234*&#123; margin: 0; padding: 0;&#125; float 浮动 浮动:div 配合 float浮动 来做 页面的布局 在 CSS 中，任何元素都可以浮动。浮动元素会生成一个块级框，而不论它本身是何种元素。 关于浮动的两个特点： 浮动的框可以向左或向右移动，直到它的外边缘碰到包含框或另一个浮动框的边框为止。 由于浮动框不在文档的普通流中，所以文档的普通流中的块框表现得就像浮动框不存在一样。 浮动标签 left：向左浮动 right：向右浮动 none：默认值，不浮动 123456789101112131415161718*&#123; margin: 0; padding: 0;&#125;.d1&#123; height: 600px; width: 20%; background-color: red; float: left;&#125;.d2&#123; height: 600px; width: 80%; background-color: green; float: left;&#125; 12345a&#123; /*内联标签设置float就会变成块级标签，可以设置长和宽*/ width: 1000px; float: left;&#125; clear 清除浮动 重要，主要解决清除浮动的副作用（父标签塌陷问题） clear属性规定元素的哪一侧不允许其他浮动元素。 出现塌陷演示1234&lt;div id="d0"&gt; &lt;div class="d1"&gt;c1&lt;/div&gt; &lt;div class="d2"&gt;c2&lt;/div&gt;&lt;/div&gt; 12345678910111213141516*&#123; margin: 0; padding: 0;&#125;#d0&#123; border: 1px solid black;&#125;.d1,.d2&#123; /*如果父标签没有高度，float浮动会脱离父标签;*/ float: left; height: 100px; width: 100px;&#125; 标签塌陷：浮动的标签脱离文档流，不在原来的位置，通过清除浮动即左边不能有标签，在浮动标签下撑起整个父标签 为什么父标签没有高度,因为父标签里没有内容 1、因为里面儿子的div 都是浮动的 不能撑起父标签 2、浮动元素 脱离文档流，不在原来的位置上 3、在父标签的最后增加一个没有高度的设置，撑起整个父标签。 伪元素 撑起父标签12345678910#d0:after&#123; /*伪元素，在父标签上内部的最后,增加一个空内容,他的左边不能有浮动*/ /*父标签就会被撑起来*/ /*不浮动的元素在浮动元素的下面*/ content: ""; clear: left; display: block; /*左右都不能有浮动元素*/ /*clear: both;*/&#125; 业界通用版123456789&lt;div id="d1" class="clearfix"&gt; &lt;div class="d2"&gt;&lt;/div&gt; &lt;div class="d2"&gt;&lt;/div&gt; &lt;div class="d2"&gt;&lt;/div&gt; &lt;div class="d2"&gt;&lt;/div&gt; &lt;!--&lt;div class="d3"&gt;&lt;/div&gt;--&gt;&lt;/div&gt;&lt;div class="d4"&gt;我是正常的内容块&lt;/div&gt; 12345678910111213141516171819202122232425262728293031*&#123; margin: 0; padding: 0;&#125;#d1&#123; /*父div没有高度*/ border: 1px solid black;&#125;.d2&#123; height: 50px; width: 50px; background-color: blue; border: 1px solid red; /*display: inline;*/ /*向左浮动left*/ float: left;&#125;.d4&#123; height: 200px; background-color: darkgreen;&#125;/*业界规定清除浮动类*/.clearfix:after&#123; content: ""; clear: both; display: block;&#125; overflow 溢出属性 12345678&lt;div class="d1"&gt; 国安国安北京国安 国安国安北京国安 国安国安北京国安 国安国安北京国安国安国安北京国安 国安国安北京国安国安国安北京国安国安国安北京国安国安国安北京国安国安国安北京国安国安国安北京国安 国安国安北京国安国安国安北京国安&lt;/div&gt; 123456789.d1&#123; width: 120px; height: 120px; border: 1px solid red; /*给溢出的内容一个滚动条*/ overflow: auto; /*超出的隐藏*/ /*overflow: hidden;*/&#125; 制作圆形头像123&lt;div class="header-img"&gt; &lt;img src="xiaoxin.jpg"&gt;&lt;/div&gt; 123456789101112.header-img&#123; height: 120px; width: 120px; border: 2px solid antiquewhite; border-radius: 100%; /*内容溢出隐藏*/ overflow: hidden;&#125;img&#123; /*max直到父标签的100%*/ max-height: 100px;&#125; position 定位 static:默认值，无定位，不能当作绝对定位的参照物，并且设置标签对象的left、top等值是不起作用的的。 relative:相对定位相对于原来的位置来说 绝对定位相对于最近的一个被定位过的祖宗标签，一直没有就是body,默认来说走左上角 relative 相对定位 相对定位是相对于该元素在文档流中的原始位置，即以自己原始位置为参照物。有趣的是，即使设定了元素的相对定位以及偏移值，元素还占有着原来的位置，即占据文档流空间。对象遵循正常文档流，但将依据top，right，bottom，left等属性在正常文档流中偏移位置。而其层叠通过z-index属性定义。 注意：position：relative的一个主要用法：方便绝对定位元素找到参照物。 12345678910111213141516171819202122232425262728*&#123; margin: 0; padding: 0;&#125;.c1,.c2,.c3&#123; height: 150px; width: 150px;&#125;.c1&#123; background-color: red;&#125;.c2&#123; background-color: green; /*相对定位，相对的是原来在的位置*/ /*左边0,上面是c1的高*/ position: relative; left: 400px; top: 150px;&#125;.c3&#123; background-color: blue;&#125; 123&lt;div class="c1"&gt;&lt;/div&gt;&lt;div class="c2"&gt;&lt;/div&gt;&lt;div class="c3"&gt;&lt;/div&gt; absolute 绝对定位 定义：设置为绝对定位的元素框从文档流完全删除，并相对于最近的已定位祖先元素定位，如果元素没有已定位的祖先元素，那么它的位置相对于最初的包含块（即body元素）。元素原先在正常文档流中所占的空间会关闭，就好像该元素原来不存在一样。元素定位后生成一个块级框，而不论原来它在正常流中生成何种类型的框。 重点：如果父级设置了position属性，例如position:relative;，那么子元素就会以父级的左上角为原始点进行定位。这样能很好的解决自适应网站的标签偏离问题，即父级为自适应的，那我子元素就设置position:absolute;父元素设置position:relative;，然后Top、Right、Bottom、Left用百分比宽度表示。另外，对象脱离正常文档流，使用top，right，bottom，left等属性进行绝对定位。而其层叠通过z-index属性定义。 父节点没有定位 12345&lt;!-- 绝对定位，父节点没有定位，所有对照body --&gt;&lt;div class="c1"&gt;&lt;/div&gt;&lt;div class="c2"&gt;&lt;/div&gt;&lt;div class="c3"&gt;&lt;/div&gt;&lt;div class="c4"&gt;&lt;/div&gt; 123456789101112131415161718192021222324252627282930313233343536*&#123; margin: 0; padding: 0;&#125;.c1,.c2,.c3,.c4&#123; height: 150px; width: 150px;&#125;.c1&#123; background-color: red;&#125;.c2&#123; background-color: green; /*相对定位，相对的是原来在的位置*/ /*左边0,上面是c1的高*/ position: relative; left: 400px; top: 150px;&#125;.c3&#123; background-color: blue;&#125;.c4&#123; background-color: deeppink; position: absolute; top: 150px; left: 400px;&#125; 父节点有定位 12345678&lt;!-- 绝对定位，父节点有定位，对照父节点 --&gt;&lt;!-- 加了一父节点div class=c4-father --&gt;&lt;div class="c1"&gt;&lt;/div&gt;&lt;div class="c2"&gt;&lt;/div&gt;&lt;div class="c3"&gt;&lt;/div&gt;&lt;div class="c4-father"&gt; &lt;div class="c4"&gt;&lt;/div&gt;&lt;/div&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243*&#123; margin: 0; padding: 0;&#125;.c1,.c2,.c3,.c4,.c4-father&#123; height: 150px; width: 150px;&#125;.c1&#123; background-color: red;&#125;.c2&#123; background-color: green; /*相对定位，相对的是原来在的位置*/ /*左边0,上面是c1的高*/ position: relative; left: 400px; top: 150px;&#125;.c3&#123; background-color: blue;&#125;.c4&#123; background-color: deeppink; position: absolute; top: 150px; left: 400px;&#125;.c4-father&#123; position: relative; left: 150px; background-color: darkgoldenrod;&#125; fixed固定 经常用于返回顶部按钮 图片制作 输入网址 图片大小 背景颜色 文字颜色 格式 最后文字内容https://dummyimage.com/600X400/695151/fff.png&amp;text=leo CSS 知识补充小米导航条123456789101112131415161718192021222324252627282930313233343536373839404142*&#123; margin: 0; padding: 0;&#125;ul&#123; list-style-type: none;&#125;.nav-left li&#123; float: left; color: white; padding: 10px;&#125;.nav-right&#123; float: right; padding: 10px;&#125;.nav&#123; background-color: black; padding: 5px 0;&#125;/*清理左右两边浮动*/.clearfix:after&#123; content: ""; display: block; clear: both;&#125;.oa&#123; width: 80%; margin: 0 auto;&#125;a&#123; color: white; text-decoration: none;&#125; 123456789101112131415161718192021&lt;div class="nav "&gt; &lt;div class="oa clearfix"&gt; &lt;div class="nav-left"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=""&gt;玉米商城&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;大米&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;小米&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;苹果&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;香蕉&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;橘子&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;西瓜&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;荔枝&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="nav-right"&gt; &lt;a href=""&gt;登录&lt;/a&gt; &lt;a href=""&gt;注册&lt;/a&gt; &lt;a href=""&gt;购物车&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 脱离文档流 脱离文档流的3种方式 浮动 float 绝对定位 position: absolute 固定定位 position: fixed 123&lt;div class="d1"&gt;&lt;/div&gt;&lt;div class="d2"&gt;&lt;/div&gt;&lt;div class="d3"&gt;&lt;/div&gt; 123456789101112131415161718192021222324252627282930313233343536373839*&#123; margin: 0; padding: 0;&#125;.d1&#123; height: 100px; width: 100px; background-color: deeppink;&#125;.d2&#123; height: 150px; width: 150px; background-color: green; /*脱离文档流 让绿色的框脱离文档原来的位置 */ /*浮动*/ /*float: right;*/ /*相对定位: 标签还占有原来的位置*/ /*position: relative;*/ /*left: 400px;*/ /*绝对定位: 没有就找左上角 蓝色框会顶上来,绿色的框脱离了原来的位置 ，脱离了文档流*/ /*position: absolute;*/ /*left: 400px;*/ /*fixed 固定 蓝色也顶上来，绿色失去了原来的位置，脱离文档流 */ position: fixed; right: 100px; top: 100px;&#125;.d3&#123; height: 200px; width: 200px; background-color: blue;&#125; 内容知识点总结1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101.伪类和伪元素 1.伪类 1.:link 2.:visited 3.:hover(重要) 4.:active 5.:focus(input标签获取光标焦点) 2.伪元素 1. :first-letter 2. :before(重要 在内容前面添加) 3. :after(重要 在内容后面添加)2.CSS属性 1.字体： 1. font-family 2. font-size 3. font-weight 2.文本: 1. text-align 对齐 2. text-decoration 装饰(去除a标签的下划线) 3. text-indent 首行缩进 3.背景 1. background-color 颜色 2. background-image 图片 url() no-repeat 4.颜色 1. red 直接写名字 2. #FF0000 3. rgb(255,0,0) --&gt; rgba(255,0,0,0)透明度[0-1] 5.边框 1. border-width 宽度 2. border-style 样式 3. border-color 颜色 border 1px solid red; 6.CSS盒子模型 4个部分 1. content 内容的宽度和高度 2. padding 内填充 调整内容和边框的距离 3. border 边框 4. margin 外边距 调整两个标签之间的距离 (注意两个挨着的标签margin取最大值) 5. 注意：要习惯看浏览器console 窗口的那个盒子模型 7.display 标签的展示形式 1. inline 2. block 菜单里的a标签可以设置成block 3. inline-block 4. none 不显示隐藏，但是不占位 8.float 浮动 1.多用于布局效果 1. 顶部的导航条 2. 页面的左右分栏 blog页面 左边20% 右边80% 3. 任何标签都可以浮动，浮动之后都会变成块级标签（可以设置高和宽） 2.float取值: 1. left 2. right 3. none 默认 9.clear 清除浮动的副作用 （内容溢出，父标签撑不起来） 1. 结合伪元素实现 .clearfix:after&#123; content:"", display:"block", clear:both (左右都清除) &#125; 2.clear 取值: 1. left 2. right 3. both 10.overflow 1.标签的内容放不下(溢出) 2.取值: 1. hidden 隐藏 2. scroll 滚轮 3. inherit 继承父标签 4. auto 自动 5. scroll-x 6. scroll-y 例子: 圆形头像例子 1. overflow:hidden 2. border-radius: 50% 边框圆角 11.position 定位 1. static 默认 2. relative 相对定位 # 相对于原来的位置 3. absolute 绝对定位 # 相对于已经定位过的前辈标签 没有就是body 4.fixed 固定 返回顶部按钮实例 补充: 12.脱离文档流的3种方式 1 浮动 float 2 绝对定位 position: absolute 3 固定定位 position: fixed 13.opacity (不透明度) 1. 和agba的区别: 取值0~1 1是原有,0是全透明 opacity 能改变元素 子标签的透明度 比如背景色和文字 rgba 给谁写才能改变 14.z-index 1. 数值越大，越靠近你 2. 只能作用于 定位过的元素标签 3. 自定义模态框示例]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML]]></title>
    <url>%2F2019%2F05%2F15%2Fweb-base%2F</url>
    <content type="text"><![CDATA[HTML 介绍前端有哪些内容1234### 三大基础基础: HTML CSS JavaScript框架: Bootstrap JQuery功能: 内容 外观 动作 HTML 是什么 超文本标记语言（Hypertext Markup Language, HTML）是一种用于创建网页的标记语言。 本质上是浏览器可识别的规则，我们按照规则写网页，浏览器根据规则渲染我们的网页。对于不同的浏览器，对同一个标签可能会有不同的解释。（兼容性问题） 网页文件的扩展名：.html或.htm HTML是一种标记语言（markup language），它不是一种编程语言。 HTML使用标签来描述网页。 思维导图: https://www.processon.com/mindmap/5a2f4ec4e4b0abb143af4a08 Web服务的本质12345678### 浏览器输入网址 回车 都发生了什么# 1. 浏览器发请求 --&gt; HTTP协议# 2. 服务端接收请求 --&gt; 服务端返回响应# 3. 服务端把HTML文件内容发给浏览器# 4. 浏览器接收渲染页面# 5. 客户端与服务端 消息的格式是约定好的 ,这个约定就叫做 HTTP协议# 6. HTTP协议:浏览器和服务器之间约定好的消息格式 123456789101112131415161718192021222324# web serverimport socketsk = socket.socket()sk.bind(('127.0.0.1',8090))sk.listen()while 1: conn,addr = sk.accept() conn.recv(1024) # 收消息 conn.send(b'HTTP:/1.1 200 ok\r\n\r\n') # 按照HTTP协议的格式 发消息 # 直接发送 # conn.send(b'&lt;h1&gt;hello!&lt;/h1&gt;') # 从文件读取,按字节 with open("data.html",'rb') as f: msg = f.read() conn.send(msg) conn.close()sk.close()# 浏览器作为客户端访问: 127.0.0.1:8090 1234# data html&lt;h1&gt; hello leo &lt;/h1&gt;&lt;img src = "https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=3054306616,2519291525&amp;fm=26&amp;gp=0.jpg"&gt;&lt;a href='http://www.sogo.com'&gt;sogo&lt;/a&gt; HTML 文档结构12345678910&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;css样式优先级&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; &lt;!DOCTYPE html&gt; 声明为HTML5文档。 &lt;html&gt;、&lt;/html&gt; 是文档的开始标记和结束的标记。是HTML页面的根元素，在它们之间是文档的头部（head）和主体（body）。 &lt;head&gt;、&lt;/head&gt; 定义了HTML文档的开头部分。它们之间的内容不会在浏览器的文档窗口显示。包含了文档的元（meta）数据。 &lt;title&gt;、&lt;/title&gt; 定义了网页标题，在浏览器标题栏显示。 &lt;body&gt;、&lt;/body&gt; 之间的文本是可见的网页主体内容。 HTML 标签HTML 标签格式之前我们学习到，HTML是一种标记语言(HTML,XML,YTAML),他的作用是使用标签来描述网页。 HTML标签是由尖括号包围的关键字，如&lt;html&gt;, &lt;div&gt;等 HTML标签通常是成对出现的，比如：&lt;div&gt;和&lt;/div&gt;，第一个标签是开始，第二个标签是结束。结束标签会有斜线。 也有一部分标签是单独呈现的，比如：&lt;br/&gt;、&lt;hr/&gt;、&lt;img src=&quot;1.jpg&quot; /&gt;等。 标签里面可以有若干属性，也可以不带属性。 标签的分类1 一般标签与自闭和标签 双标签(一般标签) &lt;h1&gt;&lt;/h1&gt; &lt;a&gt;&lt;/a&gt; 单标签(自闭和标签) &lt;img&gt; &lt;link&gt; 一般标签由于有开始符号和结束符号，可以在标签内部插入其他标签或文字； 自闭合标签由于没有结束符号，没办法在内部插入其他标签或文字，只能定义自身的一些属性。1234567891011121314### 常见的自闭合标签1、&lt;meta/&gt; 定义页面的说明信息，供搜索引擎查看。2、&lt;link/&gt; 用于连接外部的CSS文件或脚本文件。3、&lt;base/&gt; 定义页面所有链接的基础定位。4、&lt;br/&gt; 用于换行。5、&lt;hr/&gt; 水平线。6、&lt;input/&gt; 用于定义表单元素7、&lt;img/&gt; 图像标签。 标签的语法 &lt;标签名 属性1=“属性值1” 属性2=“属性值2”……&gt;内容部分&lt;/标签名&gt; &lt;标签名 属性1=“属性值1” 属性2=“属性值2”…… /&gt; 标签的属性任何标签都有三个常用属性: id：定义标签的唯一ID，HTML文档树中唯一 class：为html元素定义一个或多个类名（classname）(CSS样式类名) style：规定元素的行内样式（CSS样式） HTML注释1&lt;!--注释内容--&gt; HTML 常用标签head 常用标签 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang="zh-CN"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;!--定义网页标题--&gt; &lt;title&gt;我是标题&lt;/title&gt; &lt;!--定义内部样式表--&gt; &lt;style&gt; a &#123; color: red; &#125; &lt;/style&gt; &lt;!--引入外部样式表文件--&gt; &lt;link rel="stylesheet" href="test.css"&gt; &lt;!--定义JS代码或引入外部JS文件--&gt; &lt;script&gt; alert("hello") &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;a href="head_demo.html"&gt;点我跳转&lt;/a&gt; &lt;h1&gt;大家好&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021### Meta标签介绍：&lt;meta&gt;元素可提供有关页面的元信息（mata-information）,针对搜索引擎和更新频度的描述和关键词。&lt;meta&gt;标签位于文档的头部，不包含任何内容。&lt;meta&gt;提供的信息是用户不可见的。meta标签的组成：meta标签共有两个属性，它们分别是http-equiv属性和name 属性，不同的属性又有不同的参数值，这些不同的参数值就实现了不同的网页功能。 ### 1.http-equiv属性：相当于http的文件头作用，它可以向浏览器传回一些有用的信息，以帮助正确地显示网页内容，与之对应的属性值为content，content中的内容其实就是各个参数的变量值。&lt;!--2秒后跳转到对应的网址，注意引号--&gt;&lt;meta http-equiv="refresh" content="2;URL=https://www.oldboyedu.com"&gt;&lt;!--指定文档的编码类型--&gt;&lt;meta http-equiv="content-Type" charset=UTF8"&gt;&lt;!--告诉IE以最高级模式渲染文档--&gt;&lt;meta http-equiv="x-ua-compatible" content="IE=edge"&gt;### 2.name属性: 主要用于描述网页，与之对应的属性值为content，content中的内容主要是便于搜索引擎机器人查找信息和分类信息用的。&lt;meta name="keywords" content="meta总结,html meta,meta属性,meta跳转"&gt;&lt;meta name="description" content="老男孩教育Python学院"&gt; body 常用标签基本标签（块级标签和内联标签）1234567891011121314151617&lt;b&gt;加粗&lt;/b&gt;&lt;i&gt;斜体&lt;/i&gt;&lt;u&gt;下划线&lt;/u&gt;&lt;s&gt;删除&lt;/s&gt;&lt;p&gt;段落标签&lt;/p&gt;&lt;h1&gt;标题1&lt;/h1&gt;&lt;h2&gt;标题2&lt;/h2&gt;&lt;h3&gt;标题3&lt;/h3&gt;&lt;h4&gt;标题4&lt;/h4&gt;&lt;h5&gt;标题5&lt;/h5&gt;&lt;h6&gt;标题6&lt;/h6&gt;&lt;!--换行--&gt;&lt;br&gt;&lt;!--水平线--&gt;&lt;hr&gt; 特殊字符 div 标签和span 标签 div标签用来定义一个块级元素，并无实际的意义。主要通过CSS样式为其赋予不同的表现。 span标签用来定义内联(行内)元素，并无实际的意义。主要通过CSS样式为其赋予不同的表现。 123456&lt;!--div 和 span div占用一行, span在同一行 --&gt;&lt;div&gt;我是div&lt;/div&gt;&lt;div&gt;我是div&lt;/div&gt;&lt;span&gt;我是span&lt;/span&gt;&lt;span&gt;我是span&lt;/span&gt; 标签的分类2 块级标签与内联标签的区别根据长度分类: 块级标签: 默认占浏览器宽度,块级标签可以设置长和宽: 1h1 - h6、div、p、hr、 内联(行内)标签: 根据内容决定长度，不能设置长和宽: 1a、img、u、s、i、b、span 标签的嵌套规则: 行内标签不能嵌套块级标签 p标签不能包含块级标签，p标签也不能包含p标签。 图片 img 向网页中嵌入一幅图像 标签并不会在网页中插入图像，而是从网页上链接图像。 标签有两个必需的属性：src 属性 和 alt 属性。1&lt;img src="图片的路径" alt="图片未加载成功时的提示" title="鼠标悬浮时提示信息" width="宽" height="高(宽高两个属性只用一个会自动等比缩放)"&gt; 超链接 a 超链接标签 超链接是指从一个网页指向一个目标的连接关系，这个目标可以是另一个网页，也可以是相同网页上的不同位置，还可以是一个图片，一个电子邮件地址，一个文件，甚至是一个应用程序。 1&lt;a href="http://www.baidu.com" target="_blank" &gt;点我&lt;/a&gt; 1234567891011### 什么是URL？URL是统一资源定位器(Uniform Resource Locator)的缩写，也被称为网页地址，是因特网上标准的资源的地址。URL举例http://www.sohu.com/stu/intro.htmlURL地址由4部分组成第1部分：为协议：http://、ftp://...等 第2部分：为站点地址：可以是域名或IP地址第3部分：为页面在站点中的目录：stu第4部分：为页面名称，例如 index.html各部分之间用“/”符号隔开。 123456789### href 属性指定目标网页地址。该地址可以有几种类型：1、绝对URL - 指向另一个站点（比如 href="http://www.jd.com）2、相对URL - 指当前站点中确切的路径（href="index.htm"）3、锚URL - 指向页面中的锚（href="#top"）&lt;!--锚URL - 指向页面中的锚（href="#top"）--&gt;&lt;a href="#a2"&gt;a1跳a2&lt;/a&gt;&lt;div style="height: 1000px;background: red"&gt;&lt;/div&gt;&lt;a href="" id="a2"&gt;哈哈哈&lt;/a&gt; 123### target：_blank表示在新标签页中打开目标网页_self表示在当前标签页中打开目标网页 列表 无序列表: ui 有序列表: oi 标题列表: dl 1234567891011### 无序列表: ui&lt;ul type="circle"&gt; &lt;li&gt;第一项&lt;/li&gt; &lt;li&gt;第二项&lt;/li&gt;&lt;/ul&gt;### type属性：disc（实心圆点，默认值）circle（空心圆圈）square（实心方块）none（无样式） 123456789101112### 有序列表: oi&lt;ol&gt; &lt;li&gt;第一项&lt;/li&gt; &lt;li&gt;第二项&lt;/li&gt;&lt;/ol&gt;### type属性：1 数字列表，默认值A 大写字母a 小写字母Ⅰ大写罗马ⅰ小写罗马 12345678### 标题列表: dl&lt;dl&gt; &lt;dt&gt;标题1&lt;/dt&gt; &lt;dd&gt;内容1&lt;/dd&gt; &lt;dt&gt;标题2&lt;/dt&gt; &lt;dd&gt;内容2&lt;/dd&gt; &lt;dd&gt;内容2&lt;/dd&gt;&lt;/dl&gt; 表格 table 表格是一个二维数据空间，一个表格由若干行组成，一个行又有若干单元格组成，单元格里可以包含文字、列表、图案、表单、数字符号、预置文本和其它的表格等内容。 表格最重要的目的是显示表格类数据。表格类数据是指最适合组织为表格格式（即按行和列组织）的数据。 12345678910111213141516171819202122232425262728293031323334### 表格的基本结构&lt;table border="1" cellpadding="10" cellspacing="10"&gt; &lt;!-- 表头开始 --&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;爱好&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;!-- 表内数据开始 --&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td colspan="2"&gt;leo&lt;/td&gt; &lt;!--&lt;td&gt;28&lt;/td&gt;--&gt; &lt;td rowspan="2"&gt;游戏&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;lex&lt;/td&gt; &lt;td&gt;28&lt;/td&gt; &lt;!--&lt;td&gt;开车&lt;/td&gt;--&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;### 属性:border: 表格边框.cellpadding: 内边距cellspacing: 外边距.width: 像素 百分比.（最好通过css来设置长宽）rowspan: 单元格竖跨多少行colspan: 单元格横跨多少列（即合并单元格） 快捷语句123456781、快速格式化 : Code --&gt; reformat code2、快速建立4个标签 : h1*4 --&gt;点击tab3、快速建立标签并增加属性 : div*4&gt;a.c1&#123;a标签$&#125;### div*4&gt;a.c1&#123;a标签$&#125; == 快速建立4个div标签，里面添加a标签,class='c1',内容为&#123;a标签$&#125;,&#123;$&#125;表示从1开始的序号 &lt;div&gt;&lt;a href="" class="c1"&gt;a标签1&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;a href="" class="c1"&gt;a标签2&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;a href="" class="c1"&gt;a标签3&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;a href="" class="c1"&gt;a标签4&lt;/a&gt;&lt;/div&gt; Form 表单 form 表单的作用：表单用于收集用户的输入信息，然后向服务器传输数据，从而实现用户与Web服务器的交互 表单能够包含input系列标签，如文本字段、复选框、单选框、提交按钮等等，表单还可以包含textarea、select、fieldset和 label标签。 表单属性 123456action: 表单提交到哪.一般指向服务器端一个程序,程序接收到表单提交过来的数据（即表单元素值）作相应处理，比如https://www.sogou.com/webmethod: 表单的提交方式 post/get，默认取值就是get# 关于表单提交点击sumbit提交后，会将数据组装成字典格式—键值对如果是：与查询相关的内容，使用get提交方法，数据在地址栏用？隔开如果是：服务器修改相关的内容用post请求 数据在请求体里 http协议中可见 Djaogo接收表单数据代码12345678910111213141516171819from django.conf.urls import urlfrom django.shortcuts import HttpResponsedef upload(request): print('request.GET:',request.GET) print("request.POST:", request.POST) if request.FILES: filename = request.FILES['avatar'].name with open(filename,'wb') as f: for chunk in request.FILES['avatar'].chunks(): f.write(chunk) return HttpResponse("上传成功") return HttpResponse('收到了')urlpatterns = [ url(r'^upload/',upload)] input系列标签 text 文本框12345678910&lt;p&gt; &lt;label for="username"&gt;用户名:&lt;/label&gt; &lt;input type="text" id="username" name="username" placeholder="请输入用户名" value="默认值" disabled&gt;&lt;/p&gt;# 属性说明: 1、disabled: 禁用2、placeholder: 占位符3、value: 默认值4、request.GET: &lt;QueryDict: &#123;'username': ['leo']&#125;&gt; password 密码输入框12345678&lt;p&gt; &lt;label for="password"&gt;密码:&lt;/label&gt; &lt;input type="password" id="password" name="password" placeholder="请输入密码" &gt;&lt;/p&gt;# 属性说明:1、基本与text一致2、request.GET: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123']&#125;&gt; date 日期1234567&lt;p&gt; &lt;label for="birthday"&gt;生日:&lt;/label&gt; &lt;input type="date" id="birthday" name="birthday"&gt;&lt;/p&gt;# 属性说明:1、request.GET: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20']&#125;&gt; radio 单选按钮123456789101112&lt;p&gt; &lt;label&gt;性别:&lt;/label&gt; &lt;input type="radio" id="r1" name="gender" value="男"&gt; &lt;label for="r1"&gt;男&lt;/label&gt; &lt;input type="radio" id="r2" name="gender" value="女"&gt; &lt;label for="r2"&gt;女&lt;/label&gt;&lt;/p&gt;# 属性说明:1、checked: 默认选中2、value:选择相关联的值 3、request.GET: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20'], 'gender': ['男']&#125;&gt; checkbox 多选按钮1234567891011121314&lt;p&gt; &lt;label&gt;爱好:&lt;/label&gt; &lt;input type="checkbox" name="hobby" value="football" checked&gt;足球 &lt;input type="checkbox" name="hobby" value="basketball"&gt;篮球 &lt;input type="checkbox" name="hobby" value="doubleball"&gt;双色球&lt;/p&gt;# 属性说明1、key=name: 多选标签的name必须一致，"hobby":[basketball,doubleball]2、值=value: value值必须设置3、value:选择相关联的值 4、checked: 默认选中5、request.GET: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20'], 'gender': ['男'], 'hobby': ['basketball', 'doubleball']&#125;&gt; submit 提交按钮12345678&lt;p&gt; &lt;input type="submit" value="提交数据"&gt; &lt;input type="reset" value="重置数据"&gt;&lt;/p&gt;# 属性说明1、value:按钮上显示的文本内容2、点击sumbit会将表单中的数据组成字典(键值对)，并向服务器提交数据 reset 撤销按钮1234567&lt;p&gt; &lt;input type="submit" value="提交数据"&gt; &lt;input type="reset" value="重置数据"&gt;&lt;/p&gt;# 属性说明1、value:按钮上显示的文本内容2、点击reset撤销按钮，会将输入的内容数据重置 button 普通按钮123456&lt;p&gt; &lt;input type="button" value="普通事件按钮" onclick="alert(123)"&gt;&lt;/p&gt;# 属性说明1、通常用于JS绑定事件2、value: 按钮显示文本 hidden 隐藏标签123456&lt;p&gt; &lt;input type="hidden" name="user_id" value="leo_id"&gt;&lt;/p&gt;# 属性说明:1、每个页面中存储的用户信息比如隐藏身份证2、key=name, 值=value file 上传标签上传文件注意两点： 请求方式必须是: post form标签增加: enctype=”multipart/form-data” 1234567891011&lt;form action="http://127.0.0.1:8000/upload/" method="post" enctype="multipart/form-data"&gt; &lt;p&gt; &lt;input type="file" name="avatar" &gt; &lt;/p&gt;&lt;/form&gt;# 属性说明1、key="name",值="上传文件对象"2、request.POST: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20'], 'gender': ['男'], 'hobby': ['football', 'basketball'], 'leo': ['leo_id']&#125;&gt;3、上传的附件会根据程序代码存储到指定目录 input表单标签属性总结 name: 表单提交时的“键”，注意和id的区别 value: 表单提交时对应项的值: type=”button”, “reset”, “submit”时，为按钮上显示的文本内容 type=”text”,”password”,”hidden”时，为输入框的初始值 type=”checkbox”, “radio”, “file”，为输入相关联的值 checked: radio和checkbox默认被选中的项 readonly: text和password设置只读 disabled: 所有input均适用 其他常用标签label 标记定义： 标签为 input 元素定义标注（标记）。1234# 属性说明:1、label 元素不会向用户呈现任何特殊效果。2、&lt;label&gt; 标签的 for 属性值应当与相关元素的 id 属性值相同。3、点击label内的内容 可以进入text文本框的光标 用for关联text的id select 下拉菜单123456789101112131415161718# 普通下拉框&lt;p&gt; &lt;label for="city"&gt;城市:&lt;/label&gt; &lt;select name="city" id="city"&gt; &lt;option value="beijing"&gt;北京&lt;/option&gt; &lt;option value="shanghai"&gt;上海&lt;/option&gt; &lt;option value="hongkong" selected&gt;香港&lt;/option&gt; &lt;/select&gt;&lt;/p&gt;# 属性说明:1、select 标签中的name为键2、option 标签中的value为值，一般情况下都不会使用中文,常用的地市会使用区号 010 = 北京3、selected="selected" 默认选中4、multiple="multiple" 为多选5、optgroup 分组的下拉框6、request.POST: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20'], 'gender': ['男'], 'hobby': ['football', 'basketball'], 'city': ['beijing'], 'area': ['xc'], 'info': ['哈哈哈'], 'leo': ['leo_id']&#125;&gt; 12345678910111213141516# 分组下拉框&lt;p&gt; &lt;label for="area"&gt;区域&lt;/label&gt; &lt;select name="area" id="area"&gt; &lt;optgroup label="北京"&gt; &lt;option value="xc" selected&gt;西城&lt;/option&gt; &lt;option value="dc"&gt;东城&lt;/option&gt; &lt;option value="sjs"&gt;石景山&lt;/option&gt; &lt;/optgroup&gt; &lt;optgroup label="上海"&gt; &lt;option value="pd"&gt;浦东新区&lt;/option&gt; &lt;option value="hp"&gt;黄浦区&lt;/option&gt; &lt;/optgroup&gt; &lt;/select&gt;&lt;/p&gt; textarea 多行文本1234567891011&lt;p&gt; &lt;label for="info"&gt;自我介绍&lt;/label&gt; &lt;textarea id="info" name="info" cols="30" rows="10" placeholder="不少于100字"&gt;&lt;/textarea&gt;&lt;/p&gt;# 属性说明1、key=name,值=用户输入2、rows：行数3、cols：列数4、request.POST: &lt;QueryDict: &#123;'username': ['leo'], 'password': ['123456'], 'birthday': ['2019-05-20'], 、'gender': ['男'], 'hobby': ['football', 'basketball'], 'city': ['beijing'], 'info': ['哈哈哈'], 'leo': ['leo_id']&#125;&gt; form表单提交数据的几个注意事项 所有获取或用户输入的标签，都必须放在form表单里面 action 控制着往哪提交 input\select\textarea 都需要有name属性 提交按钮 input type=’sumbit’ file文件 需要post提交方式和enctype=”multipart/form-data”]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发爬虫]]></title>
    <url>%2F2019%2F05%2F10%2Fconcurrent-crawl%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO 模型]]></title>
    <url>%2F2019%2F05%2F08%2Fio-model%2F</url>
    <content type="text"><![CDATA[IO模型介绍Stevens在文章中一共比较了五种IO Model：12345# 1、blocking IO 阻塞IO# 2、nonblocking IO 非阻塞IO# 3、IO multiplexing IO多路复用# 4、signal driven IO 信号驱动IO# 5、asynchronous IO 异步IO 由于signal driven IO（信号驱动IO）在实际中并不常用，所以主要介绍其余四种IO Model IO发生时涉及的对象和步骤:对于一个network IO (这里我们以read举例)，它会涉及到 两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，该操作会经历两个阶段： 12#1）等待数据准备 (Waiting for the data to be ready)#2）将数据从内核拷贝到进程中(Copying the data from the kernel to the process) 阻塞 IO(blocking IO) 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。kernel要等待足够的数据到来,用户进程这边，整个进程会被阻塞。 有数据到来后，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段（等待数据和拷贝数据两个阶段）都被block了。 几乎所有的IO接口 ( 包括socket接口 ) 都是阻塞型的。这给网络编程带来了一个很大的问题，如在调用recv(1024)的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。123# 使用多线程或多进程解决阻塞？# 在服务器端使用多线程（或多进程）。# 多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。 123# 多线程或多进程的问题:# 开启多进程多线程的方式，在遇到要同时响应成百上千路的连接请求，# 则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率，而且线程与进程本身也更容易进入假死状态。 1234# 使用线程池或进程池的问题:# 线程池”和“连接池”技术也只是在一定程度上缓解了频繁调用IO接口带来的资源占用。# 而且，所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。# 所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。 对应上例中的所面临的可能同时出现的上千甚至上万次的客户端请求，“线程池”或“连接池”或许可以缓解部分压力，但是不能解决所有问题。总之，多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求，多线程模型也会遇到瓶颈，可以用非阻塞接口来尝试解决这个问题。 非阻塞 IO(non-blocking IO) 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。 用户进程判断结果是一个error时，它就知道数据还没有准备好，于是用户就可以在本次到下次再发起read询问的时间间隔内做其他事情，或者直接再次发送read操作。 一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存（这一阶段仍然是阻塞的），然后返回。 所以，在非阻塞式IO中，用户进程其实是需要不断的主动询问kernel数据准备好了没有。 但是非阻塞IO模型绝不被推荐。12345#1. 循环调用recv()将大幅度推高CPU占用率；这也是我们在代码中留一句time.sleep(2)的原因,否则在低配主机下极容易出现卡机情况#2. 任务完成的响应延迟增大了，因为每过一段时间才去轮询一次read操作，而任务可能在两次轮询之间的任意时间完成。# 这会导致整体数据吞吐量的降低。#3. recv()更多的是起到检测“操作是否完成”的作用，实际操作系统提供了更为高效的检测“操作是否完成“作用的接口，# 例如select()多路复用模式，可以一次检测多个连接是否活跃。 多路复用 IO(IO multiplexing)IO multiplexing这个词可能有点陌生，但是如果我说select/epoll，大概就都能明白了。有些地方也称这种IO方式为事件驱动IO(event driven IO)。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 在多路复用模型中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。结论: select的优势在于可以处理多个连接，不适用于单个连接 123456789101112131415161718192021222324252627282930313233343536373839404142434445# IO 多路复用# 代理帮忙去查看链接和接收发送消息,代理可以监听多个对象# windows 拥有select模块import selectimport socketsk = socket.socket()sk.bind(('127.0.0.1',8000))sk.setblocking(False) # 非阻塞模式sk.listen()read_lst = [sk] # 监听谁就放谁# print(ret) # ([&lt;socket.socket fd=204, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 8000)&gt;], [], [])# print(r_lst) # [&lt;socket.socket fd=204, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 8000)&gt;]# print(sk) # &lt;socket.socket fd=204, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 8000)&gt;# r_lst 里面存着的 sk对象while True: # 当有链接来连，获取了socket对象并链接 # 将对象再放入列表中继续监听 [sk,conn] r_lst, w_lst, x_lst = select.select(read_lst, [], []) # r_list和其他空列表 # print('******',r_lst) for i in r_lst: if i is sk: conn,addr = i.accept() # print(conn) # &lt;socket.socket fd=200, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 8000), raddr=('127.0.0.1', 61832)&gt; read_lst.append(conn) else: # 否则是conn ret = i.recv(1024) if ret == b'': i.close() read_lst.remove(i) continue print(ret) i.send(b'bye')# IO多路复用 # select Windows Linux 都是操作系统轮询每一个被监听的项，看是否有读才做 # poll Linux 可以监听的对象比select对象多 # 随着监听项的增多，导致效率降低 # epoll Linux 给每一个监听对象都绑定一个回调函数 # selectors 可以自动帮我们选适合操作系统的模块 12345678910111213141516# clientimport socketimport timeimport threadingdef func(): sk = socket.socket() sk.connect(('127.0.0.1',8000)) sk.send(b'hello') time.sleep(3) print(sk.recv(1024)) sk.close()for i in range(20): threading.Thread(target=func).start() 异步IO (Asynchronous I/O) 12345678910111213141516### 需要记住# 同步 # 提交一个任务之后，要等待这个任务执行完毕# 异步 # 只管提交任务，不等待这个任务执行完毕，就可以做其他事情# 阻塞 # recv、recvfrom、accept # 非阻塞 # 都可以正常执行# 阻塞和非阻塞 # 线程遇到阻塞 ： 运行 --&gt; 阻塞 --&gt; 就绪 --&gt; 运行 # 线程遇到非阻塞：运行 &lt;--&gt; 就绪 123456789### 需要理解# 1、异步IO 没有数据 直接返回# 2、操作系统会告诉我有新数据进来，并直接将数据交给用户# 异步IO 没有wait data 和 copy data# 其他模型只是对wait data 进行优化# 异步IO 都是操作系统来完成 wait data 和 copy data，Python并不能完成，C语言可以直接实现# Python的异步模块和框架 很多都是用底层的C语言实现 框架包括Twisted和Tornado# 异步框架 可以相应更多的需求 IO模型比较分析12# blocking和non-blocking的区别# 调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回 selectors 模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#服务端from socket import *import selectorssel=selectors.DefaultSelector()def accept(server_fileobj,mask): conn,addr=server_fileobj.accept() # accept 建立后 注册conn sel.register(conn,selectors.EVENT_READ,read) # 监听conn的读时间 , 回调函数 readdef read(conn,mask): try: data=conn.recv(1024) if not data: print('closing',conn) sel.unregister(conn) conn.close() return conn.send(data.upper()+b'_SB') except Exception: print('closing', conn) sel.unregister(conn) conn.close()# 建立socket对象sk=socket(AF_INET,SOCK_STREAM) # socket对象sk.setsockopt(SOL_SOCKET,SO_REUSEADDR,1)sk.bind(('127.0.0.1',8088))sk.listen(5)sk.setblocking(False) #设置socket的接口为非阻塞# 选择适合的IO多路复用机制# sel=selectors.DefaultSelector() selectors模块sel.register(sk,selectors.EVENT_READ,accept)# selectors.EVENT_READ 监听的是一个读事件# 相当于网select的读列表里append了一个socket对象,并且绑定了一个回调函数 accept# 有人请求链接socket了 就调用 accept方法 accept(server_fileobj,mask)while True: events=sel.select() # 检测所有的fileobj(socket|conn)，是否有完成wait data for sel_obj,mask in events: callback=sel_obj.data # callback=accpet | read callback(sel_obj.fileobj,mask) # accpet(server_fileobj,1) | read(conn,l)#客户端from socket import *c=socket(AF_INET,SOCK_STREAM)c.connect(('127.0.0.1',8088))while True: msg=input('&gt;&gt;: ') if not msg:continue c.send(msg.encode('utf-8')) data=c.recv(1024) print(data.decode('utf-8'))# selectors 模块 可以帮助我们选择适合操作系统的IO多路复用模块# 非阻塞IO一直在询问，会耗费CPU# 1、异步IO 没有数据 直接返回# 2、操作系统会告诉我有新数据进来，并直接将数据交给用户]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>IO多路复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2019%2F04%2F30%2Fcoroutines%2F</url>
    <content type="text"><![CDATA[什么是协程 在操作系统中进程是资源分配的最小单位,线程是CPU调度的最小单位。 无论是创建多进程还是创建多线程来解决问题，都要消耗一定的时间来创建进程、创建线程、以及管理他们之间的切换。 基于单线程来实现并发又成为一个新的课题，即只用一个主线程（很明显可利用的cpu只有一个）情况下实现并发。这样就可以节省创建线进程所消耗的时间。 并发的本质：切换+保存状态cpu正在运行一个任务，会在两种情况下切走去执行其他的任务（切换由操作系统强制控制），一种情况是该任务发生了阻塞，另外一种情况是该任务计算的时间过长。 协程的本质就是在单线程下，由用户自己控制一个任务遇到io阻塞了就切换另外一个任务去执行，以此来提升效率。为了实现它，我们需要找寻一种可以同时满足以下条件的解决方案： 可以控制多个任务之间的切换，切换之前将任务的状态保存下来，以便重新运行时，可以基于暂停的位置继续执行。 作为1的补充：可以检测io操作，在遇到io操作的情况下才发生切换 实现在多个任务之间切换 yield + send yield无法做到遇到io阻塞12345678910111213141516171819202122232425262728293031323334353637383940# 进程 : 启动多个进程,进程之间是由操作系统负责调用# 线程 : 启动多个线程,真正被CPU执行的最小单位实际是线程 # 开启一个线程 ：创建1个线程,属于线程的内存开销 寄存器和堆栈 # 关闭一个线程 # 线程在CPython下，由于全局GLI锁，多线程没有办法同时访问CPU，真正工作只有一个CPU# 协程 : 本质上是一个线程 # 在多个任务之间切换，来节省IO时间 # 无需切换寄存器 和 堆栈，只是正常在程序之间切换 # ***** 能在多个任务之间切换 # ***** 协程中任务之间的切换也消耗时间，但是开销，要远远小于进程线程之间的切换。# 实现并发的手段### 实现在多个任务之间切换 yield + sendimport timedef consumer(): # print(111111) while True: x = yield time.sleep(1) print('处理了数据: ' ,x)def producer(): c = consumer() # 生成器 next(c) # 激活生成器,send之前必须next for i in range(10): time.sleep(1) print('生产了数据 %s ' %i) c.send(i)producer() # 在producer控制了consumer函数，并且来回切换 即-在两个任务中切换c = consumer() # 生成器print(c) # &lt;generator object consumer at 0x00000000021C84C0&gt;c.__next__()next(c)c.send(100)c.send(200) greenlet 模块切换执行程序12345678910111213141516171819202122232425262728293031323334353637from greenlet import greenlet# pip3 install greenlet# pip3 install gevent# greenlet 学习一下 后续不用# 协程模块# 切换执行程序# 真真干的协程模块，就是使用greenlet完成的切换def eat(): print('eating start') g2.switch() # 第1次切换 在这里switch(切换)，不仅切换还会记录执行的位置 print('eating end') g2.switch()def play(): print('playing start') g1.switch() print('playing end')# 把执行的方法 交给greenletg1 = greenlet(eat)g2 = greenlet(play)g1.switch() # 首先让g1执行# 结果: 出现switch即切换程序# eating start# playing start# eating end# laying end# 进程 4c+1 = 5# 线程 4c*5 = 20# 协程 每个线程里最多可以启动500个协程# 并发 1个进程20个线程 5个进程 100个线程 100个线程 = 50000W个协程# nginx 分发任务,最大承载量 5W# 在IO等待的时候 切换执行其他任务 使用gevent模块实现协程 安装：pip3 install gevent Gevent 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程，在gevent中用到的主要模式是Greenlet, 它是以C扩展模块形式接入Python的轻量级协程。 Greenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度。 123456# g1=gevent.spawn(func,1,,2,3,x=4,y=5)创建一个协程对象g1，spawn括号内第一个参数是函数名，如eat，后面可以有多个参数，可以是位置实参或关键字实参，都是传给函数eat的# g2=gevent.spawn(func2)# g1.join() #等待g1结束# g2.join() #等待g2结束# 或者上述两步合作一步：gevent.joinall([g1,g2])# g1.value#拿到func1的返回值 使用gevent实现IO切换123456789101112131415161718192021222324252627282930313233343536373839from gevent import monkey;monkey.patch_all()# 会把后面所有的阻塞操作都打成包，就会认识到time.sleepimport timeimport geventimport threading# 遇到IO切换def eat(): print(threading.current_thread().getName()) # DummyThread-1 仿制品线程 虚拟线程 -- 协程 print('eating start') time.sleep(1) # gevent.sleep(1) print('eating end')def play(): print(threading.current_thread().getName()) print('playing start') time.sleep(1) # gevent.sleep(1) print('playing end')g1 = gevent.spawn(eat) # gevent 遇到IO会自动切换g2 = gevent.spawn(play)g1.join() # 等待g1执行结束g2.join() # 等待g2执行结束,不然则异步# 或者上述两步合作一步：gevent.joinall([g1,g2])# 结果# eating start# playing start 同时开始# 同时睡一秒# eating end 同时结束# playing end# 总结：# 1、进程和线程的任务切换 由操作系统完成，而协程任务切换由程序代码完成,只有遇到协程模块能识别的IO操作的时候，程序才会执行任务切换# 实现并发效果。# 2、协程是通过greenlet模块的switch()方法控制切换 gevent的同步和异步1234567891011121314151617181920212223242526272829# gevent.sleep(2)模拟的是gevent可以识别的io阻塞,而time.sleep(2)或其他的阻塞,# gevent是不能直接识别的需要用下面一行代码,打补丁,就可以识别了# from gevent import monkey;monkey.patch_all()必须放到被打补丁者的前面，如time，socket模块之前# 或者我们干脆记忆成：要用gevent，需要将from gevent import monkey;monkey.patch_all()放到文件的开头from gevent import monkey;monkey.patch_all()import timeimport geventdef task(): time.sleep(1) # 模拟IO阻塞 print(12345)def sync(): # synchronous = sync 同步 for i in range(10): task()def async(): # 异步 g_lst = [] for i in range(10): g = gevent.spawn(task) g_lst.append(g) # for g in g_lst:g.join() # 等待所有的协程结束 gevent.joinall(g_lst) # 接收可迭代对象 每个g都执行完join# sync() # 同步 睡一秒执行一个async() # 异步 都睡一秒一起执行# 协程 适合用于网络操作，比如爬虫 协程爬虫小例子123456789101112131415161718192021222324252627282930313233343536373839404142434445# 协程：能够在线程中实现并发效果 # 能够规避一些在任务中遇到的IO操作 # 在任务的执行过程中,检测到IO就切换到其他任务### 协程 — 爬虫例子# 正则基础# 请求过程中的IO等待from gevent import monkey;monkey.patch_all()import requestsimport geventurl = 'https://maoyan.com/board/4'# res = requests.get(url)# print(len(res.content.decode('utf-8')))# print(len(res.text))def get_url(url): headers = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) ' 'Chrome/71.0.3578.98 Safari/537.36' &#125; res = requests.get(url,headers=headers) html = res.content.decode('utf-8') return len(html)# ret = get_url('http://www.baidu.com')g1 = gevent.spawn(get_url,'http://www.tianmao.com')g2 = gevent.spawn(get_url,'http://www.taobao.com')g3 = gevent.spawn(get_url,'http://www.jd.com')g4 = gevent.spawn(get_url,'https://maoyan.com/board/4')gevent.joinall([g1,g2,g3,g4])print(g1.value)print(g2.value)print(g3.value)print(g4.value)# 总结：# 为什么用到协程 # 多线程在CPython解释器下，存在GIL锁，无法真正的使用多个CPU，多个线程之间的切换就会浪费时间 # 可以把一个线程的作用发挥到极致 1个线程可以开启500个协程,提高了CPU的利用率 # 协程相比于多线程的优势 ，切换的效率更快]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程]]></title>
    <url>%2F2019%2F04%2F25%2Fthread%2F</url>
    <content type="text"><![CDATA[线程进程内容回顾： 程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就称之为进程。 程序和进程的区别就在于：程序是指令的集合，它是进程运行的静态描述文本；进程是程序的一次执行活动，属于动态概念。 在多道编程中，我们允许多个程序同时加载到内存中，在操作系统的调度下，可以实现并发地执行。这是这样的设计，大大提高了CPU的利用率。 进程的出现让每个用户感觉到自己独享CPU，因此，进程就是为了在CPU上实现多道编程而提出的。 进程的出现，可以让一台服务器同时处理多个任务,在多个任务之间来回切换并记录任务执行状态。 进程的缺陷： 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。 线程的出现：随着计算机技术的发展，进程出现了很多弊端，一是由于进程是资源拥有者，创建、撤消与切换存在较大的时空开销，因此需要引入轻型进程；二是由于对称多处理机（SMP）出现，可以满足多个运行单位，而多个进程并行开销过大。因此在80年代，出现了能独立运行的基本单位——线程（Threads）。 注意: 进程是资源分配的最小单位,线程是CPU调度的最小单位. 每一个进程中至少有一个线程。 进程和线程的关系 地址空间和其它资源（如打开文件）：进程间相互独立，同一进程的各线程间共享。某进程内的线程在其它进程不可见。 通信：进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。 调度和切换：线程上下文切换比进程上下文切换要快得多。 在多线程操作系统中，进程不是一个可执行的实体。 线程的特点在多线程的操作系统中，通常是在一个进程中包括多个线程，每个线程都是作为利用CPU的基本单位，是花费最小开销的实体。线程具有以下属性。 轻型实体。存储的较少线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。 独立调度和分派的基本单位。 真正被操作系统调度的是线程在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 共享进程资源。 进程的数据在多线程中使用不用ipc，而是直接使用线程在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的进程id，这意味着，线程可以访问该进程的每一个内存资源；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 可并发执行。 比如6个线程可以运行不同的代码在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 内存中的线程 多个线程共享同一个进程的地址空间中的资源，是对一台计算机上多个进程的模拟，有时也称线程为轻量级的进程。而对一台计算机上多个进程，则共享物理内存、磁盘、打印机等其他物理资源。多线程的运行也多进程的运行类似，是cpu在多个线程之间的快速切换。 不同的进程之间是充满敌意的，彼此是抢占、竞争cpu的关系，如果迅雷会和QQ抢资源。而同一个进程是由一个程序员的程序创建，所以同一进程内的线程是合作关系，一个线程可以访问另外一个线程的内存地址，大家都是共享的，一个线程干死了另外一个线程的内存，那纯属程序员脑子有问题。 类似于进程，每个线程也有自己的堆栈，不同于进程，线程库无法利用时钟中断强制线程让出CPU，可以调用thread_yield运行线程自动放弃cpu，让另外一个线程运行。 线程通常是有益的，但是带来了不小程序设计难度，线程的问题是： 父进程有多个线程，那么开启的子线程是否需要同样多的线程 在同一个进程中，如果一个线程关闭了文件，而另外一个线程正准备往该文件内写内容呢？因此，在多线程的代码中，需要更多的心思来设计程序的逻辑、保护程序的数据。 开启多线程通过threading模块开启多线程123456789101112131415# Python多线程模块# thread、threading和Queue 推荐使用threading# 线程的创建Threading.Thread类import timefrom threading import Thread# 多线程并发def func(n): time.sleep(1) # 10次子线程一起执行 print(n) # 并发for i in range(10): t = Thread(target=func,args=(i,)) t.start() 通过继承threading类 开启多线程1234567891011121314import timefrom threading import Threadclass MyTread(Thread): def __init__(self,arg): super().__init__() self.arg = arg def run(self): time.sleep(1) print(self.arg)for i in range(10): t = MyTread(10) t.start() 多线程与多进程12345678910111213141516import osimport timefrom threading import Threaddef func(a,b): n = a+b print(n,os.getpid())print('主线程',os.getpid())for i in range(10): t = Thread(target=func,args=(i,5)) t.start()# 主线程和子线程的pid是一致的，说明子线程都是在一个进程里执行# 主进程中 存储 导入的模块，文件所在的位置，内置的函数，代码# 主线程存储循环的i 和 t# 子线程拥有栈，里面存储着 add a 和 b n = 0+5 内存数据共享12345678910111213141516171819# 内存数据共享import osimport timefrom threading import Threaddef func(a,b): global g g = 0 # g = 0 + a print(g,os.getpid())g = 100t_lst = []print('主线程',os.getpid())for i in range(10): t = Thread(target=func,args=(i,5)) t.start() t_lst.append(t)for t in t_lst:t.join()print(g) 12345678910111213141516171819202122232425262728293031# 总结:# 进程 是 最小的内存分配单位# 线程 是 操作系统调度的最小单位# 真正被执行的是线程，线程被CPU执行# 进程内至少含有一个线程# 进程中可以开启多个线程 # 开启一个线程 所需要的时间 要远远小于开启一个进程 # 线程占用的内存空间 也更小 # 多个线程内部有自己的数据栈，这个数据是不共享的 # 全局变量在多个线程之间是共享的# 在CPython解释器下的Python程序 # 在同一时刻，多个线程中只能有一个线程 被 CPU执行 # 高CPU：计算类 --- 高CPU利用率 执行效率不高 # 高IO: # 爬取网页 # 读写文件 处理日志文件 # 网络请求 处理web请求 # 读数据库 写数据库 # 非要处理计算类，就使用多进程 进程与进程没有锁 # 4个CPU# 不同的CPU可以执行多条线程，可能会造成数据的混乱# 对同一个数据的加减操作# CPython解释器 为了避免 存在 一种锁 GLP 全局解释器锁# 任何一个线程 想要给CPU 必须拿到钥匙，只有拿到钥匙的线程才能交给CPU执行# 重点：# 1 同一时刻，只能有一个线程，访问CPU，# 2 锁的是什么？锁的是线程# 3 锁的缺点: 多线程不能充分的利用CPU，同一时间只能用到一个C# 4 GLP是Python语言的问题么？ 不是，是CPython解释器的特性，JPython没有这个锁 多线程与多进程执行效率对比123456789101112131415161718192021222324252627282930import timefrom threading import Threadfrom multiprocessing import Processdef func(n): n + 1 # 计算操作 占用cpuif __name__ == '__main__': start = time.time() t_lst = [] for i in range(100): t = Thread(target=func,args=(i,)) t.start() t_lst.append(t) for t in t_lst:t.join() t1 = time.time() - start start = time.time() p_lst = [] for i in range(100): p = Process(target=func,args=(i,)) p.start() p_lst.append(t) for p in p_lst:p.join() t2 = time.time() - start print('线程执行',t1) # 0.009000539779663086 print('进程执行',t2) # 1.8071033954620361 # 对于高IO 和 简单的高CPU计算任务时 , 多线程的开销更少，执行效率更高 线程模块中的其他方法123456789101112131415import threadingimport timedef wahaha(n): time.sleep(0.5) print(n,threading.current_thread(),threading.get_ident()) # 子线程 名字 PIDfor i in range(10): threading.Thread(target=wahaha,args=(1,)).start()print(threading.active_count()) # 查看当前所有的活跃的线程数 # 11 (10个子线程+1个主线程)print(threading.current_thread()) # 主线程 名字 PIDprint(threading.enumerate()) # 所有的进程名字 pid 存在一个列表返回print(len(threading.enumerate())) # 11 多线程实现简单的socket服务123456789101112131415161718192021# serverimport socketfrom threading import Threaddef chat(conn): conn.send(b'hello') msg = conn.recv(1024).decode('utf-8') print(msg) conn.close()s = socket.socket()s.bind(('127.0.0.1',8090))s.listen()while True: conn,addr = s.accept() # 获取连接 t = Thread(target=chat,args=(conn,)) t.start()s.close() 1234567891011# clientimport sockets = socket.socket()s.connect(('127.0.0.1',8090))msg = s.recv(1024).decode('utf-8')print(msg)inp = input('&gt;&gt;&gt;').encode('utf-8')s.send(inp)s.close() 守护线程 daemon=True123456789101112131415161718192021222324252627282930313233343536373839404142434445import timefrom threading import Threaddef func1(): while True: print(10 * '*') time.sleep(1)def func2(): print('in func2') time.sleep(5)t = Thread(target=func1,)t.daemon = Truet.start()t2 = Thread(target=func2,)t2.start()t2.join() # 所有的t2结束后打印在执行主进程代码# 守护进程 随着主进程代码的执行结束 而结束# 守护线程 会在主线程结束之后，等待其他子线程的结束而结束# 主进程 在执行完自己的代码之后 不会立即结束，而是等待子进程结束之后 回收子进程的资源import timefrom multiprocessing import Processdef func1(): while True: print(10 * '*') time.sleep(1)def func2(): print('in func2') time.sleep(5)if __name__ == '__main__': p = Process(target=func1, ) p.daemon = True # 守护线程 主线程结束，守护线程随之结束 p.start() p2 = Process(target=func2, ) p2.start() print('主进程') 线程锁 Lock123456789101112131415161718192021222324252627282930import timefrom threading import Lock,Threaddef func(lock): global n lock.acquire() temp = n time.sleep(0.2) n = temp - 1 lock.release()n = 10t_lst = []lock = Lock()for i in range(10): t = Thread(target=func,args=(lock,)) t.start() t_lst.append(t)for i in t_lst:t.join() # 执行完所有子线程print(n)# 10个线程分别 n - 1# 1、10个线程同时执行 都去拿 n = 10# 2、都sleep 0.2秒,同时减1# 3、同时赋值回去 大家赋值的都是9# 4、为什么有了GIL锁 还是会出现混乱 : 拿到数据后 刚好时间片到了会释放锁 ，就会拿到同一数据# 5、GIL锁加给线程，为了避免多个线程同一时间对一个数据操作，但是无法避免时间片的轮转对数据的不安全性# 6、多线程里还需要用到锁# 7、加锁，牺牲了效率，保证了数据安全 递归锁解决死锁问题 RLock1234567891011121314151617181920212223242526272829303132333435363738394041import timefrom threading import Lock,Threadnoodle_lock = Lock()fork_lock = Lock()def eat1(name): noodle_lock.acquire() print('%s 拿到面条' %name) fork_lock.acquire() print('%s 拿到叉子' %name) print('%s 吃面' %name) fork_lock.release() noodle_lock.release()def eat2(name): fork_lock.acquire() print('%s 拿到叉子' %name) time.sleep(1) noodle_lock.acquire() print('%s 拿到面条' %name) print('%s 吃面' %name) noodle_lock.release() fork_lock.release()Thread(target=eat1,args=('rubin',)).start()Thread(target=eat2,args=('leo',)).start()Thread(target=eat1,args=('lex',)).start()Thread(target=eat2,args=('fily',)).start()# rubin 拿到面条# rubin 拿到叉子# rubin 吃面# leo 拿到叉子# lex 拿到面条# ... 卡住了 123456789101112131415161718192021222324252627282930313233343536373839404142# 递归锁 解决死锁问题import timefrom threading import Lock,Threadfrom threading import RLockr_fork_lock = r_noodle_lock = RLock() # 一个钥匙串上的两把钥匙def eat1(name): r_noodle_lock.acquire() # 一旦拿到其中一把钥匙，说明要是都在手里 房子1 print('%s 拿到面条' %name) r_fork_lock.acquire() # 房子2 2层钥匙 print('%s 拿到叉子' %name) print('%s 吃面' %name) r_fork_lock.release() # 释放1层 r_noodle_lock.release() # 释放2层 还完所有钥匙def eat2(name): r_fork_lock.acquire() print('%s 拿到叉子' %name) time.sleep(1) r_noodle_lock.acquire() print('%s 拿到面条' %name) print('%s 吃面' %name) r_noodle_lock.release() r_fork_lock.release()Thread(target=eat1,args=('rubin',)).start()Thread(target=eat2,args=('leo',)).start()Thread(target=eat1,args=('lex',)).start()Thread(target=eat2,args=('fily',)).start()# Lock 互斥锁 1把钥匙# RLock 递归锁 同一个线程拿多少次钥匙都可以 , 不同的线程里 一旦有一个拿到了，别的线程都无法拿到# 递归解决死锁问题，同一个线程可以被 acquire 多次。# 当同一个线程 或者 同一个进程 遇到两把以上锁的时候，就容易出现死锁，需要使用递归锁。# 线程进场遇到全局数据，全局数据又是共享，所以经常要加锁。 信号量 Semaphore12345678910111213141516import timefrom threading import Semaphore,Thread# 信号量 就是控制n个线程 访问同一段代码def func(sem,a,b): sem.acquire() time.sleep(1) print(a + b) sem.release()# KTV例子 同一时间几个人能够进入sem = Semaphore(4) # 同一时间 只能有4个线程进入for i in range(10): t = Thread(target=func,args=(sem,i,i+5)) t.start() 事件 Event123456789101112131415161718192021222324252627282930313233343536373839404142# 事件被创建的时候# 当False状态，wait() 阻塞# 当True状态的 wait() 非阻塞# clear 设置状态为False# set 设置状态为True# 之前有红绿灯例子# 链接数据库和检测数据库的可链接情况# 起两个线程# 第一个线程 : 链接数据库 # 等待一个信号,告诉我们之间的网络是通的 # 链接数据库# 第二个线程 : 检测与数据库之间的网络是否连通 # 模拟检测 time.sleep(0,2) 如果2秒钟没通 就是没通 # 将事件的状态,设置为 Trueimport timeimport randomfrom threading import Event,Threaddef connect_db(e): count = 0 # 计数器，尝试三次链接 while count &lt; 3: e.wait(0.5) # 状态为False的时候 ，只等待n秒钟就结束 if e.is_set() == True: print('链接数据库') break else: count += 1 print('第%s次链接失败' %count) else: # while 循环 + else 三次都没有被break 也就是都没连接上 raise TimeoutError('数据库连接超时') # 超时错误def check_web(e): time.sleep(random.randint(0,3)) e.set() # wait = Truee = Event()t1 = Thread(target=connect_db,args=(e,))t2 = Thread(target=check_web,args=(e,))t1.start()t2.start() 定时器 Timer123456789import timefrom threading import Timerdef func(): print('时间同步')while True: t = Timer(2,func).start() # Timer(秒数,方法) 2秒钟之后开启线程 time.sleep(5) # 和上面的Timer是同步 Timer5秒后会执行func 队列 queue1234# Queue 先进先出 队列# LifoQueue 先进后出 栈# PriorityQueue 优先级队列# 他们都是数据安全 1234567# 队列import queue q = queue.Queue() # 队列的特点 先进先出q.put() # 满了会阻塞q.get() # 空了会阻塞q.put_nowait() # + 异常处理q.get_nowait() # 1234567# 栈import queue q = queue.LifoQueue() # 栈 先进后出 (先进来的在最底下,后进来的先出去)q.put(1)q.put(2)q.put(3)print(q.get()) # 3 12345678910# 优先级队列import queue q = queue.PriorityQueue() # 优先级队列 放数据的时候，给优先级q.put((20,'a'))q.put((10,'b'))q.put((30,'c'))q.put((1,'z'))q.put((1,'d'))print(q.get()) # (1, 'd') 顺序按照 数字从小到大，如果数字相同,按照数据的ASCII码排 线程池 concurrent.futures 模块12345# 1 介绍# concurrent.futures模块提供了高度封装的异步调用接口# ThreadPoolExecutor：线程池，提供异步调用# ProcessPoolExecutor: 进程池，提供异步调用# Both implement the same interface, which is defined by the abstract Executor class. 12345678910111213141516171819202122232425# 2 基本方法# submit(fn, *args, **kwargs)# 异步提交任务# map(func, *iterables, timeout=None, chunksize=1)# 取代for循环submit的操作# shutdown(wait=True)# 相当于进程池的pool.close() + pool.join() 操作# wait=True，等待池内所有任务执行完毕回收完资源后才继续# wait=False，立即返回，并不会等待池内的任务执行完毕# 但不管wait参数为何值，整个程序都会等到所有任务执行完毕# submit和map必须在shutdown之前# result(timeout=None)# 取得结果# add_done_callback(fn)# 回调函数# done()# 判断某一个线程是否完成# cancle()# 取消某个任务 12345678910111213141516171819# 线程池import timefrom concurrent.futures import ThreadPoolExecutor # 线程池,提供异步调用from concurrent.futures import ProcessPoolExecutor # 进程池,提供异步调用 apply_async()def func(n): time.sleep(2) print(n) return n*ntpool = ThreadPoolExecutor(max_workers=5) # 1一般情况下 线程卡其 max_workers = 默认不要超过 cpu个数 * 5t_lst = [] # 任务列表for i in range(10): t = tpool.submit(func,i) # 线程池提交任务 t_lst.append(t) # 存储结果对象,就像领了个号，等任务结束后去领取结果tpool.shutdown() # pool.close() 关闭池子不让人物再提交进来 + pool.join() 阻塞直到池子中的子线程任务执行完print('主线程')for t in t_lst:print('***',t.result()) # 为什么这个地方按顺序打印，因为 t_lst 是按照顺序产生的 123456789101112# 流程总结:# 1、创建线程池# 2、线程池异步提交任务# 3、阻塞到线程池任务都结束# 4、获取结果# 如果没有shutdown:# 1、会先打印 print('主线程')# 2、谁先执行好 就先接收到结果输出 (消耗好一些)# 5线程 * 每个任务2秒 = 5任务2秒 10任务4秒 如果join 需要4秒后统一拿结果# 不加join 2秒后执行完前5个任务拿到结果# 如果有返回值的话 ，超过池子的数量 用 shutdown 效率会慢 123456789101112131415161718192021### 进程池import timefrom concurrent.futures import ThreadPoolExecutor # 线程池,提供异步调用from concurrent.futures import ProcessPoolExecutor # 进程池,提供异步调用 apply_async()def func(n): time.sleep(2) print(n) return n*nif __name__ == '__main__': p_pool = ProcessPoolExecutor(max_workers=5) p_lst = [] # 任务列表 for i in range(10): t = p_pool.submit(func, i) p_lst.append(t) p_pool.shutdown() print('主进程') for p in p_lst: print('进程池 ***', p.result()) 1234567891011### mapimport timefrom concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutordef func(n): time.sleep(2) print(n) return n*nt_pool = ThreadPoolExecutor(max_workers=5)t_pool.map(func,range(20)) # 拿不到返回值啦 123456789101112131415161718# 回调函数 call_backimport timefrom concurrent.futures import ThreadPoolExecutor # 线程池,提供异步调用from concurrent.futures import ProcessPoolExecutor # 进程池,提供异步调用 apply_async()def func(n): time.sleep(2) print(n) return n*ndef call_back(m): # 接收到了个对象 print('结果是%s' %m.result())tpool = ThreadPoolExecutor(max_workers=5)t_lst = [] # 任务列表for i in range(10): t = tpool.submit(func,i).add_done_callback(call_back) # 线程池提交任务 带回调函数 t_lst.append(t)]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多进程 multiprocess模块]]></title>
    <url>%2F2019%2F04%2F19%2Fprocess-note%2F</url>
    <content type="text"><![CDATA[开启多进程使用Process模块123456# Process([group [, target [, name [, args [, kwargs]]]]])# target表示调用对象，你可以传入方法的名字# args表示被调用对象的位置参数元组，比如target是函数a，他有两个参数m，n，那么args就传入(m, n)即可# kwargs表示调用对象的字典# name是别名，相当于给这个进程取一个名字# group分组，实际上不使用 12345678910111213141516171819# 使用process模块from multiprocessing import Processimport multiprocessingimport osdef process(num): print('PID:%s Process:%s' %(os.getpid(),num))if __name__ == '__main__': for i in range(5): p = Process(target=process,args=(i,)) p.start() print('CPU number:%d' %multiprocessing.cpu_count()) # cpu个数 # CPU number:4# PID:97764 Process:1# PID:101312 Process:2# PID:100472 Process:0# PID:101172 Process:4# PID:99964 Process:3 继承Process类12345678910111213141516171819# 继承创建多进程from multiprocessing import Processclass MyProcess(Process): def __init__(self): super().__init__() def run(self): print('开始一个新进程 PID:%s' %self.pid)if __name__ == '__main__': for i in range(5): p = MyProcess() p.start()# 开始一个新进程 PID:100524# 开始一个新进程 PID:97824# 开始一个新进程 PID:87880# 开始一个新进程 PID:93100# 开始一个新进程 PID:94960 守护进程 deamon123456789101112131415161718192021222324# 如果设置为True，当父进程结束后，子进程会自动被终止。# 注意: 守护进程 会随着 主进程的代码执行完毕 而结束# 代码运行: 主进程 在延迟一秒后只打印一句话，全部的代码已经执行完成,也直接终止了子进程的运行# 作用: 有效防止无控制地生成子进程，关闭这个主程序运行时，就无需额外担心子进程有没有被关闭了。import timefrom multiprocessing import Processclass MyProcess(Process): def __init__(self): super().__init__() def run(self): time.sleep(2) print('开启一个新进程%s' %self.pid)if __name__ == '__main__': for i in range(5): p = MyProcess() p.daemon = True p.start() time.sleep(1) print('主进程结束') # time.sleep(3) # 如果再延迟3秒，子进程会执行，因为这个时候主进程的代码还没有执行结束 # 主进程结束 主进程等待子进程执行完成 join12345678910111213141516171819202122232425262728293031323334# 主进程等待子进程执行完成 join# 每个子进程都调用了join()方法，这样父进程（主进程）就会等待子进程执行完毕。import osimport timefrom multiprocessing import Processclass MyProcess(Process): def __init__(self,num): super().__init__() self.num = num def run(self): time.sleep(1) print('子进程%s 任务编号%s 开始执行' %(os.getpid(),self.num))if __name__ == '__main__': start_time = time.time() p_lst = [] for i in range(1,6): p = MyProcess(i) p_lst.append(p) p.daemon = True p.start() # p.join() # 如果在循环里面join,则每循环一个进程都要等待进程的结束,会变成同步 [p.join() for i in p_lst] # 之前的所有进程必须在这里都执行完，才能执行后面的代码 print('主进程执行结束') print('程序运行:',time.time()-start_time) # 子进程55888 任务编号3 开始执行# 子进程16812 任务编号4 开始执行# 子进程77500 任务编号1 开始执行# 子进程62216 任务编号5 开始执行# 子进程86548 任务编号2 开始执行# 主进程执行结束# 程序运行: 1.2200696468353271]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>多进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程]]></title>
    <url>%2F2019%2F04%2F04%2Fconcurrency%2F</url>
    <content type="text"><![CDATA[进程理论基础一、操作系统的作用: 隐藏丑陋复杂的硬件接口，提供良好的抽象接口 管理、调度进程，并且将多个进程对硬件的竞争变得有序 二、多道技术: 产生背景：针对单核，实现并发ps：现在的主机一般是多核，那么每个核都会利用多道技术有4个cpu，运行于cpu1的某个程序遇到io阻塞，会等到io结束再重新调度，会被调度到4个cpu中的任意一个，具体由操作系统调度算法决定。 空间上的复用：如内存中同时有多道程序 时间上的复用：复用一个cpu的时间片强调：遇到io切，占用cpu时间过长也切，核心在于切之前将进程的状态保存下来，这样才能保证下次切换回来时，能基于上次切走的位置继续运行 三、进程: 进程即正在执行的一个过程。进程是对正在运行程序的一个抽象。 进程的概念起源于操作系统，是操作系统最核心的概念，也是操作系统提供的最古老也是最重要的抽象概念之一。操作系统的其他所有内容都是围绕进程的概念展开的。 PS：即使可以利用的cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力。将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。 内存越大，多个程序占用的空间越大；CPU核数越多，同一时间处理的任务越多。 什么是进程进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。 狭义定义：进程是正在运行的程序的实例（an instance of a computer program that is being executed）。广义定义：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元。 进程是一个实体。每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。 进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时（操作系统执行之），它才能成为一个活动的实体，我们称其为进程。[3]进程是操作系统中最基本、重要的概念。是多道程序系统出现后，为了刻画系统内部出现的动态情况，描述系统内部各道程序的活动规律引进的一个概念,所有多道程序设计操作系统都建立在进程的基础上。 进程的并行与并发 并行 : 并行是指两者同时执行，比如赛跑，两个人都在不停的往前跑；（资源够用，比如三个线程，四核的CPU ） 并发 : 并发是指资源有限的情况下，两者交替轮流使用资源，比如一段路(单核CPU资源)同时只能过一个人，A走一段后，让给B，B用完继续给A ，交替使用，目的是提高效率。 区别:并行是从微观上，也就是在一个精确的时间片刻，有不同的程序在执行，这就要求必须有多个处理器。并发是从宏观上，在一个时间段上可以看出是同时执行的，比如一个服务器同时处理多个session。 进程的三状态 在程序运行的过程中，由于被操作系统的调度算法控制，程序会进入几个状态：就绪，运行和阻塞。 （1）就绪(Ready)状态 当进程已分配到除CPU以外的所有必要的资源，只要获得处理机便可立即执行，这时的进程状态称为就绪状态。 （2）执行/运行（Running）状态当进程已获得处理机，其程序正在处理机上执行，此时的进程状态称为执行状态。 （3）阻塞(Blocked)状态正在执行的进程，由于等待某个事件发生而无法执行时，便放弃处理机而处于阻塞状态。引起进程阻塞的事件可有多种，例如，等待I/O完成、申请缓冲区不能满足、等待信件(信号)等。 同步与异步 同步: 串行处理一件事物 异步: 同时处理多件不同的事物 所谓同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。 所谓异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。 阻塞与非阻塞 阻塞: input 读写文件 产生IO操作 非阻塞: 不产生IO操作阻塞和非阻塞这两个概念与程序（线程）等待消息通知(无所谓同步或者异步)时的状态有关。也就是说阻塞与非阻塞主要是程序（线程）等待消息通知时的状态角度来说的 同步/异步与阻塞/非阻塞 同步阻塞形式 效率低 专心排队，什么别的事都不做 异步阻塞形式 领一张排队号码，不用排队等着叫号，但是等的过程中不能做其他事情。异步操作是可以被阻塞住的，只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞。 同步非阻塞形式 一边排队，一边打电话,来回切换两种不同的行为，效率低 异步非阻塞形式 效率高，等着柜台(消息触发机制)通知，去外面抽烟，做自己的事 很多人会把同步和阻塞混淆，是因为很多时候同步操作会以阻塞的形式表现出来，同样的，很多人也会把异步和非阻塞混淆，因为异步操作一般都不会在真正的IO操作处被阻塞。 进程的创建与结束但凡是硬件，都需要有操作系统去管理，只要有操作系统，就有进程的概念，就需要有创建进程的方式，一些操作系统只为一个应用程序设计，比如微波炉中的控制器，一旦启动微波炉，所有的进程都已经存在。而对于通用系统（跑很多应用程序），需要有系统运行过程中创建或撤销进程的能力，主要分为4中形式创建新的进程： 系统初始化（查看进程linux中用ps命令，windows中用任务管理器，前台进程负责与用户交互，后台运行的进程与用户无关，运行在后台并且只在需要时才唤醒的进程，称为守护进程，如电子邮件、web页面、新闻、打印） 一个进程在运行过程中开启了子进程（如nginx开启多进程，os.fork,subprocess.Popen等） 用户的交互式请求，而创建一个新进程（如用户双击暴风影音） 一个批处理作业的初始化（只在大型机的批处理系统中应用）无论哪一种，新进程的创建都是由一个已经存在的进程执行了一个用于创建进程的系统调用而创建的。 进程的结束: 正常退出（自愿，如用户点击交互式页面的叉号，或程序执行完毕调用发起系统调用正常退出，在linux中用exit，在windows中用ExitProcess） 出错退出（自愿，python a.py中a.py不存在） 严重错误（非自愿，执行非法指令，如引用不存在的内存，1/0等，可以捕捉异常，try…except…） 被其他进程杀死（非自愿，如kill -9） 在python程序中的进程操作创建进程共四个部分：创建进程部分，进程同步部分，进程池部分，进程之间数据共享。 multiprocess.process模块process模块是一个创建进程的模块，借助这个模块，就可以完成进程的创建。 Process([group [, target [, name [, args [, kwargs]]]]])，由该类实例化得到的对象，表示一个子进程中的任务（尚未启动） 强调： 需要使用关键字的方式来指定参数 args指定的为传给target函数的位置参数，是一个元组形式，必须有逗号 参数介绍：1 group参数未使用，值始终为None2 target表示调用对象，即子进程要执行的任务3 args表示调用对象的位置参数元组，args=(1,2,’egon’,)4 kwargs表示调用对象的字典,kwargs={‘name’:’egon’,’age’:18}5 name为子进程的名称 在windows中使用process模块的注意事项:必须把创建子进程的部分使用if __name__ ==‘__main__’判断保护起来 使用process模块创建进程123456# Process([group [, target [, name [, args [, kwargs]]]]])# target表示调用对象，你可以传入方法的名字# args表示被调用对象的位置参数元组，比如target是函数a，他有两个参数m，n，那么args就传入(m, n)即可# kwargs表示调用对象的字典# name是别名，相当于给这个进程取一个名字# group分组，实际上不使用 123456789101112import timefrom multiprocessing import Processdef f(name): print('hello', name) print('我是子进程')if __name__ == '__main__': p = Process(target=f, args=('leo',)) p.start() time.sleep(1) print('执行主进程的内容了') 查看进程的执行12345678910111213141516171819202122232425262728293031323334import timeimport osfrom multiprocessing import Processdef func(args,args2): print(args,args2) # print(54321) time.sleep(1) print('子进程:', os.getpid()) print('子进程的父进程:', os.getppid()) # 查看当前进程的父进程号 print(12345)if __name__ == '__main__': # 主进程(父进程)执行的 p = Process(target=func,args=('参数','参数2')) # 注册 args传参必须传元祖,里面放着参数 # p 是进程对象,此时还没有启动进程 p.start() # 开启子进程 print('*'*10) print('父进程:',os.getpid()) # 查看当前进程的进程号 print('父进程的父进程:', os.getppid()) # 查看当前进程的父进程号 在pycharm中启动就是pycharm 可以通过任务管理器查看到# 如果是同步的函数执行，会先执行完func函数在执行打印# 异步虽然要先启动子进程，但是不影响后面的代码# 进程的生命周期: # 主进程 : 从程序开启开始，直到程序执行完成 # 子进程 : 从start开始，到子进程中的代码执行完成 # 开启了子进程的主进程: # 主进程自己的代码如果长，等待自己的代码执行结束 # 子进程的执行时间长，主进程会在主进程代码执行完毕后,等待子进程执行完毕,主进程才结束# python chrom.py &amp; 终端没有关闭 进程都在后台运行# 父进程在 自己进程就在 不一定，要看怎么启动的# 子进程不一定要依赖父进程运行 123456789101112131415161718from multiprocessing import Processimport timeimport osdef func(args,args2): print('大家好,我是子进程') print(args,args2) time.sleep(3) print('子进程执行完成') print('子进程:',os.getpid()) print('子进程的父进程: ',os.getppid())if __name__ == '__main__': p = Process(target=func,args=(10,20)) # 注册 p.start() # 启动紫禁城 print('当前进程:',os.getpid()) print('当前进程的父进程: ',os.getppid()) print('*'*10) join方法 p.join([timeout]):主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程12345678910111213141516171819# join()import timefrom multiprocessing import Processdef func(arg1,arg2): print('*'*arg1) time.sleep(5) print('*'*arg2)if __name__ == '__main__': p = Process(target=func, args=(10, 20)) p.start() print('这个时候还是异步的') p.join() # 作用：感知一个子进程的结束 ，异步的程序就变成同步了 print('主进程运行完成')# join 会将异步的程序变成同步 开启多个子进程,并写入文件通过join方法，让写文件的操作编程异步123456789101112131415161718192021222324252627282930313233343536373839404142434445# 多进程写文件# for 循环500个文件,没处理1个文件需要0.1秒，500个是50秒# 同步，只有一个进程处理: 0.1 * 500 = 50 # 异步，存在500个进程: 500 * 0.1 = 0.1# 1. 先往文件夹中写文件# 2. 展示写入后文件件的所有文件名# join 会将异步的程序变成同步import timeimport osfrom multiprocessing import Processdef func(filename,context): print('子进程%s开始' %(os.getpid())) with open(filename,'w') as f: f.write(str(context))# 单过在循环外只有1个join无法确定所有子进程都结束，所以需要控制if __name__ == '__main__': p_lst = [] for i in range(1,6): p = Process(target=func,args=('inod%s'%i,i)) p_lst.append(p) # 每创建出来一个进程都加入进程列表 p.start() # p.join() # 如果在循环里面join,则每循环一个进程都要等待进程的结束,会变成同步 # 如果不用join 开启进程无法保障运行的时间，所以后面的代码一起异步执行 [p.join() for p in p_lst] # 之前的所有进程必须在这里都执行完，才能执行后面的代码 # 列表推导式，先启动所有的进程，按顺序执行，在最后之前保障所有的进程对象执行完成 print('主进程%s执行完成' %(os.getpid())) print([i for i in os.walk(r'D:\PycharmProjects\Notes\08 并发编程')])# 场景:# 同时开启多个子进程，异步执行，当我需要同步执行的时候，设置一个阻拦的手段，让所有的进程在这话话都执行完成# 结果:# 子进程8332开始# 子进程4936开始# 子进程2728开始# 子进程9832开始# 子进程4428开始# 主进程9340执行完成# [('inod1', 'inod2', 'inod3', 'inod4', 'inod5'])] 开启多进程方法212345678910111213141516171819202122232425# 自定义类继承Process，实现多进程import osfrom multiprocessing import Processclass MyProcess(Process): # 自定义类继承Process def __init__(self,arg1,arg2): # 传参需要继承父类的__init__ super().__init__() # 继承父类的__init__ self.arg1 = arg1 self.arg2 = arg2 def run(self): # 实现run方法 print(self.pid) print(self.name) print(self.arg1) print(self.arg2) print('开始一个新进程%s' %os.getpid())if __name__ == '__main__': print('主进程:', os.getpid()) p1 = MyProcess(1,2) p1.start() # start 调用 run方法 p2 = MyProcess(3,4) p2.start() 123456789101112131415161718192021222324import osfrom multiprocessing import Processclass MyProcess(Process): # 自定义类继承Process def __init__(self,arg1,arg2): # 传参需要继承父类的__init__ super().__init__() # 继承父类的__init__ self.arg1 = arg1 self.arg2 = arg2 def run(self): # 实现run方法 print(self.pid) print(self.name) print(self.arg1) print(self.arg2) print('开始一个新进程%s' %os.getpid())if __name__ == '__main__': print('主进程:', os.getpid()) p1 = MyProcess(1,2) p1.start() # start 调用 run方法 p2 = MyProcess(3,4) p2.start() 多进程之间的数据隔离12345678910111213141516171819202122# 进程 与 进程之间数据是隔离的？ 是隔离的# 微信 与 QQ之间的数据能共享么？import osfrom multiprocessing import Processdef func(): global n # 全局变量n n = 0 # 重新定义n print('子进程 : %s' %os.getpid(),n)if __name__ == '__main__': print('主进程',os.getpid()) n = 100 p = Process(target=func) p.start() p.join() # 等待子进程 结束完 print('主进程',n) # 主进程打印的n 和 子进程不一样 ，多进程之间数据有隔离# 运行结果# 主进程pid: 5748# 子进程pid: 6048 0# 主进程 100 守护进程1234567891011121314151617181920212223242526272829# 特点: 会随着主进程的结束而结束。# 主进程创建守护进程 # 其一：守护进程会在主进程代码执行结束后就终止 # 其二：守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children# 注意：进程之间是互相独立的，主进程代码运行结束，守护进程随即终止import timefrom multiprocessing import Processdef func(): while True: time.sleep(0.2) print('我还活着') # 每隔0.5秒 说下进程状态if __name__ == '__main__': p = Process(target=func) p.daemon = True # 设置子进程为守护进程 p.start() i = 0 while i &lt; 5: print('我是socket server') time.sleep(1) i += 1# 守护进程 会随着 主进程的代码执行完毕 而结束 12345678910111213141516171819202122232425262728293031323334353637383940414243# 守护进程# 子进程 转换成 守护进程# 守护进程 子进程随着主进程 一起结束import timefrom multiprocessing import Processdef func(): while True: time.sleep(0.2) print('我还活着') # 每隔0.5秒 说下进程状态def func2(): print('in func2 开始') time.sleep(8) print('in func2 结束')if __name__ == '__main__': p = Process(target=func) p.daemon = True # 设置子进程为守护进程 p.start() p2 = Process(target=func2) # 另外一个子进程 p2.start() p2.terminate() # 结束一个进程 print(p2.is_alive()) # 检查进程是否还活着 time.sleep(2) print(p2.is_alive()) print(p2.name) # i = 0 # while i &lt; 5: # print('我是socket server') # time.sleep(1) # i += 1# 守护进程 会随着 主进程的代码执行完毕 而结束# 在主进程内结束进程 p2.terminate()# 结束一个进程不是在执行方法之后立即生效，需要一个操作系统响应的过程# 检验进程是否活着的状态 p2.is_alive()# p.name p.pid 进程名 和 进程号 多进程中的方法和属性1234567891011121314151617181920212223242526272829import randomimport timefrom multiprocessing import Processclass MyProcess(Process): def __init__(self,user): super().__init__() self.user = user def run(self): print('子进程 %s 的 pid %s' %(self.name,self.pid)) print('%s 开始执行子进程'%self.user) time.sleep(random.randrange(1,5)) print('%s 子进程完成' %(self.user))if __name__ == '__main__': p1 = MyProcess(user='leo') p1.start() p1.terminate() # 关闭进程,不会立即关闭,所以is_alive立刻查看的结果可能还是存活 print(p1.is_alive()) # True time.sleep(1) print(p1.is_alive()) # False # self.name 属性是Process中的属性，标示进程的名字 # super().__init__() # 执行父类的初始化方法会覆盖name属性 # self.pid 查看id # p1.terminate() 关闭进程 # p1.is_alive() 查看进程是否存活 socket聊天并发实例123456789101112131415161718192021222324# serverimport socketfrom multiprocessing import Processdef server(conn): # ret = input('&gt;&gt;&gt;').encode('utf-8') ret = '你好'.encode('utf-8') conn.send(ret) msg = conn.recv(1024).decode('utf-8') print(msg) conn.close()if __name__ == '__main__': sk = socket.socket() sk.bind(('127.0.0.1',8090)) sk.listen() while True: conn,addr = sk.accept() # 拿到链接 放到多进程 每个链接都执行 p = Process(target=server,args=(conn,)) p.start() sk.close() 1234567891011# clientimport socketsk = socket.socket()sk.connect(('127.0.0.1',8090))msg = sk.recv(1024).decode('utf-8')print(msg)msg2 = input('&gt;&gt;&gt;').encode('utf-8')sk.send(msg2)sk.close() 进程同步 —— 锁、信号量和事件重要程度:Lock(加锁，同一时间1个进程执行) 锁 —— multiprocess.Lock加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全。虽然可以用文件共享数据实现进程间通信，但问题是：1.效率低（共享数据基于文件，而文件是硬盘上的数据）2.需要自己加锁处理12345678910111213141516171819202122232425262728293031323334353637383940414243from multiprocessing import Processfrom multiprocessing import Lock # 进程锁import jsonimport osimport timedef show(i): # 查票读取ticket文件 with open('ticket',mode='r') as f: dic = json.load(f) print('子进程%s,余票:%s'%(os.getpid(),dic['ticket']))def buy_ticket(i,lock): lock.acquire() # 拿钥匙进门 被拿走后 别的进程会在这里阻塞，直到钥匙被还 with open('ticket') as f: dic = json.load(f) time.sleep(0.1) if dic['ticket'] &gt; 0: dic['ticket'] -= 1 print('\033[32m%s 买到票了\033[0m' %i) else: print('\033[31m%s 没买到票\033[0m' %i) time.sleep(0.1) with open('ticket',mode='w') as f: # 修改票结果 json.dump(dic,f) lock.release() # 还钥匙，已经买完票了if __name__ == '__main__': for i in range(10): # 假装有10个人抢票 p = Process(target=show,args=(i,)) p.start() lock = Lock() for i in range(10): p = Process(target=buy_ticket,args=(i,lock)) p.start()# 给某一段代码加上锁 这段代码在这一段时间内只能让一个进程执行# 只要多人同时操作一个数据 就会出现数据安全问题，需要牺牲效率 保证数据安全# &#123;"ticket": 1&#125; 信号量 —— multiprocess.Semaphore1234567891011121314151617181920212223242526272829303132333435363738394041424344# 多进程中的组件 # 同步控制 # 进程间通信 # 进程间的数据共享 # 进程池# ktv 有4个门 20个人同时进出,现在要控制4个人先进# 一套资源 同一时间 只能被N个人访问# 某一段代码 在同一时间 只能被n个进程执行import timeimport randomfrom multiprocessing import Processfrom multiprocessing import Semaphoredef ktv(i,sem): sem.acquire() # 获取钥匙 print('%i 走进KTV' %i) time.sleep(random.randint(10,20)) # 模拟唱歌 1-5秒 print('%i 走出KTV' %i) sem.release() # 还钥匙if __name__ == '__main__': sem = Semaphore(4) # 实例化4把钥匙 for i in range(20): p = Process(target=ktv,args=(i,sem)) p.start()# 限定进程访问的代码 同一时间只能有几个进程来访问# 开一个们 有4把要是 前4个进程进去后 门就观赏了 直到某一个进程出来还钥匙，第5个进程获取进入# lock只有一把钥匙# 信号量有N把钥匙# 结果:# 3 走进KTV# 0 走进KTV# 8 走进KTV# 7 走进KTV# ...同一时间只有4个人# 8 走出KTV# 4 走进KTV# 3 走出KTV# 12 走进KTV 事件 —— multiprocess.Event1234567891011121314151617181920212223242526# 通过一个信号 来控制 多个进程 同时执行或者阻塞# 事件from multiprocessing import Event# 一个信号 可以使所有的进程都进入阻塞状态# 也可以控制 所有的进程 解除阻塞# 一个事件 被创建之后，默认是阻塞状态e = Event() # 创建了一个事件print(e.is_set() ) # 查看一个事件的状态 # Falsee.set() # 将这个时间的状态改为Trueprint(e.is_set() ) # Truee.wait() # 是依据e.is_set()的值，来决定是否阻塞,如果是False就阻塞,True就是不阻塞print(123456) # 正常打印e.clear() # 将事件状态改成falseprint(e.is_set()) # Falsee.wait() # 阻塞print('123456') # 不打印# set 和 clear # 分别用来修改一个事件的状态：True / False# is_set # 用来查看一个事件的状态# wait # 依据事件的状态来决定自己是否阻塞 True:不阻塞，False:阻塞 1234567891011121314151617181920212223242526272829303132# 红绿灯事件# 两个进程，车怎么才能感知到灯？import timeimport randomfrom multiprocessing import Eventfrom multiprocessing import Processdef car(e,i): if not e.is_set(): print('car%s 等红灯'%i) e.wait() # 阻塞,直到得到一个事件状态改变，编程True的信号 print('\033[0;32;40mcar%s 通过\033[0m' %i)def light(e): while True: if e.is_set(): # Ture e.clear() # False print('\033[31m红灯亮了\033[0m') else: e.set() # True print('\033[32m绿灯亮了\033[0m') time.sleep(2)if __name__ == '__main__': e = Event() p = Process(target=light,args=(e,)) p.start() for i in range(20): cars = Process(target=car,args=(e,i)) cars.start() time.sleep(random.random()) # 0 - 1秒 进程间的通信 —— 队列和管道进程间通信:IPC(Inter-Process Communication) 队列 multiprocessing.Queue1234567891011121314151617181920212223242526# 队列 先进先出import timefrom multiprocessing import Queueq = Queue(5)# for i in range(6): # 0 - 5 超过队列大小了 会阻塞# q.put(i)q.put(1)q.put(2)q.put(3)q.put(4)q.put(5)print(q.full()) # True 队列是否满了print(q.get()) # 1print(q.get()) # 2print(q.get()) # 3print(q.get()) # 4print(q.get()) # 5print(q.empty()) # True 队列是否空while True: try: q.get_nowait() except: print('队列已空') time.sleep(1)# 放到满 和 空了再取值 都会阻塞 生产者与消费者模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 队列# 生产者与消费者模型:# 买包子# 蒸包子 # 买包子# 生产数据 # 消费数据# 爬虫 爬500个网页 生产数据快 ——&gt; 放到内存里 # 处理爬虫 处理数据 慢# 1、容器满了就不允许再放# 2、增加处理进程# 3、如果爬取数据慢，比如网络延迟或者需要验证破解,那么增加生产者,解决数据供需不平衡# 生产者 进程# 消费者 进程import timeimport randomimport osfrom multiprocessing import Queuefrom multiprocessing import Process# 消费者def consumer(q,name): # 数据一直处理 while True: food = q.get() if food is None: # 当获取到空了说明生产者完成了所有生产 print('%s 获取到空了' %name) break print('\033[31m%s消费了%s\033[0m' % (name,food)) time.sleep(random.randint(1,3))# 生产者def producer(name,food,q): for i in range(5): time.sleep(random.randint(1,3)) f = '%s生产了%s%s' %(name,food,i) print(f,os.getpid()) q.put(f) # 放到队列中if __name__ == '__main__': q = Queue(20) pro1 = Process(target=producer,args=('小红','包子',q)) pro2 = Process(target=producer,args=('小兰','汽水',q)) c1 = Process(target=consumer,args=(q,'小黑')) c2 = Process(target=consumer,args=(q,'小金')) pro1.start() pro2.start() c1.start() c2.start() pro1.join() pro2.join() q.put(None) # 等生产者都生产完毕后 放入一个空值 q.put(None) # 等生产者都生产完毕后 放入一个空值# Queue 是进程安全的，在队列里的数据只能被一个进程取走# 三个不同的进程 有可能在同一个时间去队列取值，这样是不安全的# None被其中一个进程取走，另外一个并没有拿到，所以会阻塞 JoinableQueue队列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import timeimport osimport randomfrom multiprocessing import Processfrom multiprocessing import JoinableQueue# 消费者def consumer(q,name): # 数据一直处理 while True: food = q.get() if food is None: # 当获取到空了说明生产者完成了所有生产 print('%s 获取到空了' %name) break print('\033[31m%s消费了%s\033[0m' % (name,food)) time.sleep(random.randint(1,3)) q.task_done() # count - 1 ,直到队列中的所有数据都执行了task_done# 生产者def producer(name,food,q): for i in range(5): time.sleep(random.randint(1,3)) f = '%s生产了%s%s' %(name,food,i) print(f,os.getpid()) q.put(f) # 放到队列中 count + 1 1.,20 q.join() # 阻塞，直到一听歌队列中的所有数据 全部被处理完毕,这个进程才结束if __name__ == '__main__': q = JoinableQueue(20) pro1 = Process(target=producer,args=('小红','包子',q)) pro2 = Process(target=producer,args=('小兰','汽水',q)) c1 = Process(target=consumer,args=(q,'小黑')) c2 = Process(target=consumer,args=(q,'小金')) pro1.start() pro2.start() c1.daemon = True # 主进程中的代码执行完毕后 该守护进程结束 进程结束了 q.get()也不会阻塞了。 c2.daemon = True c1.start() c2.start() pro1.join() # 感知一个进程的结束 pro2.join() # 生产进程 q.join()结束，需要等待消费者都处理完才能结束# JoinableQueue 比 Queue多了两个方法# 1、获取数据要提交回执 q.task_done() q.join()# c1.daemon = True 守护进程 主进程中的代码执行完毕后 该守护进程结束# 在消费者这一端: # 每次获取一个数据 # 处理一个数据 # 发送一个记号：标志一个数据被处理成功# 在生产者这一端: # 每一次生产一个数据 # 且每一次生产的数据都放在队列里 # 在队列中刻上一个记号 # 当生产者全部生产完毕之后 # join信号:已经停止生产数据,且要等待之前被刻上的记号都被消费完 # 当数据都被处理完事，join阻塞结束# consumer 把所有的任务消耗完# producer 端的join感知到，停止阻塞# 所有的producer 进程结束# 主进程中的p.join结束# 主进程的代码结束# 守护进程(c1,c2消费者进程)结束 管道 multiprocessing.Pipe作用：在进程之间通信 123456789from multiprocessing import Pipe,Processdef func(conn): conn.send('吃了么')if __name__ == '__main__': conn1,conn2 = Pipe() p = Process(target=func,args=(conn1,)) p.start() print(conn2.recv()) 123456789101112131415# 发送多条消息from multiprocessing import Pipe,Processdef func(conn): while True: msg = conn.recv() if msg is None: break print(msg)if __name__ == '__main__': conn1,conn2 = Pipe() p = Process(target=func,args=(conn1,)) p.start() for i in range(20): conn2.send('吃了么%s' %i) conn2.send(None) 1234567891011121314151617181920# 作为两端通信# conn2发送,conn1接收def func(conn1,conn2): conn2.close() while True: try: msg = conn1.recv() print(msg) except EOFError: # 没有数据仍然recv的时候报错 conn1.close() breakif __name__ == '__main__': conn1,conn2 = Pipe() p = Process(target=func,args=(conn1,conn2)) p.start() conn1.close() for i in range(20): conn2.send('吃了么%s' %i) conn2.close() # 主进程两边都关闭 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import timeimport randomfrom multiprocessing import Pipefrom multiprocessing import Processfrom multiprocessing import Lockdef producer(con,pro,name,food): con.close() # 用不到消费者管道 for i in range(6): time.sleep(random.randint(1,3)) f = '%s生产%s %s' %(name,food,i) print(f) pro.send(f) # 生产放入管道 pro.close() # 生产完成后关闭管道def consumer(con,pro,name,lock): pro.close() while True: try: lock.acquire() food = con.recv() # 从管道拿产品 lock.release() print('%s 购买了 %s' %(name,food)) time.sleep(random.randint(1,3)) except EOFError: con.close() breakif __name__ == '__main__': con,pro = Pipe() # 生产者和消费者的管道 lock = Lock() # 加锁 p = Process(target=producer,args=(con,pro,'rubin','汽水')) p.start() c1 = Process(target=consumer,args=(con,pro,'leo',lock)) c1.start() c2 = Process(target=consumer,args=(con,pro,'lex',lock)) c2.start() con.close() pro.close()# pipe 数据不安全性# 生产者 消费者1 1个放1个取# 生产者 消费者1、2 1个放2个取 其他消费也来抢占数据,数据在管道中是混乱的,没被拿走之前，消费者都可以来申请# 多个消费者同时取一个数据，数据不安全# 管道是进程数据不安全的，解决方式：加锁# 队列是进程之间数据安全的，因为队里基于管道+锁 实现的,所以之后会更多使用队列 进程之间的数据共享 multiprocessing.Manager 基于消息传递的并发编程是大势所趋 即便是使用线程，推荐做法也是将程序设计为大量独立的线程集合，通过消息队列交换数据。 这样极大地减少了对使用锁定和其他同步手段的需求，还可以扩展到分布式系统中。 但进程间应该尽量避免通信，即便需要通信，也应该选择进程安全的工具来避免加锁带来的问题。 以后我们会尝试使用数据库来解决现在进程之间的数据共享问题。 进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的 虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此 12345678910111213141516# 进程之间的数据共享from multiprocessing import Manager,Process,Lockdef func(dic): dic['count'] -= 1 print(dic)if __name__ == '__main__': m = Manager() print(m) # &lt;multiprocessing.managers.SyncManager object at 0x0000000001D7A3C8&gt; dic = m.dict(&#123;'count':100&#125;) # dic会变成数据共享的字典 p_lst = [] p = Process(target=func, args=(dic,)) p.start() p.join() print('主进程:',dic) 1234567891011121314151617181920212223242526def func(dic,lock): lock.acquire() dic['count'] -= 1 lock.release() # print(dic)if __name__ == '__main__': m = Manager() lock = Lock() # 不加锁而操作共享的数据,肯定会出现数据错乱 # print(m) # &lt;multiprocessing.managers.SyncManager object at 0x0000000001D7A3C8&gt; dic = m.dict(&#123;'count':100&#125;) # dic会变成数据共享的字典 p_lst = [] for i in range(50): p = Process(target=func, args=(dic,lock)) p.start() p_lst.append(p) [ i.join() for i in p_lst ] # 等待所以子进程都结束 print('主进程:',dic)# 总结# 进程同步控制：锁、信号量、事件 -- 控制进程怎么执行，能不能一起执行，几个一起执行，什么时候一起执行 -- 控制# 进程间通信：队列和管道 -- 通信# 进程间数据共享: Manager -- 共享# 以后真正会用到的只有，进程控制，通信方面只用队列,# 未来使用 -- kafak,rabbitmq memcache (消息中间件) kafak(大数据消息中间件,会保留数据)# 进程服务器(多台) --&gt; 服务器(memcache) 进程池 Poolmultiprocessing.Pool为什么要有进程池?进程池的概念。 创建进程需要消耗时间，销毁进程也需要消耗时间。不能无限制的根据任务开启或者结束进程。 进程池的概念:定义一个池子，在里面放上固定数量的进程，有需求来了，就拿一个池中的进程来处理任务，等到处理完毕，进程并不关闭，而是将进程再放回进程池中继续等待任务。如果有很多任务需要执行，池中的进程数量不够，任务就要等待之前的进程执行任务完毕归来，拿到空闲进程才能继续执行。 Pool([numprocess [,initializer [, initargs]]]):创建进程池1234# 参数介绍:# 1 numprocess:要创建的进程数，如果省略，将默认使用cpu_count()的值# 2 initializer：是每个工作进程启动时要执行的可调用对象，默认为None# 3 initargs：是要传给initializer的参数组 map()方法 进程池和进程效率测试1234567891011121314151617181920212223242526272829# 为什么会有进程池的概念 # 提高效率 # 1、每次开启进程，都需要创建一个新的属于这个进程的内存空间，耗时 # 寄存器 堆栈 都是存储代码和变量的 # 2、进程过多，造成操作系统调度，切换过程较多 # 进程不能无限制的开放，而是需要进程池# 进程池 # python:在还没有启动程序之前，先创建一个属于进程的池子 # 这个池子指定能存放多少个进程 # 先将这些进程创建好 # 有50个任务，池子里有5个进程，任务需要排队，按顺序先执行5个任务，结束后不消失回到进程池里接收新的任务，后面依次执行 # 现象：同一时间操作系统中，只执行了这5个进程，减少了进程开销，使5个进程的内存空间循环被利用 # 信号量，同一时间N个进程执行 ,有点像进程池,区别是信号量多个进程排队，在等着执行一段代码，实际上信号量有N个进程被创建了。 # 一个是进程排队，一个是任务排队。 # 进程池既减少了操作系统的调度,且减少了进程开销。# 高级进程池(弹性伸缩) # python中没有 # n,m 上限和下限 # 3 三个进程 # 用户量增多,+1进程，一直加到上线m 20个,最多到20个 # 当任务不断减少的时候，再减到3个进程 # 好处：有效的介绍操作系统负担，减少进程# 开启进程的个数 # CPU核数 + 1 = 进程开启个数 1234567891011121314151617181920212223242526272829303132333435# 使用进程池import timefrom multiprocessing import Pool,Process# Process 超过5个进程,需要使用进程池def func(n): for i in range(10): print(n + 1)def func2(n): print(n) # ('leo', 1) # rubinif __name__ == '__main__': start = time.time() pool = Pool(5) # 5个进程 # pool.map(func,range(100)) # 100个任务 （方法名,可迭代类型）map方法自带join() pool.map(func2,[('leo',1),'rubin']) # 第二个任务 t1 = time.time() - start # 启100个进程 start = time.time() p_lst = [] for i in range(100): p = Process(target=func,args=(i,)) p_lst.append(p) p.start() for i in p_lst:i.join() t2 = time.time() - start print(t1,t2) # 0.21701264381408691 3.6672096252441406 开启100个进程 并没有5个进程交替执行的快# 进程池提高了执行效率 进程池中的同步和异步调用1234567891011121314151617181920212223242526272829# 进程池的同步调用import timeimport randomimport osfrom multiprocessing import Pooldef func(n): print('start func %s' %n,os.getpid()) time.sleep(1) print('end func %s' %n,os.getpid())if __name__ == '__main__': pool = Pool(5) for i in range(10): # pool.apply(func,args=(i,)) # apply同步提交任务 (方法,参数) pool.apply_async(func,args=(i,)) # apply_async 异步提交任务 async在python就代表着异步 # 真异步，主进程执行完了，不等待子进程 pool.close() # 结束进程池接收任务 pool.join() # 感知进程池中的任务执行结束# start func 0 8572# start func 1 6000# start func 2 1208# start func 3 10492# start func 4 9232 # end func 0 8572# start func 5 8572# ... 123456789101112131415161718192021222324252627282930313233# 使用进程池创建socket_server# serverimport socketfrom multiprocessing import Pooldef func(conn): conn.send(b'hello') print(conn.recv(1024).decode('utf-8')) conn.close()if __name__ == '__main__': pool = Pool(5) sk = socket.socket() sk.bind(('127.0.0.1',8080)) sk.listen() while True: conn,addr = sk.accept() pool.apply_async(func,args=(conn,)) sk.close()# clientimport socketsk = socket.socket()sk.connect(('127.0.0.1',8080))ret =sk.recv(1024).decode('utf-8')print(ret)msg = input('&gt;&gt;&gt;').encode('utf-8')sk.send(msg)sk.close() 进程池的返回值12345678import requestsimport timefrom multiprocessing import Pool# p = Pool()# p.map(funcname,iterable) 默认异步的执行任务,且自带close和join# p.apply 同步调用# p.apply_async 异步调用 和 主进程 完全异步,主进程结束不会等待子进程,需要手动close和join 12345678910111213141516171819# 进程池的返回值# 进程池特有的# 使用队列实现def func(i): time.sleep(0.5) return i*iif __name__ == '__main__': p = Pool(5) res_l = [] for i in range(10): # res = p.apply(func,args=(i,)) # apply的结果就是func的返回值 # print(res) res = p.apply_async(func,args=(i,)) # 异步提交 # print(res.get()) # res进程的对象 get会阻塞等待结果,等着func的结算结果 res_l.append(res) for res in res_l:print(res.get()) # 一次获取5个结果 123456789101112131415# mapdef func(i): time.sleep(0.5) return i*iif __name__ == '__main__': p = Pool(5) ret = p.map(func,range(10)) # map自带join和close print(ret) # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 任务都运行完毕后 返回一个列表# map 和 apply，apply_async的区别:# apply_async : 5个5个打印# apply : 一次返回# map : 任务计算完,返回一个列表# 任务很多 使用 apply_async更好 不用等都执行完，拿到结果更快 回调函数 callback123456789101112131415161718192021222324252627import osfrom multiprocessing import Pooldef func1(n): print('in func1',os.getpid()) return n*ndef func2(nn): print('in func2',os.getpid()) print(nn)if __name__ == '__main__': p = Pool(5) print('主进程pid: ',os.getpid()) for i in range(10): p.apply_async(func1,args=(10,),callback=func2) p.close() p.join()# in func1# in func2# 100# 1、执行func1 他的返回值 作为回调函数的参数# 2、执行回调函数 func2# 3、回调函数不传参数,他的参数只能是func1的返回值# 4、回调函数在主进程中执行 回调函数 – 爬虫1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import requestsfrom multiprocessing import Pool# response = requests.get('https://maoyan.com/board/4')# print(response) # 网页结果对象# # print(response.__dict__)# print(response.status_code) # 200# print(response.text)# 爬虫# 耗时最长 网络延迟# 1、访问网址# 2、将数据从网址上下载下来 urllib * 耗时最长 发送请求，拿到代码，接收代码# 3、数据就是bytes 转成 字符串# 4、处理字符串# 5个进程,任务是200个（访问200个）# 同时跑5个进程,一起享受网络延迟,如果在这个时候处理字符串，那么195个进程都在排队# 如果处理交给主进程来做，这5个进程的处理字符串,那么可以省出5个进程继续下载网页# - 下载网页1# - 下载网页2# - 下载网页3# - 下载网页4# - 下载网页5# ----- 处理字符串# 一般情况下,爬虫的时候，容易用到回调函数# 访问网页，爬取网页的过程用爬虫# 处理数据，使用回调函数# 流程:# 多进程去访问页面，拿到结果,返回url和页面内容# 回调函数打印url和网页内容长度# 使用进程池下载页面# 子进程处理下载页面def get_page(url): res = requests.get(url) if res.status_code == 200 : return url,res.text# 回调函数，接收网页内容# 打印页面内容长度def call_back(args): url,content = args print(url,len(content))if __name__ == '__main__': url_lst = [ 'https://www.baidu.com', 'https://www.sogou.com', 'http://www.sohu.com/', 'https://maoyan.com/board/4', ] p = Pool(5) for url in url_lst: p.apply_async(get_page,args=(url,),callback=call_back) p.close() p.join()# https://www.sogou.com 23447# https://www.baidu.com 2443# http://www.sohu.com/ 180835# https://maoyan.com/board/4 20754]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix自定义脚本监控redis]]></title>
    <url>%2F2019%2F04%2F02%2Fpro2%2F</url>
    <content type="text"><![CDATA[编写监控redis qps 脚本结合zabbix展示之前公司的redis info显示执行了6亿次命令，造成cpu过高问题，现写下shell采集redis执行命令次数和ops然后通过zabbix做监控 监控脚本123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/bashREDISCLI="/usr/local/bin/redis-cli"HOST="127.0.0.1"PORT=7007PASS="redis_pwd"if [[ $# == 1 ]];then case $1 in version) result=`$REDISCLI -h $HOST -a $PASS -p $PORT info server | grep -w "redis_version" | awk -F':' '&#123;print $2&#125;'` echo $result ;; connected_clients) result=`$REDISCLI -h $HOST -a $PASS -p $PORT info clients | grep -w "connected_clients" | awk -F':' '&#123;print $2&#125;'` echo $result ;; instantaneous_ops_per_sec) result=`$REDISCLI -h $HOST -a $PASS -p $PORT info Stats | grep -w "instantaneous_ops_per_sec" | awk -F':' '&#123;print $2&#125;'` echo $result ;; total_commands_processed) result=`$REDISCLI -h $HOST -a $PASS -p $PORT info Stats | grep -w "total_commands_processed" | awk -F':' '&#123;print $2&#125;'` echo $result ;; *) echo -e "\033[33mUsage: $0 &#123;version&#125;\033[0m" ;; esacelif [[ $# == 2 ]];then case $2 in keys) result=`$REDISCLI -h $HOST -a $PASS -p $PORT info | grep -w "$1" | grep -w "keys" | awk -F'=|,' '&#123;print $2&#125;'` echo $result ;; *) echo -e "33[33mUsage: $0 &#123;db0 keys|db0 expires|db0 avg_ttl&#125;33[0m" ;; esacfi 在本地做测试1234[work@scripts]$ sh /data/backup/zabbix/scripts/redis_status_7007.sh total_commands_processed11958[work@scripts]$ sh /data/backup/zabbix/scripts/redis_status_7007.sh instantaneous_ops_per_sec0 修改zabbix_agent配置文件1234567vim /etc/zabbix/zabbix_agentd.confInclude=/etc/zabbix/zabbix_agentd.d/ # 简易脚本的执行目录...# 自定义脚本执行UserParameter=Redis_7007.Info[*],/data/backup/zabbix/scripts/redis_status_7007.sh $1 $2UserParameter=Redis_7010.Info[*],/data/backup/zabbix/scripts/redis_status_7010.sh $1 $2 重启zabbix agent服务1sudo /etc/init.d/zabbix-agent restart 在zabbix server端通过get测试12[work@scripts]$ zabbix_get -s 内网监控IP -p 10050 -k "Redis_7007.Info[total_commands_processed]"12051 12[work@scripts]$ zabbix_get -s 内网监控IP -p 10050 -k "Redis_7007.Info[instantaneous_ops_per_sec]"0 在zabbix平台配置监控项和触发器]]></content>
      <categories>
        <category>Linux运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看java进程占用cpu过高]]></title>
    <url>%2F2019%2F04%2F01%2Fpro1%2F</url>
    <content type="text"><![CDATA[查看java进程占用cpu过高之前平台出现过一次java进程占用cpu 100%的问题，发现后排查的几率如下: 使用top查找哪个继承占用cpu过高 记录下进程如:14492 把进程的栈dump到文件里，以便后面的分析 1jstack 14492 &gt; cpu0401.log 看看这个进程里面哪些线程在占用cpu 1top -p 14492 -H # 选择占用最高的 PID=5159 接着要看刚才dump出来的cpu日志了，里面会有14492这个进程下面每个线程的栈信息，但是是十六进制显示的，所以先把5159转换成16进制 1printf %0x 5159 # 1427 在cpu日志里找PID=1427的线程 1vim cpu0401.log]]></content>
      <categories>
        <category>Linux运维</category>
      </categories>
      <tags>
        <tag>进程占用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket 网络编程]]></title>
    <url>%2F2019%2F03%2F26%2Fsocket-server%2F</url>
    <content type="text"><![CDATA[tcp协议 和 udp协议TCP（Transmission Control Protocol）可靠的、面向连接的协议（eg:打电话）、传输效率低全双工通信（发送缓存&amp;接收缓存）、面向字节流。使用TCP的应用：Web浏览器；电子邮件、文件传输程序。 UDP（User Datagram Protocol）不可靠的、无连接的服务，传输效率高（发送前时延小），一对一、一对多、多对一、多对多、面向报文，尽最大努力服务，无拥塞控制。使用UDP的应用：域名系统 (DNS)；视频流；IP语音(VoIP)。 socket的基本使用TCP 的socket — 基础对话版12345678910111213141516171819# server端import socketsk = socket.socket() # 创建套接字sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1) # 避免重用ip和端口sk.bind(('127.0.0.1',8090)) # 绑定端口sk.listen() # 监听链接conn,addr = sk.accept() # 接收客户端链接ret = conn.recv(1024) # 接收客户端消息print(ret) # 打印客户端消息conn.send(b'hello') # 向客户端发送消息 在网络上传输的只有二进制1010,所以必须是bytes类型ret = conn.recv(1024).decode('utf-8')print(ret)conn.send('吃面条吧'.encode('utf-8'))conn.close() # 关闭客户端链接sk.close() # 关闭服务端 123456789101112131415# client端import socketsk = socket.socket() # 创建客户端套接字sk.connect(('127.0.0.1',8090)) # 链接服务端# 接收|发送消息sk.send(b'hey') # 发送消息ret = sk.recv(1024) # 接收消息print(ret)sk.send('中午吃什么'.encode('utf-8'))ret = sk.recv(1024).decode('utf-8')print(ret)sk.close() 循环对话版12345678910111213141516171819202122232425262728# server端import socketsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端开始监听...')conn,addr = sk.accept() # 获取到一个客户端的链接，已经完成了三次握手建立了一个连接 # 阻塞print('有新的链接进入&#123;&#125;'.format(addr))# 对话while 1: ret = conn.recv(1024).decode('utf-8') # 阻塞，直到收到客户端发来的消息 print(ret) if ret == 'bye': conn.send('bye'.encode('utf-8')) break msg = input('server:&gt;&gt;&gt;') if msg == 'bye': conn.send('bye'.encode('utf-8')) break conn.send(msg.encode('utf-8'))conn.close()sk.close() 1234567891011121314151617# client端import socketsk = socket.socket()sk.connect(('127.0.0.1',8090))while True: msg = input('client:&gt;&gt;&gt;') if msg == 'bye': sk.send('bye'.encode('utf-8')) break sk.send(msg.encode('utf-8')) ret = sk.recv(1024).decode('utf-8') if ret == 'bye': break print(ret)sk.close() 时间戳转换1234567891011121314151617181920212223242526272829303132ip_port = ('127.0.0.1',8090)# server端# server 接收时间戳时间，转化成格式化时间# client 每10秒time.time() 吧时间戳时间发给serverimport sysimport ossys.path.insert(0,os.path.dirname(os.getcwd()))import socketimport timefrom conf import settingssk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(settings.ip_port)sk.listen()print('服务端开始监听...')conn,addr = sk.accept()print('有新的链接进入&#123;&#125;'.format(addr))# 收发消息while 1: res = conn.recv(1024).decode('utf-8') if res == '': break print('client:%s' %res) msg = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(float(res))) print(type(msg),msg) conn.send(('server:%s' %msg).encode('utf-8'))conn.close()sk.close() 12345678910111213141516171819202122# client端import timeimport socket# msg = time.time()# print(type(msg),msg) # float类型sk = socket.socket()sk.connect(('127.0.0.1',8090))count = 1while 1: if count &gt; 3: break now_time = str(time.time()) print(now_time) sk.send(now_time.encode('utf-8')) ret = sk.recv(1024).decode('utf-8') print(ret) time.sleep(3) count += 1sk.close() UDP 的socket — 基础对话版1234567891011# server import socketsk = socket.socket(type=socket.SOCK_DGRAM) # 创建一个服务器的套接字sk.bind(('127.0.0.1',8090)) # 绑定服务器套接字# 对话(接收与发送)msg,addr = sk.recvfrom(1024)print(msg.decode('utf-8'))sk.sendto(b'bye',addr) # 发送消息要带着地址sk.close() # 关闭服务器套接字 1234567891011# clientimport socketip_port = ('127.0.0.1',8090)sk = socket.socket(type=socket.SOCK_DGRAM)# 对话(接收与发送)sk.sendto(b'hello',ip_port) # 发送消息要带着地址ret,addr = sk.recvfrom(1024)print(ret.decode('utf-8'))sk.close() 在udp的消息通信的时候: 不需要进行监听 (listen) 不需要建立链接 (accept) 在启动服务之后，只能被动的等到客户端发送消息过来 客户端发送消息的同时，还会带着地址信息过来 服务端进行消息服务的时候，不仅需要发送消息，还需要带着对方的地址回去 UDP 实现简易QQ1234567891011121314151617# server import socketudp_sk = socket.socket(type=socket.SOCK_DGRAM)ip_port = ('127.0.0.1',9999)udp_sk.bind(ip_port)print('Bind UDP on 9999...')while True: # 接收数据: data,addr = udp_sk.recvfrom(1024) data = data.decode('utf-8') print(('Received from %s:%s') %(addr,data)) msg = input('server:&gt;&gt;&gt;') udp_sk.sendto(msg.encode('utf-8'),addr)udp_sk.close() 123456789101112131415# client import socketip_port = ('127.0.0.1',9999)udp_sk = socket.socket(type=socket.SOCK_DGRAM)while True: msg = input('client:&gt;&gt;&gt;') msg = '\033[32mform client1:%s\033[0m' %msg udp_sk.sendto(msg.encode('utf-8'),ip_port) data,addr = udp_sk.recvfrom(1024) data = data.decode('utf-8') print(('Received from %s:%s') %(addr,data))udp_sk.close() UDP 实现简易时间同步123456789101112131415161718192021222324# server # server端提供时间同步服务# 接收信息 时间的格式# 将server端时间 转换成 接收到的格式# 返回给clientimport socketimport timesk = socket.socket(type=socket.SOCK_DGRAM)ip_port = ('127.0.0.1',9999)sk.bind(ip_port)print('Bind UDP on 9999...')while True: data,addr = sk.recvfrom(1024) data = data.decode('utf-8') # print(type(data),data) print('form %s:%s' %(addr,data)) # time_str = time.strftime(data,time.localtime(time.time())) time_str = time.strftime(data) print(type(time_str),time_str) sk.sendto(str(time_str).encode('utf-8'),addr)sk.close() 1234567891011121314151617# client import socketsk = socket.socket(type=socket.SOCK_DGRAM)ip_port = ('127.0.0.1',9999)time_format = '%Y-%m-%d %H:%M:%S'sk.sendto(time_format.encode('utf-8'),ip_port)data,addr = sk.recvfrom(1024)data = data.decode('utf-8')print(data)sk.close()# 操作方式:# 1、操作系统定时任务 + python代码# 2、while True + time.sleep 黏包现象subprocess 远程执行命令12345678import subprocessres = subprocess.Popen('dir',shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)print('stdout:' + res.stdout.read().decode('gbk')) # windows默认控制台输出 gbkprint('stderr:' + res.stderr.read().decode('gbk')) # windows默认控制台输出 gbk# stdout=subprocess.PIPE 标准输出放入管道# stderr=subprocess.PIPE 错误输出放入管道# 结果的编码是以当前所在的系统为准的，如果是windows，那么res.stdout.read()读出的就是GBK编码的，在接收端需要用GBK解码 TCP的黏包现象12345678910111213141516171819202122232425# 在server端下发命令import socketsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务器端口8090开始监听')conn,addr = sk.accept()print('有新的链接请求&#123;&#125;'.format(addr))while 1: cmd = input('cmd:&gt;&gt;&gt;') conn.send(cmd.encode('utf-8')) ret = conn.recv(1024).decode('utf-8') # windows控制台是GBK print(ret)conn.close()sk.close()# 执行的命令# 1. dir;ls# 2. ipconfig# 发生的问题:# 象数据没有接收完全 或者 接收多了 这种现象就是“黏包” 12345678910111213141516171819202122# 在client端接收命令并执行返回import socketimport subprocesssk = socket.socket()sk.connect(('127.0.0.1',8090))while True: cmd = sk.recv(1024).decode('gbk') res = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) # print('stdout:' + res.stdout.read().decode('gbk')) # windows默认控制台输出 gbk # print('stderr:' + res.stderr.read().decode('gbk')) # windows默认控制台输出 gbk # 这里的res得到的是gbk格式，需要解码整体转换成str std_out = 'stdout: ' + (res.stdout.read()).decode('gbk') std_err = 'stderr: ' + (res.stderr.read()).decode('gbk') print(type(std_out),std_out) print(type(std_err),std_err) sk.send(std_out.encode('utf-8')) sk.send(std_err.encode('utf-8'))sk.close() UDP的黏包现象12345678910111213141516171819202122232425262728293031# serverimport socketsk = socket.socket(type=socket.SOCK_DGRAM)ip_port = ('127.0.0.1',9999)sk.bind(ip_port)print('Bind UDP on 9999...')data,addr = sk.recvfrom(1024)while 1: cmd = input('cmd:&gt;&gt;&gt;') if cmd == 'q': break sk.sendto(cmd.encode('utf-8'),addr) data,addr = sk.recvfrom(10240) print(data.decode('utf-8'))sk.close()# 操作：# 1. ipconfig# 2. dir# udp# udp不会黏包# udp会丢包# tcp# tcp会黏包# tcp不会丢包# 内部优化算法 让整个程序发送数据和接收数据没有边界 12345678910111213141516171819202122# clientimport socketimport subprocesssk = socket.socket(type=socket.SOCK_DGRAM)ip_port = ('127.0.0.1',9999)sk.sendto('吃了吗？'.encode('utf-8'),ip_port)while 1: cmd,addr = sk.recvfrom(1024) # bytes if cmd == 'q': break res = subprocess.Popen(cmd.decode('gbk'),shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) std_out = 'std_out:' + res.stdout.read().decode('gbk') std_err = 'std_err:' + res.stderr.read().decode('gbk') print(std_out) print(std_err) sk.sendto(std_out.encode('utf-8'),addr) sk.sendto(std_err.encode('utf-8'),addr)sk.close() 触发黏包会发生黏包的两种情况 发送方的缓存机制发送端需要等缓冲区满才发送出去，造成粘包（发送数据时间间隔很短，数据很小，会合到一起，产生粘包） 123456789101112131415161718# server# 1. server 先把数据给操作,操作系统再传给对面的操作系统# 2. client 操作系统把接收到的消息 给client程序# 3. 如果发送了10个数据 由于第一次服务端接收2个，还剩下8个# 4. tcp协议在接收端有缓存机制，直到下次接收再给# 5. 第一次没有全部接收，后面就全发给接收方import socketsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')conn,addr = sk.accept()ret = conn.recv(2)ret2 = conn.recv(10)print(ret) # b'he'print(ret2) # b'llo,egg' 1234567import socketsk = socket.socket()sk.connect(('127.0.0.1',8090))sk.send(b'hello,egg')sk.close() 接收方的缓存机制 1234567891011121314151617# server # 1. 优化算法，连续的小数据包会被合并# 2. windows系统上 客户端在结束的时候会发送一个空消息 低版本会报错# 3. 多个send 小的数据连在一起，会发生黏包现象，是Tcp协议内部的优化算法造成的# 4. 连续使用了send引起的import socketsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')conn,addr = sk.accept()ret = conn.recv(12)print(ret) # b'helloegg'ret2 = conn.recv(12)print(ret2) 1234567891011# clietn import socketimport timesk = socket.socket()sk.connect(('127.0.0.1',8090))sk.send(b'hello')# time.sleep(3)sk.send(b'egg')sk.close() 总结黏包现象黏包现象只发生在tcp协议中： 从表面上看，黏包问题主要是因为发送方和接收方的缓存机制、tcp协议面向流通信的特点。 实际上，主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。 解决黏包发送消息长度 黏包问题的根源在于，接收端不知道发送端将要传送的字节流的长度，所以解决粘包的方法就是围绕，如何让发送端在发送数据前，把自己将要发送的字节流总大小让接收端知晓，然后接收端来一个循环接收完所有数据。 存在的问题:程序的运行速度远快于网络传输速度，所以在发送一段字节前，先用send去发送该字节流长度，这种方式会放大网络延迟带来的性能损耗。 12345678910111213141516171819202122232425262728# server# 黏包的本质问题 — 你不知道到底要接收多大的数据# 解决: # 首先发送,这个数据到底有多大# 再按照数据的长度,接收数据import socketsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')conn,addr = sk.accept()print('有新的客户端链接&#123;&#125;'.format(addr))while True: cmd = input('server: ').encode('gbk') if cmd == 'q': break conn.send(cmd) num = conn.recv(4) # 接收消息的长度 # print('接收消息长度:%d' %(int(num))) print(num) conn.send(b'ok') # 发送消息应答 ret = conn.recv(int(num)).decode('gbk') # 1024修改成要接收的数据长度 print(ret)conn.close()sk.close() 1234567891011121314151617181920212223# clientimport socketimport subprocesssk = socket.socket()sk.connect(('127.0.0.1',8090))while True: cmd = sk.recv(1024).decode('gbk') res = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) std_out = res.stdout.read() std_err = res.stderr.read() # print(type(std_out)) # &lt;class 'bytes'&gt; # print(type(std_err)) print(str(len(std_out)+len(std_err)).encode('utf-8')) # 得到消息长度 b'455' sk.send(str(len(std_out)+len(std_err)).encode('utf-8')) # 发送消息长度 # 发送一次消息的长度 sk.recv(4096) # 接收应答 sk.send(std_out) sk.send(std_err)sk.close() 进阶方法—使用struct模块12345678910111213141516# struct模块作用:把一个类型，如数字，转成固定长度的bytes# 1、什么是固定长度的bytesimport structret = struct.pack('i',20491) # i 代表int,即将要把一个数字转换成固定长度的bytes类型print(len(ret),ret) # 4 b'\x01\x08\x00\x00' 太长的数据不够位数会有字母和符号代替，超过长度会报错# struct.pack('i',1111111111111)# truct.error: 'i' format requires -2147483648 &lt;= number &lt;= 2147483647 #这个是范围num = struct.unpack('i',ret)print(type(num),num) # &lt;class 'tuple'&gt; (2049,)print(num[0]) # 2049# 2、为什么要转成固定长度的# 发送数据的时候# 客户端先发送长度，服务端先接收长度 123456789101112131415161718192021222324252627282930# serverimport socketimport structsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')conn,addr = sk.accept()print('有新的客户端链接&#123;&#125;'.format(addr))while True: cmd = input('server:&gt;&gt;&gt;').encode('gbk') if cmd == 'q': conn.send(b'bye') break conn.send(cmd) # 接收消息长度 客户端使用struct固定传值4个bytes num = conn.recv(4) # 4 num = struct.unpack('i',num)[0] # 消息长度大小 print(num) ret = conn.recv(int(num)).decode('gbk') print(ret)conn.close()sk.close() 1234567891011121314151617181920212223242526# clientimport socketimport subprocessimport structsk = socket.socket()sk.connect(('127.0.0.1',8090))while True: cmd = sk.recv(1024).decode('gbk') if cmd == 'q': break res = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) std_out = res.stdout.read() std_err = res.stderr.read() len_num = len(std_out)+len(std_err) num_by = struct.pack('i',len_num) # print(type(num_by),num_by) # &lt;class 'bytes'&gt; b'\xc8\x01\x00\x00' sk.send(num_by) # 发送固定4个字节，消息长度 sk.send(std_out) sk.send(std_err)sk.close() 使用struct解决黏包借助struct模块，我们知道长度数字可以被转换成一个标准大小的4字节数字。因此可以利用这个特点来预先发送数据长度。 我们还可以把报头做成字典，字典里包含将要发送的真实数据的详细信息，然后json序列化，然后用struck将序列化后的数据长度打包成4个字节（4个自己足够用了） 实现大文件上传和下载1234567891011121314151617181920212223242526272829303132333435363738394041424344# server 接收端import socketimport jsonimport structsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')buffer = 1024 # 按照服务器性能调整conn,addr = sk.accept()print('有新的客户端链接&#123;&#125;'.format(addr))# 1. 接收报头长度pack_head = conn.recv(4) # 接收报头len_head = struct.unpack('i',pack_head)[0] # 使用struct.unpack得到报头长度# print(len_head)# 2. 接收数据报头js_head = conn.recv(len_head).decode('utf-8') # 接收bytes类型的数据报文js_head = json.loads(js_head) # json报文转字典# print(js_head)# 3. 获取文件长度和文件名称file_size = js_head['file_size']file_name = js_head['file_name']# 4. 打开文件,接收数据with open(file_name,mode='wb') as f: while file_size: print(file_size) if file_size &gt;= buffer: context = conn.recv(buffer) f.write(context) file_size -= buffer else: context = conn.recv(buffer) f.write(context) breakconn.close()sk.close() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# client 发送端import socketimport osimport structimport jsonsk = socket.socket()sk.connect(('127.0.0.1',8090))# 发送文件 定制报头# 发送流程:# 1. 先发报头长度# 2. 再发by_head报头# 3. 发送报文# 1. 组织数据字典head = &#123; 'file_name': '01 软件简介 软件分类.avi', 'file_path': 'D:\\', 'file_size': None&#125;file_path = os.path.join(head['file_path'],head['file_name']) # 获取文件路径# print(file_path)file_size = os.path.getsize(file_path) # 获取文件大小head['file_size'] = file_sizehead['all_file_path'] = file_sizebuffer = 4096# 2. 数据报头长度js_head = json.dumps(head,ensure_ascii=False)# print(js_head)by_head = js_head.encode('utf-8')len_head = len(by_head)pack_head = struct.pack('i',len_head)# 3. 发送报头长度sk.send(pack_head)# 4. 发送数据报头sk.send(by_head)# 5. 打开文件发送文件with open(file_path,mode='rb') as f: while file_size: print(file_size) if file_size &gt;= buffer: context = f.read(buffer) sk.send(context) file_size -= buffer else: context = f.read(buffer) sk.send(context) breaksk.close() 自定制报头 发送数据12345678910111213141516171819202122232425262728293031323334# serverimport socketimport structimport jsonsk = socket.socket()sk.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sk.bind(('127.0.0.1',8090))sk.listen()print('服务端8090端口开始监听...')conn,adrr = sk.accept()print('有新的客户端链接&#123;&#125;'.format(adrr))while True: cmd = input('server:&gt;&gt;&gt;').encode('gbk') conn.send(cmd) if cmd == 'q': break len_head = conn.recv(4) len_head = struct.unpack('i',len_head)[0] print(int(len_head)) # 接收报头 by_head = conn.recv(int(len_head)).decode('utf-8') head = json.loads(by_head) print(head,type(head)) # 通过报头长度接收消息 res = conn.recv(head['info_size']).decode('gbk') print(res)conn.close()sk.close() 1234567891011121314151617181920212223242526272829303132333435363738394041# clientimport socketimport structimport jsonimport subprocess# 组织报头head = &#123; 'info_size':None&#125;sk = socket.socket()sk.connect(('127.0.0.1',8090))while True: cmd = sk.recv(1024).decode('gbk') if cmd == 'q': break res = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE) std_out = res.stdout.read() std_err = res.stderr.read() # 先发报头长度 # 1. 将字典转成by类型 head['info_size'] = len(std_out)+len(std_err) js_head = json.dumps(head,ensure_ascii=False) by_head = js_head.encode('utf-8') # 2. 使用struct将报头长度转成固定字节 len_head = len(by_head) print(len_head) pack_len = struct.pack('i',len_head) # 3. 发送报头长度 sk.send(pack_len) # 4. 发送报头 sk.send(by_head) # 5. 发送信息 sk.send(std_out) sk.send(std_err)sk.close()]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lambda 匿名函数]]></title>
    <url>%2F2019%2F03%2F25%2Fanonymous%2F</url>
    <content type="text"><![CDATA[匿名函数基础语法匿名函数的作用: 为了解决那些功能很简单的需求而设计的一句话函数 语法：func(函数名) = lambda(关键字) 参数:返回值 参数可以有多个，用逗号隔开 匿名函数不管逻辑多复杂，只能写一行，且逻辑执行结束后的内容就是返回值 返回值和正常的函数一样可以是任意数据类型 123456# 把以下函数变成匿名函数def add(x,y): return x+y add = lambda x,y:x+yprint(add(1,2)) # 3 匿名函数和其他功能函数合作lambda 常常和几个内置函数一起使用 max min sorted filter map 和max函数配合12345678910111213141516171819202122# 字典取最大值的key# 字典取最大值dic=&#123;'k1':10,'k2':100,'k3':30&#125;ret = max(dic,key=dic.get)print(dic[ret]) # 100# 字典取最大值的键dic2=&#123;'k1':10,'k2':100,'k3':30&#125;ret1 = max(dic2)print(ret1) # k3 key按照文本大小的最大值# 字典取最大值的键dic2=&#123;'k1':10,'k2':100,'k3':30&#125;def max_dict_key(key): return dic2[key]ret = max(dic2,key=max_dict_key)print(ret) # k2# lambda 匿名一句话函数ret = max(dic2,key=lambda key:dic2[key])print(ret) # k2 和filter函数配合123456789101112# filter 配合 lambda#取出数组中大于10的数据 [5,8,11,9,15]def func(x): return x &gt; 10ret = filter(func,[5,8,11,9,15])for i in ret: print(i) # 11 15# lambda 匿名一句话函数ret = filter(lambda x:x &gt; 10,[5,8,11,9,15])for i in ret: print(i) # 11 15 和map函数配合1234567891011121314ret = map(abs,[-1,2,-3,4])for i in ret: print(i) # 1,2,3,4def func_map(x): return x**2ret = map(func_map,[-1,2,-3,4])for i in ret: print('func_map:',i)# lambda 匿名一句话函数ret = map(lambda x:x**2,[-1,2,-3,4])for i in ret: print('func_map_lanbda:',i) 匿名函数面试题12345678# 1.下面程序的输出结果是：d = lambda p:p*2t = lambda p:p*3x = 2x = d(x) # 4x = t(x) # 12x = d(x) # 24print(x) # 24 1234567891011121314151617# 2.现有两元组(('a'),('b')),(('c'),('d')) ,请使用python中匿名函数生成列表[&#123;'a':'c'&#125;,&#123;'b':'d'&#125;]# lambda# zipret = zip((('a'),('b')),(('c'),('d')))# for i in ret:# print(i)# def func(tup):# return &#123;tup[0]:tup[1]&#125;# res = map(func,ret)# for i in res:# print(i)res = map(lambda tup:&#123;tup[0]:tup[1]&#125;,ret)print(list(res)) # [&#123;'a': 'c'&#125;, &#123;'b': 'd'&#125;]]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>匿名函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内置函数]]></title>
    <url>%2F2019%2F03%2F19%2Ffunction%2F</url>
    <content type="text"><![CDATA[内置函数什么是内置函数? 就是python给你提供的. 拿来直接用的函数,比如print., input等等.截止 到python版本3.6.2 python一共提供了68个内置函数. 有一些我们已经用过了.有一些还没有用过. 还有一些需要学完了面向对象才能继续学习的. 作用域相关(2) locals() 和 globals() 基于字典的形式获取局部变量和全局变量12print(locals()) # 返回本地作用域中的所有名字print(globals()) # 返回全局作用域中的所有名字 迭代器/生成器相关(3) range(),next() 和 iter()Python3 range() 函数返回的是一个可迭代对象（类型是对象），而不是列表类型， 所以打印的时候不会打印列表。1234567# range# range(10)# range(1,11)# range(1,11,2) # 步长取值print('__iter__' in dir(range)) # True 可迭代的print('__next__' in dir(range)) # False 不是迭代器print('__next__' in dir(iter(range(1,11,2)))) # True next(迭代器),迭代器向下执行一次,内部实际上调用迭代器.__next__()方法iter(可迭代的),用来生成一个迭代器12345678910# next 和 iterl = [1,2,3,4,5]it = iter(l) # 转成迭代器 print(type(it)) # &lt;class 'list_iterator'&gt;while 1: try: print(next(it)) except StopIteration: # 遇到StopIteration就退出循环 break 其他(12) 查看内置属性(1) dir() 默认查看全局空间内的属性，也接受一个参数，查看这个参数内的方法或变量12print(dir()) # 获得当前模块的属性列表print(dir([])) # 查看列表的方法 调用相关(1) callable() callable 用来检查一个对象是否可被调用对于函数、方法、lambda 函式、 类以及实现了 __call__ 方法的类实例, 它都返回 True。123a = 1print(callable(a)) # Falseprint(callable(print)) # True 帮助(1) help() 用于查看函数或模块用途的详细说明在控制台执行help()进入帮助模式。可以随意输入变量或者变量的类型。输入q退出或者直接执行help(o)，o是参数，查看和变量o有关的操作。。。1help(str) 模块相关(1) import() 导入模块1import time 文件操作相关(1) open() 打开一个文件，返回一个文件操作符(文件句柄)操作文件的模式有r,w,a,r+,w+,a+ 共6种，每一种方式都可以用二进制的形式操作(rb,wb,ab,rb+,wb+,ab+)可以用encoding指定编码.123f = open('01 内置函数.py')print(f.writable()) # 判断当前文件是否可写print(f.readable()) # 判断当前文件是否可读 内存相关(2) id() id()函数用于获取对象的内存地址。123# id(o) o是参数，返回一个变量的内存地址a = 100print(id(a)) # 1497027344 hash() 用于获取取一个对象（字符串或者数值等）的哈希值。hash() 函数可以应用于数字、字符串和对象，不能直接应用于 list、set、dictionary。获取到对象的哈希值(int, str, bool, tuple) hash函数会根据一个内部的算法对当前可hash变量进行处理，返回一个int数字。 每一次执行程序，内容相同的变量hash值在这一次执行过程中不会发生改变。1234print(hash(12345)) # 12345print(hash('abcde')) # -5832084034581495945print(hash(('a','b'))) # -3079515087831999849# print(hash(['a',1,'b',2])) # 报错:TypeError: unhashable type: 'list' 不可哈希 输入输出(2) input() 获取用户输入12content = input('&gt;&gt;&gt;')print(type(content),content) # input得到的是字符串类型 print() 打印输出12345# 关键字传参 end默认为'\n'，指定不是回车即可# 这就是我们为什么使用print的时候会出现换行,end的值修改成了空字符串print('我们的祖国是花园\n',end='')print('我们的祖国是花园\n',end='')print('我们的祖国是花园\n') 123# sep 打印多个值之间的分隔符，默认为空格print(1,2,3,4,5) # 1 2 3 4 5 多个值之间空格隔开print(1,2,3,4,5,sep='|') # 1|2|3|4|5 指定分隔符 12345# file: 默认是输出到屏幕，如果设置为文件句柄，输出到文件# flush: 立即把内容输出到流文件，不作缓存f = open('file','w')print('aaa',file=f,flush=True)f.close() 字符串类型代码的执行(3) eval() eval() 将字符串类型的代码执行并返回结果1print(eval('1+2+3+4')) # 10 有返回值 ——有结果的简单计算 exec() exec() 将自字符串类型的代码执行12345print(exec('1+2+3+4')) # None 没有返回值 ——简单的流程控制# exec 和eval都可以执行 字符串类型的代码# 区别是eval有返回值，exec没有# eval只能用在明确知道要执行的代码 compile() 将字符串类型的代码编译。代码对象能够通过exec语句来执行或者eval()进行求值。1234#流程语句使用exec# code1 = 'for i in range(0,10): print (i)'# compile1 = compile(code1,'','exec')# exec (compile1) 1234#简单求值表达式用eval# code2 = '1 + 2 + 3 + 4'# compile2 = compile(code2,'','eval')# print(eval(compile2)) 12345#交互语句用singlecode3 = 'name = input("please input your name:")'compile3 = compile(code3,'','single')exec(compile3)# print(name) # leo 执行exec后就存在name 基础数据类型相关(38)和数字相关(14) bool() bool() 函数用于将给定参数转换为布尔类型，如果没有参数，返回 False。int –&gt; bool 非0为True，0为False1234n1 = 10print(bool(n1)) # Truen2 = 0print(bool(n2)) # False int() int() 函数用于将一个字符串或数字转换为整型。​int() 将给定的数据转换成int值. 如果不给值, 返回012345678# int# str 转换成 int 只能转换数字字符串str1 = '10'print(int(str1)) # 10# int 转换成 str 数字可以转换成任何字符串num = 5print(str(num)) float() ﬂoat() 将给定的数据转换成ﬂoat值. 也就是浮点数 浮点数包括: 有限循环小数 无线循环小数 小数包括: 有限循环小数 无线循环小数 无线不循环小数12345# 浮点数# 354.123 == 3.54123 * 10**2 == 35.4123 * 10 在这个过程中点是浮动的 所以才叫浮点数f = 1.78789787079889 # 当小数特别长的时候 就有可能不准了 二进制转小数会有问题就会不准print(float(1)) # 1.0print(float('123') ) # 转换字符串 123.0 complex() complex() 创建一个复数. 第一个参数为实部, 第二个参数为虚部. 或者第一个参数直接 用字符串来描述复数 实数: 有理数 ： 整数 有限循环小数 无线循环小数 无理数 ： 无线不循环小数 π 虚数: 虚无缥缈的数123# python里面的虚数 = 12j (j是单位)# 5 + 12j === 复合的数 == 复数 (复数之间是无法比较大小的)print(complex(1, 2)) # (1+2j) bin() 1print(bin(10)) # 十进制转二进制 # 0b1010 oct() 1print(oct(10)) # 十进制转十进制 # 0o12 hex() 1print(hex(10)) # 十进制转十六进制 # 0xa abs() abs() 函数返回数字的绝对值。123# abs求绝对值 负的转正的 正的还是正的print(abs(-5)) # 5print(abs(10)) # 10 divmod() 12345# divmod 接收两个参数 div 除法 mod 取余# 除余方法# 分页的时候 会用到print(divmod(7,2)) # (3, 1)print(divmod(9,5)) # (1, 4) round() round() 方法返回浮点数x的四舍五入值。1print(round(3.14159,2)) # 3.14 2代表保留两位 支持四舍五入 pow() 1234# pow 求幂运算print(pow(2,3)) # 8print(pow(3,2,1)) # 0 三个参数就是 == 3的2次幂 对 1取余 幂运算之后再取余print(pow(2,3,3)) # 2 sum() ​sum() 求和12345678# sum(iterable[, start])# start 从几开始相加# 列表计算总和后再加10ret = sum([1,2,3],10)print(ret) # 16ret = sum((1,2,3))print(ret) # 6 min() min()计算最小值12345# min(iterable,key,default)# min(*args,key,default)print(min([1,2,3])) # 1print(min(1,2,3)) # 1print(min((1,2,3,-4),key=abs)) # 1 key=abs 以绝对值的方法来计算 max() min()计算最大值12345# max(iterable,key,default)# max(*args,key,default)print(max([1,2,3])) # 3print(max(1,2,3)) # 3print(max((1,2,3,-4),key=abs)) # -4 key=abs 以绝对值的方法来计算 和数据结构相关(24) list() list() 用于将元组或字符串转换为列表。注：元组与列表是非常类似的，区别在于元组的元素值不能修改，元组是放在括号中，列表是放于方括号中。1234str1="Hello World"print(list(str1)) # ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd']tup1 = (1,2,3,'leo')print(list(tup1)) # [1, 2, 3, 'leo'] tuple() tuple()函数将列表转换为元组。。12l1= ['Google', 'Taobao', 'Runoob', 'Baidu']print(tuple(l1)) # ('Google', 'Taobao', 'Runoob', 'Baidu') reversed() reversed() 返回一个反向的迭代器12345678910# list.reverse() # 列表的反转方法l2 = [1,2,3,4]l2.reverse()print(l2) # [4, 3, 2, 1] 原本的列表发生变化l3 = [1,2,3,4,5]iter_l = reversed(l3) # 保留原列表,返回一个反向的迭代器print(iter_l) # &lt;list_reverseiterator object at 0x000000000288CF98&gt; 迭代器for i in iter_l: print(i) slice() slice() 函数实现切片对象，主要用在切片操作函数里的参数传递。1234l = (1,2,23,213,5612,342,43)sli = slice(1,5,2) # 切片规则print(l[sli]) # (2, 213)print(l[1:5:2]) # (2, 213) str() str() 将数据转化成字符串1234l4 = [1,2,3]print(str(l4)) # [1, 2, 3]dict4 = &#123;'name':'leo'&#125;print(str(dict4)) # &#123;'name': 'leo'&#125; format() format() 字符串格式化1234567print("&#123;&#125;,&#123;&#125;".format('leo','lex')) # leo,lex 不设置指定位置，按默认顺序print("&#123;0&#125;,&#123;1&#125;,&#123;0&#125;".format('leo','lex')) # leo,lex,leo 设置指定位置print("名字:&#123;name&#125;,年龄&#123;age&#125;".format(name='leo',age='30')) # 名字:leo,年龄30# 通过字典设置参数info = &#123;'name':'leo',"age":29&#125;print("名字:&#123;name&#125;,年龄&#123;age&#125;".format(**info)) # 名字:leo,年龄29 bytes() bytes 将数据转换成bytes类型12345# 拿到的事gbk编码，想要转换成utf-8编码print(bytes('您好',encoding='GBK')) # b'\xc4\xfa\xba\xc3' unicode转成 GBKprint(bytes('您好',encoding='utf-8')) # b'\xe6\x82\xa8\xe5\xa5\xbd' unicode 转 utf-8# gbk -&gt; decode（解码） unicode -&gt; encode(编码) utf-8 bytearray() bytes类型的数组123b_array = bytearray('您好',encoding='utf-8')print(b_array) # bytearray(b'\xe6\x82\xa8\xe5\xa5\xbd')print(b_array[0]) # 230 memoryview() memoryview() 函数返回给定参数的内存查看对象(Momory view)。所谓内存查看对象，是指对支持缓冲区协议的数据进行包装，在不需要复制对象基础上允许Python代码访问。1234ret = memoryview(bytes('你好',encoding='utf-8'))print(len(ret))print(bytes(ret[:3]).decode('utf-8'))print(bytes(ret[3:]).decode('utf-8')) ord() 字符按照unicode转数字123print(ord('A')) # 65print(ord('a')) # 97print(ord('1')) # 49 chr() 数字按照unicode转字符12print(chr(65)) # Aprint(chr(49)) # 1 ascii() 只要是ascii码(字母、数字、符号、拉丁文)就显示，不是的话就打印\u类型12print(ascii('好')) # '\u597d'print(ascii('l')) # l repr() repr格式化,原形毕露12345name = 'egg'print('你好%s' %name) # 你好egg %s ==&gt; strprint('你好%r' %name) # 你好'egg' %r ==&gt; reprprint(repr('1')) # '1'print(repr(1)) # 1 dict() 创造字典12print(dict()) # &#123;&#125;print(dict(a=1,b=2,c=3)) # &#123;'a': 1, 'b': 2, 'c': 3&#125; set() set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等123456x = set('runoob')y = set('google')print((set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) ) # 重复的被删除print(x &amp; y) # 交集 &#123;'o'&#125;print(x | y ) # 并集 &#123;'r', 'o', 'l', 'n', 'e', 'u', 'b', 'g'&#125;print(x - y ) # 差集 &#123;'r', 'b', 'n', 'u'&#125; frozenset() 生成一个新的不可变集合,它可以作为字典的key12a = frozenset(range(10)) # 生成一个新的不可变集合b = frozenset('runoob') 重要的内置参数 len() 返回对象的长度或者元素个数1234test = 'abcde'list1 = [1,2,3]print(len(test)) # 5 字符串长度print(len(list1)) # 3 列表元素个数 enumerate() enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中enumerate(sequence, [start=0]) sequence – 一个序列、迭代器或其他支持迭代对象。 start – 下标起始位置。123456seasons = ['Spring', 'Summer', 'Fall', 'Winter']list2 = list(enumerate(seasons))print(list2) # [(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]list3 = list(enumerate(seasons, start=1)) # 小标从 1 开始print(list3) # [(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')] all() 有任何一个空内容就是false123print(all(['a','',123])) # Falseprint(all(['a',123])) # Trueprint(all([0,123])) # False any() 有一个正确的就是True1print(any(['',True,0,[]])) # True zip() zip 拉链方法,如果少一个元素无法对应上，就不加入,以最小的数据类型为准12345678910l1 = [1,2,3]l2 = ['a','b','c','d']t3 = ('*','**',[1,2])d4 = &#123;'k1':'1','k2':'2'&#125;print(zip(l1,l2)) # &lt;zip object at 0x00000000028422C8&gt;for i in zip(l1,l2,t3,d4): print(i)# (1, 'a', '*', 'k1')# (2, 'b', '**', 'k2') filter() filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象,如果要转换为列表，可以使用 list() 来转换。filter() 函数接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。1234567891011121314151617# 过滤列表中的奇数def is_odd(x): return x % 2 == 1 # 奇数# 第一个参数 函数方法# 第二个参数 可迭代的# 可迭代的里面的每个数据都会传入前面的函数# 根据函数的结算结果筛选,为True的才会留下，最后返回一个迭代器ret = filter(is_odd,[1,3,5,8,10])print(ret) # &lt;filter object at 0x00000000027A8780&gt; 迭代器# for i in ret:# print(i) # 迭代器节省内存print(list(ret)) # [1, 3, 5]# 相当于列表推导式l2 = [i for i in [1,3,5,8,10] if i % 2 == 1]print(l2) # [1, 3, 5] 12345678# 名字有两个e的结果names = ['leo','leex','rubin','lee']def func(name): return name.count('e') &gt;= 2iter_names = filter(func,names)for i in iter_names: print(i) # leex,lee 12345678910# 只保留字符串l3 = [1,'leo',2,'rubin',3]def is_str(s):# if type(s) == str:# return True return type(s) == striter_names = filter(is_str,l3)for i in iter_names: print(i) # leo,rubin 123456789# 删除列表中的None和空字符串l4 = ['test', None, '', 'str', ' ', 'END']def is_none(s): if type(s) != int: return s and str(s).strip()iter_none = filter(is_none,l4)for i in iter_none: print(i) 12345678# 练习：请利用filter()过滤出1~100中平方根是整数的数，即结果应该是：from math import sqrt # 开平方print(sqrt(64)) # 8.0def init_sqrt(num): res = sqrt(num) return res % 1 == 0 # 除1余0就是整数iter_sqrt = filter(init_sqrt,range(1,101))print(list(iter_sqrt)) # [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] map() Python中的map函数应用于每一个可迭代的项，返回的是一个结果list。如果有其他的可迭代参数传进来，map函数则会把每一个参数都以相应的处理函数进行迭代处理。map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。1234567891011# map 有点像 [i for i in [1,2,3]]ret = map(abs,[-1,-2,3,-8])for i in ret: print(i) # 1,2,3,8# filter 执行了filter之后的记过集合 &lt;= 执行之前的个数 # filter只管筛选，不会改变原来的值# map 执行前后元素个数不变，值变了 # 值可能发生改变 # 要注意配合匿名函数 sorted() 对List、Dict进行排序，Python提供了两个方法对给定的List L进行排序，方法1.用List的成员函数sort进行排序，在本地进行排序，不返回副本方法2.用built-in函数sorted进行排序（从2.4开始），返回副本，原始输入不变1234567891011121314151617181920# sortl = [1,-4,6,5,-10]l.sort(key=abs) # 在原列表的基础上进行排序print(l) # [1, -4, 5, 6, -10]# sorted# 会生成一个新的数据,保留原来数据# 排序的过程中负载的算法不支持产生一个迭代器l = [1,-4,6,5,-10]print(sorted(l)) # [-10, -4, 1, 5, 6] # 生成一个新列表，不改变原列表 占内存print(l) # [1, -4, 6, 5, -10] # 源列表不变# reversed() 倒叙 返回一个反向的迭代器# sorted() 排序 返回listprint(sorted(l,key=abs)) # [1, -4, 5, 6, -10]# 列表按照每一个元素的len排序l = [[1,2],[3,4,5,6],(7,),'123']print(sorted(l,key=len)) # [(7,), [1, 2], '123', [3, 4, 5, 6]]]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>内置函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成器面试题]]></title>
    <url>%2F2019%2F03%2F18%2Fcase-generator%2F</url>
    <content type="text"><![CDATA[生成器面试题12345678910111213141516171819def demo(): # 生成器函数 for i in range(4): yield ig=demo() # 生成器# 生成器表达式g1=(i for i in g) # 没执行g2=(i for i in g1)# 数据类型强转print(list(g1)) # [0, 1, 2, 3] # 执行了,g1生成器找g取值print(list(g2)) # [] # g2找g1拿值，g1已经没有值了.所以是空,把g1注释掉,g2就有值了# 一个生成器里面的能用只能取一次,你不找他要，他也不会给你# g1=(i for i in g) 没干活，因为他返回了个 生成器#def g1():# for i in g:# yield i 123456789101112131415161718192021222324252627def add(n,i): return n+idef test(): for i in range(4): yield ig=test()for n in [1,10]: g=(add(n,i) for i in g) # 生成器表达式# 当遇见这种问题的时候 先将循拆开# for循环套生成器表达式# n = 1# g=(add(n,i) for i in g) # test()# n = 10# g=(add(n,i) for i in g) # (add(n,i) for i in g)# 2 带入方程# g=(add(n,i) for i in test())# g=(add(n,i) for i in (add(10,i) for i in test()))# n = 10# g=(add(n,i) for i in (add(10,(0,1,2,3))# n = 10# g=(add(10,i) for i in (10,11,12,13))# 20,21,22,23print(list(g)) # 开始执行 12345678910111213141516171819202122def add(n,i): return n+idef test(): for i in range(4): yield ig=test()for n in [1,10,5]: g=(add(n,i) for i in g) # 生成器表达式# n = 1# g=(add(n,i) for i in test())# n = 10# g=(add(n,i) for i in (add(n,i) for i in test()))# n = 5# g=(add(n,i) for i in (add(n,i) for i in (add(n,i) for i in test())))# g=(add(n,i) for i in (add(n,i) for i in (add(n,i) for i in (0,1,2,3))))# g=(add(n,i) for i in (add(n,i) for i in (5,6,7,8)))# g=(add(n,i) for i in (10,11,12,13)# 15,16,17,18print(list(g)) # 开始执行]]></content>
      <categories>
        <category>Python三大利器</category>
      </categories>
      <tags>
        <tag>生成器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种推导式]]></title>
    <url>%2F2019%2F03%2F15%2Fcomprehensions%2F</url>
    <content type="text"><![CDATA[推导式的套路: 新的列表 = [ 每一个元素或者是和元素相关的操作 for 元素 in 可迭代数据类型 ] # 遍历之后挨个处理 新的列表 = [ 满足条件的元素相关的操作 for 元素 in 可迭代数据类型 if 元素相关的条件 ] # 筛选功能 列表推导式12345678910111213141516171819202122# 例一：30以内所有能被3整除的数l = [ i for i in range(30) if i % 3 == 0 ]print(l)# 例二：30以内所有能被3整除的数的平方l = [ i**2 for i in range(30) if i % 3 == 0 ]print(l)# 例三:找到嵌套列表中名字含有两个‘e’的所有名字names = [['Tom', 'Billy', 'Jefferson', 'Andrew', 'Wesley', 'Steven', 'Joe'], ['Alice', 'Jill', 'Ana', 'Wendy', 'Jennifer', 'Sherry', 'Eva']]# 常规循环#for l in names:# for name in l:# if name.count('e') &gt;= 2:# print(name)# 二维列表# 注意遍历顺序，这是实现的关键l = [name for l in names for name in l if name.count('e') &gt;= 2]print(l) 字典推导式12345678910111213141516171819# 字典推导式# 例一：将一个字典的key和value对调mcase = &#123;'a': 10, 'b': 34&#125;# 常规循环# for k in mcase:# mcase[k] : k # v : k# print(mcase)mcase = &#123;mcase[k]:k for k in mcase&#125;print(mcase)# 例二：合并大小写对应的value值，将k统一成小写# 所有的推导式都从for开始看mcase = &#123;'a': 10, 'b': 34, 'A': 7, 'Z': 3&#125;# &#123;'a':10+7,'b':34,'z':3&#125;mcase_frequency = &#123;k.lower(): mcase.get(k.lower(), 0) + mcase.get(k.upper(), 0) for k in mcase.keys()&#125;# 拿到所有key# key值就是要小写k.lower():# mcase.get(k.lower(), 0)没有小写的就默认写一个0 + mcase.get(k.upper(), 0) 相当于小写+大写的值,没有就给0print(mcase_frequency) 集合推导式12345# 集合推导式 可以替重# &#123;&#125;# 例：计算列表中每个值的平方，自带去重功能squared = &#123;x**2 for x in [1, -1, 2]&#125;print(squared) # -1的平方和1的平方一样 所以去重之后2个结果，如果是列表就不会自动去重 总结各种推导式:生成器 列表 字典 集合 遍历 筛选 带if 优点 推导式能让你的代码更简洁， 可读性提高 惰性运算: 懒 不去找他要值不会执行，要一个执行一个 生成器(自己写的，可见)和迭代器（不可见的，python提供）特点 同一个迭代器,从头到尾取值只能取一次 不找他要值的时候不干活 练习1234567891011121314151617# 例1: 过滤掉长度小于3的字符串列表，并将剩下的转换成大写字母names = ['Tom', 'Billy', 'Jefferson', 'Andrew', 'Wesley', 'Steven', 'Joe']new_l = [name.upper() for name in names if len(name) &gt; 3]print(new_l)# 例2: 求(x,y)其中x是0-5之间的偶数，y是0-5之间的奇数组成的元祖列表l1 = [(x,y) for x in range(5) if x%2==0 for y in range(5) if y %2==1]l2 = [(x,y) for x in range(5) if x % 2 == 0 for y in range(5) if y % 2 == 1]print(l1)print(l2)# 求M中3,6,9组成的列表M = [[1,2,3],[4,5,6],[7,8,9]]# for i in M:# print(i[2])l3 = [row[2] for row in M]print(l3)]]></content>
      <categories>
        <category>Python三大利器</category>
      </categories>
      <tags>
        <tag>列表推导式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python三大利器 — 装饰器]]></title>
    <url>%2F2019%2F03%2F13%2Fdecorator%2F</url>
    <content type="text"><![CDATA[简单的装饰器使用比如现在公司有一个需求，每个函数都要计算运行时间，我们可以调用time模块实现一个简单的计算执行时间的方法 1234567891011import time# 统计每个函数的执行时间 1def func(): start_time = time.time() print('func 1') time.sleep(3) now_time = time.time() return now_time - start_timeret = func() # func 1print(ret) # 3.000171661376953 那如果要是有200多个函数呢，难道要一个个加入，然后在一个个删除？我们想到计算时间可以单独写一个函数去调用。123456789101112# 调用统计时间函数def timmer(f): start_time = time.time() f() end_time = time.time() print(end_time - start_time)def func(): time.sleep(3) print('func 1')timmer(func) 这样以后的200个函数都要使用timmer去调用执行么？也是不合理的，应该是func方法来调用时间函数，比较合理。 装饰器的形成过程1234567891011121314151617181920# 我们想要做到的:# 1 不想修改函数的调用方式 但是还想再原来的函数前后添加功能# 2 timmer就是一个装饰器函数，只是对一个函数 有一些装饰作用def func(): time.sleep(3) print('func 1')# 调用统计时间函数# 闭包 内部函数inner,调用了外部变量f,f是传进来的def timmer(f): # 装饰器函数 def inner(): start_time = time.time() f() # 被装饰的函数 end_time = time.time() print(end_time - start_time) return innerfunc = timmer(func)func() 运行过程流程图: 原来的函数为func 最后我还是要调用func 中间增加的计时功能timmer 通过func = timmer(func) 和 闭包函数 来进行修饰 最终通过闭包函数来返回内部函数 交给 外部的func接收，接收的变量还是原本func的方法 最后执行外部的func()，他会自动去找装饰函数inner(),再去找到原本被装饰的函数func() 总结:装饰器的本质：一个闭包函数装饰器的功能：在不修改原函数及其调用方式的情况下对原函数功能进行扩展装饰器的意义： 装饰器既没有改变函数的调用方式，又在函数的前后增加了装饰功能 开放封闭原则开放: 对扩展是开放的,任何一个程序，不可能在设计之初就已经想好了所有的功能并且未来不做任何更新和修改。所以我们必须允许代码扩展、添加新功能。 封闭: 对修改是封闭的,因为我们写的一个函数，很有可能已经交付给其他人使用了，如果这个时候我们对其进行了修改，很有可能影响其他已经在使用该函数的用户。 装饰器完美的遵循了这个开放封闭原则 语法糖@装饰器函数 == 重新定义被装饰函数=装饰器函数（被装饰函数）12345678910111213141516171819import timedef timmer(f): # 装饰器函数 def inner(): start_time = time.time() f() # 被装饰的函数 end_time = time.time() print(end_time - start_time) return inner# 语法糖 @timmer 让代码更好看 更便捷# 在被装饰的函数上面贴着加上 @装饰器函数名 # 就相当于写了func = timmer(func)@timmerdef func(): time.sleep(3) print('func 1')# func = timmer(func)func() 装饰带返回值的函数的装饰器12345678910111213141516171819202122import timedef timmer(f): # 装饰器函数 def inner(): start_time = time.time() ret = f() # 被装饰的函数 带有返回值 end_time = time.time() print(end_time - start_time) return ret # 返回被装饰的函数的返回值 return inner@timmerdef func(): time.sleep(3) print('func 1') return '新年好' # 被装饰的函数的返回值# func = timmer(func)ret = func()print(ret)# 因为现在的func不是原来的func 而是inner ，所有要对inner中增加返回值# 现在的func就是inner,ret接收的事inner的返回值 装饰带一个参数的函数12345678910111213def wrapper(func): def inner(name): ret = func(name) return ret return inner@wrapper # func = wrapper(func)def func(name): return '新年好,%s'%nameret = func('leo')print(ret) 接收万能参数装饰器1234567891011121314151617def wrapper(func): def inner(*args,**kwargs): # print('函数被装饰之前要做的事') print(*args) # leo python print(kwargs) # &#123;'age': 30&#125; print(kwargs['age']) # 30 ret = func(*args,**kwargs) # print('函数被装饰之前要做的事') return ret return inner@wrapperdef func(name,course,age): return '大家好,我是%s,今年%d,现在正在学习%s'%(name,age,course)ret = func('leo','python',age=30)print(ret) # 大家好,我是leo,今年30,现在正在学习python 装饰器的固定格式123456789101112131415161718import time# 单纯就叫装饰器的时候 -- wrapperdef wrapper(f): # 装饰器函数,f是被装饰的函数,装饰器函数里面的参数永远是被装饰的函数 def inner(*args,**kwargs): # 内部函数inner,*args,**kwargs动态参数原封不动的传给被装饰的函数 # 被装饰函数执行之前要做的事 ret = f(*args,**kwargs) # 被装饰的函数,执行完成后，给外面返回值 # 被装饰函数执行之后要做的事 return ret return inner # 对应内部函数inner 不加括号执行@wrapper # func = timmer(func)def func(a,b): time.sleep(2) print('func1',a,b) return '新年好'ret = func(1,2)print(ret) 1234567891011def wrapper(func): # func = qqxing def inner(*args,**kwargs): ret = func(*args,**kwargs) # 被装饰的函数 qqxing return ret return inner@wrapper # qqxing = wrapper(qqxing)def qqxing(a,b): print(123)ret = qqxing(1,2) # 实际上执行的是inner() 装饰器的固定格式 - wraps首先先了解函数的name和doc方法:函数名.__name__ = 查看字符串格式的函数名函数名.__doc__ = 查看函数注释123456789def wahaha(): ''' 一个打印娃哈哈的函数 :return: ''' print('娃哈哈')print(wahaha.__name__) # 查看字符串格式的函数名print(wahaha.__doc__) # 查看函数注释 在执行使用装饰器之后，我们打印函数的name发现是装饰器的函数名称了，这个时候就需要使用wraps来解决。12345678910111213141516171819202122232425from functools import wrapsdef wrapper(func): # func = holiday @wraps(func) # 装饰inner函数 def inner(*args,**kwargs): print('在被装饰的函数执行前做的事') ret = func(*args,**kwargs) print('在被装饰的函数执行后做的事') return ret return inner@wrapper # holiday = wrapper(holiday)def holiday(day): ''' 这是一个放假通知 :param day: :return: ''' return '还有%s天放假'%dayprint(holiday.__name__) # inner...因为现在的holiday已经是inner了，由于之前说装饰器最好不要影响被装饰的函数，需要用wraps装饰inner函数,才可以正常显示回去print(holiday.__doc__)ret = holiday(3) # innerprint(ret)# wraps并不影响wrapper装饰器的使用 带参数的装饰器比如现在有500个函数,都使用装饰器，那么怎么一次性的去控制500个装饰器的增加和删除，怎么办？我们可以使用带参数的参数器，通过标志位参数去控制装饰器是否执行。带参数的装饰器，也就是三层装饰器，在外部多一次调用传入状态标记。123456789101112131415161718192021222324252627282930import timeFLAGE = True # 标识位，True执行，Fales不执行def timmer_out(flag): # 在原有装饰器之外再来一层 def timmer(func): def inner(*args,**kwargs): if flag: # 如果flag = True 那么我就走装饰器，否则我就只运行被装饰的函数 start_time = time.time() ret = func(*args,**kwargs) end_time = time.time() print(end_time - start_time) return ret else: ret = func(*args, **kwargs) return ret return inner return timmer# timmer = timmer_out(FLAGE)@timmer_out(FLAGE)def wahaha(): time.sleep(2) print('wahaha')@timmer_out(FLAGE)def qqxing(): time.sleep(1) print('qqxing')ret = wahaha()ret = qqxing() 多个装饰器装饰一个函数多个装饰器执行的过程有点像套娃，装饰器在后的先执行装饰1234567891011121314151617181920212223242526272829303132def wrapper1(func): # f def inner1(): print('wrapper1装饰器 start') # 3 func() # 执行f # 4 print('wrapper1装饰器 end') # 5 return inner1def wrapper2(func): # inner1 def inner2(): print('wrapper2装饰器 start') # 1 先执行他 func() # inner1() # 2 print('wrapper2装饰器 end') # 6 return inner2# 先看装饰器执行先后@wrapper2 # f = wrapper2(f)==&gt; f = inner1 ==&gt; inner1 = wrapper2(inner1) ==&gt; inner2 ，传进去的是inner1,最后返回得到的是inner2@wrapper1 # f = wrapper1(f) = inner1def f(): print('in f')f() # ==&gt; 调用开始现在是 inner2# wrapper2装饰器 start# wrapper1装饰器 start# in f# wrapper1装饰器 end# wrapper2装饰器 end# 1. 先看装饰器执行先后，wrapper2没有找到要被修饰的函数，所以现在wrapper1# 2. f = wrapper1(f) = inner1# 3. # f(下面赢变成inner1) ==&gt; inner1 = wrapper2(inner1) = inner2，但是传进去的是inner1, 有时候会遇见两个需求: 记录用户的登录情况 记录函数的执行时间仔细思考下先后执行顺序:先登录成功之后 才能开始执行程序记录函数的执行时间]]></content>
      <categories>
        <category>Python三大利器</category>
      </categories>
      <tags>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python三大利器 — 迭代器]]></title>
    <url>%2F2019%2F03%2F11%2Fiterator%2F</url>
    <content type="text"><![CDATA[for循环是如何工作的当我们拥有一个列表 l = [1,2,3,4,5],想取列表中的内容，有几种方式？ 1234# 1 通过索引下标和切片取值l = [1,2,3,4,5]print(l[0]) # 1print(l[0:2]) # [1, 2] 123# 2 通过for循环取值for i in l: print(i) 他们的区别是，使用索引取值可以取到任意位置的值，前提是我知道这个值在什么位置，而for循环是取到每一个值，不需要关心这个值在什么位置，也不能跳过任何一个值去取其他位置的值，我们可以称作循环遍历。那么for循环到底是怎么工作的呢？ 都有哪些数据类型可以被for循环123456789101112131415for s in 'abcde': print(s) # 返回字符串中每一个字符 a b c d edic = &#123;'name':'leo','age':26&#125;for key in dic: print(key) # 默认返回字典中的键 name , agefor value in dic.values(): print(value) # 返回字典中的值 leo,26 for k,v in dic.items(): print(k,v) # 返回字典中的键值对 name leo age 26for i in 12345: print(i) # TypeError: 'int' object is not iterable 当我们循环数字类型的时候报错了，说int类型不是 iterable（可迭代的） 迭代和可迭代协议通过对数字类型的报错，不可被for循环的数据类型会报错 不是一个可迭代的，那么是不是说可迭代的数据类型就可以被for循环，如何判断数据类型是否可以被迭代？123456789101112131415161718from collections import Iterablel = [1,2,3,4]t = (1,2,3,4)d = &#123;1:2,3:4&#125;s = &#123;1,2,3,4&#125;num = 123money = 10.10print(isinstance(l,Iterable)) # Trueprint(isinstance(t,Iterable)) # Trueprint(isinstance(d,Iterable)) # Trueprint(isinstance(s,Iterable)) # Trueprint(isinstance(num,Iterable)) # Falseprint(isinstance(money,Iterable)) # False# 下面这三种也是可以被循环遍历# f = open() # range()# enumerate 枚举 可以将某个数据集内的数据“一个挨着一个的取出来”，就叫做迭代总结出一条规律来：能被for循环的就是“可迭代的”。但是如果正着想，for怎么知道谁是可迭代的呢？为什么能被for循环？ 123456789101112131415# 为什么能够被循环# dir 可以返回这个数据类型的拥有的所有方法# 查看列表、字典、字符串、和range的双下方法 有什么共同方法# 求交集ret = set(dir([]))&amp;set(dir(&#123;&#125;))&amp;set(dir(''))&amp;set(dir(range(10)))print(ret) # 我们找一个和iterable比较相似的方法， '__iter__',# 我们再来看看无法被迭代的数据类型 有没有__iter__方法print('__iter__' in dir(int)) # Falseprint('__iter__' in dir(bool)) # Falseprint('__iter__' in dir(list)) # Trueprint('__iter__' in dir(dict)) # Trueprint('__iter__' in dir(set)) # Trueprint('__iter__' in dir(tuple)) # Trueprint('__iter__' in dir(range(10))) # Trueprint('__iter__' in dir(enumerate([]))) # True 再总结出一条新的规律: 能被for循环的就是“可迭代的”,只要是能被for循环的数据类型，就一定拥有__iter__双下方法 双下方法__iter__做了什么123456789101112131415161718print([].__iter__()) # &lt;list_iterator object at 0x0000000002308940&gt; 迭代器 iterator# [].__iter__() 得到了一个list_iterator# 那么迭代器有什么作用呢？让我们来看看列表list和转换成列表_迭代器所有方法的差集print(set(dir([].__iter__()))- set(dir([])) ) # &#123;'__setstate__', '__length_hint__', '__next__'&#125;# 迭代器多出来的这三个方法的作用:#__length_hint__ 获取迭代器中元素的长度# print([1,2,3,4,5].__iter__().__length_hint__()) # 5 元素个数# __setstate__ 可以指定从其他位置取值# __next__ 一个一个的取值# 迭代器取值l = [1,2,3] # 列表iterator = l.__iter__() # iterator现在是一个迭代器,他内部有.__next__()方法print(iterator.__next__()) # 1print(iterator.__next__()) # 2print(iterator.__next__()) # 3print(iterator.__next__()) # 报错 StopIteration 通过上面的例子我们发现,当一个可迭代的对象调用了iter()方法会生成一个 iterator (迭代器) 迭代器中含有_next__()方法，他可以一个一个的取值,如果我们一直取next取到迭代器里已经没有元素了，就会抛出一个异常StopIteration，告诉我们，列表中已经没有有效的元素了 可迭代协议 与 迭代器协议根据上面的例子我们总结出以下概念： 能被for循环的数据类型都是 可迭代的 (iterable) 当这个数据类型调用.__iter__()方法会生成一个 迭代器(iterator) 迭代器.next()可以一个一个的取值 for循环其实就是在使用迭代器，只有是可迭代对象或者迭代器，才能用for循环 for循环的本质就是迭代器 12345for i in l: pass # 首先会去找l.__iter__() ==&gt; iterator = l.__iter__() # i = iterator.__next__() # 当没有值的时候 自动停止结束 也不会报错 12345# 模拟for循环l = [1,2,3,4,5]iterator = l.__iter__() # 变成一个迭代器while True: print(iterator.__next__()) 可迭代协议: 只要含有__iter__()方法的都是可迭代的迭代器协议: 内部含有__next__()方法和__iter__()方法的就是迭代器 可迭代的不一定就是迭代器 迭代器：内部有__iter__和__next__方法 ，所以他一定是可迭代的 可迭代的不一定是迭代器，要看有没有__next__方法12345678910from collections import Iterablefrom collections import Iteratorprint(isinstance([],Iterable)) # 可迭代的 # Trueprint(isinstance([],Iterator)) # 迭代器 # False ,list是可迭代的，但不是一个迭代器print('__iter__' in dir(range(12))) # Trueprint('__next__' in dir(range(12))) # Falseprint(isinstance(range(100000000),Iterable)) # Trueprint(isinstance(range(100000000),Iterator)) # False , range是可迭代器，但不是一个迭代器，因为它没有__next__()方法 迭代器的好处 迭代器会从容器类型中 一个一个的取值，会把所有的值都取到。 它可以节省内存空间,迭代器并不会在内存中再占用一大块内存，而是随着循环每次生成一个,或者每次next()每次给我一个12# print(range(10000000)) # 很快，但是并不会在内存中真正的生成数据# print(list(range(10000000))) # 强制转列表会导致崩溃,list是真正存在并存储在内存里 ，range是要一个给一个]]></content>
      <categories>
        <category>Python三大利器</category>
      </categories>
      <tags>
        <tag>迭代器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python三大利器 — 生成器]]></title>
    <url>%2F2019%2F03%2F11%2Fgenerator%2F</url>
    <content type="text"><![CDATA[什么是生成器之前我们学习过迭代器，它的好处之一就是可以节省能存，在某些情况下，我们需要自己定义一个方法去实现迭代器功能，这个方法就是生成器。在Python中生成器又分成两类: 生成器函数 生成器表达式 生成器函数生成器Generator： 本质：迭代器(所以自带了iter方法和next方法，不需要我们去实现) 特点：惰性运算,开发者自定义 生成器函数：一个包含yield关键字的函数就是一个生成器函数。yield可以为我们从函数中返回值，但是yield又不同于return，return的执行意味着程序的结束，调用生成器函数不会得到返回的具体的值，而是得到一个可迭代的对象。每一次获取这个可迭代对象的值，就能推动函数的执行，获取新的返回值。直到函数执行结束。 1234567891011121314# 只要含有yield关键字的函数都是生成器函数# yield 必须写在函数里，且无法和return共用def generator(): a = 1 yield a b = 2 yield b g = generator() # 得到一个“生成器”作为返回值print(g) # &lt;generator object generator at 0x0000000001E9A308&gt; generator 生成器# g.__next__ # 生成器带有__next__方法和__iter__方法# g.__iter__ # 生成器是迭代器 用next方法取值print(g.__next__()) # 1print(g.__next__()) # 2 运行过程总结： 由于函数中有yield，所以现在内存中会有一个生成器函数 generator g = generator() 发生了函数调用，生成器函数的特点：函数中的代码不执行 g 得到了一个生成器 生成器里面即有iter方法也有next方法，说明它其实是一个迭代器 生成器就可以使用next方法取值,这时程序才第一次触发了生成器里面的代码 yield 不会结束函数，return会直接结束 生成器函数的使用生成器的最大好处就是不会在内存中一次性的生成所有数据1234567891011121314151617181920212223def factoy(): for i in range(100): yield '生成%s次'%ig = factoy()# __next__() ,一次一次的提取print(g.__next__())print(g.__next__())print(g.__next__())# for循环遍历提取for i in g: print(i)# 取50次g = factoy()count = 0for i in g: count += 1 print(i) if count &gt; 50: breakprint('*****',g.__next__()) # ***** 生成51次 可以继续从生成器中取值 列表为什么不能继续取值123456789101112131415161718192021# 列表是可迭代的,并不是一个迭代器,在两次for循环的时候会产生两个迭代器# for循环自动将可迭代的转换成迭代器l = [1,2,3,4,5]for i in l: print(i) # 1,2 if i == 2: breakfor i in l: print(i) # 1,2,3,4,5 # 获取两个生成器l = [1,2,3,4,5]def generator(): for i in l: yield i g = generator()g1 = generator()print(g,g1) # &lt;generator object generator at 0x0000000001F7A150&gt; &lt;generator object generator at 0x0000000001F7A200&gt;print(g.__next__()) # 1print(g1.__next__()) # 1 拿到两个生成器，自己执行自己的 监听文件的输入123456789101112def tail(filename): f = open(filename, encoding='utf-8') while True: line = f.readline() # 每次读一行 if line.strip(): # 不为空就打印 # print('****',line.strip('\n')) yield line.strip() # 返回这行g = tail('file') # 获取生成器for i in g: if 'python' in i: print('*****',i,'*****') # 可以对这个结果做任何操作了,用生成器实现就可以想要的结果 爬虫时的使用123456789101112131415161718192021222324252627def parse_one_page(html): rule = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?data-src="(.*?)".*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?&gt;(.*?)&lt;/p&gt;.*?' 'releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;' ,re.S) items = re.findall(rule, html) # 通过findall方法根据规则得到html文本 # print(items) for item in items: yield &#123; 'index':item[0], 'image':item[1], 'title':item[2].strip(), 'actor':item[3].strip(), 'time':item[4].strip(), 'score':item[5].strip() + item[6].strip() &#125; # 循环整个html文本列表,每一条数据都生成yield返回一个字典，里面拼接成想要的数据类型# 使用的时候传递一个页面进去，循环调用生成器，item里就是生成的每条数据def main(offset): url = 'https://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) # print(html) for item in parse_one_page(html): print(item) write_to_file(item) 生成器函数的进阶数据类型的强制转换 — 列表(生成器)1234567891011121314def generator(): for i in range(20): yield 'wahaha%s'%ig = generator()# list(g)# list是列表，代表将g生成器直接转换成列表，列表中的每一个值都是实际存在的# 一个一个从生成器里取出来，全部取完放入列表，列表会在内存中生成print(list(g))# ['wahaha0', 'wahaha1', 'wahaha2', 'wahaha3'...'wahaha19']# 从生成器取值的几个方法： # next # for # 数据类型的强制转换 (不推荐，占用内存) 123456789101112def generator(): print(123) yield 1 print(456) yield 2 print(789)g = generator() # 得到一个生成器ret = g.__next__()print('***',ret) # 先打印123，然后拿到yield返回的1print('***',ret)print('***',ret) # 执行了789,由于后面没有yield,会报错StopIteration 生成器函数 — send123456789101112131415161718192021222324def generator(): print(123) send_msg = yield 1 print('======',send_msg) yield 2g = generator()ret = g.__next__()print('***',ret)ret = g.send('send_hello')print('***',ret)# 123# *** 1# ====== send_hello# *** 2# send用法总结# 1. send的获取下一个值的效果与next基本一致# 2. 只是在获取下一个值的时候给上一个yield的位置,传递一个数据# 使用send的注意事项# 1. 第一次使用生成器的时候,必须使用next获取下一个值# 2. 最后一个yield 不能接收外部的值,但是可以在接收arg=yield 2...最后返回一个空yield send实例 — 计算移动平均值123456789101112131415161718192021# 接收一次值计算平均值# 移动平均值# num: 10 20 30# avg: 10 15 20# 公式: avg = num / countdef avg_generator(): sum = 0 count = 0 avg = 0 num = yield # 第一次返回空，为了后面send传值(num)进来,10 sum += num # 总数有更新 10 count += 1 # 次数更新 avg = sum / count yield avg # send执行到这avg_g = avg_generator()avg_g.__next__() # 使用send第一次必须next，得到空avg1 = avg_g.send(10) # 传值(num)10进去print(avg1) 那么如何多次计算呢，需要加上循环123456789101112131415161718192021222324252627# 移动平均值# num: 10 20 30# avg: 10 15 20# 公式: avg = num / countdef avg_generator(): sum = 0 count = 0 avg = 0 while 1: # num = yield # 第一次返回空，为了后面send传值(num)进来,10 num = yield avg # 第一次的avg = 0 ，num = 传值 sum += num # 总数有更新 10 count += 1 # 次数更新 avg = sum / countavg_g = avg_generator()avg_g.__next__() # 使用send第一次必须next，得到空avg = avg_g.send(10) # 传值(num)10进去avg = avg_g.send(20)avg = avg_g.send(30)print(avg)# 每次计算方法:# 如果我加上while循环,现在我有两个yield,第一次结束到yield avg,第二次执行什么？# 如果执行next num = yield 相当于 num = 0# 下面再用一次send 传值20 再返回打印 计算移动平均值(2)_预激协程的装饰器123456789101112131415161718192021222324252627# 计算移动平均值# 用装饰器 激活__next__()def init(func): def inner(*args,**kwargs): g = func(*args,**kwargs) # g = generator() 拿到装饰器 g.__next__() # 执行__next__() return g # 返回装饰器 return inner@init # avg_generator = init(avg_generator) ==&gt; innerdef avg_generator(): sum = 0 count = 0 avg = 0 while True: # num = yield num = yield avg # num = 10,20,30 sum += num # sum = 10,30,60 count += 1 # count = 1,2,3 avg = sum / count # avg = 10,15,20g = avg_generator() # inner() # 执行这里 得到一个执行过next的装饰器# g.__next__() # 我不在这调用 而是在装饰器里avg = g.send(10) # 开始向生成器里里传值avg = g.send(20)avg = g.send(30)print(avg) yield fromyield from : 从一个容器类型里取值,不需要一个个返回，而是集体返回接收123456789101112131415# python 3# 将结果按个返回def generator(): a = 'abcde' b = '12345' # 单个字符串返回 for i in a: yield i for i in b: yield ig = generator()# print(g.__next__())for i in g: print(i) 12345678910111213# yield from 将结果按个返回def generator(): a = 'abcde' l = [1,2,3,4,5] # 单个字符串返回 yield from a # 生成器函数语法 yield from lg = generator()for i in g: print(i)# yield from 从一个容器类型里取值,不需要一个个返回，而是集体返回接收 1234567# 将两个类型的数据list转化成同一个def generator(): yield from range(0,5) yield from 'abcde'l = list(generator())print(l) # [0, 1, 2, 3, 4, 'a', 'b', 'c', 'd', 'e'] 生成器表达式列表推导式我们先写一个获取鸡蛋的程序1234egg_list = []for i in range(10): egg_list.append('鸡蛋%s'%i)print(egg_list) 在这里循环获取得到一个鸡蛋筐(列表),里面存着10个鸡蛋,列表推导式的写法如下12345egg_list = ['鸡蛋%s' %i for i in range(10)]print(egg_list)# 1. for i in range(10) 循环# 2. 将想要的 放在for前面# 3. 用列表括起来 列表推导式可以做一些简单的循环工作,那么这个时候我们就想,列表生成后可是存在内存里的，那如果是大数据怎么办，很占用内存，占用内存我们就想到了 生成器 生成器推导式生成器表达式 与 列表表达式 的不同 括号不一样 返回的值不一样 列表推导式得到的还是一个列表，一次性得到所有的值，占用内存 生成器表达式几乎不占用内存，但是不能直接应用,需要遍历循环取值，程序应该更关心内存123456789101112131415161718192021# 生成器表达式g = (i for i in range(10))print(g) # &lt;generator object &lt;genexpr&gt; at 0x0000000001EB92B0&gt; 生成器for i in g: print(i)# 获取鸡蛋例子egg_g = ('鸡蛋%s'%i for i in range(10)) # 生成器表达式for i in egg_g: # 相当于老母鸡,然后下蛋 print(i) # 每个数字都取平方# g里面的代码一句话没执行，直到for循环取值__next__,for循环每走一次,上面的range10的循环才走一次g = (i*i for i in range(10))for i in g: print(i) #列表解析sum([i for i in range(100000000)])#内存占用大,机器容易卡死#生成器表达式sum(i for i in range(100000000))#几乎不占内存 迭代器与生成器总结可迭代对象: 拥有__iter__方法 特点：惰性运算 例如: range(), str, list, tuple, dict, set 迭代器Iterator： 拥有__iter__方法和__next__方法 例如: iter(range()), iter(str), iter(list), iter(tuple), iter(dict), iter(set), reversed(list_o), map(func,list_o), filter(func, list_o), file_o 生成器Generator：本质：迭代器，所以拥有__iter__方法和__next__方法特点：惰性运算, 开发者自定义 使用生成器的优点： 延迟计算，一次返回一个结果。也就是说，它不会一次生成所有的结果，这对于大数据量处理，将会非常有用。 提高代码可读性]]></content>
      <categories>
        <category>Python三大利器</category>
      </categories>
      <tags>
        <tag>生成器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 包的使用]]></title>
    <url>%2F2019%2F03%2F05%2Fpackage%2F</url>
    <content type="text"><![CDATA[什么是包包就是把解决一类问题的模块放在同一个文件目录里，这个文件目录就叫做包包是一种通过使用‘.模块名’来组织python模块名称空间的方式。 包的导入方法 – import1234567891011121314151617soft/ ├── bin │ ├── __init__.py│ └── start.py ├── core │ ├── __init__.py│ └── Manage.py │ └── login() └── cook.py 最外层是soft软件工程目录 bin 和 core 是两个包 bin 下有着start.py 程序入口文件 core 下有着Manage模块,里面带有一个login( )方法 整个目录还有一个cook.py文件 在soft目录下 12345678910# 想要在start.py 使用 Manage的login()方法# 通过查看sys.path路径发现只能找到soft的bin目录,所以不能直接import core,需要从soft开始import sysprint(sys.path) # D:\\PycharmProjects\\Notes\\soft\\binimport soft.core.Managesoft.core.Manage.login() # login in Manageimport soft.core.Manage as MM.login() # login in Manage 关于包相关的导入语句也分为import和from … import …两种，但是无论哪种，无论在什么位置，在导入时都必须遵循一个原则：凡是在导入时带点的，点的左边都必须是一个包，否则非法。可以带有一连串的点，如,soft.core.Manage但都必须遵循这个原则。 对于导入后，在使用时就没有这种限制了，点的左边可以是包,模块，函数，类(它们都可以用点的方式调用自己的属性)。 from … import …123456789import osimport sys# 把soft目录加入到sys.path里去，然后就可以直接找到coresys.path.append(os.path.dirname(os.getcwd()))print(sys.path) # 'D:\\PycharmProjects\\Notes\\soft'from core import ManageManage.login() # login in Manage 1234567891011# 在core包下的文件引入其他包中的方法或者配置# 记得由于我们执行开始是在start.py中执行,所以一定要在里面导入路径# settings 文件中加入 DB_PATH = 'D:\PycharmProjects\Notes\soft\db'# Manageimport osfrom conf import settingsdef login(): print('login in Manage') file_name = os.path.join(settings.DB_PATH,'info.log') with open(file_name,'w') as f: f.write('Hello') 需要注意的是from后import导入的模块，必须是明确的一个不能带点，否则会有语法错误可以用import导入内置或者第三方模块（已经在sys.path中），但是要绝对避免使用import来导入自定义包的子模块(没有在sys.path中)，应该使用from… import …的绝对或者相对导入,且包的相对导入只能用from的形式。]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 模块的使用]]></title>
    <url>%2F2019%2F03%2F04%2Fmodule%2F</url>
    <content type="text"><![CDATA[什么是模块在Python中，一个.py文件就被称之为一个模块 模块的调用模块一旦被调用，即相当于执行另外一个另外一个py文件中的代码，多次import导入同一个模块，只会执行一次。 模块的导入 – import1234# demo.pymoney = 100def read(): print('in read',money) 12345678# func.pyimport demodef read(): print('my read func')money = 200read() # 自己本地的print(demo.money) # 调用模块的demo.read() # 调用模块的 模块导入流程 先从sys.modules里查看是否已经被导入 如果没有被导入就依据sys.path路径去寻找模块 找到了就导入，没有找到就报错 创建这个模块的命名空间 执行文件,把文件中的名字都放到命名空间中 sys.modules会加入这个模块，当这个模块再被impoet时就不会被重复导入 123456import sysimport demo# sys.modules 是一个字典，内部包含模块名与模块对象的映射，该字典决定了导入模块时是否需要重新导入print(sys.modules.keys())# sys.path 是python的搜索模块的路径listprint(sys.path) 导入模块的顺序 内置模块 time,re 扩展模块 pip3安装的 django 自定义模块 demo 单独导入 – from…import…123from time import sleepsleep(1)print('hey') 123456from demo import read,moneymoney = 200def read(): print('my read',money)read() # my read 200print(money) # 200 查看结果1. 如果本地有read()方法则优先调用本地的。2. 即使导入了money这个变量也无法替代本地变量,会发生冲突。为什么要使用 from…import?1. import是导入了模块里面的所有名字 比较占用内存。2. 导入一个变量进来，占用的内容较少,只要不用重名即可。 1234# 导入多个名字from demo import money,readprint(money) # 100print(read) # &lt;function read at 0x0000000000B62620&gt; 1234567891011121314151617# 导入全部* 配合 __all__# 如果在模块中定义了__all__ 那么如果import * ，只有存在这个列表中的名字才能调用# import demo 不受这个约束# demo__all__ = ['money','read'] # 只和 from demo import * 能配合起来money = 100def read(): print('in read',money)def read2(): print('in read2',money)# funcfrom demo import *print(money) # 100read() # in read 100read2() # NameError: name 'read2' is not defined 把模块当做脚本执行我们可以通过模块的全局变量name来查看模块名：12345# demo2print(__name__) # __main__# funcimport demo2 # demo2 当我在文件中import demo2的时候,demo2中的print(__name__)执行返回的是demo2 当我在demo模块中print(__name__)的时候返回的是 __main__ 在哪个页面页面上点的run 在哪个页面上就是__main__ 如果不在本页面上执行,在其他页面上调用模块名导入执行,那么就返回模块名 if name == ‘main‘: 作用:用来控制.py文件在不同的应用场景下执行不同的逻辑123456789101112131415# demo2def login(): print('正在执行登录程序')if __name__ == '__main__': print('in demo2.py') # in demo2.py ret = login() # 正在执行登录程序 print(ret) # None # funcimport demo2ret = demo2.login() # 正在执行登录程序print(ret) # None# 这样就符合了：模块导入的时候什么都不执行,所有想做的都放在模块里面去,在执行程序的时候就不会受影响，也能调用模块中想要的名字]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Readme]]></title>
    <url>%2F2019%2F03%2F04%2FReadme%2F</url>
    <content type="text"></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
</search>
